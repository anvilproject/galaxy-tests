galaxy.jobs.handler DEBUG 2025-04-18 12:55:33,012 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 3
tpv.core.entities DEBUG 2025-04-18 12:55:33,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:55:33,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (3) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:55:33,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (3) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:55:33,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (3) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:55:33,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (3) Working directory for job is: /galaxy/server/database/jobs_directory/000/3
galaxy.jobs.runners DEBUG 2025-04-18 12:55:33,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [3] queued (40.153 ms)
galaxy.jobs.handler INFO 2025-04-18 12:55:33,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (3) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:33,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 3
galaxy.jobs DEBUG 2025-04-18 12:55:33,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [3] prepared (64.255 ms)
galaxy.jobs.command_factory INFO 2025-04-18 12:55:33,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/3/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/3/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/3/configs/tmpb66qzm5u']
galaxy.jobs.runners DEBUG 2025-04-18 12:55:33,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (3) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/3/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/3/galaxy_3.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:33,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 3 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:33,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 3 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:33,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:44,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mcghh with k8s id: gxy-mcghh succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:55:44,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 3: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:55:54,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 3 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:55:54,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '1000trimmed.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/3/working/data_fetch_upload_81x4hece', 'object_id': 3}]}]}]
galaxy.jobs INFO 2025-04-18 12:55:54,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 3 in /galaxy/server/database/jobs_directory/000/3
galaxy.jobs DEBUG 2025-04-18 12:55:54,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 3 executed (134.871 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:54,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 3 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:55:54,485 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 4
tpv.core.entities DEBUG 2025-04-18 12:55:54,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*, abstract=False, cores=min(max(int(input_size * 4), 1), 16), mem=min(max(int(input_size * 7), 5.6), 64), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:55:54,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (4) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:55:54,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (4) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:55:54,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (4) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:55:54,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (4) Working directory for job is: /galaxy/server/database/jobs_directory/000/4
galaxy.jobs.runners DEBUG 2025-04-18 12:55:54,559 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [4] queued (35.997 ms)
galaxy.jobs.handler INFO 2025-04-18 12:55:54,561 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (4) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:54,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 4
galaxy.security.object_wrapper WARNING 2025-04-18 12:55:54,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to create dynamic subclass SafeStringWrapper__galaxy.model.none_like.None__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis for <class 'galaxy.model.none_like.NoneDataset'>, None: type() doesn't support MRO entry resolution; use types.new_class()
cheetah_DynamicallyCompiledCheetahTemplate_1744980954_6180274_86219.py:87: SyntaxWarning: invalid escape sequence '\w'
galaxy.jobs DEBUG 2025-04-18 12:55:54,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [4] prepared (68.208 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 12:55:54,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:55:54,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:55:54,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 12:55:54,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/4/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/d/a/f/dataset_dafe2300-cff8-4125-80cc-a088eee8603f.dat' '1000trimmed_fastq' && mkdir -p '/galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a_files' && fastqc --outdir '/galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a_files'   --threads ${GALAXY_SLOTS:-2} --dir ${TEMP:-$_GALAXY_JOB_TMP_DIR} --quiet --extract  --kmers 7 -f 'fastq' '1000trimmed_fastq'  && cp '/galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a_files'/*/fastqc_data.txt output.txt && cp '/galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a_files'/*\.html output.html]
galaxy.jobs.runners DEBUG 2025-04-18 12:55:54,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (4) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/4/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/4/galaxy_4.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/4/working/output.txt" -a -f "/galaxy/server/database/objects/c/7/4/dataset_c744931c-2715-41c8-9bbc-e6f2eb852ed5.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/4/working/output.txt" "/galaxy/server/database/objects/c/7/4/dataset_c744931c-2715-41c8-9bbc-e6f2eb852ed5.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/4/working/output.html" -a -f "/galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/4/working/output.html" "/galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:54,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 4 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 12:55:54,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:55:54,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:55:54,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:54,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 4 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:55:55,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:23,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xb6s2 with k8s id: gxy-xb6s2 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:56:23,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 4: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:56:32,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 4 finished
galaxy.model.metadata DEBUG 2025-04-18 12:56:32,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 5
galaxy.model.metadata DEBUG 2025-04-18 12:56:32,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 4
galaxy.util WARNING 2025-04-18 12:56:32,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/7/4/dataset_c744931c-2715-41c8-9bbc-e6f2eb852ed5.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/7/4/dataset_c744931c-2715-41c8-9bbc-e6f2eb852ed5.dat'
galaxy.util WARNING 2025-04-18 12:56:32,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/d/8/dataset_2d882f1f-12f9-4df6-a844-f48b6ecb2c1a.dat'
galaxy.jobs INFO 2025-04-18 12:56:32,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 4 in /galaxy/server/database/jobs_directory/000/4
galaxy.jobs DEBUG 2025-04-18 12:56:32,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 4 executed (121.505 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:32,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 4 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:56:35,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 6, 5
tpv.core.entities DEBUG 2025-04-18 12:56:35,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:56:35,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:56:35,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:56:35,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:56:35,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Working directory for job is: /galaxy/server/database/jobs_directory/000/5
galaxy.jobs.runners DEBUG 2025-04-18 12:56:35,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [5] queued (32.647 ms)
galaxy.jobs.handler INFO 2025-04-18 12:56:35,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:35,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 5
tpv.core.entities DEBUG 2025-04-18 12:56:35,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:56:35,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:56:35,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:56:35,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:56:35,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Working directory for job is: /galaxy/server/database/jobs_directory/000/6
galaxy.jobs.runners DEBUG 2025-04-18 12:56:35,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [6] queued (37.470 ms)
galaxy.jobs.handler INFO 2025-04-18 12:56:35,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:35,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 6
galaxy.jobs DEBUG 2025-04-18 12:56:35,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [5] prepared (68.379 ms)
galaxy.jobs.command_factory INFO 2025-04-18 12:56:35,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/5/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/5/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/5/configs/tmpt222sfm6']
galaxy.jobs.runners DEBUG 2025-04-18 12:56:35,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (5) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/5/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/5/galaxy_5.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-18 12:56:35,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [6] prepared (56.756 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:35,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:35,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 12:56:35,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/6/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/6/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/6/configs/tmpx3t67bcu']
galaxy.jobs.runners DEBUG 2025-04-18 12:56:35,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (6) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/6/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/6/galaxy_6.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:35,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:35,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:36,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:36,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:47,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m272g with k8s id: gxy-m272g succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:47,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qxgfk with k8s id: gxy-qxgfk succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:56:47,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 5: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:56:47,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 6: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:56:57,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 6 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:56:57,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'fastqc_contaminants.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/6/working/data_fetch_upload_seldddh1', 'object_id': 7}]}]}]
galaxy.jobs.runners DEBUG 2025-04-18 12:56:57,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 5 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:56:57,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '1000trimmed.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/5/working/data_fetch_upload_we6jhde5', 'object_id': 6}]}]}]
galaxy.jobs INFO 2025-04-18 12:56:57,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 6 in /galaxy/server/database/jobs_directory/000/6
galaxy.jobs INFO 2025-04-18 12:56:57,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 5 in /galaxy/server/database/jobs_directory/000/5
galaxy.jobs DEBUG 2025-04-18 12:56:57,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 6 executed (146.705 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:57,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 12:56:57,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 5 executed (139.963 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:57,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:56:58,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 7
tpv.core.entities DEBUG 2025-04-18 12:56:58,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*, abstract=False, cores=min(max(int(input_size * 4), 1), 16), mem=min(max(int(input_size * 7), 5.6), 64), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:56:58,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:56:58,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:56:58,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:56:58,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Working directory for job is: /galaxy/server/database/jobs_directory/000/7
galaxy.jobs.runners DEBUG 2025-04-18 12:56:58,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [7] queued (34.423 ms)
galaxy.jobs.handler INFO 2025-04-18 12:56:58,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:58,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 7
galaxy.jobs DEBUG 2025-04-18 12:56:59,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [7] prepared (60.844 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 12:56:59,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:56:59,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:56:59,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 12:56:59,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/7/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/6/8/a/dataset_68ad4b5b-e003-4901-85df-51d1003fd538.dat' '1000trimmed_fastq' && mkdir -p '/galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea_files' && fastqc --outdir '/galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea_files' --contaminants '/galaxy/server/database/objects/9/5/b/dataset_95bfd224-ad99-4f00-97a5-e87d2991cedf.dat'   --threads ${GALAXY_SLOTS:-2} --dir ${TEMP:-$_GALAXY_JOB_TMP_DIR} --quiet --extract  --kmers 7 -f 'fastq' '1000trimmed_fastq'  && cp '/galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea_files'/*/fastqc_data.txt output.txt && cp '/galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea_files'/*\.html output.html]
galaxy.jobs.runners DEBUG 2025-04-18 12:56:59,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (7) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/7/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/7/galaxy_7.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/7/working/output.txt" -a -f "/galaxy/server/database/objects/6/2/5/dataset_625f07ed-bd51-491d-aeee-303aa4b4f298.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/7/working/output.txt" "/galaxy/server/database/objects/6/2/5/dataset_625f07ed-bd51-491d-aeee-303aa4b4f298.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/7/working/output.html" -a -f "/galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/7/working/output.html" "/galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:59,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 12:56:59,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:56:59,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:56:59,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:59,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:56:59,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:07,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f44m7 with k8s id: gxy-f44m7 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:57:08,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 7: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:57:17,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 7 finished
galaxy.model.metadata DEBUG 2025-04-18 12:57:17,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 9
galaxy.model.metadata DEBUG 2025-04-18 12:57:17,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 8
galaxy.util WARNING 2025-04-18 12:57:17,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/2/5/dataset_625f07ed-bd51-491d-aeee-303aa4b4f298.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/2/5/dataset_625f07ed-bd51-491d-aeee-303aa4b4f298.dat'
galaxy.util WARNING 2025-04-18 12:57:17,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/3/c/dataset_33c64a7b-185c-4520-af50-0ba6d53257ea.dat'
galaxy.jobs INFO 2025-04-18 12:57:17,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 7 in /galaxy/server/database/jobs_directory/000/7
galaxy.jobs DEBUG 2025-04-18 12:57:17,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 7 executed (128.045 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:17,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:57:19,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 8, 9
tpv.core.entities DEBUG 2025-04-18 12:57:19,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:57:19,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:57:19,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:57:19,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:57:19,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Working directory for job is: /galaxy/server/database/jobs_directory/000/8
galaxy.jobs.runners DEBUG 2025-04-18 12:57:19,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [8] queued (30.219 ms)
galaxy.jobs.handler INFO 2025-04-18 12:57:19,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:19,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 8
tpv.core.entities DEBUG 2025-04-18 12:57:19,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:57:19,374 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:57:19,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:57:19,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:57:19,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Working directory for job is: /galaxy/server/database/jobs_directory/000/9
galaxy.jobs.runners DEBUG 2025-04-18 12:57:19,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [9] queued (40.248 ms)
galaxy.jobs.handler INFO 2025-04-18 12:57:19,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:19,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 9
galaxy.jobs DEBUG 2025-04-18 12:57:19,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [8] prepared (66.846 ms)
galaxy.jobs.command_factory INFO 2025-04-18 12:57:19,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/8/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/8/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/8/configs/tmpny1c9_1s']
galaxy.jobs.runners DEBUG 2025-04-18 12:57:19,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (8) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/8/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/8/galaxy_8.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:19,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 12:57:19,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [9] prepared (62.232 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:19,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 12:57:19,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/9/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/9/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/9/configs/tmp15awg6ru']
galaxy.jobs.runners DEBUG 2025-04-18 12:57:19,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (9) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/9/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/9/galaxy_9.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:19,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:19,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:19,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:20,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:30,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-g8lg4 with k8s id: gxy-g8lg4 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:57:30,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 9: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:31,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s5jzp with k8s id: gxy-s5jzp succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:57:31,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 8: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:57:39,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 9 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:57:39,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'fastqc_adapters.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/9/working/data_fetch_upload_ha2a4juy', 'object_id': 11}]}]}]
galaxy.jobs INFO 2025-04-18 12:57:39,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 9 in /galaxy/server/database/jobs_directory/000/9
galaxy.jobs DEBUG 2025-04-18 12:57:39,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 9 executed (122.192 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:39,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-18 12:57:40,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 8 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:57:40,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '1000trimmed.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/8/working/data_fetch_upload_k2v5rviy', 'object_id': 10}]}]}]
galaxy.jobs INFO 2025-04-18 12:57:41,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 8 in /galaxy/server/database/jobs_directory/000/8
galaxy.jobs DEBUG 2025-04-18 12:57:41,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 8 executed (142.341 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:41,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:57:41,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 10
tpv.core.entities DEBUG 2025-04-18 12:57:41,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*, abstract=False, cores=min(max(int(input_size * 4), 1), 16), mem=min(max(int(input_size * 7), 5.6), 64), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:57:41,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:57:41,873 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:57:41,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:57:41,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Working directory for job is: /galaxy/server/database/jobs_directory/000/10
galaxy.jobs.runners DEBUG 2025-04-18 12:57:41,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [10] queued (35.974 ms)
galaxy.jobs.handler INFO 2025-04-18 12:57:41,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:41,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 10
galaxy.jobs DEBUG 2025-04-18 12:57:41,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [10] prepared (55.400 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 12:57:41,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:57:41,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:57:42,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 12:57:42,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/10/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/4/f/3/dataset_4f3e106e-8568-428f-8c1e-4503c8d675d3.dat' '1000trimmed_fastq' && mkdir -p '/galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0_files' && fastqc --outdir '/galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0_files'  --adapters '/galaxy/server/database/objects/e/a/1/dataset_ea127957-73fd-4bd5-887e-5018a4aada74.dat'  --threads ${GALAXY_SLOTS:-2} --dir ${TEMP:-$_GALAXY_JOB_TMP_DIR} --quiet --extract  --kmers 7 -f 'fastq' '1000trimmed_fastq'  && cp '/galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0_files'/*/fastqc_data.txt output.txt && cp '/galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0_files'/*\.html output.html]
galaxy.jobs.runners DEBUG 2025-04-18 12:57:42,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/10/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/10/galaxy_10.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/10/working/output.txt" -a -f "/galaxy/server/database/objects/a/5/7/dataset_a57b3448-c059-45f0-b352-3d23c462b89b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/10/working/output.txt" "/galaxy/server/database/objects/a/5/7/dataset_a57b3448-c059-45f0-b352-3d23c462b89b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/10/working/output.html" -a -f "/galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/10/working/output.html" "/galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:42,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 12:57:42,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:57:42,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:57:42,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:42,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:42,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:57:50,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbxfs with k8s id: gxy-vbxfs succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:57:50,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 10: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:58:00,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 10 finished
galaxy.model.metadata DEBUG 2025-04-18 12:58:00,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 13
galaxy.model.metadata DEBUG 2025-04-18 12:58:00,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 12
galaxy.util WARNING 2025-04-18 12:58:00,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/5/7/dataset_a57b3448-c059-45f0-b352-3d23c462b89b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/5/7/dataset_a57b3448-c059-45f0-b352-3d23c462b89b.dat'
galaxy.util WARNING 2025-04-18 12:58:00,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/8/f/dataset_e8f1f5fb-cde6-4ba3-add6-9a94323b74e0.dat'
galaxy.jobs INFO 2025-04-18 12:58:00,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 10 in /galaxy/server/database/jobs_directory/000/10
galaxy.jobs DEBUG 2025-04-18 12:58:00,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 10 executed (116.558 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:00,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:58:02,371 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 11, 12
tpv.core.entities DEBUG 2025-04-18 12:58:02,400 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:58:02,400 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:58:02,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:58:02,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:58:02,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Working directory for job is: /galaxy/server/database/jobs_directory/000/11
galaxy.jobs.runners DEBUG 2025-04-18 12:58:02,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [11] queued (31.475 ms)
galaxy.jobs.handler INFO 2025-04-18 12:58:02,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:02,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 11
tpv.core.entities DEBUG 2025-04-18 12:58:02,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:58:02,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:58:02,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:58:02,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:58:02,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Working directory for job is: /galaxy/server/database/jobs_directory/000/12
galaxy.jobs.runners DEBUG 2025-04-18 12:58:02,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [12] queued (41.745 ms)
galaxy.jobs.handler INFO 2025-04-18 12:58:02,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:02,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 12
galaxy.jobs DEBUG 2025-04-18 12:58:02,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [11] prepared (66.917 ms)
galaxy.jobs.command_factory INFO 2025-04-18 12:58:02,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/11/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/11/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/11/configs/tmp_uh244hh']
galaxy.jobs.runners DEBUG 2025-04-18 12:58:02,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (11) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/11/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/11/galaxy_11.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:02,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 12:58:02,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [12] prepared (66.489 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:02,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 12:58:02,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/12/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/12/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/12/configs/tmpm_8_d_tm']
galaxy.jobs.runners DEBUG 2025-04-18 12:58:02,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (12) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/12/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/12/galaxy_12.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:02,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:02,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:03,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:03,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:14,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sjb2x with k8s id: gxy-sjb2x succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:58:14,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 12: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:15,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dnrcf with k8s id: gxy-dnrcf succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:58:15,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 11: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:58:23,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 12 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:58:24,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'fastqc_customlimits.txt', 'dbkey': '?', 'ext': 'txt', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded txt file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/12/working/data_fetch_upload_s3trfm8j', 'object_id': 15}]}]}]
galaxy.jobs INFO 2025-04-18 12:58:24,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 12 in /galaxy/server/database/jobs_directory/000/12
galaxy.jobs DEBUG 2025-04-18 12:58:24,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 12 executed (101.841 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:24,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-18 12:58:24,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 11 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:58:24,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '1000trimmed.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/11/working/data_fetch_upload_wtzmq6yl', 'object_id': 14}]}]}]
galaxy.jobs INFO 2025-04-18 12:58:24,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 11 in /galaxy/server/database/jobs_directory/000/11
galaxy.jobs DEBUG 2025-04-18 12:58:24,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 11 executed (132.581 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:24,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:58:25,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 13
tpv.core.entities DEBUG 2025-04-18 12:58:25,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*, abstract=False, cores=min(max(int(input_size * 4), 1), 16), mem=min(max(int(input_size * 7), 5.6), 64), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:58:25,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:58:25,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:58:25,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:58:25,998 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Working directory for job is: /galaxy/server/database/jobs_directory/000/13
galaxy.jobs.runners DEBUG 2025-04-18 12:58:26,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [13] queued (35.673 ms)
galaxy.jobs.handler INFO 2025-04-18 12:58:26,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:26,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 13
galaxy.jobs DEBUG 2025-04-18 12:58:26,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [13] prepared (63.985 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 12:58:26,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:58:26,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:58:26,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 12:58:26,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/13/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/0/7/6/dataset_07627a75-9776-4dbb-bff0-53d0f4c0579c.dat' '1000trimmed_fastq' && mkdir -p '/galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2_files' && fastqc --outdir '/galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2_files'   --limits '/galaxy/server/database/objects/6/a/f/dataset_6af3c205-4fdd-4b71-a400-0d4abd155e77.dat' --threads ${GALAXY_SLOTS:-2} --dir ${TEMP:-$_GALAXY_JOB_TMP_DIR} --quiet --extract  --kmers 7 -f 'fastq' '1000trimmed_fastq'  && cp '/galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2_files'/*/fastqc_data.txt output.txt && cp '/galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2_files'/*\.html output.html]
galaxy.jobs.runners DEBUG 2025-04-18 12:58:26,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (13) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/13/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/13/galaxy_13.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/13/working/output.txt" -a -f "/galaxy/server/database/objects/a/9/0/dataset_a9028933-23f7-49bd-b097-4567d2738ee1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/13/working/output.txt" "/galaxy/server/database/objects/a/9/0/dataset_a9028933-23f7-49bd-b097-4567d2738ee1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/13/working/output.html" -a -f "/galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/13/working/output.html" "/galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:26,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 12:58:26,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:58:26,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:58:26,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:26,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:27,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:33,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7xsqq with k8s id: gxy-7xsqq succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:58:33,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 13: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:58:42,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 13 finished
galaxy.model.metadata DEBUG 2025-04-18 12:58:43,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 16
galaxy.model.metadata DEBUG 2025-04-18 12:58:43,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 17
galaxy.util WARNING 2025-04-18 12:58:43,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/9/0/dataset_a9028933-23f7-49bd-b097-4567d2738ee1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/9/0/dataset_a9028933-23f7-49bd-b097-4567d2738ee1.dat'
galaxy.util WARNING 2025-04-18 12:58:43,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/d/4/dataset_8d461c61-515d-4cab-9807-b2369e69bca2.dat'
galaxy.jobs INFO 2025-04-18 12:58:43,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 13 in /galaxy/server/database/jobs_directory/000/13
galaxy.jobs DEBUG 2025-04-18 12:58:43,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 13 executed (131.774 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:43,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:58:45,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 14, 15
tpv.core.entities DEBUG 2025-04-18 12:58:45,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:58:45,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:58:45,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:58:45,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:58:45,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Working directory for job is: /galaxy/server/database/jobs_directory/000/14
galaxy.jobs.runners DEBUG 2025-04-18 12:58:45,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [14] queued (37.403 ms)
galaxy.jobs.handler INFO 2025-04-18 12:58:45,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:45,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 14
tpv.core.entities DEBUG 2025-04-18 12:58:45,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:58:45,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:58:45,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:58:45,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:58:45,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Working directory for job is: /galaxy/server/database/jobs_directory/000/15
galaxy.jobs.runners DEBUG 2025-04-18 12:58:45,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [15] queued (38.687 ms)
galaxy.jobs.handler INFO 2025-04-18 12:58:45,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:45,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 15
galaxy.jobs DEBUG 2025-04-18 12:58:45,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [14] prepared (70.136 ms)
galaxy.jobs.command_factory INFO 2025-04-18 12:58:45,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/14/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/14/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/14/configs/tmpm9rtimkz']
galaxy.jobs.runners DEBUG 2025-04-18 12:58:45,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (14) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/14/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/14/galaxy_14.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:45,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 12:58:45,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [15] prepared (65.564 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:45,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 12:58:45,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/15/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/15/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/15/configs/tmp6ipq1n_y']
galaxy.jobs.runners DEBUG 2025-04-18 12:58:45,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (15) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/15/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/15/galaxy_15.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:45,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:45,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:45,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:46,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:57,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9vk8g with k8s id: gxy-9vk8g succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:58:57,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pp6r4 with k8s id: gxy-pp6r4 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:58:57,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 14: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:58:57,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 15: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:59:07,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 14 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:59:07,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '1000trimmed.fastq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/14/working/data_fetch_upload_k020rcju', 'object_id': 18}]}]}]
galaxy.jobs.runners DEBUG 2025-04-18 12:59:07,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 15 finished
galaxy.jobs INFO 2025-04-18 12:59:07,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 14 in /galaxy/server/database/jobs_directory/000/14
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:59:07,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'fastqc_customlimits.txt', 'dbkey': '?', 'ext': 'txt', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded txt file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/15/working/data_fetch_upload_m80axrpt', 'object_id': 19}]}]}]
galaxy.jobs DEBUG 2025-04-18 12:59:07,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 14 executed (146.144 ms)
galaxy.jobs INFO 2025-04-18 12:59:07,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 15 in /galaxy/server/database/jobs_directory/000/15
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:07,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 12:59:07,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 15 executed (113.385 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:07,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:59:08,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 16
tpv.core.entities DEBUG 2025-04-18 12:59:08,070 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*, abstract=False, cores=min(max(int(input_size * 4), 1), 16), mem=min(max(int(input_size * 7), 5.6), 64), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:59:08,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:59:08,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:59:08,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:59:08,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Working directory for job is: /galaxy/server/database/jobs_directory/000/16
galaxy.jobs.runners DEBUG 2025-04-18 12:59:08,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [16] queued (35.987 ms)
galaxy.jobs.handler INFO 2025-04-18 12:59:08,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:08,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 16
galaxy.jobs DEBUG 2025-04-18 12:59:08,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [16] prepared (50.001 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:08,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:59:08,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:08,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 12:59:08,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/16/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/b/9/d/dataset_b9dffe74-271c-4eec-93b0-6f36d3a03b5d.dat' '1000trimmed_fastq' && mkdir -p '/galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba_files' && fastqc --outdir '/galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba_files'   --limits '/galaxy/server/database/objects/c/7/a/dataset_c7a796a0-d97c-4bbf-a61c-c7257e7c3b0d.dat' --threads ${GALAXY_SLOTS:-2} --dir ${TEMP:-$_GALAXY_JOB_TMP_DIR} --quiet --extract  --kmers 3 -f 'fastq' '1000trimmed_fastq'  && cp '/galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba_files'/*/fastqc_data.txt output.txt && cp '/galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba_files'/*\.html output.html]
galaxy.jobs.runners DEBUG 2025-04-18 12:59:08,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (16) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/16/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/16/galaxy_16.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/16/working/output.html" -a -f "/galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/16/working/output.html" "/galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/16/working/output.txt" -a -f "/galaxy/server/database/objects/a/c/6/dataset_ac6116bb-ec49-48cc-bcc9-21266ff30aae.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/16/working/output.txt" "/galaxy/server/database/objects/a/c/6/dataset_ac6116bb-ec49-48cc-bcc9-21266ff30aae.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:08,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:08,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:59:08,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:08,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:08,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:09,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:16,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s5b7l failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:16,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:16,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-s5b7l.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:16,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 12:59:17,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-s5b7l

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-s5b7l": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (16/gxy-s5b7l) tool_stdout: null

galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (16/gxy-s5b7l) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (16/gxy-s5b7l) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (16/gxy-s5b7l) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-s5b7l.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 16 (gxy-s5b7l)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-s5b7l to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:17,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (16/gxy-s5b7l) Terminated at user's request
galaxy.util WARNING 2025-04-18 12:59:17,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/8/b/dataset_38b1584f-6739-4009-9e0a-6a69adab08ba.dat'
galaxy.util WARNING 2025-04-18 12:59:17,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/c/6/dataset_ac6116bb-ec49-48cc-bcc9-21266ff30aae.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/c/6/dataset_ac6116bb-ec49-48cc-bcc9-21266ff30aae.dat'
galaxy.jobs.handler DEBUG 2025-04-18 12:59:19,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 17
tpv.core.entities DEBUG 2025-04-18 12:59:19,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:59:19,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:59:19,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:59:19,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:59:19,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Working directory for job is: /galaxy/server/database/jobs_directory/000/17
galaxy.jobs.runners DEBUG 2025-04-18 12:59:19,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [17] queued (32.521 ms)
galaxy.jobs.handler INFO 2025-04-18 12:59:19,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:19,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 17
galaxy.jobs DEBUG 2025-04-18 12:59:19,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [17] prepared (54.072 ms)
galaxy.jobs.command_factory INFO 2025-04-18 12:59:19,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/17/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/17/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/17/configs/tmpdc31xkhd']
galaxy.jobs.runners DEBUG 2025-04-18 12:59:19,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (17) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/17/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/17/galaxy_17.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:19,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:19,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:20,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ztb9v failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-ztb9v.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 12:59:30,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-ztb9v

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-ztb9v": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (17/gxy-ztb9v) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (17/gxy-ztb9v) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (17/gxy-ztb9v) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (17/gxy-ztb9v) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-ztb9v.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 17 (gxy-ztb9v)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-ztb9v to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:30,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (17/gxy-ztb9v) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-18 12:59:31,589 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 18
tpv.core.entities DEBUG 2025-04-18 12:59:31,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:59:31,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:59:31,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:59:31,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:59:31,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Working directory for job is: /galaxy/server/database/jobs_directory/000/18
galaxy.jobs.runners DEBUG 2025-04-18 12:59:31,647 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [18] queued (29.639 ms)
galaxy.jobs.handler INFO 2025-04-18 12:59:31,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:31,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 18
galaxy.jobs DEBUG 2025-04-18 12:59:31,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [18] prepared (53.113 ms)
galaxy.jobs.command_factory INFO 2025-04-18 12:59:31,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/18/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/18/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/18/configs/tmp2ff89s1h']
galaxy.jobs.runners DEBUG 2025-04-18 12:59:31,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (18) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/18/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/18/galaxy_18.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:31,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:31,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:32,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:43,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qjx8l with k8s id: gxy-qjx8l succeeded
galaxy.jobs.runners DEBUG 2025-04-18 12:59:43,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 18: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 12:59:52,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 18 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 12:59:53,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '1000trimmed.fastq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/18/working/data_fetch_upload_8mfqyb1r', 'object_id': 23}]}]}]
galaxy.jobs INFO 2025-04-18 12:59:53,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 18 in /galaxy/server/database/jobs_directory/000/18
galaxy.jobs DEBUG 2025-04-18 12:59:53,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 18 executed (141.051 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:53,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 12:59:54,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 19
tpv.core.entities DEBUG 2025-04-18 12:59:54,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*, abstract=False, cores=min(max(int(input_size * 4), 1), 16), mem=min(max(int(input_size * 7), 5.6), 64), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 12:59:54,113 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 12:59:54,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 12:59:54,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 12:59:54,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Working directory for job is: /galaxy/server/database/jobs_directory/000/19
galaxy.jobs.runners DEBUG 2025-04-18 12:59:54,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [19] queued (38.722 ms)
galaxy.jobs.handler INFO 2025-04-18 12:59:54,156 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:54,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 19
galaxy.jobs DEBUG 2025-04-18 12:59:54,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [19] prepared (49.768 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:54,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:59:54,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:54,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 12:59:54,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/19/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/b/9/7/dataset_b97d7eb9-52e6-4e8d-98ef-00742967142d.dat' '1000trimmed_fastq' && mkdir -p '/galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5_files' && fastqc --outdir '/galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5_files'   --threads ${GALAXY_SLOTS:-2} --dir ${TEMP:-$_GALAXY_JOB_TMP_DIR} --quiet --extract --nogroup --kmers 7 -f 'fastq' '1000trimmed_fastq'  && cp '/galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5_files'/*/fastqc_data.txt output.txt && cp '/galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5_files'/*\.html output.html]
galaxy.jobs.runners DEBUG 2025-04-18 12:59:54,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/19/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/19/galaxy_19.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/19/working/output.html" -a -f "/galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/19/working/output.html" "/galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/19/working/output.txt" -a -f "/galaxy/server/database/objects/c/7/5/dataset_c75ddfd5-0e37-471a-bb0e-60e29ee1268d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/19/working/output.txt" "/galaxy/server/database/objects/c/7/5/dataset_c75ddfd5-0e37-471a-bb0e-60e29ee1268d.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:54,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:54,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 12:59:54,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 12:59:54,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:54,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 12:59:54,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:02,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6k96b failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:02,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:02,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-6k96b.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:02,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:00:03,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-6k96b

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-6k96b": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19/gxy-6k96b) tool_stdout: null

galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19/gxy-6k96b) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19/gxy-6k96b) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19/gxy-6k96b) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-6k96b.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 19 (gxy-6k96b)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-6k96b to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:03,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19/gxy-6k96b) Terminated at user's request
galaxy.util WARNING 2025-04-18 13:00:03,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/e/9/dataset_ae956151-f0b9-4474-938c-971370da7fd5.dat'
galaxy.util WARNING 2025-04-18 13:00:03,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/7/5/dataset_c75ddfd5-0e37-471a-bb0e-60e29ee1268d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/7/5/dataset_c75ddfd5-0e37-471a-bb0e-60e29ee1268d.dat'
galaxy.jobs.handler DEBUG 2025-04-18 13:00:04,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 20
tpv.core.entities DEBUG 2025-04-18 13:00:04,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:00:04,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:00:04,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:00:04,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:00:04,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Working directory for job is: /galaxy/server/database/jobs_directory/000/20
galaxy.jobs.runners DEBUG 2025-04-18 13:00:04,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [20] queued (33.695 ms)
galaxy.jobs.handler INFO 2025-04-18 13:00:04,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:04,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 20
galaxy.jobs DEBUG 2025-04-18 13:00:04,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [20] prepared (59.491 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:00:04,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/20/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/20/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/20/configs/tmpr__7l_fz']
galaxy.jobs.runners DEBUG 2025-04-18 13:00:04,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (20) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/20/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/20/galaxy_20.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:04,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:04,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:05,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:16,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f7z9b with k8s id: gxy-f7z9b succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:00:16,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 20: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:00:25,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 20 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:00:25,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'hisat_output_1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/20/working/gxupload_0', 'object_id': 26}]}]}]
galaxy.jobs INFO 2025-04-18 13:00:25,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 20 in /galaxy/server/database/jobs_directory/000/20
galaxy.jobs DEBUG 2025-04-18 13:00:25,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 20 executed (123.655 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:25,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:00:26,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 21
tpv.core.entities DEBUG 2025-04-18 13:00:26,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/.*, abstract=False, cores=min(max(int(input_size * 4), 1), 16), mem=min(max(int(input_size * 7), 5.6), 64), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:00:26,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:00:26,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:00:26,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:00:26,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Working directory for job is: /galaxy/server/database/jobs_directory/000/21
galaxy.jobs.runners DEBUG 2025-04-18 13:00:26,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [21] queued (37.272 ms)
galaxy.jobs.handler INFO 2025-04-18 13:00:26,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:26,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 21
galaxy.jobs DEBUG 2025-04-18 13:00:26,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [21] prepared (49.704 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:00:26,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:00:26,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 13:00:27,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:00:27,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/21/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/1/3/7/dataset_137fac73-4640-4fa8-accc-3c41366688e4.dat' 'hisat_output_1_bam' && mkdir -p '/galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1_files' && fastqc --outdir '/galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1_files'   --threads ${GALAXY_SLOTS:-2} --dir ${TEMP:-$_GALAXY_JOB_TMP_DIR} --quiet --extract  --kmers 7 -f 'bam' 'hisat_output_1_bam'  && cp '/galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1_files'/*/fastqc_data.txt output.txt && cp '/galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1_files'/*\.html output.html]
galaxy.jobs.runners DEBUG 2025-04-18 13:00:27,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (21) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/21/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/21/galaxy_21.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/21/working/output.html" -a -f "/galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/21/working/output.html" "/galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/21/working/output.txt" -a -f "/galaxy/server/database/objects/a/c/5/dataset_ac59c11c-5495-48fb-8590-041b0e685ce8.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/21/working/output.txt" "/galaxy/server/database/objects/a/c/5/dataset_ac59c11c-5495-48fb-8590-041b0e685ce8.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:27,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:00:27,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:00:27,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/0.74+galaxy1: fastqc:0.12.1
galaxy.tool_util.deps.containers INFO 2025-04-18 13:00:27,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:27,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:28,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:38,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pfvb2 with k8s id: gxy-pfvb2 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:00:38,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 21: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:00:47,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 21 finished
galaxy.model.metadata DEBUG 2025-04-18 13:00:47,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 27
galaxy.model.metadata DEBUG 2025-04-18 13:00:47,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 28
galaxy.util WARNING 2025-04-18 13:00:47,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/a/1/dataset_8a13d843-6447-4e25-83ec-40b042e765e1.dat'
galaxy.util WARNING 2025-04-18 13:00:47,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/c/5/dataset_ac59c11c-5495-48fb-8590-041b0e685ce8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/c/5/dataset_ac59c11c-5495-48fb-8590-041b0e685ce8.dat'
galaxy.jobs INFO 2025-04-18 13:00:48,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 21 in /galaxy/server/database/jobs_directory/000/21
galaxy.jobs DEBUG 2025-04-18 13:00:48,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 21 executed (124.761 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:48,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:00:51,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 22
tpv.core.entities DEBUG 2025-04-18 13:00:51,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:00:51,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:00:51,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:00:51,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:00:51,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Working directory for job is: /galaxy/server/database/jobs_directory/000/22
galaxy.jobs.runners DEBUG 2025-04-18 13:00:51,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [22] queued (33.739 ms)
galaxy.jobs.handler INFO 2025-04-18 13:00:51,458 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:51,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 22
galaxy.jobs DEBUG 2025-04-18 13:00:51,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [22] prepared (66.238 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:00:51,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/22/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/22/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/22/configs/tmpk4m452a_']
galaxy.jobs.runners DEBUG 2025-04-18 13:00:51,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (22) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/22/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/22/galaxy_22.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:51,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:51,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:00:52,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:03,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j9ncf with k8s id: gxy-j9ncf succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:01:04,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 22: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:01:13,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 22 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:01:13,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '5_markdup.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/22/working/data_fetch_upload_vp6i43hs', 'object_id': 29}]}]}]
galaxy.jobs INFO 2025-04-18 13:01:13,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 22 in /galaxy/server/database/jobs_directory/000/22
galaxy.jobs DEBUG 2025-04-18 13:01:13,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 22 executed (107.726 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:13,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:01:13,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 23
tpv.core.entities DEBUG 2025-04-18 13:01:13,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:01:13,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:01:13,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:01:13,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:01:13,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Working directory for job is: /galaxy/server/database/jobs_directory/000/23
galaxy.jobs.runners DEBUG 2025-04-18 13:01:13,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [23] queued (35.882 ms)
galaxy.jobs.handler INFO 2025-04-18 13:01:13,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:13,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 23
galaxy.jobs DEBUG 2025-04-18 13:01:14,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [23] prepared (53.243 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:14,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:01:14,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:14,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:01:14,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/23/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/23/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T "${TMPDIR:-.}" -O sam -o coordsort.sam '/galaxy/server/database/objects/6/1/8/dataset_618f40fa-88c3-47ed-9c62-cabbf577ab66.dat' &&  samtools markdup  -@ $addthreads    coordsort.sam '/galaxy/server/database/objects/1/3/a/dataset_13a6aac1-fb05-4a3a-b167-29839b25bbed.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:01:14,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (23) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/23/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/23/galaxy_23.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:14,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:14,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:01:14,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:14,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:14,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:14,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sg4td failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-sg4td.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:01:23,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-sg4td

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-sg4td": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (23/gxy-sg4td) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (23/gxy-sg4td) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (23/gxy-sg4td) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (23/gxy-sg4td) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-sg4td.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 23 (gxy-sg4td)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-sg4td to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:23,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (23/gxy-sg4td) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-18 13:01:25,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 24
tpv.core.entities DEBUG 2025-04-18 13:01:25,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:01:25,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:01:25,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:01:25,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:01:25,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Working directory for job is: /galaxy/server/database/jobs_directory/000/24
galaxy.jobs.runners DEBUG 2025-04-18 13:01:25,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [24] queued (33.373 ms)
galaxy.jobs.handler INFO 2025-04-18 13:01:25,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:25,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 24
galaxy.jobs DEBUG 2025-04-18 13:01:25,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [24] prepared (76.550 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:01:25,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/24/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/24/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/24/configs/tmpsub788mk']
galaxy.jobs.runners DEBUG 2025-04-18 13:01:25,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (24) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/24/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/24/galaxy_24.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:25,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:25,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:26,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:36,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nwb2q with k8s id: gxy-nwb2q succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:01:36,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 24: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:01:45,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 24 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:01:45,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '6_remove_dups.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/24/working/data_fetch_upload_4em6z63t', 'object_id': 31}]}]}]
galaxy.jobs INFO 2025-04-18 13:01:45,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 24 in /galaxy/server/database/jobs_directory/000/24
galaxy.jobs DEBUG 2025-04-18 13:01:45,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 24 executed (98.226 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:45,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:01:46,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 25
tpv.core.entities DEBUG 2025-04-18 13:01:46,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:01:46,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:01:46,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:01:46,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:01:46,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Working directory for job is: /galaxy/server/database/jobs_directory/000/25
galaxy.jobs.runners DEBUG 2025-04-18 13:01:46,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [25] queued (32.511 ms)
galaxy.jobs.handler INFO 2025-04-18 13:01:46,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:46,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 25
galaxy.jobs DEBUG 2025-04-18 13:01:46,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [25] prepared (44.315 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:46,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:01:46,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:46,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:01:46,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/25/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/25/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T "${TMPDIR:-.}" -O sam -o coordsort.sam '/galaxy/server/database/objects/5/1/7/dataset_51718217-8406-4a6b-9cb2-0ff4f51c98b4.dat' &&  samtools markdup  -@ $addthreads -r   coordsort.sam '/galaxy/server/database/objects/b/9/b/dataset_b9bd148f-7cc4-4167-9683-5b3d02ec96fd.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:01:46,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (25) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/25/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/25/galaxy_25.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:46,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:46,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:01:46,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:01:46,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:46,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:47,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:01:51,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2qbvb with k8s id: gxy-2qbvb succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:01:51,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 25: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:02:01,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 25 finished
galaxy.model.metadata DEBUG 2025-04-18 13:02:01,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 32
galaxy.jobs INFO 2025-04-18 13:02:01,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 25 in /galaxy/server/database/jobs_directory/000/25
galaxy.jobs DEBUG 2025-04-18 13:02:01,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 25 executed (162.117 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:01,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:02:02,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 26
tpv.core.entities DEBUG 2025-04-18 13:02:02,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:02:02,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:02:02,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:02:02,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:02:02,067 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Working directory for job is: /galaxy/server/database/jobs_directory/000/26
galaxy.jobs.runners DEBUG 2025-04-18 13:02:02,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [26] queued (33.173 ms)
galaxy.jobs.handler INFO 2025-04-18 13:02:02,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:02,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 26
galaxy.jobs DEBUG 2025-04-18 13:02:02,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [26] prepared (60.530 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:02:02,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/26/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/26/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/26/configs/tmpwd91l_0g']
galaxy.jobs.runners DEBUG 2025-04-18 13:02:02,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (26) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/26/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/26/galaxy_26.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:02,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:02,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:02,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:14,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vdc7n with k8s id: gxy-vdc7n succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:02:15,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 26: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:02:24,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 26 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:02:24,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '7_mark_supp_dup.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/26/working/gxupload_0', 'object_id': 33}]}]}]
galaxy.jobs INFO 2025-04-18 13:02:24,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 26 in /galaxy/server/database/jobs_directory/000/26
galaxy.jobs DEBUG 2025-04-18 13:02:24,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 26 executed (123.241 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:24,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:02:25,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 27
tpv.core.entities DEBUG 2025-04-18 13:02:25,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:02:25,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:02:25,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:02:25,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:02:25,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Working directory for job is: /galaxy/server/database/jobs_directory/000/27
galaxy.jobs.runners DEBUG 2025-04-18 13:02:25,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [27] queued (46.655 ms)
galaxy.jobs.handler INFO 2025-04-18 13:02:25,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:25,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 27
galaxy.jobs DEBUG 2025-04-18 13:02:25,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [27] prepared (58.486 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:02:25,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:02:25,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:02:25,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:02:25,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/27/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/27/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  ln -s '/galaxy/server/database/objects/1/3/f/dataset_13f8eccb-355a-4d96-83be-f23c809a4c7c.dat' coordsort.sam &&  samtools markdup  -@ $addthreads   -S coordsort.sam '/galaxy/server/database/objects/f/9/d/dataset_f9db4cec-acf2-4a4d-97ec-8eaed54d067d.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:02:25,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (27) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/27/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/27/galaxy_27.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:25,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:02:25,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:02:25,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:02:25,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:25,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:26,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:31,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-42p5b with k8s id: gxy-42p5b succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:02:31,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 27: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:02:40,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 27 finished
galaxy.model.metadata DEBUG 2025-04-18 13:02:40,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 34
galaxy.jobs INFO 2025-04-18 13:02:40,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 27 in /galaxy/server/database/jobs_directory/000/27
galaxy.jobs DEBUG 2025-04-18 13:02:40,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 27 executed (102.882 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:40,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:02:41,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 28
tpv.core.entities DEBUG 2025-04-18 13:02:41,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:02:41,998 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:02:42,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:02:42,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:02:42,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Working directory for job is: /galaxy/server/database/jobs_directory/000/28
galaxy.jobs.runners DEBUG 2025-04-18 13:02:42,042 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [28] queued (39.710 ms)
galaxy.jobs.handler INFO 2025-04-18 13:02:42,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:42,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 28
galaxy.jobs DEBUG 2025-04-18 13:02:42,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [28] prepared (60.669 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:02:42,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/28/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/28/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/28/configs/tmpvmunbq3m']
galaxy.jobs.runners DEBUG 2025-04-18 13:02:42,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (28) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/28/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/28/galaxy_28.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:42,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:42,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:43,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:02:53,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hcwgj with k8s id: gxy-hcwgj succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:02:53,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 28: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:03:02,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 28 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:03:02,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '5_markdup.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/28/working/data_fetch_upload_h4bif_4h', 'object_id': 35}]}]}]
galaxy.jobs INFO 2025-04-18 13:03:02,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 28 in /galaxy/server/database/jobs_directory/000/28
galaxy.jobs DEBUG 2025-04-18 13:03:02,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 28 executed (101.864 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:02,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:03:03,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 29
tpv.core.entities DEBUG 2025-04-18 13:03:03,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:03:03,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:03:03,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:03:03,506 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:03:03,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Working directory for job is: /galaxy/server/database/jobs_directory/000/29
galaxy.jobs.runners DEBUG 2025-04-18 13:03:03,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [29] queued (32.710 ms)
galaxy.jobs.handler INFO 2025-04-18 13:03:03,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:03,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 29
galaxy.jobs DEBUG 2025-04-18 13:03:03,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [29] prepared (48.024 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:03,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:03:03,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:03,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:03:03,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/29/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/29/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T "${TMPDIR:-.}" -O sam -o coordsort.sam '/galaxy/server/database/objects/7/a/3/dataset_7a3a2b4d-0e9c-40f1-a237-c8ef842999ce.dat' &&  samtools markdup  -@ $addthreads  -s  coordsort.sam '/galaxy/server/database/objects/8/d/c/dataset_8dcb97ec-8635-4849-bc46-52b8bec4c430.dat' 2> '/galaxy/server/database/objects/d/c/d/dataset_dcdffd40-c701-410f-a77e-2808d6f01237.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:03:03,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (29) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/29/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/29/galaxy_29.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:03,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:03,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:03:03,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_markdup/samtools_markdup/1.9+galaxy2: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:03,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:03,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:04,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:08,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5n4wj with k8s id: gxy-5n4wj succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:03:08,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 29: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:03:17,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 29 finished
galaxy.model.metadata DEBUG 2025-04-18 13:03:17,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 36
galaxy.model.metadata DEBUG 2025-04-18 13:03:17,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 37
galaxy.jobs INFO 2025-04-18 13:03:17,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 29 in /galaxy/server/database/jobs_directory/000/29
galaxy.jobs DEBUG 2025-04-18 13:03:17,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 29 executed (119.703 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:17,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:03:21,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 30
tpv.core.entities DEBUG 2025-04-18 13:03:21,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:03:21,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:03:21,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:03:21,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:03:21,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Working directory for job is: /galaxy/server/database/jobs_directory/000/30
galaxy.jobs.runners DEBUG 2025-04-18 13:03:21,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [30] queued (35.525 ms)
galaxy.jobs.handler INFO 2025-04-18 13:03:21,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:21,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 30
galaxy.jobs DEBUG 2025-04-18 13:03:21,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [30] prepared (58.444 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:03:21,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/30/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/30/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/30/configs/tmpk7ly9wt0']
galaxy.jobs.runners DEBUG 2025-04-18 13:03:21,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (30) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/30/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/30/galaxy_30.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:21,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:21,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:21,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:32,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vc87q with k8s id: gxy-vc87q succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:03:32,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 30: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:03:41,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 30 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:03:41,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'cnv.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/30/working/data_fetch_upload_5bidkpj1', 'object_id': 38}]}]}]
galaxy.jobs INFO 2025-04-18 13:03:42,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 30 in /galaxy/server/database/jobs_directory/000/30
galaxy.jobs DEBUG 2025-04-18 13:03:42,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 30 executed (125.222 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:42,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:03:42,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 31
tpv.core.entities DEBUG 2025-04-18 13:03:42,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:03:42,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:03:42,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:03:42,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:03:42,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Working directory for job is: /galaxy/server/database/jobs_directory/000/31
galaxy.jobs.runners DEBUG 2025-04-18 13:03:42,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [31] queued (45.085 ms)
galaxy.jobs.handler INFO 2025-04-18 13:03:42,539 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:42,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 31
galaxy.jobs DEBUG 2025-04-18 13:03:42,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [31] prepared (86.115 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:42,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:03:42,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:42,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:03:42,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/31/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/31/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/1/4/2/dataset_1426c970-29ff-4d3f-aaeb-90cc4afd5e03.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp    -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/3/7/6/dataset_376bf92e-f87b-4f3b-986f-67ce480752cc.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/f/b/f/dataset_fbfb58fc-738e-48d9-bfba-d05dab24db6f.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/5/1/1/dataset_511c4b8d-74ee-43a5-89bf-9734706b4cbd.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/5/1/1/dataset_511c4b8d-74ee-43a5-89bf-9734706b4cbd.dat; python -m base64 $plot >> /galaxy/server/database/objects/5/1/1/dataset_511c4b8d-74ee-43a5-89bf-9734706b4cbd.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/5/1/1/dataset_511c4b8d-74ee-43a5-89bf-9734706b4cbd.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/5/1/1/dataset_511c4b8d-74ee-43a5-89bf-9734706b4cbd.dat;)]
galaxy.jobs.runners DEBUG 2025-04-18 13:03:42,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (31) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/31/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/31/galaxy_31.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:42,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:42,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:03:42,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:03:42,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:42,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:03:43,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:26,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c88mp with k8s id: gxy-c88mp succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:04:26,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 31: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:04:35,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 31 finished
galaxy.model.metadata DEBUG 2025-04-18 13:04:36,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 40
galaxy.model.metadata DEBUG 2025-04-18 13:04:36,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 39
galaxy.model.metadata DEBUG 2025-04-18 13:04:36,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 41
galaxy.util WARNING 2025-04-18 13:04:36,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/b/f/dataset_fbfb58fc-738e-48d9-bfba-d05dab24db6f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/b/f/dataset_fbfb58fc-738e-48d9-bfba-d05dab24db6f.dat'
galaxy.util WARNING 2025-04-18 13:04:36,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/7/6/dataset_376bf92e-f87b-4f3b-986f-67ce480752cc.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/7/6/dataset_376bf92e-f87b-4f3b-986f-67ce480752cc.dat'
galaxy.jobs INFO 2025-04-18 13:04:36,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 31 in /galaxy/server/database/jobs_directory/000/31
galaxy.jobs DEBUG 2025-04-18 13:04:36,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 31 executed (139.708 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:36,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:04:37,589 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 32
tpv.core.entities DEBUG 2025-04-18 13:04:37,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:04:37,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:04:37,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:04:37,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:04:37,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Working directory for job is: /galaxy/server/database/jobs_directory/000/32
galaxy.jobs.runners DEBUG 2025-04-18 13:04:37,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [32] queued (32.506 ms)
galaxy.jobs.handler INFO 2025-04-18 13:04:37,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:37,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 32
galaxy.jobs DEBUG 2025-04-18 13:04:37,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [32] prepared (56.499 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:04:37,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/32/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/32/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/32/configs/tmphixgw4ee']
galaxy.jobs.runners DEBUG 2025-04-18 13:04:37,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (32) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/32/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/32/galaxy_32.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:37,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:37,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:38,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:48,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4gr7x with k8s id: gxy-4gr7x succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:04:49,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 32: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:04:58,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 32 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:04:58,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'cnv.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/32/working/data_fetch_upload_z1k_g3ga', 'object_id': 42}]}]}]
galaxy.jobs INFO 2025-04-18 13:04:58,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 32 in /galaxy/server/database/jobs_directory/000/32
galaxy.jobs DEBUG 2025-04-18 13:04:58,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 32 executed (110.008 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:58,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:04:59,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 33
tpv.core.entities DEBUG 2025-04-18 13:04:59,113 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:04:59,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:04:59,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:04:59,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:04:59,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Working directory for job is: /galaxy/server/database/jobs_directory/000/33
galaxy.jobs.runners DEBUG 2025-04-18 13:04:59,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [33] queued (36.080 ms)
galaxy.jobs.handler INFO 2025-04-18 13:04:59,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:59,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 33
galaxy.jobs DEBUG 2025-04-18 13:04:59,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [33] prepared (51.807 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:04:59,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:04:59,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:04:59,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:04:59,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/33/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/33/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/a/e/5/dataset_ae5851ff-4a7f-45bf-8479-c7c55a572d18.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp     --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/d/a/d/dataset_dadbb6e3-f80b-487c-b2f1-69ef00db601a.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/d/e/a/dataset_deac64cf-500b-47a9-8e06-2e8c042f8262.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:04:59,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (33) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/33/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/33/galaxy_33.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:59,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:04:59,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:04:59,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:04:59,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:59,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:04:59,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:04,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4lccs with k8s id: gxy-4lccs succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:05:04,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 33: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:05:13,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 33 finished
galaxy.model.metadata DEBUG 2025-04-18 13:05:13,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 43
galaxy.model.metadata DEBUG 2025-04-18 13:05:13,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 44
galaxy.util WARNING 2025-04-18 13:05:13,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/a/d/dataset_dadbb6e3-f80b-487c-b2f1-69ef00db601a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/a/d/dataset_dadbb6e3-f80b-487c-b2f1-69ef00db601a.dat'
galaxy.util WARNING 2025-04-18 13:05:13,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/e/a/dataset_deac64cf-500b-47a9-8e06-2e8c042f8262.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/e/a/dataset_deac64cf-500b-47a9-8e06-2e8c042f8262.dat'
galaxy.jobs INFO 2025-04-18 13:05:13,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 33 in /galaxy/server/database/jobs_directory/000/33
galaxy.jobs DEBUG 2025-04-18 13:05:13,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 33 executed (119.846 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:13,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:05:14,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 34
tpv.core.entities DEBUG 2025-04-18 13:05:14,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:05:14,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:05:14,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:05:14,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:05:14,508 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Working directory for job is: /galaxy/server/database/jobs_directory/000/34
galaxy.jobs.runners DEBUG 2025-04-18 13:05:14,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [34] queued (32.046 ms)
galaxy.jobs.handler INFO 2025-04-18 13:05:14,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:14,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 34
galaxy.jobs DEBUG 2025-04-18 13:05:14,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [34] prepared (58.649 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:05:14,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/34/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/34/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/34/configs/tmpnw8rx9tu']
galaxy.jobs.runners DEBUG 2025-04-18 13:05:14,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (34) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/34/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/34/galaxy_34.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:14,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:14,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:15,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:26,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xjpv5 with k8s id: gxy-xjpv5 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:05:26,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 34: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:05:35,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 34 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:05:35,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'cnv.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/34/working/data_fetch_upload__wmsub4j', 'object_id': 45}]}]}]
galaxy.jobs INFO 2025-04-18 13:05:35,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 34 in /galaxy/server/database/jobs_directory/000/34
galaxy.jobs DEBUG 2025-04-18 13:05:35,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 34 executed (118.815 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:35,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:05:36,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 35
tpv.core.entities DEBUG 2025-04-18 13:05:36,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:05:36,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:05:36,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:05:37,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:05:37,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Working directory for job is: /galaxy/server/database/jobs_directory/000/35
galaxy.jobs.runners DEBUG 2025-04-18 13:05:37,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [35] queued (40.214 ms)
galaxy.jobs.handler INFO 2025-04-18 13:05:37,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:37,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 35
galaxy.jobs DEBUG 2025-04-18 13:05:37,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [35] prepared (56.560 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:05:37,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:05:37,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:05:37,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:05:37,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/35/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/35/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/f/7/1/dataset_f714f9b1-3524-4214-8281-499e7e18485a.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp  -c 'test' -s 'test'   -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/5/c/7/dataset_5c7fdbec-a486-46cb-bbb9-22b73f17173e.dat' && mv cnv_tmp/summary.tab '/galaxy/server/database/objects/4/9/e/dataset_49e53c21-c99f-4487-97fb-70bb28be1a33.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/c/d/6/dataset_cd68ac01-0b28-433b-ac83-1b5b160123cc.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/c/d/6/dataset_cd68ac01-0b28-433b-ac83-1b5b160123cc.dat; python -m base64 $plot >> /galaxy/server/database/objects/c/d/6/dataset_cd68ac01-0b28-433b-ac83-1b5b160123cc.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/c/d/6/dataset_cd68ac01-0b28-433b-ac83-1b5b160123cc.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/c/d/6/dataset_cd68ac01-0b28-433b-ac83-1b5b160123cc.dat;)]
galaxy.jobs.runners DEBUG 2025-04-18 13:05:37,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (35) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/35/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/35/galaxy_35.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:37,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:05:37,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:05:37,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:05:37,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:37,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:38,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:45,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-758wt with k8s id: gxy-758wt succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:05:45,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 35: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:05:54,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 35 finished
galaxy.model.metadata DEBUG 2025-04-18 13:05:54,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 46
galaxy.model.metadata DEBUG 2025-04-18 13:05:54,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 47
galaxy.model.metadata DEBUG 2025-04-18 13:05:54,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 48
galaxy.util WARNING 2025-04-18 13:05:54,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/c/7/dataset_5c7fdbec-a486-46cb-bbb9-22b73f17173e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/c/7/dataset_5c7fdbec-a486-46cb-bbb9-22b73f17173e.dat'
galaxy.util WARNING 2025-04-18 13:05:54,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/9/e/dataset_49e53c21-c99f-4487-97fb-70bb28be1a33.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/9/e/dataset_49e53c21-c99f-4487-97fb-70bb28be1a33.dat'
galaxy.jobs INFO 2025-04-18 13:05:54,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 35 in /galaxy/server/database/jobs_directory/000/35
galaxy.jobs DEBUG 2025-04-18 13:05:54,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 35 executed (125.493 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:55,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:05:56,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 36
tpv.core.entities DEBUG 2025-04-18 13:05:56,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:05:56,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:05:56,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:05:56,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:05:56,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Working directory for job is: /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-04-18 13:05:56,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [36] queued (34.150 ms)
galaxy.jobs.handler INFO 2025-04-18 13:05:56,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:56,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 36
galaxy.jobs DEBUG 2025-04-18 13:05:56,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [36] prepared (58.580 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:05:56,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/36/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/36/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/36/configs/tmp77mt283z']
galaxy.jobs.runners DEBUG 2025-04-18 13:05:56,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (36) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/36/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/36/galaxy_36.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:56,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:56,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:05:57,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:08,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6wtkw with k8s id: gxy-6wtkw succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:06:08,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 36: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:06:18,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 36 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:06:18,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'cnv_baf_only.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/36/working/data_fetch_upload_6pr27lkt', 'object_id': 49}]}]}]
galaxy.jobs INFO 2025-04-18 13:06:18,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs DEBUG 2025-04-18 13:06:18,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 36 executed (107.693 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:18,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:06:18,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 37
tpv.core.entities DEBUG 2025-04-18 13:06:18,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:06:18,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:06:19,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:06:19,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:06:19,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.runners DEBUG 2025-04-18 13:06:19,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (48.944 ms)
galaxy.jobs.handler INFO 2025-04-18 13:06:19,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:19,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 37
galaxy.jobs DEBUG 2025-04-18 13:06:19,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [37] prepared (54.627 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:19,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:06:19,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:19,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:06:19,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/37/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/3/9/dataset_b39a994f-1a75-47c6-a4b1-ef84d16ceb00.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp     --aberrant "1.0,1.0" --BAF-weight 1 --BAF-dev "0.04,0.04" --LRR-weight 0 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/0/2/a/dataset_02ab00f3-839e-4554-9938-a8479352879f.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/e/9/6/dataset_e9602176-16c0-4057-9c94-4e91a7870213.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:06:19,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:19,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:19,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:06:19,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:19,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:19,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:19,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bhctc failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-bhctc.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:06:23,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-bhctc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-bhctc": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (37/gxy-bhctc) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (37/gxy-bhctc) tool_stderr: Number of lines: total/processed: 184/184

galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (37/gxy-bhctc) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (37/gxy-bhctc) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-bhctc.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 37 (gxy-bhctc)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-bhctc to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:23,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:24,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (37/gxy-bhctc) Terminated at user's request
galaxy.util WARNING 2025-04-18 13:06:24,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/2/a/dataset_02ab00f3-839e-4554-9938-a8479352879f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/2/a/dataset_02ab00f3-839e-4554-9938-a8479352879f.dat'
galaxy.util WARNING 2025-04-18 13:06:24,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/9/6/dataset_e9602176-16c0-4057-9c94-4e91a7870213.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/9/6/dataset_e9602176-16c0-4057-9c94-4e91a7870213.dat'
galaxy.jobs.handler DEBUG 2025-04-18 13:06:25,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38
tpv.core.entities DEBUG 2025-04-18 13:06:25,195 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:06:25,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:06:25,200 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:06:25,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:06:25,227 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-04-18 13:06:25,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (35.197 ms)
galaxy.jobs.handler INFO 2025-04-18 13:06:25,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:25,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 38
galaxy.jobs DEBUG 2025-04-18 13:06:25,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [38] prepared (74.416 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:06:25,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/38/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/38/configs/tmpvjxxiz5v']
galaxy.jobs.runners DEBUG 2025-04-18 13:06:25,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:25,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:25,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:25,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:37,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-98vs2 with k8s id: gxy-98vs2 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:06:37,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 38: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:06:47,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 38 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:06:47,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'cnv.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/38/working/data_fetch_upload_uqmxpcg9', 'object_id': 52}]}]}]
galaxy.jobs INFO 2025-04-18 13:06:47,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 38 in /galaxy/server/database/jobs_directory/000/38
galaxy.jobs DEBUG 2025-04-18 13:06:47,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 38 executed (141.302 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:47,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:06:47,699 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 39
tpv.core.entities DEBUG 2025-04-18 13:06:47,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:06:47,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:06:47,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:06:47,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:06:47,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.runners DEBUG 2025-04-18 13:06:47,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (41.272 ms)
galaxy.jobs.handler INFO 2025-04-18 13:06:47,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:47,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 39
galaxy.jobs DEBUG 2025-04-18 13:06:47,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [39] prepared (54.573 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:47,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:06:47,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:47,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:06:47,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/39/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/3/8/dataset_b388cc95-aa0a-4599-adf4-77f0201f0b4c.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp    -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09    --regions-overlap 1          input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/a/7/a/dataset_a7a50b99-b1c6-45ac-bb5a-e5dab57b64fb.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/0/a/2/dataset_0a2205f4-bb5f-4c44-b8f0-5f45ea12a3c2.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/2/d/9/dataset_2d9a71cc-e47f-4891-83ed-c962464cea0f.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/2/d/9/dataset_2d9a71cc-e47f-4891-83ed-c962464cea0f.dat; python -m base64 $plot >> /galaxy/server/database/objects/2/d/9/dataset_2d9a71cc-e47f-4891-83ed-c962464cea0f.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/2/d/9/dataset_2d9a71cc-e47f-4891-83ed-c962464cea0f.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/2/d/9/dataset_2d9a71cc-e47f-4891-83ed-c962464cea0f.dat;)]
galaxy.jobs.runners DEBUG 2025-04-18 13:06:47,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:47,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:47,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:06:47,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-04-18 13:06:47,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:47,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:48,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:06:55,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cvzl7 with k8s id: gxy-cvzl7 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:06:55,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 39: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:07:04,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 39 finished
galaxy.model.metadata DEBUG 2025-04-18 13:07:04,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 53
galaxy.model.metadata DEBUG 2025-04-18 13:07:04,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 54
galaxy.model.metadata DEBUG 2025-04-18 13:07:04,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 55
galaxy.util WARNING 2025-04-18 13:07:04,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/7/a/dataset_a7a50b99-b1c6-45ac-bb5a-e5dab57b64fb.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/7/a/dataset_a7a50b99-b1c6-45ac-bb5a-e5dab57b64fb.dat'
galaxy.util WARNING 2025-04-18 13:07:04,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/a/2/dataset_0a2205f4-bb5f-4c44-b8f0-5f45ea12a3c2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/a/2/dataset_0a2205f4-bb5f-4c44-b8f0-5f45ea12a3c2.dat'
galaxy.jobs INFO 2025-04-18 13:07:04,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 39 in /galaxy/server/database/jobs_directory/000/39
galaxy.jobs DEBUG 2025-04-18 13:07:05,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 39 executed (165.744 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:05,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:07:07,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 40
tpv.core.entities DEBUG 2025-04-18 13:07:07,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:07:07,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:07:07,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:07:07,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:07:07,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-04-18 13:07:07,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (38.705 ms)
galaxy.jobs.handler INFO 2025-04-18 13:07:07,233 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:07,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 40
galaxy.jobs DEBUG 2025-04-18 13:07:07,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [40] prepared (71.491 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:07:07,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/40/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/40/configs/tmp34cedbfg']
galaxy.jobs.runners DEBUG 2025-04-18 13:07:07,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:07,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:07,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:08,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:18,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-flq2s with k8s id: gxy-flq2s succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:07:18,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 40: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:07:28,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 40 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:07:28,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'roh.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/40/working/data_fetch_upload_8chphwem', 'object_id': 56}]}]}]
galaxy.jobs INFO 2025-04-18 13:07:28,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 40 in /galaxy/server/database/jobs_directory/000/40
galaxy.jobs DEBUG 2025-04-18 13:07:28,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 40 executed (428.856 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:28,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:07:29,682 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-04-18 13:07:29,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:07:29,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:07:29,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:07:29,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:07:29,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-04-18 13:07:29,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (33.724 ms)
galaxy.jobs.handler INFO 2025-04-18 13:07:29,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:29,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 41
galaxy.jobs DEBUG 2025-04-18 13:07:29,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [41] prepared (79.794 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:07:29,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:07:29,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:07:30,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:07:30,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/41/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/f/9/a/dataset_f9a5d5eb-53c8-4e16-a010-3ff949eeac5f.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"                 input.vcf.gz  > '/galaxy/server/database/objects/0/8/c/dataset_08c63010-57dc-4bde-a5f8-fa52032a97e9.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:07:30,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:30,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:07:30,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:07:30,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:07:30,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:30,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:30,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:43,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lwjxm with k8s id: gxy-lwjxm succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:07:43,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:07:52,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 41 finished
galaxy.model.metadata DEBUG 2025-04-18 13:07:52,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 57
galaxy.jobs INFO 2025-04-18 13:07:52,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-04-18 13:07:52,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 41 executed (104.695 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:52,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:07:54,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 42
tpv.core.entities DEBUG 2025-04-18 13:07:54,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:07:54,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:07:54,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:07:54,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:07:54,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-04-18 13:07:54,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (31.860 ms)
galaxy.jobs.handler INFO 2025-04-18 13:07:54,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:54,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 42
galaxy.jobs DEBUG 2025-04-18 13:07:54,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [42] prepared (59.520 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:07:54,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/42/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/42/configs/tmpit638f3u']
galaxy.jobs.runners DEBUG 2025-04-18 13:07:54,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:54,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:54,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:07:55,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:05,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w2ljj with k8s id: gxy-w2ljj succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:08:05,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:08:14,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 42 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:08:14,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'roh.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/42/working/data_fetch_upload_mbwgf8lh', 'object_id': 58}]}]}]
galaxy.jobs INFO 2025-04-18 13:08:15,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs DEBUG 2025-04-18 13:08:15,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 42 executed (424.028 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:15,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:08:15,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43
tpv.core.entities DEBUG 2025-04-18 13:08:15,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:08:15,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:08:15,755 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:08:15,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:08:15,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-04-18 13:08:15,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (34.679 ms)
galaxy.jobs.handler INFO 2025-04-18 13:08:15,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:15,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 43
galaxy.jobs DEBUG 2025-04-18 13:08:15,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [43] prepared (47.846 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:15,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:08:15,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:15,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:08:15,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/43/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/7/c/3/dataset_7c3d025d-5740-4f36-992e-2e2467a356da.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"               --output-type r   input.vcf.gz  > '/galaxy/server/database/objects/3/2/b/dataset_32b508ae-4aa4-4664-b02e-a55d08b0cbd7.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:08:15,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:15,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:15,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:08:15,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:15,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:15,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:16,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:20,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z9pgt with k8s id: gxy-z9pgt succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:08:21,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:08:29,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 43 finished
galaxy.model.metadata DEBUG 2025-04-18 13:08:29,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 59
galaxy.jobs INFO 2025-04-18 13:08:29,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-04-18 13:08:30,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 43 executed (92.513 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:30,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:08:31,100 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 44
tpv.core.entities DEBUG 2025-04-18 13:08:31,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:08:31,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:08:31,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:08:31,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:08:31,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.runners DEBUG 2025-04-18 13:08:31,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (31.381 ms)
galaxy.jobs.handler INFO 2025-04-18 13:08:31,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:31,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 44
galaxy.jobs DEBUG 2025-04-18 13:08:31,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [44] prepared (57.226 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:08:31,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/44/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/44/configs/tmpbkwlgs6j']
galaxy.jobs.runners DEBUG 2025-04-18 13:08:31,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:31,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:31,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:33,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:44,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h9c2l with k8s id: gxy-h9c2l succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:08:44,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:08:53,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 44 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:08:53,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'roh.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/44/working/data_fetch_upload_91yksyn7', 'object_id': 60}]}]}]
galaxy.jobs INFO 2025-04-18 13:08:53,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-04-18 13:08:53,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 44 executed (412.436 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:53,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:08:54,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-04-18 13:08:54,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:08:54,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:08:54,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:08:54,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:08:54,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-04-18 13:08:54,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (30.972 ms)
galaxy.jobs.handler INFO 2025-04-18 13:08:54,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:54,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-04-18 13:08:54,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [45] prepared (44.901 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:54,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:08:54,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:54,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:08:54,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/45/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/5/f/dataset_b5f4e36f-70e1-4341-a012-75658823a9b7.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"  --ignore-homref --include-noalt  --hw-to-az "6.7e-08" --az-to-hw "5e-09"               --output-type r   input.vcf.gz  > '/galaxy/server/database/objects/6/b/f/dataset_6bf4051d-e267-4996-8b14-e770e835b2ba.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:08:54,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:54,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:54,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:08:54,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:08:54,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:55,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:08:55,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:00,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xg6h4 with k8s id: gxy-xg6h4 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:09:00,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:09:09,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 45 finished
galaxy.model.metadata DEBUG 2025-04-18 13:09:09,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 61
galaxy.jobs INFO 2025-04-18 13:09:09,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-04-18 13:09:09,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 45 executed (93.947 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:09,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:09:11,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-04-18 13:09:11,228 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:09:11,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:09:11,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:09:11,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:09:11,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-04-18 13:09:11,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (36.143 ms)
galaxy.jobs.handler INFO 2025-04-18 13:09:11,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:11,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-04-18 13:09:11,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [46] prepared (58.742 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:09:11,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/46/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/46/configs/tmp0uc3a6ym']
galaxy.jobs.runners DEBUG 2025-04-18 13:09:11,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:11,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:11,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:12,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:22,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-49x58 with k8s id: gxy-49x58 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:09:22,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:09:31,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 46 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:09:31,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'roh.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/46/working/data_fetch_upload_w1ak0cs2', 'object_id': 62}]}]}]
galaxy.jobs INFO 2025-04-18 13:09:32,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-04-18 13:09:32,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 46 executed (398.020 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:32,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:09:32,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-04-18 13:09:32,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:09:32,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:09:32,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:09:32,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:09:32,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-04-18 13:09:32,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (32.622 ms)
galaxy.jobs.handler INFO 2025-04-18 13:09:32,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:32,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 47
galaxy.jobs DEBUG 2025-04-18 13:09:32,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [47] prepared (48.998 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:09:32,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:09:32,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:09:32,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:09:32,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/47/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/5/0/c/dataset_50ca4fdb-d8c8-49b7-bb8a-36a19d66d9c3.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"     --regions-overlap 1             input.vcf.gz  > '/galaxy/server/database/objects/1/1/0/dataset_1105703e-cc85-4695-a8b4-e88904f55a9f.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:09:32,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:32,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:09:32,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:09:32,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-18 13:09:32,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:32,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:33,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:38,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-28b4v with k8s id: gxy-28b4v succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:09:38,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:09:48,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 47 finished
galaxy.model.metadata DEBUG 2025-04-18 13:09:48,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 63
galaxy.jobs INFO 2025-04-18 13:09:48,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs DEBUG 2025-04-18 13:09:48,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 47 executed (102.874 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:48,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:09:51,108 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 48, 49
tpv.core.entities DEBUG 2025-04-18 13:09:51,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:09:51,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:09:51,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:09:51,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:09:51,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-04-18 13:09:51,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (35.575 ms)
galaxy.jobs.handler INFO 2025-04-18 13:09:51,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 48
tpv.core.entities DEBUG 2025-04-18 13:09:51,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:09:51,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:09:51,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:09:51,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:09:51,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-04-18 13:09:51,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (45.721 ms)
galaxy.jobs.handler INFO 2025-04-18 13:09:51,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-04-18 13:09:51,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [48] prepared (82.131 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:09:51,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/48/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/48/configs/tmpsy29x6dr']
galaxy.jobs.runners DEBUG 2025-04-18 13:09:51,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:09:51,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [49] prepared (68.097 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:09:51,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/49/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/49/configs/tmpym9k72eq']
galaxy.jobs.runners DEBUG 2025-04-18 13:09:51,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:09:51,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:03,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7d9lg with k8s id: gxy-7d9lg succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:03,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cxmrq with k8s id: gxy-cxmrq succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:10:03,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:10:03,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:10:12,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 48 finished
galaxy.jobs.runners DEBUG 2025-04-18 13:10:12,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 49 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:10:12,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/48/working/data_fetch_upload_w_54ym6o', 'object_id': 64}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:10:12,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcfannotate.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/49/working/data_fetch_upload_2af1xltp', 'object_id': 65}]}]}]
galaxy.jobs INFO 2025-04-18 13:10:12,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs INFO 2025-04-18 13:10:12,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs DEBUG 2025-04-18 13:10:13,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 48 executed (144.290 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:13,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:10:13,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 49 executed (142.610 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:13,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:10:13,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-04-18 13:10:14,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:10:14,005 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:10:14,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:10:14,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:10:14,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-04-18 13:10:14,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (33.621 ms)
galaxy.jobs.handler INFO 2025-04-18 13:10:14,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:14,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-04-18 13:10:14,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [50] prepared (50.915 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:10:14,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:10:14,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-18 13:10:14,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:10:14,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [vcfintersect -b '/galaxy/server/database/objects/8/e/d/dataset_8ed1a360-0fb3-4fb5-be0a-b9ca666edaf3.dat'  '/galaxy/server/database/objects/d/f/c/dataset_dfcbade0-8a3b-4778-9080-55366d51a9c5.dat' > '/galaxy/server/database/objects/e/0/7/dataset_e0708c57-725c-4dc6-a549-e5e8d2698b45.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:10:14,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:14,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:10:14,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:10:14,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-18 13:10:14,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:14,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:15,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:27,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dxcdw with k8s id: gxy-dxcdw succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:10:27,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:10:36,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 50 finished
galaxy.model.metadata DEBUG 2025-04-18 13:10:36,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 66
galaxy.jobs INFO 2025-04-18 13:10:36,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-04-18 13:10:36,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 50 executed (82.082 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:36,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:10:38,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 51
tpv.core.entities DEBUG 2025-04-18 13:10:38,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:10:38,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:10:38,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:10:38,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:10:38,568 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-04-18 13:10:38,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (31.678 ms)
galaxy.jobs.handler INFO 2025-04-18 13:10:38,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:38,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 51
galaxy.jobs DEBUG 2025-04-18 13:10:38,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [51] prepared (60.838 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:10:38,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/51/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/51/configs/tmplnhz4oha']
galaxy.jobs.runners DEBUG 2025-04-18 13:10:38,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:38,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:38,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:39,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:49,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bhzv7 with k8s id: gxy-bhzv7 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:10:49,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:10:58,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 51 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:10:58,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/51/working/data_fetch_upload_ubhy68o5', 'object_id': 67}]}]}]
galaxy.jobs INFO 2025-04-18 13:10:59,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs DEBUG 2025-04-18 13:10:59,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 51 executed (108.832 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:10:59,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:10:59,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52
tpv.core.entities DEBUG 2025-04-18 13:11:00,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:11:00,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:11:00,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:11:00,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:11:00,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-04-18 13:11:00,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (34.812 ms)
galaxy.jobs.handler INFO 2025-04-18 13:11:00,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:00,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-04-18 13:11:00,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [52] prepared (43.545 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:11:00,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:11:00,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-18 13:11:00,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:11:00,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [vcfintersect -R '20:1-30000'  '/galaxy/server/database/objects/2/9/6/dataset_296c752a-79f1-4dc0-ad6b-f757a2902c2e.dat' > '/galaxy/server/database/objects/4/e/b/dataset_4ebc0c07-ba31-4ac7-89e9-542e03ecf030.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:11:00,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:00,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:11:00,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:11:00,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-18 13:11:00,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:00,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:00,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:04,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hcnxc with k8s id: gxy-hcnxc succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:11:04,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:11:13,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 52 finished
galaxy.model.metadata DEBUG 2025-04-18 13:11:13,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 68
galaxy.jobs INFO 2025-04-18 13:11:13,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-04-18 13:11:13,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 52 executed (94.581 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:13,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:11:16,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 53, 54, 55
tpv.core.entities DEBUG 2025-04-18 13:11:16,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:11:16,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:11:16,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:11:16,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:11:16,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-04-18 13:11:16,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (31.882 ms)
galaxy.jobs.handler INFO 2025-04-18 13:11:16,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 53
tpv.core.entities DEBUG 2025-04-18 13:11:16,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:11:16,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:11:16,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:11:16,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:11:16,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-04-18 13:11:16,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (41.871 ms)
galaxy.jobs.handler INFO 2025-04-18 13:11:16,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 54
tpv.core.entities DEBUG 2025-04-18 13:11:16,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:11:16,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:11:16,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:11:16,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:11:16,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [53] prepared (82.680 ms)
galaxy.jobs DEBUG 2025-04-18 13:11:16,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-04-18 13:11:16,554 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (44.871 ms)
galaxy.jobs.handler INFO 2025-04-18 13:11:16,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.command_factory INFO 2025-04-18 13:11:16,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/53/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/53/configs/tmpma8rencu']
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 55
galaxy.jobs.runners DEBUG 2025-04-18 13:11:16,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:11:16,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [54] prepared (84.698 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:11:16,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/54/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/54/configs/tmp2pn33g2a']
galaxy.jobs.runners DEBUG 2025-04-18 13:11:16,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:11:16,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [55] prepared (71.980 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:11:16,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/55/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/55/configs/tmpyz81hf10']
galaxy.jobs.runners DEBUG 2025-04-18 13:11:16,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:16,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:17,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:18,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:18,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:27,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mqrv9 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:27,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:27,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-mqrv9.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:11:28,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-mqrv9

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-mqrv9": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-mqrv9) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-mqrv9) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-mqrv9) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-mqrv9) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-mqrv9.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 54 (gxy-mqrv9)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-mqrv9 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7xvnj with k8s id: gxy-7xvnj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8w252 with k8s id: gxy-8w252 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:28,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-mqrv9) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-04-18 13:11:28,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:11:28,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:11:37,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 53 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:11:37,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcfvcfintersect-input1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/53/working/data_fetch_upload_zl4fihyt', 'object_id': 69}]}]}]
galaxy.jobs.runners DEBUG 2025-04-18 13:11:37,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 55 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:11:37,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib-test-genome-phix.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/55/working/data_fetch_upload_ffb3xuxi', 'object_id': 71}]}]}]
galaxy.jobs INFO 2025-04-18 13:11:37,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs INFO 2025-04-18 13:11:37,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-04-18 13:11:37,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 53 executed (141.471 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:37,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:11:37,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 55 executed (106.980 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:37,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:11:39,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56, 58, 57
tpv.core.entities DEBUG 2025-04-18 13:11:39,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:11:39,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:11:39,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:11:39,104 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:11:39,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-04-18 13:11:39,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (31.162 ms)
galaxy.jobs.handler INFO 2025-04-18 13:11:39,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 56
tpv.core.entities DEBUG 2025-04-18 13:11:39,140 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:11:39,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:11:39,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:11:39,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:11:39,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-04-18 13:11:39,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (40.653 ms)
galaxy.jobs.handler INFO 2025-04-18 13:11:39,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 57
tpv.core.entities DEBUG 2025-04-18 13:11:39,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:11:39,200 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:11:39,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:11:39,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [56] prepared (84.217 ms)
galaxy.jobs DEBUG 2025-04-18 13:11:39,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:11:39,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.command_factory INFO 2025-04-18 13:11:39,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/56/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/56/configs/tmp5pbc8e3d']
galaxy.jobs.runners DEBUG 2025-04-18 13:11:39,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (48.039 ms)
galaxy.jobs.handler INFO 2025-04-18 13:11:39,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 58
galaxy.jobs.runners DEBUG 2025-04-18 13:11:39,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:11:39,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [57] prepared (82.597 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:11:39,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/57/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/57/configs/tmpauh7jnw4']
galaxy.jobs.runners DEBUG 2025-04-18 13:11:39,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-18 13:11:39,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [58] prepared (73.464 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:11:39,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/58/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/58/configs/tmp4fg6r7cv']
galaxy.jobs.runners DEBUG 2025-04-18 13:11:39,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:39,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:40,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:40,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:40,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:50,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wdr7d with k8s id: gxy-wdr7d succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:50,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-twvxs with k8s id: gxy-twvxs succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:11:51,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 56: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:11:51,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 57: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:11:51,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s7cp2 with k8s id: gxy-s7cp2 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:11:52,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:12:04,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 56 finished
galaxy.jobs.runners DEBUG 2025-04-18 13:12:05,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 57 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:12:05,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcfvcfintersect-input1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/56/working/data_fetch_upload_fjk7oj2e', 'object_id': 72}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:12:05,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcfvcfintersect-input2.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/57/working/data_fetch_upload_1lp41sd9', 'object_id': 73}]}]}]
galaxy.jobs INFO 2025-04-18 13:12:05,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 56 in /galaxy/server/database/jobs_directory/000/56
galaxy.jobs INFO 2025-04-18 13:12:05,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 57 in /galaxy/server/database/jobs_directory/000/57
galaxy.jobs DEBUG 2025-04-18 13:12:05,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 56 executed (190.533 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:05,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:12:05,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 57 executed (134.673 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:05,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-18 13:12:05,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 58 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:12:05,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib-test-genome-phix.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/58/working/data_fetch_upload_ztkjv0so', 'object_id': 74}]}]}]
galaxy.jobs INFO 2025-04-18 13:12:05,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.jobs DEBUG 2025-04-18 13:12:05,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 58 executed (107.461 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:06,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:12:07,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-04-18 13:12:07,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:12:07,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:12:07,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:12:07,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:12:07,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-04-18 13:12:07,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (38.994 ms)
galaxy.jobs.handler INFO 2025-04-18 13:12:07,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:07,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-04-18 13:12:07,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [59] prepared (56.025 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:07,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:12:07,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:07,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:12:07,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/4/f/5/dataset_4f5f4b94-77c6-46cf-8cd0-4ece110f0493.dat' 'localref.fa' &&  vcfintersect   -r 'localref.fa' -w "30" -u '/galaxy/server/database/objects/b/b/a/dataset_bbabfd0e-601b-4149-81d2-30b6bb9d75b5.dat' '/galaxy/server/database/objects/f/4/a/dataset_f4a51e26-04b4-471b-bac3-380e57b4a68e.dat' > '/galaxy/server/database/objects/5/f/1/dataset_5f184ef4-e2d5-4b5d-ab00-4cda39841b7c.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:12:07,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:07,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:07,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:12:07,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:07,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:07,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:07,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:12,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7mth4 with k8s id: gxy-7mth4 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:12:12,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:12:21,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 59 finished
galaxy.model.metadata DEBUG 2025-04-18 13:12:21,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 75
galaxy.jobs INFO 2025-04-18 13:12:21,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-04-18 13:12:21,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 59 executed (91.203 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:21,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:12:23,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-04-18 13:12:23,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:12:23,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:12:23,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:12:23,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:12:23,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-04-18 13:12:23,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (29.886 ms)
galaxy.jobs.handler INFO 2025-04-18 13:12:23,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:23,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-04-18 13:12:23,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [60] prepared (56.972 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:12:23,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/60/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/60/configs/tmpghrkocj8']
galaxy.jobs.runners DEBUG 2025-04-18 13:12:23,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:23,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:23,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:24,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5svdb failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-5svdb.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:12:34,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-5svdb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-5svdb": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60/gxy-5svdb) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60/gxy-5svdb) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60/gxy-5svdb) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60/gxy-5svdb) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-5svdb.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 60 (gxy-5svdb)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-5svdb to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:34,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60/gxy-5svdb) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-18 13:12:35,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-04-18 13:12:35,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:12:35,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:12:35,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:12:35,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:12:35,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-04-18 13:12:35,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (30.671 ms)
galaxy.jobs.handler INFO 2025-04-18 13:12:35,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:35,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-04-18 13:12:35,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [61] prepared (55.372 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:12:35,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/61/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/61/configs/tmph66ycl3d']
galaxy.jobs.runners DEBUG 2025-04-18 13:12:35,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:35,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:35,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:36,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:46,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dj9hf with k8s id: gxy-dj9hf succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:12:46,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 61: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:12:55,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 61 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:12:55,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/61/working/gxupload_0', 'object_id': 77}]}]}]
galaxy.jobs INFO 2025-04-18 13:12:56,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 61 in /galaxy/server/database/jobs_directory/000/61
galaxy.jobs DEBUG 2025-04-18 13:12:56,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 61 executed (343.996 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:56,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:12:57,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-04-18 13:12:57,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:12:57,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:12:57,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:12:57,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:12:57,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-04-18 13:12:57,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (36.522 ms)
galaxy.jobs.handler INFO 2025-04-18 13:12:57,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:57,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-04-18 13:12:57,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [62] prepared (47.536 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:57,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:12:57,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:57,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:12:57,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools hist '/galaxy/server/database/objects/f/c/b/dataset_fcb372b0-d31d-4de4-825c-3399ebce9c1e.dat' --saveas histogram.pdf --min-length 0 --max-length 1000000000 --num-bins 50  && mv histogram.pdf '/galaxy/server/database/objects/0/3/3/dataset_03369c51-7db9-49af-b1ae-aade0da5d61d.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:12:57,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:57,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:57,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:12:57,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-18 13:12:57,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:57,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:12:58,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:13:37,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7jkd7 with k8s id: gxy-7jkd7 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:13:37,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:13:46,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 62 finished
galaxy.model.metadata DEBUG 2025-04-18 13:13:46,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.util WARNING 2025-04-18 13:13:46,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/3/3/dataset_03369c51-7db9-49af-b1ae-aade0da5d61d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/3/3/dataset_03369c51-7db9-49af-b1ae-aade0da5d61d.dat'
galaxy.jobs INFO 2025-04-18 13:13:46,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.jobs DEBUG 2025-04-18 13:13:46,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 62 executed (94.413 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:13:46,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:13:48,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-04-18 13:13:48,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:13:48,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:13:48,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:13:48,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:13:48,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-04-18 13:13:48,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (31.675 ms)
galaxy.jobs.handler INFO 2025-04-18 13:13:48,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:13:48,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-04-18 13:13:48,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [63] prepared (60.979 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:13:48,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/63/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/63/configs/tmp4bfl_5j3']
galaxy.jobs.runners DEBUG 2025-04-18 13:13:48,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:13:48,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:13:48,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:13:49,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:13:59,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d6dt8 with k8s id: gxy-d6dt8 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:13:59,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 63: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:14:08,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 63 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:14:08,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/63/working/gxupload_0', 'object_id': 79}]}]}]
galaxy.jobs INFO 2025-04-18 13:14:09,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 63 in /galaxy/server/database/jobs_directory/000/63
galaxy.jobs DEBUG 2025-04-18 13:14:09,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 63 executed (359.206 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:09,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:14:09,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-04-18 13:14:09,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:14:09,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:14:09,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:14:09,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:14:09,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-04-18 13:14:09,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (33.962 ms)
galaxy.jobs.handler INFO 2025-04-18 13:14:09,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:09,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-04-18 13:14:09,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [64] prepared (42.772 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:14:09,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:14:09,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-18 13:14:09,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:14:09,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools hist '/galaxy/server/database/objects/9/2/3/dataset_9230cf52-5790-49e3-a5a6-85fdcef64b81.dat' --saveas histogram.svg --min-length 0 --max-length 1000000000 --num-bins 50  && mv histogram.svg '/galaxy/server/database/objects/2/3/1/dataset_231a4636-4ea0-4497-a2c9-fca0406a70b3.dat']
galaxy.jobs.runners DEBUG 2025-04-18 13:14:09,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:09,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:14:10,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:14:10,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-18 13:14:10,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:10,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:10,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:16,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rgwmz failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:16,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-rgwmz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:14:17,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-rgwmz

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-rgwmz": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64/gxy-rgwmz) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64/gxy-rgwmz) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64/gxy-rgwmz) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64/gxy-rgwmz) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-rgwmz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 64 (gxy-rgwmz)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-rgwmz to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:17,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64/gxy-rgwmz) Terminated at user's request
galaxy.util WARNING 2025-04-18 13:14:17,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/3/1/dataset_231a4636-4ea0-4497-a2c9-fca0406a70b3.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/3/1/dataset_231a4636-4ea0-4497-a2c9-fca0406a70b3.dat'
galaxy.jobs.handler DEBUG 2025-04-18 13:14:20,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66, 65
tpv.core.entities DEBUG 2025-04-18 13:14:20,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:14:20,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:14:20,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:14:20,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:14:20,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-04-18 13:14:20,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (35.540 ms)
galaxy.jobs.handler INFO 2025-04-18 13:14:20,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:20,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 65
tpv.core.entities DEBUG 2025-04-18 13:14:20,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:14:20,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:14:20,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:14:20,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:14:20,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-04-18 13:14:20,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (40.097 ms)
galaxy.jobs.handler INFO 2025-04-18 13:14:20,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:20,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-04-18 13:14:20,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [65] prepared (70.247 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:14:20,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/65/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/65/configs/tmpdojibjkh']
galaxy.jobs.runners DEBUG 2025-04-18 13:14:20,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:20,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:14:20,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [66] prepared (64.558 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:20,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:14:20,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/66/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/66/configs/tmpjndfxkrq']
galaxy.jobs.runners DEBUG 2025-04-18 13:14:20,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:20,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:20,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:21,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:21,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:31,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z8w6f failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:31,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-z8w6f.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:14:32,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-z8w6f

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-z8w6f": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66/gxy-z8w6f) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66/gxy-z8w6f) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66/gxy-z8w6f) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66/gxy-z8w6f) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-z8w6f.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 66 (gxy-z8w6f)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-z8w6f to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:32,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66/gxy-z8w6f) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:33,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-l7jwm with k8s id: gxy-l7jwm succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:14:33,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:14:42,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 65 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:14:42,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/65/working/data_fetch_upload_gfzatvgg', 'object_id': 81}]}]}]
galaxy.jobs INFO 2025-04-18 13:14:42,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-04-18 13:14:42,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 65 executed (120.816 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:42,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:14:43,627 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67, 68
tpv.core.entities DEBUG 2025-04-18 13:14:43,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:14:43,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:14:43,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:14:43,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:14:43,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-04-18 13:14:43,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (32.082 ms)
galaxy.jobs.handler INFO 2025-04-18 13:14:43,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:43,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 67
tpv.core.entities DEBUG 2025-04-18 13:14:43,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:14:43,705 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:14:43,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:14:43,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:14:43,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-04-18 13:14:43,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (39.604 ms)
galaxy.jobs.handler INFO 2025-04-18 13:14:43,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:43,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-04-18 13:14:43,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [67] prepared (68.051 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:14:43,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/67/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/67/configs/tmpkg3nlh_l']
galaxy.jobs.runners DEBUG 2025-04-18 13:14:43,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:43,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:14:43,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [68] prepared (62.590 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:43,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:14:43,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/68/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/68/configs/tmp4i73wo44']
galaxy.jobs.runners DEBUG 2025-04-18 13:14:43,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:43,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:43,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:44,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:44,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:54,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-btdq6 with k8s id: gxy-btdq6 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:14:54,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-x8xxc with k8s id: gxy-x8xxc succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:14:54,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:14:54,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 68: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:15:04,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 68 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:15:04,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix2.bw', 'dbkey': '?', 'ext': 'bigwig', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bigwig file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/68/working/gxupload_0', 'object_id': 84}]}]}]
galaxy.jobs INFO 2025-04-18 13:15:04,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 68 in /galaxy/server/database/jobs_directory/000/68
galaxy.jobs DEBUG 2025-04-18 13:15:04,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 68 executed (100.312 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:04,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-18 13:15:04,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 67 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:15:04,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix2.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/67/working/data_fetch_upload_r0vzd5w2', 'object_id': 83}]}]}]
galaxy.jobs INFO 2025-04-18 13:15:04,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs DEBUG 2025-04-18 13:15:04,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 67 executed (101.316 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:04,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:15:05,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69
tpv.core.entities DEBUG 2025-04-18 13:15:05,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:15:05,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:15:05,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:15:05,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:15:05,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-04-18 13:15:05,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (33.487 ms)
galaxy.jobs.handler INFO 2025-04-18 13:15:05,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:05,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 69
cheetah_DynamicallyCompiledCheetahTemplate_1744982105_326829_81363.py:93: SyntaxWarning: invalid escape sequence '\.'
cheetah_DynamicallyCompiledCheetahTemplate_1744982105_326829_81363.py:109: SyntaxWarning: invalid escape sequence '\.'
cheetah_DynamicallyCompiledCheetahTemplate_1744982105_326829_81363.py:131: SyntaxWarning: invalid escape sequence '\s'
galaxy.jobs DEBUG 2025-04-18 13:15:05,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [69] prepared (75.969 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:15:05,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:15:05,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:15:05,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:15:05,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [computeMatrix --version > /galaxy/server/database/jobs_directory/000/69/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/b/0/3/dataset_b037226b-cf2c-4ede-9bcc-49b9f355e796.dat' 'computeMatrix2.bw_0.bw' &&    ln -f -s '/galaxy/server/database/objects/d/0/9/dataset_d09ce5d8-c26e-4e7a-9866-ba62338919f0.dat' 'computeMatrix2_bed_0.bed' &&  computeMatrix  reference-point --regionsFileName 'computeMatrix2_bed_0.bed'  --scoreFileName 'computeMatrix2.bw_0.bw'  --outFileName '/galaxy/server/database/objects/c/8/7/dataset_c87694a0-77c9-43f1-9ac7-53c35834e0dd.dat' --samplesLabel 'computeMatrix2.bw'  --numberOfProcessors "${GALAXY_SLOTS:-4}"   --referencePoint TSS  --beforeRegionStartLength 10 --afterRegionStartLength 10  --sortRegions 'keep' --sortUsing 'mean' --averageTypeBins 'mean'   --binSize 10     --transcriptID transcript --exonID exon --transcript_id_designator transcript_id]
galaxy.jobs.runners DEBUG 2025-04-18 13:15:05,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:05,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:15:05,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:15:05,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:15:05,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:05,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:05,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:29,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-69v65 with k8s id: gxy-69v65 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:15:29,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:15:38,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 69 finished
galaxy.model.metadata DEBUG 2025-04-18 13:15:38,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 85
galaxy.util WARNING 2025-04-18 13:15:38,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/8/7/dataset_c87694a0-77c9-43f1-9ac7-53c35834e0dd.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/8/7/dataset_c87694a0-77c9-43f1-9ac7-53c35834e0dd.dat'
galaxy.jobs INFO 2025-04-18 13:15:38,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs DEBUG 2025-04-18 13:15:38,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 69 executed (98.929 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:38,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:15:40,074 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 70, 71
tpv.core.entities DEBUG 2025-04-18 13:15:40,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:15:40,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:15:40,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:15:40,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:15:40,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-04-18 13:15:40,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (34.883 ms)
galaxy.jobs.handler INFO 2025-04-18 13:15:40,151 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:40,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 70
tpv.core.entities DEBUG 2025-04-18 13:15:40,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:15:40,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:15:40,169 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:15:40,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:15:40,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-04-18 13:15:40,210 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (40.718 ms)
galaxy.jobs.handler INFO 2025-04-18 13:15:40,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:40,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-04-18 13:15:40,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [70] prepared (70.704 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:15:40,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/70/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/70/configs/tmpqs73rkep']
galaxy.jobs.runners DEBUG 2025-04-18 13:15:40,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:40,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:15:40,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [71] prepared (64.820 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:40,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:15:40,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/71/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/71/configs/tmpvbzhhrhl']
galaxy.jobs.runners DEBUG 2025-04-18 13:15:40,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:40,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:40,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:41,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:41,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:52,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6jspf with k8s id: gxy-6jspf succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:15:52,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-k57m8 with k8s id: gxy-k57m8 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:15:52,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:15:52,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 71: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:16:02,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 70 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:16:02,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix2.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/70/working/data_fetch_upload_y3brhv24', 'object_id': 86}]}]}]
galaxy.jobs.runners DEBUG 2025-04-18 13:16:02,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 71 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:16:02,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix2.bw', 'dbkey': '?', 'ext': 'bigwig', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bigwig file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/71/working/gxupload_0', 'object_id': 87}]}]}]
galaxy.jobs INFO 2025-04-18 13:16:02,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs INFO 2025-04-18 13:16:02,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 71 in /galaxy/server/database/jobs_directory/000/71
galaxy.jobs DEBUG 2025-04-18 13:16:02,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 70 executed (146.445 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:02,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:16:02,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 71 executed (138.525 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:02,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:16:03,682 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72
tpv.core.entities DEBUG 2025-04-18 13:16:03,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:16:03,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:16:03,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:16:03,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:16:03,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-04-18 13:16:03,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (38.324 ms)
galaxy.jobs.handler INFO 2025-04-18 13:16:03,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:03,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 72
galaxy.jobs DEBUG 2025-04-18 13:16:03,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [72] prepared (58.483 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:03,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:16:03,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:04,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:16:04,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [computeMatrix --version > /galaxy/server/database/jobs_directory/000/72/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/3/a/2/dataset_3a23acb6-ab1e-48dd-9405-93a28badd8fa.dat' 'computeMatrix2.bw_0.bw' &&    ln -f -s '/galaxy/server/database/objects/0/b/5/dataset_0b563d4a-410b-4a91-a799-61eb73b72c73.dat' 'computeMatrix2_bed_0.bed' &&  computeMatrix  scale-regions --regionsFileName 'computeMatrix2_bed_0.bed'  --scoreFileName 'computeMatrix2.bw_0.bw'  --outFileName '/galaxy/server/database/objects/a/9/9/dataset_a99ad9ed-a7f8-49f0-a69b-95668410efe8.dat' --samplesLabel 'computeMatrix2.bw'  --numberOfProcessors "${GALAXY_SLOTS:-4}"   --regionBodyLength 500   --sortRegions 'keep' --sortUsing 'mean' --averageTypeBins 'mean'   --binSize 50     --transcriptID transcript --exonID exon --transcript_id_designator transcript_id]
galaxy.jobs.runners DEBUG 2025-04-18 13:16:04,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:04,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:04,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:16:04,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:04,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:04,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:04,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:09,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b2v2b with k8s id: gxy-b2v2b succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:16:09,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:16:19,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 72 finished
galaxy.model.metadata DEBUG 2025-04-18 13:16:19,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 88
galaxy.util WARNING 2025-04-18 13:16:19,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/9/9/dataset_a99ad9ed-a7f8-49f0-a69b-95668410efe8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/9/9/dataset_a99ad9ed-a7f8-49f0-a69b-95668410efe8.dat'
galaxy.jobs INFO 2025-04-18 13:16:19,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs DEBUG 2025-04-18 13:16:19,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 72 executed (96.016 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:19,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:16:22,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 73
tpv.core.entities DEBUG 2025-04-18 13:16:22,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:16:22,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:16:22,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:16:22,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:16:22,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-04-18 13:16:22,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (33.130 ms)
galaxy.jobs.handler INFO 2025-04-18 13:16:22,208 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:22,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-04-18 13:16:22,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [73] prepared (59.416 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:16:22,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/73/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/73/configs/tmpu88nsyz6']
galaxy.jobs.runners DEBUG 2025-04-18 13:16:22,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:22,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:22,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:22,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:35,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mfbnq with k8s id: gxy-mfbnq succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:16:35,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:16:44,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 73 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:16:44,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/73/working/gxupload_0', 'object_id': 89}]}]}]
galaxy.jobs INFO 2025-04-18 13:16:44,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs DEBUG 2025-04-18 13:16:44,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 73 executed (125.296 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:44,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:16:45,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-04-18 13:16:45,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=14, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:16:45,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:16:45,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:16:45,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:16:45,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-04-18 13:16:45,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (30.125 ms)
galaxy.jobs.handler INFO 2025-04-18 13:16:45,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:45,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-04-18 13:16:45,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [74] prepared (61.894 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:45,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:16:45,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:45,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:16:45,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/74/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/d/7/8/dataset_d7844213-7f1f-4b9c-aa30-41ccf544c76f.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/b/5/d/metadata_b5dfcf00-3222-4bc4-b055-4d84116e305a.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/7/9/1/dataset_791e676a-7777-4042-933c-14c8415d9d66.dat' --outFileFormat 'bigwig'  --binSize 10]
galaxy.jobs.runners DEBUG 2025-04-18 13:16:45,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:45,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:45,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:16:45,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:16:45,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:45,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:46,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:16:52,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lb72z with k8s id: gxy-lb72z succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:16:52,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 74: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:17:01,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 74 finished
galaxy.model.metadata DEBUG 2025-04-18 13:17:01,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 90
galaxy.util WARNING 2025-04-18 13:17:01,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/9/1/dataset_791e676a-7777-4042-933c-14c8415d9d66.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/9/1/dataset_791e676a-7777-4042-933c-14c8415d9d66.dat'
galaxy.jobs INFO 2025-04-18 13:17:01,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 74 in /galaxy/server/database/jobs_directory/000/74
galaxy.jobs DEBUG 2025-04-18 13:17:01,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 74 executed (84.321 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:01,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:17:03,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 75
tpv.core.entities DEBUG 2025-04-18 13:17:03,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:17:03,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:17:03,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:17:03,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:17:03,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-04-18 13:17:03,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (35.890 ms)
galaxy.jobs.handler INFO 2025-04-18 13:17:03,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:03,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 75
galaxy.jobs DEBUG 2025-04-18 13:17:03,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [75] prepared (58.581 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:17:03,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/75/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/75/configs/tmp1pn94ycd']
galaxy.jobs.runners DEBUG 2025-04-18 13:17:03,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:03,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:03,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:04,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:15,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hfdcr with k8s id: gxy-hfdcr succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:17:15,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:17:24,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 75 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:17:24,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/75/working/gxupload_0', 'object_id': 91}]}]}]
galaxy.jobs INFO 2025-04-18 13:17:24,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-04-18 13:17:24,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 75 executed (121.779 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:24,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:17:25,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76
tpv.core.entities DEBUG 2025-04-18 13:17:25,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=14, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:17:25,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:17:25,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:17:25,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:17:25,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-04-18 13:17:25,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (33.919 ms)
galaxy.jobs.handler INFO 2025-04-18 13:17:25,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:25,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-04-18 13:17:25,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [76] prepared (51.895 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:25,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:17:25,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:25,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:17:25,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/76/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/8/c/4/dataset_8c433b7f-5b71-4392-8348-b04c7dcd0154.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/4/f/4/metadata_4f4b8c26-ffd6-40a3-91da-bb82b524e960.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/8/c/1/dataset_8c1b956b-93f5-4167-9ad1-28c55d6ea96a.dat' --outFileFormat 'bigwig'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000]
galaxy.jobs.runners DEBUG 2025-04-18 13:17:25,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:25,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:25,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:17:25,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:25,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:25,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:26,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:31,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fnsj7 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:31,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fnsj7.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:17:32,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-fnsj7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-fnsj7": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (76/gxy-fnsj7) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (76/gxy-fnsj7) tool_stderr: normalization: 1x (effective genome size 2451960000)
bamFilesList: ['one.bam']
binLength: 10
numberOfSamples: None
blackListFileName: None
skipZeroOverZero: False
bed_and_bin: False
genomeChunkSize: None
defaultFragmentLength: read length
numberOfProcessors: 8
verbose: False
region: None
bedFile: None
minMappingQuality: None
ignoreDuplicates: False
chrsToSkip: []
stepSize: 10
center_read: False
samFlag_include: None
samFlag_exclude: None
minFragmentLength: 0
maxFragmentLength: 0
zerosToNans: False
smoothLength: None
save_data: False
out_file_for_raw_data: None
maxPairedFragmentLength: 1000

galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (76/gxy-fnsj7) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (76/gxy-fnsj7) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fnsj7.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 76 (gxy-fnsj7)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-fnsj7 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:32,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (76/gxy-fnsj7) Terminated at user's request
galaxy.util WARNING 2025-04-18 13:17:32,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/c/1/dataset_8c1b956b-93f5-4167-9ad1-28c55d6ea96a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/c/1/dataset_8c1b956b-93f5-4167-9ad1-28c55d6ea96a.dat'
galaxy.jobs.handler DEBUG 2025-04-18 13:17:33,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-04-18 13:17:33,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:17:33,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:17:33,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:17:33,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:17:33,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-04-18 13:17:33,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (31.308 ms)
galaxy.jobs.handler INFO 2025-04-18 13:17:33,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:33,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-04-18 13:17:33,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [77] prepared (54.523 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:17:33,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/77/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/77/configs/tmp09w3e654']
galaxy.jobs.runners DEBUG 2025-04-18 13:17:33,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:33,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:34,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:34,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:45,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rrll9 with k8s id: gxy-rrll9 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:17:45,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:17:54,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 77 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:17:54,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/77/working/gxupload_0', 'object_id': 93}]}]}]
galaxy.jobs INFO 2025-04-18 13:17:54,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-04-18 13:17:55,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 77 executed (125.439 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:55,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:17:56,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 78
tpv.core.entities DEBUG 2025-04-18 13:17:56,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=14, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:17:56,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:17:56,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:17:56,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:17:56,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-04-18 13:17:56,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (32.437 ms)
galaxy.jobs.handler INFO 2025-04-18 13:17:56,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:56,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 78
galaxy.jobs DEBUG 2025-04-18 13:17:56,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [78] prepared (51.057 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:56,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:17:56,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:56,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:17:56,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/78/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/9/9/a/dataset_99a01b5e-10b5-40f8-af73-4ea404f54519.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/8/9/8/metadata_8984df80-214e-49d1-9d0d-6b56c9850013.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/d/3/2/dataset_d32407ff-8b76-481b-9b53-2ce2ad341528.dat' --outFileFormat 'bedgraph'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000]
galaxy.jobs.runners DEBUG 2025-04-18 13:17:56,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:56,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:56,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:17:56,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:17:56,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:56,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:17:57,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:01,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8lpjb failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:01,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:01,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8lpjb.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:01,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:18:02,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-8lpjb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-8lpjb": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (78/gxy-8lpjb) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (78/gxy-8lpjb) tool_stderr: normalization: 1x (effective genome size 2451960000)
bamFilesList: ['one.bam']
binLength: 10
numberOfSamples: None
blackListFileName: None
skipZeroOverZero: False
bed_and_bin: False
genomeChunkSize: None
defaultFragmentLength: read length
numberOfProcessors: 8
verbose: False
region: None
bedFile: None
minMappingQuality: None
ignoreDuplicates: False
chrsToSkip: []
stepSize: 10
center_read: False
samFlag_include: None
samFlag_exclude: None
minFragmentLength: 0
maxFragmentLength: 0
zerosToNans: False
smoothLength: None
save_data: False
out_file_for_raw_data: None
maxPairedFragmentLength: 1000

galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (78/gxy-8lpjb) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (78/gxy-8lpjb) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8lpjb.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 78 (gxy-8lpjb)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-8lpjb to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:02,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (78/gxy-8lpjb) Terminated at user's request
galaxy.util WARNING 2025-04-18 13:18:02,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/3/2/dataset_d32407ff-8b76-481b-9b53-2ce2ad341528.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/3/2/dataset_d32407ff-8b76-481b-9b53-2ce2ad341528.dat'
galaxy.jobs.handler DEBUG 2025-04-18 13:18:03,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79
tpv.core.entities DEBUG 2025-04-18 13:18:03,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:18:03,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:18:03,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:18:03,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:18:03,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-04-18 13:18:03,548 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (31.480 ms)
galaxy.jobs.handler INFO 2025-04-18 13:18:03,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:03,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-04-18 13:18:03,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [79] prepared (58.812 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:18:03,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/79/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/79/configs/tmp9ulb765g']
galaxy.jobs.runners DEBUG 2025-04-18 13:18:03,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:03,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:03,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:04,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:15,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kg7kr with k8s id: gxy-kg7kr succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:18:15,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:18:24,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 79 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:18:24,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'phiX.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/79/working/gxupload_0', 'object_id': 95}]}]}]
galaxy.jobs INFO 2025-04-18 13:18:24,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-04-18 13:18:24,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 79 executed (105.806 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:24,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:18:26,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 80
tpv.core.entities DEBUG 2025-04-18 13:18:26,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=14, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:18:26,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:18:26,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:18:26,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:18:26,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-04-18 13:18:26,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (33.020 ms)
galaxy.jobs.handler INFO 2025-04-18 13:18:26,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:26,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 80
galaxy.jobs DEBUG 2025-04-18 13:18:26,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [80] prepared (55.436 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:18:26,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:18:26,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:18:26,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:18:26,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/80/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/3/0/2/dataset_3024c805-06db-4a2d-8ff7-85775a028ec7.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/f/1/0/metadata_f102cd7c-dde9-48a7-ae82-ca0ec809aa61.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/a/8/e/dataset_a8e94d98-e3dd-4458-91c2-44e30924ca5b.dat' --outFileFormat 'bigwig'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000]
galaxy.jobs.runners DEBUG 2025-04-18 13:18:26,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:26,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:18:26,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:18:26,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:18:26,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:26,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:26,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:30,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r8x6r with k8s id: gxy-r8x6r succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:18:30,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:18:39,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 80 finished
galaxy.model.metadata DEBUG 2025-04-18 13:18:39,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 96
galaxy.util WARNING 2025-04-18 13:18:39,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/8/e/dataset_a8e94d98-e3dd-4458-91c2-44e30924ca5b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/8/e/dataset_a8e94d98-e3dd-4458-91c2-44e30924ca5b.dat'
galaxy.jobs INFO 2025-04-18 13:18:39,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-04-18 13:18:39,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 80 executed (92.779 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:39,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:18:41,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81
tpv.core.entities DEBUG 2025-04-18 13:18:41,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:18:41,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:18:41,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:18:41,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:18:41,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-04-18 13:18:41,427 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (31.456 ms)
galaxy.jobs.handler INFO 2025-04-18 13:18:41,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:41,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-04-18 13:18:41,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [81] prepared (61.753 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:18:41,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/81/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/81/configs/tmpxtld3tng']
galaxy.jobs.runners DEBUG 2025-04-18 13:18:41,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:41,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:41,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:42,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:18:53,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xxcpz with k8s id: gxy-xxcpz succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:18:53,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:19:02,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 81 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:19:03,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'phiX.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/81/working/gxupload_0', 'object_id': 97}]}]}]
galaxy.jobs INFO 2025-04-18 13:19:03,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-04-18 13:19:03,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 81 executed (131.683 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:03,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:19:03,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-04-18 13:19:03,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=14, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:19:03,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:19:03,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:19:03,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:19:03,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-04-18 13:19:03,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (31.512 ms)
galaxy.jobs.handler INFO 2025-04-18 13:19:03,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:03,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-04-18 13:19:03,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [82] prepared (53.293 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:03,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:19:03,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:04,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:19:04,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/82/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/a/4/f/dataset_a4f19f88-f0ed-419b-9a4c-cfd8a787af13.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/e/1/2/metadata_e12ac71e-9aa3-49a2-a8d4-99fd86bbbf68.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/0/6/8/dataset_06882d8e-dd42-4948-ba25-e940cdefa052.dat' --outFileFormat 'bedgraph'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000    --scaleFactor 1.0      --minMappingQuality '1']
galaxy.jobs.runners DEBUG 2025-04-18 13:19:04,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:04,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:04,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:19:04,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:04,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:04,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:04,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:08,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fvcf5 with k8s id: gxy-fvcf5 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:19:08,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 82: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:19:18,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 82 finished
galaxy.model.metadata DEBUG 2025-04-18 13:19:18,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 98
galaxy.util WARNING 2025-04-18 13:19:18,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/6/8/dataset_06882d8e-dd42-4948-ba25-e940cdefa052.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/6/8/dataset_06882d8e-dd42-4948-ba25-e940cdefa052.dat'
galaxy.jobs INFO 2025-04-18 13:19:18,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 82 in /galaxy/server/database/jobs_directory/000/82
galaxy.jobs DEBUG 2025-04-18 13:19:18,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 82 executed (95.866 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:18,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:19:20,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83
tpv.core.entities DEBUG 2025-04-18 13:19:20,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:19:20,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:19:20,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:19:20,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:19:20,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-04-18 13:19:20,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (33.366 ms)
galaxy.jobs.handler INFO 2025-04-18 13:19:20,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:20,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 83
galaxy.jobs DEBUG 2025-04-18 13:19:20,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [83] prepared (58.615 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:19:20,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/83/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/83/configs/tmphx3z07qw']
galaxy.jobs.runners DEBUG 2025-04-18 13:19:20,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:20,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:20,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:20,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fbkc8 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fbkc8.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:19:32,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-fbkc8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-fbkc8": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (83/gxy-fbkc8) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (83/gxy-fbkc8) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (83/gxy-fbkc8) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (83/gxy-fbkc8) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fbkc8.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 83 (gxy-fbkc8)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-fbkc8 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:32,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (83/gxy-fbkc8) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-18 13:19:33,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 84
tpv.core.entities DEBUG 2025-04-18 13:19:33,621 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:19:33,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:19:33,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:19:33,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:19:33,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-04-18 13:19:33,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (30.804 ms)
galaxy.jobs.handler INFO 2025-04-18 13:19:33,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:33,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-04-18 13:19:33,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [84] prepared (86.556 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:19:33,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/84/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/84/configs/tmp9z212ann']
galaxy.jobs.runners DEBUG 2025-04-18 13:19:33,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:33,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:33,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:34,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:45,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8llw9 with k8s id: gxy-8llw9 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:19:45,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:19:54,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 84 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:19:54,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/84/working/gxupload_0', 'object_id': 100}]}]}]
galaxy.jobs INFO 2025-04-18 13:19:54,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-04-18 13:19:55,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 84 executed (121.287 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:55,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:19:55,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-04-18 13:19:55,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=14, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:19:55,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:19:55,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:19:55,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:19:55,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-04-18 13:19:55,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (35.613 ms)
galaxy.jobs.handler INFO 2025-04-18 13:19:55,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:55,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-04-18 13:19:55,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [85] prepared (48.510 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:55,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:19:55,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:55,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:19:55,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/85/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/a/6/f/dataset_a6f09c35-eb32-4e87-ba01-82f03499ec31.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/6/c/9/metadata_6c93c9b7-c9a3-4bb9-96a7-c299ab520b09.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/1/a/5/dataset_1a506c32-7de0-4961-bc26-1e49cbff09ed.dat' --outFileFormat 'bigwig'  --binSize 10     --scaleFactor 1.0    --extendReads        --Offset -4 -1]
galaxy.jobs.runners DEBUG 2025-04-18 13:19:55,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:55,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:55,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:19:55,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-18 13:19:55,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:55,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:19:55,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:00,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tlqjq with k8s id: gxy-tlqjq succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:20:00,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:20:09,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 85 finished
galaxy.model.metadata DEBUG 2025-04-18 13:20:10,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 101
galaxy.util WARNING 2025-04-18 13:20:10,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/a/5/dataset_1a506c32-7de0-4961-bc26-1e49cbff09ed.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/a/5/dataset_1a506c32-7de0-4961-bc26-1e49cbff09ed.dat'
galaxy.jobs INFO 2025-04-18 13:20:10,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-04-18 13:20:10,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 85 executed (98.844 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:10,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:20:12,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 86
tpv.core.entities DEBUG 2025-04-18 13:20:12,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:20:12,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:20:12,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:20:12,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:20:12,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-04-18 13:20:12,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (37.241 ms)
galaxy.jobs.handler INFO 2025-04-18 13:20:12,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:12,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 86
galaxy.jobs DEBUG 2025-04-18 13:20:12,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [86] prepared (63.699 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:20:12,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/86/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/86/configs/tmpsz4ak5bn']
galaxy.jobs.runners DEBUG 2025-04-18 13:20:12,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:12,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:12,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:13,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:24,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4blss with k8s id: gxy-4blss succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:20:25,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:20:34,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 86 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:20:34,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/86/working/gxupload_0', 'object_id': 102}]}]}]
galaxy.jobs INFO 2025-04-18 13:20:34,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.jobs DEBUG 2025-04-18 13:20:34,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 86 executed (129.755 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:34,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:20:35,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87
tpv.core.entities DEBUG 2025-04-18 13:20:35,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:20:35,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:20:35,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:20:35,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:20:35,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-04-18 13:20:35,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (33.392 ms)
galaxy.jobs.handler INFO 2025-04-18 13:20:35,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:35,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-04-18 13:20:35,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [87] prepared (50.400 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:20:35,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:20:35,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-18 13:20:35,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:20:35,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/87/outputs/COMMAND_VERSION 2>&1;
lofreq indelqual --uniform 20,30 -o output.bam /galaxy/server/database/objects/a/c/0/dataset_ac01f931-98d3-4df7-ac8e-dee9c36b585f.dat]
galaxy.jobs.runners DEBUG 2025-04-18 13:20:35,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/87/working/output.bam" -a -f "/galaxy/server/database/objects/f/4/d/dataset_f4d1c59b-76b9-4d06-a547-73596b6aba67.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/87/working/output.bam" "/galaxy/server/database/objects/f/4/d/dataset_f4d1c59b-76b9-4d06-a547-73596b6aba67.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:35,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:20:35,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:20:35,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-18 13:20:35,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:35,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:36,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:20:51,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7fhnh with k8s id: gxy-7fhnh succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:20:51,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:21:00,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 87 finished
galaxy.model.metadata DEBUG 2025-04-18 13:21:00,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 103
galaxy.util WARNING 2025-04-18 13:21:00,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/4/d/dataset_f4d1c59b-76b9-4d06-a547-73596b6aba67.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/4/d/dataset_f4d1c59b-76b9-4d06-a547-73596b6aba67.dat'
galaxy.jobs INFO 2025-04-18 13:21:00,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs DEBUG 2025-04-18 13:21:00,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 87 executed (101.115 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:00,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:21:01,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-04-18 13:21:01,701 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:21:01,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:21:01,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:21:01,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:21:01,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-04-18 13:21:01,740 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (32.763 ms)
galaxy.jobs.handler INFO 2025-04-18 13:21:01,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:01,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-04-18 13:21:01,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [88] prepared (53.936 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:21:01,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/88/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/88/configs/tmp_hn2uz74']
galaxy.jobs.runners DEBUG 2025-04-18 13:21:01,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:01,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:01,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:02,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-04-18 13:21:02,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 89
tpv.core.entities DEBUG 2025-04-18 13:21:02,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:21:02,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:21:02,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:21:02,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:21:02,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-04-18 13:21:02,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (36.699 ms)
galaxy.jobs.handler INFO 2025-04-18 13:21:02,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:02,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 89
galaxy.jobs DEBUG 2025-04-18 13:21:02,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [89] prepared (63.485 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:21:02,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/89/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/89/configs/tmpoi7vu4fq']
galaxy.jobs.runners DEBUG 2025-04-18 13:21:02,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:02,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:02,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:03,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:14,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9t5w6 with k8s id: gxy-9t5w6 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:14,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sfkbk with k8s id: gxy-sfkbk succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:21:14,804 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:21:14,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:21:24,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 89 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:21:24,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pBR322.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/89/working/data_fetch_upload_qgw125mo', 'object_id': 105}]}]}]
galaxy.jobs INFO 2025-04-18 13:21:24,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-04-18 13:21:24,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 89 executed (127.514 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:24,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-18 13:21:24,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 88 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:21:24,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/88/working/gxupload_0', 'object_id': 104}]}]}]
galaxy.jobs INFO 2025-04-18 13:21:24,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-04-18 13:21:24,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 88 executed (116.091 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:24,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:21:25,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90
tpv.core.entities DEBUG 2025-04-18 13:21:25,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:21:25,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:21:25,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:21:25,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:21:25,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-04-18 13:21:25,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (36.370 ms)
galaxy.jobs.handler INFO 2025-04-18 13:21:25,554 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:25,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-04-18 13:21:25,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [90] prepared (50.794 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:21:25,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:21:25,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-18 13:21:25,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:21:25,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/90/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/6/e/2/dataset_6e27c1b9-61d2-4eac-a280-56898e29ee46.dat' reference.fa && lofreq faidx reference.fa 2>&1 || echo "Error running samtools faidx for indexing fasta reference for lofreq" >&2 &&  lofreq indelqual --dindel --ref reference.fa -o output.bam /galaxy/server/database/objects/c/7/4/dataset_c746b54c-0b33-43db-bdff-10806bb763bd.dat]
galaxy.jobs.runners DEBUG 2025-04-18 13:21:25,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/90/working/output.bam" -a -f "/galaxy/server/database/objects/5/7/d/dataset_57d6852b-5e25-4668-8e3e-dc9759230f24.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/90/working/output.bam" "/galaxy/server/database/objects/5/7/d/dataset_57d6852b-5e25-4668-8e3e-dc9759230f24.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:25,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:21:25,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:21:25,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-18 13:21:25,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:26,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:26,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:30,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4vzr7 with k8s id: gxy-4vzr7 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:21:30,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:21:39,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 90 finished
galaxy.model.metadata DEBUG 2025-04-18 13:21:39,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 106
galaxy.util WARNING 2025-04-18 13:21:39,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/7/d/dataset_57d6852b-5e25-4668-8e3e-dc9759230f24.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/7/d/dataset_57d6852b-5e25-4668-8e3e-dc9759230f24.dat'
galaxy.jobs INFO 2025-04-18 13:21:39,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-04-18 13:21:40,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 90 executed (112.011 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:40,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:21:41,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-04-18 13:21:41,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:21:41,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:21:41,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:21:41,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:21:41,925 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-04-18 13:21:41,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (33.496 ms)
galaxy.jobs.handler INFO 2025-04-18 13:21:41,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:41,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-04-18 13:21:42,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [91] prepared (65.353 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:21:42,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/91/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/91/configs/tmpipzz2_1s']
galaxy.jobs.runners DEBUG 2025-04-18 13:21:42,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:42,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:42,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:42,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:21:54,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pgs6g with k8s id: gxy-pgs6g succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:21:54,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:22:03,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 91 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:22:03,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/91/working/gxupload_0', 'object_id': 107}]}]}]
galaxy.jobs INFO 2025-04-18 13:22:03,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-04-18 13:22:03,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 91 executed (134.333 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:03,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:22:04,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92
tpv.core.entities DEBUG 2025-04-18 13:22:04,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:22:04,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:22:04,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:22:04,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:22:04,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-04-18 13:22:04,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (32.429 ms)
galaxy.jobs.handler INFO 2025-04-18 13:22:04,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:04,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 92
galaxy.jobs DEBUG 2025-04-18 13:22:04,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [92] prepared (45.480 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:04,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:22:04,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:04,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:22:04,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/92/outputs/COMMAND_VERSION 2>&1;
lofreq indelqual --uniform 20 -o output.bam /galaxy/server/database/objects/a/6/b/dataset_a6bbbdf6-3bb4-4b18-9522-f25a1437e767.dat]
galaxy.jobs.runners DEBUG 2025-04-18 13:22:04,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/92/working/output.bam" -a -f "/galaxy/server/database/objects/d/e/b/dataset_debf2487-a727-4efe-b798-4a618351e0f3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/92/working/output.bam" "/galaxy/server/database/objects/d/e/b/dataset_debf2487-a727-4efe-b798-4a618351e0f3.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:04,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:04,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:22:04,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:04,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:04,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:05,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:09,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-g4tg4 with k8s id: gxy-g4tg4 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:22:09,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:22:18,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 92 finished
galaxy.model.metadata DEBUG 2025-04-18 13:22:18,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 108
galaxy.util WARNING 2025-04-18 13:22:18,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/e/b/dataset_debf2487-a727-4efe-b798-4a618351e0f3.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/e/b/dataset_debf2487-a727-4efe-b798-4a618351e0f3.dat'
galaxy.jobs INFO 2025-04-18 13:22:18,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs DEBUG 2025-04-18 13:22:18,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 92 executed (101.269 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:18,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:22:22,925 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94, 93
tpv.core.entities DEBUG 2025-04-18 13:22:22,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:22:22,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:22:22,966 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:22:22,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:22:22,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-04-18 13:22:23,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (39.593 ms)
galaxy.jobs.handler INFO 2025-04-18 13:22:23,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:23,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 93
tpv.core.entities DEBUG 2025-04-18 13:22:23,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:22:23,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:22:23,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:22:23,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:22:23,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-04-18 13:22:23,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (42.447 ms)
galaxy.jobs.handler INFO 2025-04-18 13:22:23,070 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:23,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-04-18 13:22:23,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [93] prepared (73.627 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:22:23,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/93/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/93/configs/tmpirty0x5o']
galaxy.jobs.runners DEBUG 2025-04-18 13:22:23,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:23,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:22:23,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [94] prepared (71.164 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:23,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:22:23,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/94/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/94/configs/tmpf9k42ktf']
galaxy.jobs.runners DEBUG 2025-04-18 13:22:23,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:23,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:23,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:24,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:24,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:34,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7ntnk with k8s id: gxy-7ntnk succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:35,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b5stp with k8s id: gxy-b5stp succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:22:35,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:22:35,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:22:44,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 93 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:22:44,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'human_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/93/working/data_fetch_upload_nbsw889c', 'object_id': 109}]}]}]
galaxy.jobs INFO 2025-04-18 13:22:44,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-04-18 13:22:44,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 94 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:22:44,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mouse_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/94/working/data_fetch_upload_55zcflbb', 'object_id': 110}]}]}]
galaxy.jobs DEBUG 2025-04-18 13:22:44,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 93 executed (125.940 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:44,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-04-18 13:22:44,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.jobs DEBUG 2025-04-18 13:22:44,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 94 executed (116.050 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:44,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:22:45,570 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95
tpv.core.entities DEBUG 2025-04-18 13:22:45,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/.*, abstract=False, cores=5, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:22:45,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:22:45,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:22:45,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:22:45,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-04-18 13:22:45,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (36.309 ms)
galaxy.jobs.handler INFO 2025-04-18 13:22:45,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:45,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 95
galaxy.jobs DEBUG 2025-04-18 13:22:45,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [95] prepared (65.130 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:45,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:22:45,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:45,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:22:45,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [ln -s /galaxy/server/database/objects/b/2/8/dataset_b2866f52-4b0e-46a0-ab03-eaff1120d29e.dat reference.fa && ln -s /galaxy/server/database/objects/d/0/5/dataset_d052da54-669d-49c6-a811-17bf76f5b5bc.dat query.fa && nucmer  -b '200' -c '65' -D '5' -d '0.12'   -g '90' -l '20' -L '0'   --threads "${GALAXY_SLOTS:-1}" 'reference.fa' 'query.fa' && mummerplot -R '/galaxy/server/database/objects/b/2/8/dataset_b2866f52-4b0e-46a0-ab03-eaff1120d29e.dat' -Q '/galaxy/server/database/objects/d/0/5/dataset_d052da54-669d-49c6-a811-17bf76f5b5bc.dat'  -b '20'     -s 'small' -terminal png -title 'Title'  'out.delta' && gnuplot < out.gp]
galaxy.jobs.runners DEBUG 2025-04-18 13:22:45,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/95/working/out.delta" -a -f "/galaxy/server/database/objects/4/c/a/dataset_4ca95da1-5e2e-4ae8-bae5-6655389be385.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/95/working/out.delta" "/galaxy/server/database/objects/4/c/a/dataset_4ca95da1-5e2e-4ae8-bae5-6655389be385.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/95/working/out.png" -a -f "/galaxy/server/database/objects/4/f/5/dataset_4f5a160a-be8b-4908-bd67-b7716e565ffe.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/95/working/out.png" "/galaxy/server/database/objects/4/f/5/dataset_4f5a160a-be8b-4908-bd67-b7716e565ffe.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:45,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:45,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:22:45,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-04-18 13:22:45,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:45,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:22:47,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:18,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dwcbz with k8s id: gxy-dwcbz succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:23:18,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:23:28,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 95 finished
galaxy.model.metadata DEBUG 2025-04-18 13:23:28,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 111
galaxy.model.metadata DEBUG 2025-04-18 13:23:28,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 112
galaxy.jobs INFO 2025-04-18 13:23:28,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-04-18 13:23:28,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 95 executed (105.605 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:28,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:23:30,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97, 96
tpv.core.entities DEBUG 2025-04-18 13:23:30,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:23:30,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:23:30,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:23:30,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:23:30,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-04-18 13:23:30,660 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (33.536 ms)
galaxy.jobs.handler INFO 2025-04-18 13:23:30,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:30,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 96
tpv.core.entities DEBUG 2025-04-18 13:23:30,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:23:30,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:23:30,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:23:30,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:23:30,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-04-18 13:23:30,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (45.480 ms)
galaxy.jobs.handler INFO 2025-04-18 13:23:30,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:30,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-04-18 13:23:30,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [96] prepared (80.862 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:23:30,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/96/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/96/configs/tmp8c2z95l2']
galaxy.jobs.runners DEBUG 2025-04-18 13:23:30,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:30,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:23:30,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [97] prepared (108.749 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:30,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:23:30,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/97/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/97/configs/tmpkczhengy']
galaxy.jobs.runners DEBUG 2025-04-18 13:23:30,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:30,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:30,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:32,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:32,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:43,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v2k94 with k8s id: gxy-v2k94 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:43,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h87xs with k8s id: gxy-h87xs succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:23:43,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:23:43,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:23:53,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 97 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:23:53,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mouse_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/97/working/data_fetch_upload_j_bu_2p9', 'object_id': 114}]}]}]
galaxy.jobs.runners DEBUG 2025-04-18 13:23:53,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 96 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:23:53,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'human_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/96/working/data_fetch_upload_jld6wheg', 'object_id': 113}]}]}]
galaxy.jobs INFO 2025-04-18 13:23:53,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs INFO 2025-04-18 13:23:53,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs DEBUG 2025-04-18 13:23:53,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 97 executed (130.436 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:53,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:23:53,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 96 executed (117.106 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:53,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:23:54,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98
tpv.core.entities DEBUG 2025-04-18 13:23:54,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/.*, abstract=False, cores=5, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:23:54,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:23:54,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:23:54,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:23:54,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-04-18 13:23:54,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (35.860 ms)
galaxy.jobs.handler INFO 2025-04-18 13:23:54,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:54,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 98
galaxy.jobs DEBUG 2025-04-18 13:23:54,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [98] prepared (46.668 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:23:54,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:23:54,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-04-18 13:23:54,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:23:54,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [ln -s /galaxy/server/database/objects/f/b/2/dataset_fb2dfa60-ba34-4f25-a70b-dc9375e0c889.dat reference.fa && ln -s /galaxy/server/database/objects/6/4/e/dataset_64e3695b-0c77-434c-86a8-59dc557f541c.dat query.fa && nucmer  --sam-long=outsam.sam -b '200' -c '65' -D '5' -d '0.12'   -g '90' -l '20' -L '0'   --threads "${GALAXY_SLOTS:-1}" 'reference.fa' 'query.fa' && samtools dict reference.fa > outsamhead && tail -n +3 outsam.sam >> outsamhead && samtools sort  -@ ${GALAXY_SLOTS:-1} -T "${TMPDIR:-.}" outsamhead | samtools calmd -b --threads {GALAXY_SLOTS:-1} - reference.fa > outsam]
galaxy.jobs.runners DEBUG 2025-04-18 13:23:54,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/98/working/outsam" -a -f "/galaxy/server/database/objects/3/2/e/dataset_32e9987b-b843-4e54-bd31-792135b7b4ef.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/98/working/outsam" "/galaxy/server/database/objects/3/2/e/dataset_32e9987b-b843-4e54-bd31-792135b7b4ef.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:54,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:23:54,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:23:54,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-04-18 13:23:54,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:54,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:54,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j7zwb failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-j7zwb.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-18 13:23:58,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-18-12-37-1/jobs/gxy-j7zwb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-j7zwb": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (98/gxy-j7zwb) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (98/gxy-j7zwb) tool_stderr: [bam_fillmd1] different MD for read 'NC_000070.6:c41098183-41092724': '5c3a0g6c30c2g2t0c0c2c2c11c20c2a11c23g10g77' -> '5C3A0G6C30C2G2T0C0C2C2C11C20C2A11C23G10G4'
[bam_fillmd1] different MD for read 'NC_000070.6:c41098183-41092724': '7c0a0g6c2t4c2a23t7c8g8t8c2c8c2c5a11c5c26c17c2c23g8c2c5t2t2a5t2g11t1c19c5^cccag2^ca7c6t5^g0c6c0c2^a4c4c1a0t0g0a4^c1g4c7c4a3t11g5c2g14c5t2g20g5c2c28a27t0g4g5105' -> '7C0A0G6C2T4C2A23T7C8G8T8C2C8C2C5A11C5C26C17C2C23G8C2C5T2T2A5T2G11T1C19C5^CCCAG2^CA7C6T5^G0C6C0C2^A4C4C1A0T0G0A4^C1G4C7C4A3T11G5C2G14C5T2G20G5C2C28A27T0G4G26'

galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (98/gxy-j7zwb) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (98/gxy-j7zwb) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-j7zwb.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 98 (gxy-j7zwb)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-j7zwb to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:58,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:23:59,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (98/gxy-j7zwb) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-18 13:24:00,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100, 99
tpv.core.entities DEBUG 2025-04-18 13:24:00,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:24:00,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:24:00,580 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:24:00,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:24:00,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-04-18 13:24:00,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (32.592 ms)
galaxy.jobs.handler INFO 2025-04-18 13:24:00,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:00,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 99
tpv.core.entities DEBUG 2025-04-18 13:24:00,627 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:24:00,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:24:00,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:24:00,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:24:00,670 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-04-18 13:24:00,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (45.883 ms)
galaxy.jobs.handler INFO 2025-04-18 13:24:00,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:00,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-04-18 13:24:00,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [99] prepared (90.262 ms)
galaxy.jobs.command_factory INFO 2025-04-18 13:24:00,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/99/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/99/configs/tmp13isfbsj']
galaxy.jobs.runners DEBUG 2025-04-18 13:24:00,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:00,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-18 13:24:00,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [100] prepared (69.455 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:00,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-18 13:24:00,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/100/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/100/configs/tmpfdohi67p']
galaxy.jobs.runners DEBUG 2025-04-18 13:24:00,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:00,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:00,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:01,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:01,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:12,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-57grz with k8s id: gxy-57grz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:12,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6fht4 with k8s id: gxy-6fht4 succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:24:12,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:24:12,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:24:22,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 99 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:24:22,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'human_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/99/working/data_fetch_upload_2m8cnbtm', 'object_id': 116}]}]}]
galaxy.jobs INFO 2025-04-18 13:24:22,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-04-18 13:24:22,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 100 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-18 13:24:22,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mouse_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/100/working/data_fetch_upload_o0c2m586', 'object_id': 117}]}]}]
galaxy.jobs DEBUG 2025-04-18 13:24:22,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 99 executed (135.201 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:22,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-04-18 13:24:22,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-04-18 13:24:22,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 100 executed (116.828 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:22,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-18 13:24:23,233 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 101
tpv.core.entities DEBUG 2025-04-18 13:24:23,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/.*, abstract=False, cores=5, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-18 13:24:23,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-18 13:24:23,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-18 13:24:23,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-18 13:24:23,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-04-18 13:24:23,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (34.528 ms)
galaxy.jobs.handler INFO 2025-04-18 13:24:23,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:23,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 101
galaxy.jobs DEBUG 2025-04-18 13:24:23,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [101] prepared (49.023 ms)
galaxy.tool_util.deps.containers INFO 2025-04-18 13:24:23,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:24:23,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-04-18 13:24:23,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-18 13:24:23,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [ln -s /galaxy/server/database/objects/0/e/a/dataset_0eaa03f7-4aa3-47e3-8093-fd772fe10bce.dat reference.fa && ln -s /galaxy/server/database/objects/0/1/6/dataset_016499c7-307e-4185-b360-9bdbf0ed6a88.dat query.fa && nucmer  --sam-long=outsam.sam -b '200' -c '65' -D '5' -d '0.12'   -g '90' -l '20' -L '0'   --threads "${GALAXY_SLOTS:-1}" 'reference.fa' 'query.fa' && samtools dict reference.fa > outsamhead && tail -n +3 outsam.sam >> outsamhead && samtools sort  -@ ${GALAXY_SLOTS:-1} -T "${TMPDIR:-.}" outsamhead | samtools view -C --reference reference.fa -o outsam -]
galaxy.jobs.runners DEBUG 2025-04-18 13:24:23,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/101/working/outsam" -a -f "/galaxy/server/database/objects/4/8/e/dataset_48ebb5f3-6fe4-49f6-bbcc-54e7e491f017.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/101/working/outsam" "/galaxy/server/database/objects/4/8/e/dataset_48ebb5f3-6fe4-49f6-bbcc-54e7e491f017.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:23,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-18 13:24:23,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-18 13:24:23,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-04-18 13:24:23,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:23,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:24,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:27,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f45pp with k8s id: gxy-f45pp succeeded
galaxy.jobs.runners DEBUG 2025-04-18 13:24:27,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-18 13:24:36,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 101 finished
galaxy.model.metadata DEBUG 2025-04-18 13:24:36,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 118
galaxy.jobs INFO 2025-04-18 13:24:36,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-04-18 13:24:37,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 101 executed (108.422 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-18 13:24:37,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
