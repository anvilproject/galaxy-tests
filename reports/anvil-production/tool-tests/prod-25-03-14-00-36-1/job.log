galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:10,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sprkg with k8s id: gxy-sprkg succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:55:10,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 4: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:55:17,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 4 finished
galaxy.model.metadata DEBUG 2025-03-14 00:55:17,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 6
galaxy.model.metadata DEBUG 2025-03-14 00:55:17,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 5
galaxy.model.metadata DEBUG 2025-03-14 00:55:17,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 4
galaxy.util WARNING 2025-03-14 00:55:17,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/9/9/dataset_6992b4d0-3a35-45b2-b3f5-d4067dac18d8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/9/9/dataset_6992b4d0-3a35-45b2-b3f5-d4067dac18d8.dat'
galaxy.util WARNING 2025-03-14 00:55:17,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/8/6/dataset_d86633bb-022b-4c74-ae6d-92ef3d40c153.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/8/6/dataset_d86633bb-022b-4c74-ae6d-92ef3d40c153.dat'
galaxy.util WARNING 2025-03-14 00:55:17,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/e/4/dataset_3e4d38da-03dc-4ee5-b8b5-09d1b3dd81c7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/e/4/dataset_3e4d38da-03dc-4ee5-b8b5-09d1b3dd81c7.dat'
galaxy.jobs INFO 2025-03-14 00:55:17,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 4 in /galaxy/server/database/jobs_directory/000/4
galaxy.jobs DEBUG 2025-03-14 00:55:17,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 4 executed (131.074 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:17,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 4 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:55:19,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 6, 5
tpv.core.entities DEBUG 2025-03-14 00:55:19,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:55:19,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:55:19,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:55:19,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:55:19,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Working directory for job is: /galaxy/server/database/jobs_directory/000/5
galaxy.jobs.runners DEBUG 2025-03-14 00:55:19,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [5] queued (37.609 ms)
galaxy.jobs.handler INFO 2025-03-14 00:55:19,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:19,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 5
tpv.core.entities DEBUG 2025-03-14 00:55:19,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:55:19,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:55:19,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:55:19,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:55:19,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Working directory for job is: /galaxy/server/database/jobs_directory/000/6
galaxy.jobs.runners DEBUG 2025-03-14 00:55:20,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [6] queued (44.981 ms)
galaxy.jobs.handler INFO 2025-03-14 00:55:20,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:20,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 6
galaxy.jobs DEBUG 2025-03-14 00:55:20,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [5] prepared (84.055 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:55:20,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/5/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/5/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/5/configs/tmpzk2f24aq']
galaxy.jobs.runners DEBUG 2025-03-14 00:55:20,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (5) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/5/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/5/galaxy_5.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:20,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 00:55:20,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [6] prepared (80.131 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:20,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 00:55:20,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/6/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/6/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/6/configs/tmpxvlrxgs9']
galaxy.jobs.runners DEBUG 2025-03-14 00:55:20,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (6) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/6/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/6/galaxy_6.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:20,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:20,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:20,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:20,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:30,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-g6x2g with k8s id: gxy-g6x2g succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:30,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8s8mp with k8s id: gxy-8s8mp succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:55:30,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 5: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:55:30,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 6: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:55:37,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 5 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:55:37,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/5/working/data_fetch_upload_c5rf8rzb', 'object_id': 7}]}]}]
galaxy.jobs INFO 2025-03-14 00:55:37,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 5 in /galaxy/server/database/jobs_directory/000/5
galaxy.jobs DEBUG 2025-03-14 00:55:37,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 5 executed (131.135 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:37,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 00:55:37,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 6 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:55:37,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/6/working/data_fetch_upload__8zebjq_', 'object_id': 8}]}]}]
galaxy.jobs INFO 2025-03-14 00:55:37,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 6 in /galaxy/server/database/jobs_directory/000/6
galaxy.jobs DEBUG 2025-03-14 00:55:38,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 6 executed (125.246 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:38,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:55:38,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 7
tpv.core.entities DEBUG 2025-03-14 00:55:38,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:55:38,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:55:38,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:55:38,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:55:38,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Working directory for job is: /galaxy/server/database/jobs_directory/000/7
galaxy.jobs.runners DEBUG 2025-03-14 00:55:38,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [7] queued (36.496 ms)
galaxy.jobs.handler INFO 2025-03-14 00:55:38,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:38,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 7
galaxy.jobs DEBUG 2025-03-14 00:55:38,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [7] prepared (67.024 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:55:38,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:55:38,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:55:38,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:55:38,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/7/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/7/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/2/b/4/dataset_2b4ca4ce-15a4-435a-a762-720c7c510933.dat' 'Unnamed Collection.fastqsanger' && ln -sf '/galaxy/server/database/objects/d/b/c/dataset_dbc23e31-3e50-4ac3-b6e9-d0080874b49e.dat' 'Unnamed Collection_R2.fastqsanger' &&   fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for Unnamed Collection.fastqsanger'  -i 'Unnamed Collection.fastqsanger'   -I 'Unnamed Collection_R2.fastqsanger' -o first.fastqsanger -O second.fastqsanger                                                && mv first.fastqsanger '/galaxy/server/database/objects/9/8/0/dataset_98093ef9-b735-44b8-8e80-e63259d820c9.dat' && mv second.fastqsanger '/galaxy/server/database/objects/7/b/6/dataset_7b6683fa-1a4c-4c62-a2f0-7e94e422a5c9.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:55:38,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (7) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/7/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/7/galaxy_7.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/7/working/fastp.html" -a -f "/galaxy/server/database/objects/f/8/f/dataset_f8f456eb-63eb-4f9d-a693-33b572b74313.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/7/working/fastp.html" "/galaxy/server/database/objects/f/8/f/dataset_f8f456eb-63eb-4f9d-a693-33b572b74313.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:38,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:55:38,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:55:38,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:55:38,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:38,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:39,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:43,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qz2x8 with k8s id: gxy-qz2x8 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:55:43,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 7: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:55:50,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 7 finished
galaxy.model.metadata DEBUG 2025-03-14 00:55:50,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 11
galaxy.model.metadata DEBUG 2025-03-14 00:55:50,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 10
galaxy.model.metadata DEBUG 2025-03-14 00:55:50,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 9
galaxy.util WARNING 2025-03-14 00:55:50,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/8/f/dataset_f8f456eb-63eb-4f9d-a693-33b572b74313.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/8/f/dataset_f8f456eb-63eb-4f9d-a693-33b572b74313.dat'
galaxy.util WARNING 2025-03-14 00:55:50,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/b/6/dataset_7b6683fa-1a4c-4c62-a2f0-7e94e422a5c9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/b/6/dataset_7b6683fa-1a4c-4c62-a2f0-7e94e422a5c9.dat'
galaxy.util WARNING 2025-03-14 00:55:50,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/8/0/dataset_98093ef9-b735-44b8-8e80-e63259d820c9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/8/0/dataset_98093ef9-b735-44b8-8e80-e63259d820c9.dat'
galaxy.jobs INFO 2025-03-14 00:55:50,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 7 in /galaxy/server/database/jobs_directory/000/7
galaxy.jobs DEBUG 2025-03-14 00:55:50,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 7 executed (134.079 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:50,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:55:52,701 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 8
tpv.core.entities DEBUG 2025-03-14 00:55:52,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:55:52,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:55:52,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:55:52,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:55:52,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Working directory for job is: /galaxy/server/database/jobs_directory/000/8
galaxy.jobs.runners DEBUG 2025-03-14 00:55:52,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [8] queued (29.564 ms)
galaxy.jobs.handler INFO 2025-03-14 00:55:52,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:52,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 8
galaxy.jobs DEBUG 2025-03-14 00:55:52,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [8] prepared (51.856 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:55:52,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/8/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/8/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/8/configs/tmp61gbqw_l']
galaxy.jobs.runners DEBUG 2025-03-14 00:55:52,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (8) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/8/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/8/galaxy_8.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:52,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:52,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:55:53,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:02,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8msm4 with k8s id: gxy-8msm4 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:56:02,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 8: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:56:09,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 8 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:56:09,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'R1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/8/working/data_fetch_upload_r58k6aw7', 'object_id': 12}]}]}]
galaxy.jobs INFO 2025-03-14 00:56:09,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 8 in /galaxy/server/database/jobs_directory/000/8
galaxy.jobs DEBUG 2025-03-14 00:56:09,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 8 executed (118.605 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:09,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:56:11,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 9
tpv.core.entities DEBUG 2025-03-14 00:56:11,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:56:11,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:56:11,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:56:11,100 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:56:11,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Working directory for job is: /galaxy/server/database/jobs_directory/000/9
galaxy.jobs.runners DEBUG 2025-03-14 00:56:11,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [9] queued (40.035 ms)
galaxy.jobs.handler INFO 2025-03-14 00:56:11,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:11,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 9
galaxy.jobs DEBUG 2025-03-14 00:56:11,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [9] prepared (62.594 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:11,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:56:11,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:11,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:56:11,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/9/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/9/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/8/4/7/dataset_847ad74d-e387-4aab-82a1-e2c8d637247d.dat' 'R1_fq.fastqsanger' &&    fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for R1_fq.fastqsanger'  -i 'R1_fq.fastqsanger'   -o first.fastqsanger     --adapter_sequence 'ATCG'                                           && mv first.fastqsanger '/galaxy/server/database/objects/9/9/0/dataset_990fa754-e61e-4c04-b9cf-b0ac8a72aae8.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:56:11,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (9) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/9/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/9/galaxy_9.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/9/working/fastp.html" -a -f "/galaxy/server/database/objects/8/3/b/dataset_83b887dc-30dd-482e-a487-dcb0add194a5.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/9/working/fastp.html" "/galaxy/server/database/objects/8/3/b/dataset_83b887dc-30dd-482e-a487-dcb0add194a5.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:11,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:11,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:56:11,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:11,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:11,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:11,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:15,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n75pm with k8s id: gxy-n75pm succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:56:16,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 9: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:56:23,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 9 finished
galaxy.model.metadata DEBUG 2025-03-14 00:56:23,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 14
galaxy.model.metadata DEBUG 2025-03-14 00:56:23,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 13
galaxy.util WARNING 2025-03-14 00:56:23,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/3/b/dataset_83b887dc-30dd-482e-a487-dcb0add194a5.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/3/b/dataset_83b887dc-30dd-482e-a487-dcb0add194a5.dat'
galaxy.util WARNING 2025-03-14 00:56:23,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/9/0/dataset_990fa754-e61e-4c04-b9cf-b0ac8a72aae8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/9/0/dataset_990fa754-e61e-4c04-b9cf-b0ac8a72aae8.dat'
galaxy.jobs INFO 2025-03-14 00:56:23,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 9 in /galaxy/server/database/jobs_directory/000/9
galaxy.jobs DEBUG 2025-03-14 00:56:23,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 9 executed (127.714 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:23,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:56:24,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 10
tpv.core.entities DEBUG 2025-03-14 00:56:24,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:56:24,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:56:24,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:56:24,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:56:24,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Working directory for job is: /galaxy/server/database/jobs_directory/000/10
galaxy.jobs.runners DEBUG 2025-03-14 00:56:24,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [10] queued (30.763 ms)
galaxy.jobs.handler INFO 2025-03-14 00:56:24,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:24,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 10
galaxy.jobs DEBUG 2025-03-14 00:56:24,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [10] prepared (65.495 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:56:24,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/10/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/10/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/10/configs/tmp3pxzhcqq']
galaxy.jobs.runners DEBUG 2025-03-14 00:56:24,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/10/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/10/galaxy_10.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:24,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:24,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:24,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:34,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d6c52 with k8s id: gxy-d6c52 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:56:34,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 10: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:56:41,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 10 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:56:41,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'R1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/10/working/data_fetch_upload_inxwdje5', 'object_id': 15}]}]}]
galaxy.jobs INFO 2025-03-14 00:56:41,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 10 in /galaxy/server/database/jobs_directory/000/10
galaxy.jobs DEBUG 2025-03-14 00:56:41,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 10 executed (132.665 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:41,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:56:42,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 11
tpv.core.entities DEBUG 2025-03-14 00:56:42,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:56:42,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:56:42,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:56:42,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:56:42,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Working directory for job is: /galaxy/server/database/jobs_directory/000/11
galaxy.jobs.runners DEBUG 2025-03-14 00:56:42,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [11] queued (37.075 ms)
galaxy.jobs.handler INFO 2025-03-14 00:56:42,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:42,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 11
galaxy.jobs DEBUG 2025-03-14 00:56:42,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [11] prepared (52.111 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:42,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:56:42,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:42,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:56:42,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/11/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/11/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/4/2/e/dataset_42e11a7d-6a6d-4ce3-8815-ab873f718990.dat' 'R1_fq.fastqsanger' &&    fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for R1_fq.fastqsanger'  -i 'R1_fq.fastqsanger'   -o first.fastqsanger                                         -U --umi_loc 'read1' --umi_len 8       && mv first.fastqsanger '/galaxy/server/database/objects/a/b/e/dataset_abeb9034-778e-403f-839e-129c8940d603.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:56:42,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (11) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/11/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/11/galaxy_11.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/11/working/fastp.html" -a -f "/galaxy/server/database/objects/d/0/f/dataset_d0f80747-eb04-47af-94aa-799d230cb881.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/11/working/fastp.html" "/galaxy/server/database/objects/d/0/f/dataset_d0f80747-eb04-47af-94aa-799d230cb881.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:42,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:42,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:56:42,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:56:42,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:42,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:44,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:48,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h6zfc with k8s id: gxy-h6zfc succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:56:48,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 11: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:56:55,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 11 finished
galaxy.model.metadata DEBUG 2025-03-14 00:56:55,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 17
galaxy.model.metadata DEBUG 2025-03-14 00:56:55,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 16
galaxy.util WARNING 2025-03-14 00:56:55,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/0/f/dataset_d0f80747-eb04-47af-94aa-799d230cb881.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/0/f/dataset_d0f80747-eb04-47af-94aa-799d230cb881.dat'
galaxy.util WARNING 2025-03-14 00:56:55,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/b/e/dataset_abeb9034-778e-403f-839e-129c8940d603.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/b/e/dataset_abeb9034-778e-403f-839e-129c8940d603.dat'
galaxy.jobs INFO 2025-03-14 00:56:55,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 11 in /galaxy/server/database/jobs_directory/000/11
galaxy.jobs DEBUG 2025-03-14 00:56:55,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 11 executed (130.130 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:55,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:56:57,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 12
tpv.core.entities DEBUG 2025-03-14 00:56:57,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:56:57,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:56:57,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:56:57,257 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:56:57,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Working directory for job is: /galaxy/server/database/jobs_directory/000/12
galaxy.jobs.runners DEBUG 2025-03-14 00:56:57,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [12] queued (35.954 ms)
galaxy.jobs.handler INFO 2025-03-14 00:56:57,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:57,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 12
galaxy.jobs DEBUG 2025-03-14 00:56:57,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [12] prepared (62.920 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:56:57,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/12/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/12/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/12/configs/tmp51rr2xnp']
galaxy.jobs.runners DEBUG 2025-03-14 00:56:57,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (12) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/12/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/12/galaxy_12.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:57,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:57,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:56:58,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:07,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4sq4w with k8s id: gxy-4sq4w succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:57:07,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 12: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:57:14,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 12 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:57:14,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'R1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/12/working/data_fetch_upload_sdri9o9h', 'object_id': 18}]}]}]
galaxy.jobs INFO 2025-03-14 00:57:14,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 12 in /galaxy/server/database/jobs_directory/000/12
galaxy.jobs DEBUG 2025-03-14 00:57:14,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 12 executed (105.555 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:14,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:57:15,584 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 13
tpv.core.entities DEBUG 2025-03-14 00:57:15,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:57:15,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:57:15,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:57:15,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:57:15,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Working directory for job is: /galaxy/server/database/jobs_directory/000/13
galaxy.jobs.runners DEBUG 2025-03-14 00:57:15,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [13] queued (36.168 ms)
galaxy.jobs.handler INFO 2025-03-14 00:57:15,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:15,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 13
galaxy.jobs DEBUG 2025-03-14 00:57:15,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [13] prepared (48.015 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:15,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:57:15,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:15,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:57:15,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/13/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/13/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/a/c/f/dataset_acf8d7de-6bf7-4301-a889-5c9a60914597.dat' 'R1_fq.fastqsanger' &&    fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for R1_fq.fastqsanger'  -i 'R1_fq.fastqsanger'   -o first.fastqsanger                                         -U --umi_loc 'read1' --umi_len 12       && mv first.fastqsanger '/galaxy/server/database/objects/5/d/b/dataset_5dba4739-f32b-4975-b4d9-b458b4f2be44.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:57:15,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (13) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/13/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/13/galaxy_13.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/13/working/fastp.html" -a -f "/galaxy/server/database/objects/d/4/3/dataset_d4386ce1-9229-4c04-a2fa-48e16d42a8f0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/13/working/fastp.html" "/galaxy/server/database/objects/d/4/3/dataset_d4386ce1-9229-4c04-a2fa-48e16d42a8f0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:15,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:15,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:57:15,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:15,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:15,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:16,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:20,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mdfjt with k8s id: gxy-mdfjt succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:57:20,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 13: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:57:27,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 13 finished
galaxy.model.metadata DEBUG 2025-03-14 00:57:27,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 19
galaxy.model.metadata DEBUG 2025-03-14 00:57:27,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 20
galaxy.util WARNING 2025-03-14 00:57:27,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/d/b/dataset_5dba4739-f32b-4975-b4d9-b458b4f2be44.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/d/b/dataset_5dba4739-f32b-4975-b4d9-b458b4f2be44.dat'
galaxy.util WARNING 2025-03-14 00:57:27,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/4/3/dataset_d4386ce1-9229-4c04-a2fa-48e16d42a8f0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/4/3/dataset_d4386ce1-9229-4c04-a2fa-48e16d42a8f0.dat'
galaxy.jobs INFO 2025-03-14 00:57:27,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 13 in /galaxy/server/database/jobs_directory/000/13
galaxy.jobs DEBUG 2025-03-14 00:57:27,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 13 executed (129.105 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:27,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:57:28,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 14
tpv.core.entities DEBUG 2025-03-14 00:57:28,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:57:28,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:57:28,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:57:28,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:57:28,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Working directory for job is: /galaxy/server/database/jobs_directory/000/14
galaxy.jobs.runners DEBUG 2025-03-14 00:57:28,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [14] queued (40.252 ms)
galaxy.jobs.handler INFO 2025-03-14 00:57:28,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:28,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 14
galaxy.jobs DEBUG 2025-03-14 00:57:29,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [14] prepared (57.230 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:57:29,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/14/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/14/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/14/configs/tmpkxuqpfun']
galaxy.jobs.runners DEBUG 2025-03-14 00:57:29,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (14) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/14/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/14/galaxy_14.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:29,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:29,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:29,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-14 00:57:29,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 15
tpv.core.entities DEBUG 2025-03-14 00:57:29,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:57:29,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:57:29,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:57:29,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:57:29,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Working directory for job is: /galaxy/server/database/jobs_directory/000/15
galaxy.jobs.runners DEBUG 2025-03-14 00:57:30,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [15] queued (34.887 ms)
galaxy.jobs.handler INFO 2025-03-14 00:57:30,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:30,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 15
galaxy.jobs DEBUG 2025-03-14 00:57:30,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [15] prepared (66.542 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:57:30,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/15/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/15/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/15/configs/tmpiunoucri']
galaxy.jobs.runners DEBUG 2025-03-14 00:57:30,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (15) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/15/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/15/galaxy_15.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:30,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:30,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:30,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:37,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t4vn9 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:37,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:37,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-t4vn9.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:37,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 00:57:38,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-t4vn9

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-t4vn9": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (14/gxy-t4vn9) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (14/gxy-t4vn9) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (14/gxy-t4vn9) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (14/gxy-t4vn9) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-t4vn9.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 14 (gxy-t4vn9)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-t4vn9 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:38,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (14/gxy-t4vn9) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:39,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-986t6 with k8s id: gxy-986t6 succeeded
galaxy.jobs.handler DEBUG 2025-03-14 00:57:39,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 16, 17
galaxy.jobs.runners DEBUG 2025-03-14 00:57:39,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 15: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
tpv.core.entities DEBUG 2025-03-14 00:57:39,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:57:39,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:57:39,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:57:39,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:57:39,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Working directory for job is: /galaxy/server/database/jobs_directory/000/16
galaxy.jobs.runners DEBUG 2025-03-14 00:57:39,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [16] queued (35.150 ms)
galaxy.jobs.handler INFO 2025-03-14 00:57:39,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:39,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 16
tpv.core.entities DEBUG 2025-03-14 00:57:39,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:57:39,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:57:39,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:57:39,260 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:57:39,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Working directory for job is: /galaxy/server/database/jobs_directory/000/17
galaxy.jobs.runners DEBUG 2025-03-14 00:57:39,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [17] queued (49.020 ms)
galaxy.jobs.handler INFO 2025-03-14 00:57:39,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:39,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 17
galaxy.jobs DEBUG 2025-03-14 00:57:39,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [16] prepared (80.820 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:57:39,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/16/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/16/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/16/configs/tmp9enun7l4']
galaxy.jobs.runners DEBUG 2025-03-14 00:57:39,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (16) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/16/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/16/galaxy_16.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 00:57:39,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [17] prepared (61.527 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:39,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:39,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 00:57:39,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/17/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/17/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/17/configs/tmphl36hxc_']
galaxy.jobs.runners DEBUG 2025-03-14 00:57:39,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (17) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/17/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/17/galaxy_17.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:39,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:39,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:40,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:40,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-14 00:57:46,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 15 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:57:46,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/15/working/data_fetch_upload_uq2eof92', 'object_id': 22}]}]}]
galaxy.jobs INFO 2025-03-14 00:57:46,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 15 in /galaxy/server/database/jobs_directory/000/15
galaxy.jobs DEBUG 2025-03-14 00:57:46,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 15 executed (123.218 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:46,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:49,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j9kkk with k8s id: gxy-j9kkk succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:49,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2qkcp with k8s id: gxy-2qkcp succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:57:49,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 16: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:57:49,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 17: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:57:56,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 17 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:57:57,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/17/working/data_fetch_upload_6gqr2e3j', 'object_id': 24}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 00:57:57,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 16 finished
galaxy.jobs INFO 2025-03-14 00:57:57,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 17 in /galaxy/server/database/jobs_directory/000/17
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:57:57,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/16/working/data_fetch_upload_mzrg2pmf', 'object_id': 23}]}]}]
galaxy.jobs DEBUG 2025-03-14 00:57:57,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 17 executed (155.608 ms)
galaxy.jobs INFO 2025-03-14 00:57:57,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 16 in /galaxy/server/database/jobs_directory/000/16
galaxy.jobs DEBUG 2025-03-14 00:57:57,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 16 executed (124.091 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:57,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:57,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:57:57,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 18
tpv.core.entities DEBUG 2025-03-14 00:57:57,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:57:57,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:57:57,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:57:57,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:57:57,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Working directory for job is: /galaxy/server/database/jobs_directory/000/18
galaxy.jobs.runners DEBUG 2025-03-14 00:57:57,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [18] queued (42.158 ms)
galaxy.jobs.handler INFO 2025-03-14 00:57:57,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:57,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 18
galaxy.jobs DEBUG 2025-03-14 00:57:57,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [18] prepared (71.555 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:57,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:57:57,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:57,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:57:57,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/18/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/18/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/a/e/1/dataset_ae15ce11-d6c0-4926-9864-a6efa3468f26.dat' 'bwa-mem-fastq1_fq.fastqsanger' &&  ln -sf '/galaxy/server/database/objects/9/a/f/dataset_9af9f226-a438-4915-9d17-111c24f5a93b.dat' 'bwa-mem-fastq2_fq_R2.fastqsanger' &&   fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for bwa-mem-fastq1_fq.fastqsanger'  -i 'bwa-mem-fastq1_fq.fastqsanger'   -I 'bwa-mem-fastq2_fq_R2.fastqsanger' -o first.fastqsanger -O second.fastqsanger                                          -U --umi_loc 'read1' --umi_len 8       && mv first.fastqsanger '/galaxy/server/database/objects/e/1/3/dataset_e130a718-6ab9-48b9-aaf3-f6c429d32daa.dat' && mv second.fastqsanger '/galaxy/server/database/objects/2/a/a/dataset_2aae6a93-ef94-478b-ae0d-9b268b2150cb.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:57:58,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (18) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/18/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/18/galaxy_18.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/18/working/fastp.html" -a -f "/galaxy/server/database/objects/a/a/e/dataset_aae706ae-38ff-43dd-8a97-82b635d90de7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/18/working/fastp.html" "/galaxy/server/database/objects/a/a/e/dataset_aae706ae-38ff-43dd-8a97-82b635d90de7.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:58,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:58,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:57:58,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:57:58,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:58,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:57:58,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:02,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hq26g with k8s id: gxy-hq26g succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:58:02,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 18: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:58:09,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 18 finished
galaxy.model.metadata DEBUG 2025-03-14 00:58:09,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 25
galaxy.model.metadata DEBUG 2025-03-14 00:58:09,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 26
galaxy.model.metadata DEBUG 2025-03-14 00:58:09,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 27
galaxy.util WARNING 2025-03-14 00:58:09,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/1/3/dataset_e130a718-6ab9-48b9-aaf3-f6c429d32daa.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/1/3/dataset_e130a718-6ab9-48b9-aaf3-f6c429d32daa.dat'
galaxy.util WARNING 2025-03-14 00:58:09,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/a/a/dataset_2aae6a93-ef94-478b-ae0d-9b268b2150cb.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/a/a/dataset_2aae6a93-ef94-478b-ae0d-9b268b2150cb.dat'
galaxy.util WARNING 2025-03-14 00:58:09,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/a/e/dataset_aae706ae-38ff-43dd-8a97-82b635d90de7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/a/e/dataset_aae706ae-38ff-43dd-8a97-82b635d90de7.dat'
galaxy.jobs INFO 2025-03-14 00:58:09,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 18 in /galaxy/server/database/jobs_directory/000/18
galaxy.jobs DEBUG 2025-03-14 00:58:09,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 18 executed (109.421 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:09,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:58:12,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 20, 19
tpv.core.entities DEBUG 2025-03-14 00:58:12,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:58:12,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:58:12,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:58:12,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:58:12,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Working directory for job is: /galaxy/server/database/jobs_directory/000/20
galaxy.jobs.runners DEBUG 2025-03-14 00:58:12,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [20] queued (38.081 ms)
galaxy.jobs.handler INFO 2025-03-14 00:58:12,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:12,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 20
tpv.core.entities DEBUG 2025-03-14 00:58:12,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:58:12,193 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:58:12,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:58:12,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:58:12,233 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Working directory for job is: /galaxy/server/database/jobs_directory/000/19
galaxy.jobs.runners DEBUG 2025-03-14 00:58:12,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [19] queued (43.224 ms)
galaxy.jobs.handler INFO 2025-03-14 00:58:12,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:12,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 19
galaxy.jobs DEBUG 2025-03-14 00:58:12,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [20] prepared (75.215 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:58:12,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/20/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/20/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/20/configs/tmp0fpjxli1']
galaxy.jobs.runners DEBUG 2025-03-14 00:58:12,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (20) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/20/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/20/galaxy_20.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:12,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 00:58:12,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [19] prepared (72.123 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:12,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 00:58:12,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/19/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/19/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/19/configs/tmp6m_uttrg']
galaxy.jobs.runners DEBUG 2025-03-14 00:58:12,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/19/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/19/galaxy_19.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:12,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:12,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:13,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:13,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:23,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ksqvp with k8s id: gxy-ksqvp succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:23,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zsd5d with k8s id: gxy-zsd5d succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:58:24,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 19: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:58:24,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 20: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:58:31,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 20 finished
galaxy.jobs.runners DEBUG 2025-03-14 00:58:31,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 19 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:58:31,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/19/working/data_fetch_upload_i13px0lz', 'object_id': 28}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:58:31,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/20/working/data_fetch_upload_rb7h8ird', 'object_id': 29}]}]}]
galaxy.jobs INFO 2025-03-14 00:58:31,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 20 in /galaxy/server/database/jobs_directory/000/20
galaxy.jobs INFO 2025-03-14 00:58:31,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 19 in /galaxy/server/database/jobs_directory/000/19
galaxy.jobs DEBUG 2025-03-14 00:58:31,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 20 executed (148.037 ms)
galaxy.jobs DEBUG 2025-03-14 00:58:31,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 19 executed (153.662 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:31,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:31,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:58:32,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 21
tpv.core.entities DEBUG 2025-03-14 00:58:32,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:58:32,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:58:32,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:58:32,683 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:58:32,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Working directory for job is: /galaxy/server/database/jobs_directory/000/21
galaxy.jobs.runners DEBUG 2025-03-14 00:58:32,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [21] queued (45.573 ms)
galaxy.jobs.handler INFO 2025-03-14 00:58:32,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:32,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 21
galaxy.jobs DEBUG 2025-03-14 00:58:32,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [21] prepared (56.634 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:58:32,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:58:32,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:58:32,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:58:32,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/21/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/21/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/5/d/6/dataset_5d602cda-0fd9-4970-a409-4098d49657e0.dat' 'bwa-mem-fastq1_fq.fastqsanger' &&  ln -sf '/galaxy/server/database/objects/6/4/4/dataset_644b658f-d286-41d2-9cf1-1152b61e7399.dat' 'bwa-mem-fastq2_fq_R2.fastqsanger' &&   fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for bwa-mem-fastq1_fq.fastqsanger'  -i 'bwa-mem-fastq1_fq.fastqsanger'   -I 'bwa-mem-fastq2_fq_R2.fastqsanger' -o first.fastqsanger -O second.fastqsanger                                          -U --umi_loc 'read2' --umi_len 8       && mv first.fastqsanger '/galaxy/server/database/objects/a/7/0/dataset_a70a4d74-6f83-4325-b0a7-cd5c3344e901.dat' && mv second.fastqsanger '/galaxy/server/database/objects/c/f/5/dataset_cf541065-5a38-4790-b3de-d3a245d84678.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:58:32,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (21) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/21/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/21/galaxy_21.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/21/working/fastp.html" -a -f "/galaxy/server/database/objects/f/9/9/dataset_f993f1a1-f942-4b61-98b0-27c144337424.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/21/working/fastp.html" "/galaxy/server/database/objects/f/9/9/dataset_f993f1a1-f942-4b61-98b0-27c144337424.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/21/working/fastp.json" -a -f "/galaxy/server/database/objects/1/d/e/dataset_1de4578a-db60-4e46-9cc1-38e36b6fa743.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/21/working/fastp.json" "/galaxy/server/database/objects/1/d/e/dataset_1de4578a-db60-4e46-9cc1-38e36b6fa743.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:32,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:58:32,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:58:32,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:58:32,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:32,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:33,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:38,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jbszr with k8s id: gxy-jbszr succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:58:38,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 21: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:58:45,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 21 finished
galaxy.model.metadata DEBUG 2025-03-14 00:58:45,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 30
galaxy.model.metadata DEBUG 2025-03-14 00:58:45,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 31
galaxy.model.metadata DEBUG 2025-03-14 00:58:45,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 32
galaxy.model.metadata DEBUG 2025-03-14 00:58:45,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 33
galaxy.util WARNING 2025-03-14 00:58:45,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/7/0/dataset_a70a4d74-6f83-4325-b0a7-cd5c3344e901.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/7/0/dataset_a70a4d74-6f83-4325-b0a7-cd5c3344e901.dat'
galaxy.util WARNING 2025-03-14 00:58:45,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/f/5/dataset_cf541065-5a38-4790-b3de-d3a245d84678.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/f/5/dataset_cf541065-5a38-4790-b3de-d3a245d84678.dat'
galaxy.util WARNING 2025-03-14 00:58:45,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/9/9/dataset_f993f1a1-f942-4b61-98b0-27c144337424.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/9/9/dataset_f993f1a1-f942-4b61-98b0-27c144337424.dat'
galaxy.util WARNING 2025-03-14 00:58:45,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/d/e/dataset_1de4578a-db60-4e46-9cc1-38e36b6fa743.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/d/e/dataset_1de4578a-db60-4e46-9cc1-38e36b6fa743.dat'
galaxy.jobs INFO 2025-03-14 00:58:45,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 21 in /galaxy/server/database/jobs_directory/000/21
galaxy.jobs DEBUG 2025-03-14 00:58:45,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 21 executed (150.281 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:45,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:58:47,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 22
tpv.core.entities DEBUG 2025-03-14 00:58:47,140 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:58:47,140 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:58:47,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:58:47,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:58:47,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Working directory for job is: /galaxy/server/database/jobs_directory/000/22
galaxy.jobs.runners DEBUG 2025-03-14 00:58:47,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [22] queued (34.898 ms)
galaxy.jobs.handler INFO 2025-03-14 00:58:47,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:47,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 22
galaxy.jobs DEBUG 2025-03-14 00:58:47,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [22] prepared (53.653 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:58:47,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/22/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/22/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/22/configs/tmpudgzboqm']
galaxy.jobs.runners DEBUG 2025-03-14 00:58:47,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (22) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/22/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/22/galaxy_22.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:47,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:47,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:48,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xx2f8 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xx2f8.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 00:58:56,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-xx2f8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-xx2f8": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (22/gxy-xx2f8) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (22/gxy-xx2f8) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (22/gxy-xx2f8) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (22/gxy-xx2f8) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xx2f8.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 22 (gxy-xx2f8)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-xx2f8 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:56,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (22/gxy-xx2f8) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-14 00:58:58,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 23
tpv.core.entities DEBUG 2025-03-14 00:58:58,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:58:58,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:58:58,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:58:58,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:58:58,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Working directory for job is: /galaxy/server/database/jobs_directory/000/23
galaxy.jobs.runners DEBUG 2025-03-14 00:58:58,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [23] queued (29.629 ms)
galaxy.jobs.handler INFO 2025-03-14 00:58:58,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:58,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 23
galaxy.jobs DEBUG 2025-03-14 00:58:58,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [23] prepared (53.298 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:58:58,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/23/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/23/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/23/configs/tmpeo0mmfgr']
galaxy.jobs.runners DEBUG 2025-03-14 00:58:58,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (23) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/23/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/23/galaxy_23.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:58,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:58,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:58:59,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:07,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lzngs with k8s id: gxy-lzngs succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:59:07,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 23: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:59:15,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 23 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:59:15,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'R1.fq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/23/working/gxupload_0', 'object_id': 35}]}]}]
galaxy.jobs INFO 2025-03-14 00:59:15,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 23 in /galaxy/server/database/jobs_directory/000/23
galaxy.jobs DEBUG 2025-03-14 00:59:15,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 23 executed (113.698 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:15,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:59:15,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 24
tpv.core.entities DEBUG 2025-03-14 00:59:15,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:59:15,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:59:15,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:59:15,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:59:15,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Working directory for job is: /galaxy/server/database/jobs_directory/000/24
galaxy.jobs.runners DEBUG 2025-03-14 00:59:15,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [24] queued (41.511 ms)
galaxy.jobs.handler INFO 2025-03-14 00:59:15,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:15,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 24
galaxy.jobs DEBUG 2025-03-14 00:59:15,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [24] prepared (55.132 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:15,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:59:15,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:15,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:59:15,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/24/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/24/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/5/6/3/dataset_563b2459-af58-412c-88b7-ebb63784aa34.dat' 'R1_fq_gz.fastqsanger.gz' &&    fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for R1_fq_gz.fastqsanger.gz'  -i 'R1_fq_gz.fastqsanger.gz'   -o first.fastqsanger.gz                                    --poly_g_min_len 10 -g           && mv first.fastqsanger.gz '/galaxy/server/database/objects/5/a/c/dataset_5ac96075-387d-4b33-9d73-656e596e6804.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:59:15,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (24) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/24/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/24/galaxy_24.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/24/working/fastp.html" -a -f "/galaxy/server/database/objects/0/a/b/dataset_0ab86c4c-8846-4dd6-b543-71855593f87e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/24/working/fastp.html" "/galaxy/server/database/objects/0/a/b/dataset_0ab86c4c-8846-4dd6-b543-71855593f87e.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/24/working/fastp.json" -a -f "/galaxy/server/database/objects/f/a/f/dataset_faf9da5a-75ed-4cad-9128-ad2740ce7fc9.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/24/working/fastp.json" "/galaxy/server/database/objects/f/a/f/dataset_faf9da5a-75ed-4cad-9128-ad2740ce7fc9.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:15,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:15,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:59:15,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:15,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:15,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:16,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:20,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kqsv8 with k8s id: gxy-kqsv8 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:59:20,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 24: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:59:27,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 24 finished
galaxy.model.metadata DEBUG 2025-03-14 00:59:27,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 36
galaxy.model.metadata DEBUG 2025-03-14 00:59:27,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 38
galaxy.model.metadata DEBUG 2025-03-14 00:59:27,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 37
galaxy.util WARNING 2025-03-14 00:59:27,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/a/c/dataset_5ac96075-387d-4b33-9d73-656e596e6804.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/a/c/dataset_5ac96075-387d-4b33-9d73-656e596e6804.dat'
galaxy.util WARNING 2025-03-14 00:59:27,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/a/b/dataset_0ab86c4c-8846-4dd6-b543-71855593f87e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/a/b/dataset_0ab86c4c-8846-4dd6-b543-71855593f87e.dat'
galaxy.util WARNING 2025-03-14 00:59:27,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/a/f/dataset_faf9da5a-75ed-4cad-9128-ad2740ce7fc9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/a/f/dataset_faf9da5a-75ed-4cad-9128-ad2740ce7fc9.dat'
galaxy.jobs INFO 2025-03-14 00:59:28,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 24 in /galaxy/server/database/jobs_directory/000/24
galaxy.jobs DEBUG 2025-03-14 00:59:28,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 24 executed (138.323 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:28,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:59:30,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 25
tpv.core.entities DEBUG 2025-03-14 00:59:30,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:59:30,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:59:30,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:59:30,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:59:30,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Working directory for job is: /galaxy/server/database/jobs_directory/000/25
galaxy.jobs.runners DEBUG 2025-03-14 00:59:30,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [25] queued (33.288 ms)
galaxy.jobs.handler INFO 2025-03-14 00:59:30,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:30,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 25
galaxy.jobs DEBUG 2025-03-14 00:59:30,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [25] prepared (62.356 ms)
galaxy.jobs.command_factory INFO 2025-03-14 00:59:30,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/25/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/25/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/25/configs/tmp4woiur_a']
galaxy.jobs.runners DEBUG 2025-03-14 00:59:30,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (25) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/25/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/25/galaxy_25.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:30,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:30,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:30,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:39,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-27t4w with k8s id: gxy-27t4w succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:59:40,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 25: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 00:59:47,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 25 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 00:59:47,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'R1.fq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/25/working/gxupload_0', 'object_id': 39}]}]}]
galaxy.jobs INFO 2025-03-14 00:59:47,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 25 in /galaxy/server/database/jobs_directory/000/25
galaxy.jobs DEBUG 2025-03-14 00:59:47,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 25 executed (133.810 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:47,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 00:59:48,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 26
tpv.core.entities DEBUG 2025-03-14 00:59:48,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 00:59:48,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 00:59:48,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 00:59:48,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 00:59:48,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Working directory for job is: /galaxy/server/database/jobs_directory/000/26
galaxy.jobs.runners DEBUG 2025-03-14 00:59:48,481 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [26] queued (40.650 ms)
galaxy.jobs.handler INFO 2025-03-14 00:59:48,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:48,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 26
galaxy.jobs DEBUG 2025-03-14 00:59:48,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [26] prepared (47.212 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:48,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:59:48,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:48,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 00:59:48,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/26/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/26/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/6/e/c/dataset_6ecb10d1-714b-4198-b917-c76f1aef08e7.dat' 'R1_fq_gz.fastqsanger.gz' &&    fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for R1_fq_gz.fastqsanger.gz'  -i 'R1_fq_gz.fastqsanger.gz'   -o first.fastqsanger.gz                                      -x --poly_x_min_len 10         && mv first.fastqsanger.gz '/galaxy/server/database/objects/c/1/3/dataset_c1311e06-f20e-479b-8076-23a38887ca27.dat']
galaxy.jobs.runners DEBUG 2025-03-14 00:59:48,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (26) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/26/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/26/galaxy_26.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/26/working/fastp.html" -a -f "/galaxy/server/database/objects/7/a/0/dataset_7a03ea3c-6697-4eda-aed8-8473f535c8df.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/26/working/fastp.html" "/galaxy/server/database/objects/7/a/0/dataset_7a03ea3c-6697-4eda-aed8-8473f535c8df.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/26/working/fastp.json" -a -f "/galaxy/server/database/objects/e/8/0/dataset_e8088747-b597-494d-856d-fb7f9c2801ec.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/26/working/fastp.json" "/galaxy/server/database/objects/e/8/0/dataset_e8088747-b597-494d-856d-fb7f9c2801ec.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:48,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:48,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 00:59:48,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 00:59:48,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:48,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:49,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 00:59:54,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8hksb with k8s id: gxy-8hksb succeeded
galaxy.jobs.runners DEBUG 2025-03-14 00:59:54,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 26: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:00:01,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 26 finished
galaxy.model.metadata DEBUG 2025-03-14 01:00:01,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 40
galaxy.model.metadata DEBUG 2025-03-14 01:00:01,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 41
galaxy.model.metadata DEBUG 2025-03-14 01:00:01,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 42
galaxy.util WARNING 2025-03-14 01:00:01,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/1/3/dataset_c1311e06-f20e-479b-8076-23a38887ca27.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/1/3/dataset_c1311e06-f20e-479b-8076-23a38887ca27.dat'
galaxy.util WARNING 2025-03-14 01:00:01,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/a/0/dataset_7a03ea3c-6697-4eda-aed8-8473f535c8df.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/a/0/dataset_7a03ea3c-6697-4eda-aed8-8473f535c8df.dat'
galaxy.util WARNING 2025-03-14 01:00:01,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/8/0/dataset_e8088747-b597-494d-856d-fb7f9c2801ec.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/8/0/dataset_e8088747-b597-494d-856d-fb7f9c2801ec.dat'
galaxy.jobs INFO 2025-03-14 01:00:01,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 26 in /galaxy/server/database/jobs_directory/000/26
galaxy.jobs DEBUG 2025-03-14 01:00:02,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 26 executed (179.658 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:02,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:00:03,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 28, 27
tpv.core.entities DEBUG 2025-03-14 01:00:03,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:00:03,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:00:03,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:00:03,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:00:03,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Working directory for job is: /galaxy/server/database/jobs_directory/000/28
galaxy.jobs.runners DEBUG 2025-03-14 01:00:03,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [28] queued (30.999 ms)
galaxy.jobs.handler INFO 2025-03-14 01:00:03,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:03,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 28
tpv.core.entities DEBUG 2025-03-14 01:00:03,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:00:03,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:00:03,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:00:03,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:00:03,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Working directory for job is: /galaxy/server/database/jobs_directory/000/27
galaxy.jobs.runners DEBUG 2025-03-14 01:00:03,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [27] queued (45.943 ms)
galaxy.jobs.handler INFO 2025-03-14 01:00:03,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:03,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 27
galaxy.jobs DEBUG 2025-03-14 01:00:03,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [28] prepared (82.205 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:00:03,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/28/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/28/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/28/configs/tmpnzqc1tr_']
galaxy.jobs.runners DEBUG 2025-03-14 01:00:03,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (28) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/28/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/28/galaxy_28.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:03,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:00:03,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [27] prepared (79.157 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:03,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:00:04,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/27/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/27/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/27/configs/tmpvwohunus']
galaxy.jobs.runners DEBUG 2025-03-14 01:00:04,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (27) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/27/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/27/galaxy_27.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:04,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:04,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:05,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:05,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:14,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cmmlp with k8s id: gxy-cmmlp succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:14,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jhzpn with k8s id: gxy-jhzpn succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:00:14,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 28: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:00:14,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 27: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:00:22,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 27 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:00:22,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/27/working/data_fetch_upload_0qcbc1lb', 'object_id': 43}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:00:22,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 28 finished
galaxy.jobs INFO 2025-03-14 01:00:22,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 27 in /galaxy/server/database/jobs_directory/000/27
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:00:22,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2_too_long.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/28/working/data_fetch_upload_reerlqvg', 'object_id': 44}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:00:22,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 27 executed (168.625 ms)
galaxy.jobs INFO 2025-03-14 01:00:22,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 28 in /galaxy/server/database/jobs_directory/000/28
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:22,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:00:22,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 28 executed (147.807 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:22,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:00:23,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 29
tpv.core.entities DEBUG 2025-03-14 01:00:23,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:00:23,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:00:23,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:00:23,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:00:23,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Working directory for job is: /galaxy/server/database/jobs_directory/000/29
galaxy.jobs.runners DEBUG 2025-03-14 01:00:23,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [29] queued (45.206 ms)
galaxy.jobs.handler INFO 2025-03-14 01:00:23,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:23,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 29
galaxy.jobs DEBUG 2025-03-14 01:00:23,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [29] prepared (74.087 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:00:23,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:00:23,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 01:00:23,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:00:23,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/29/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/29/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/e/6/2/dataset_e62b986e-dd95-4c9b-9bd3-eb78d8b64a3f.dat' 'Unnamed Collection.fastqsanger' && ln -sf '/galaxy/server/database/objects/d/f/0/dataset_df0bf8d5-6ae2-4991-a4f7-1c7274329a65.dat' 'Unnamed Collection_R2.fastqsanger' &&   fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for Unnamed Collection.fastqsanger'  -i 'Unnamed Collection.fastqsanger'   -I 'Unnamed Collection_R2.fastqsanger' -o first.fastqsanger -O second.fastqsanger                                                && mv first.fastqsanger '/galaxy/server/database/objects/9/1/c/dataset_91c98263-34b5-45cb-8336-50aef2fe26b6.dat' && mv second.fastqsanger '/galaxy/server/database/objects/f/a/b/dataset_fab5ca36-65ef-44ff-8e2b-6ab1c7932e6e.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:00:23,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (29) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/29/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/29/galaxy_29.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/29/working/fastp.html" -a -f "/galaxy/server/database/objects/0/e/9/dataset_0e9b6b32-6696-4bba-bf06-c33128698f8f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/29/working/fastp.html" "/galaxy/server/database/objects/0/e/9/dataset_0e9b6b32-6696-4bba-bf06-c33128698f8f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/29/working/fastp.json" -a -f "/galaxy/server/database/objects/5/9/1/dataset_5917831c-0d29-4c7d-918f-8c7c5be173da.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/29/working/fastp.json" "/galaxy/server/database/objects/5/9/1/dataset_5917831c-0d29-4c7d-918f-8c7c5be173da.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:23,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:00:23,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:00:23,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 01:00:23,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:23,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:23,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:27,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zmfl8 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:27,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:28,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job: gxy-zmfl8 failed due to an unknown exit code from the tool.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:28,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:00:28,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-zmfl8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-zmfl8": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners DEBUG 2025-03-14 01:00:28,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 29: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:00:36,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 29 finished
galaxy.tool_util.output_checker INFO 2025-03-14 01:00:36,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job error detected, failing job. Reasons are [{'type': 'exit_code', 'desc': 'Fatal error: Exit code 255 ()', 'exit_code': 255, 'code_desc': '', 'error_level': 3}]
galaxy.jobs DEBUG 2025-03-14 01:00:36,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (29) setting dataset 45 state to ERROR
galaxy.jobs DEBUG 2025-03-14 01:00:36,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (29) setting dataset 46 state to ERROR
galaxy.jobs DEBUG 2025-03-14 01:00:36,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (29) setting dataset 47 state to ERROR
galaxy.jobs DEBUG 2025-03-14 01:00:36,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (29) setting dataset 48 state to ERROR
galaxy.jobs INFO 2025-03-14 01:00:36,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 29 in /galaxy/server/database/jobs_directory/000/29
galaxy.jobs DEBUG 2025-03-14 01:00:36,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 29 executed (238.902 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:36,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:00:37,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 30, 31
tpv.core.entities DEBUG 2025-03-14 01:00:37,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:00:37,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:00:37,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:00:37,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:00:37,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Working directory for job is: /galaxy/server/database/jobs_directory/000/30
galaxy.jobs.runners DEBUG 2025-03-14 01:00:37,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [30] queued (42.820 ms)
galaxy.jobs.handler INFO 2025-03-14 01:00:37,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:37,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 30
tpv.core.entities DEBUG 2025-03-14 01:00:37,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:00:37,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:00:37,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:00:37,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:00:37,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Working directory for job is: /galaxy/server/database/jobs_directory/000/31
galaxy.jobs.runners DEBUG 2025-03-14 01:00:37,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [31] queued (51.043 ms)
galaxy.jobs.handler INFO 2025-03-14 01:00:37,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:37,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 31
galaxy.jobs DEBUG 2025-03-14 01:00:37,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [30] prepared (101.676 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:00:37,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/30/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/30/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/30/configs/tmpot_141lv']
galaxy.jobs.runners DEBUG 2025-03-14 01:00:37,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/30/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/30/galaxy_30.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:00:37,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [31] prepared (88.590 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:37,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:37,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:00:37,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/31/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/31/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/31/configs/tmp7_3x0r0i']
galaxy.jobs.runners DEBUG 2025-03-14 01:00:37,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (31) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/31/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/31/galaxy_31.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:37,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:37,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:38,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:38,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8l4hc failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8l4hc.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:00:47,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-8l4hc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-8l4hc": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hz8d8 with k8s id: gxy-hz8d8 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30/gxy-8l4hc) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30/gxy-8l4hc) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30/gxy-8l4hc) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30/gxy-8l4hc) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8l4hc.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 30 (gxy-8l4hc)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-8l4hc to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:47,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30/gxy-8l4hc) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-03-14 01:00:47,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 31: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-14 01:00:48,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 32, 33
tpv.core.entities DEBUG 2025-03-14 01:00:48,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:00:48,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:00:48,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:00:48,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:00:48,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Working directory for job is: /galaxy/server/database/jobs_directory/000/32
galaxy.jobs.runners DEBUG 2025-03-14 01:00:48,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [32] queued (37.652 ms)
galaxy.jobs.handler INFO 2025-03-14 01:00:49,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 32
tpv.core.entities DEBUG 2025-03-14 01:00:49,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:00:49,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:00:49,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:00:49,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:00:49,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Working directory for job is: /galaxy/server/database/jobs_directory/000/33
galaxy.jobs.runners DEBUG 2025-03-14 01:00:49,065 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [33] queued (44.516 ms)
galaxy.jobs.handler INFO 2025-03-14 01:00:49,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 33
galaxy.jobs DEBUG 2025-03-14 01:00:49,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [32] prepared (87.283 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:00:49,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/32/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/32/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/32/configs/tmpz8yea9y_']
galaxy.jobs.runners DEBUG 2025-03-14 01:00:49,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (32) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/32/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/32/galaxy_32.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:00:49,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [33] prepared (80.003 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:00:49,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/33/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/33/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/33/configs/tmprnkyv82a']
galaxy.jobs.runners DEBUG 2025-03-14 01:00:49,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (33) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/33/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/33/galaxy_33.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:49,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-14 01:00:55,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 31 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:00:55,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/31/working/data_fetch_upload_10j90e95', 'object_id': 50}]}]}]
galaxy.jobs INFO 2025-03-14 01:00:55,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 31 in /galaxy/server/database/jobs_directory/000/31
galaxy.jobs DEBUG 2025-03-14 01:00:55,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 31 executed (127.273 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:55,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:59,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hnqd9 with k8s id: gxy-hnqd9 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:00:59,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sgrhl with k8s id: gxy-sgrhl succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:00:59,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 32: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:00:59,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 33: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:01:07,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 33 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:01:07,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/33/working/data_fetch_upload_styk0ezb', 'object_id': 52}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:01:07,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 32 finished
galaxy.jobs INFO 2025-03-14 01:01:07,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 33 in /galaxy/server/database/jobs_directory/000/33
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:01:07,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/32/working/data_fetch_upload_8mgnl2dg', 'object_id': 51}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:01:07,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 33 executed (160.453 ms)
galaxy.jobs INFO 2025-03-14 01:01:07,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 32 in /galaxy/server/database/jobs_directory/000/32
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:07,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:01:07,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 32 executed (148.487 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:07,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:01:08,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 34
tpv.core.entities DEBUG 2025-03-14 01:01:08,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:01:08,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:01:08,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:01:08,485 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:01:08,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Working directory for job is: /galaxy/server/database/jobs_directory/000/34
galaxy.jobs.runners DEBUG 2025-03-14 01:01:08,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [34] queued (40.208 ms)
galaxy.jobs.handler INFO 2025-03-14 01:01:08,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:08,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 34
galaxy.jobs DEBUG 2025-03-14 01:01:08,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [34] prepared (83.282 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:08,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:01:08,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:08,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:01:08,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/34/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/34/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/9/2/2/dataset_92245b7c-6b0b-49cf-a615-9d3cfeef520e.dat' 'Unnamed Collection.fastqsanger' && ln -sf '/galaxy/server/database/objects/1/d/1/dataset_1d1b2c9e-1133-4baf-a38b-d1a762597c1f.dat' 'Unnamed Collection_R2.fastqsanger' &&   fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for Unnamed Collection.fastqsanger'  -i 'Unnamed Collection.fastqsanger'   -I 'Unnamed Collection_R2.fastqsanger' --merge --merged_out '/galaxy/server/database/objects/0/d/b/dataset_0db821da-6432-4aba-b5cb-71c9918f80e8.dat' --include_unmerged]
galaxy.jobs.runners DEBUG 2025-03-14 01:01:08,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (34) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/34/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/34/galaxy_34.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/34/working/fastp.json" -a -f "/galaxy/server/database/objects/c/3/b/dataset_c3beeeb3-40ac-4c43-9595-d0b5f7d44fc7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/34/working/fastp.json" "/galaxy/server/database/objects/c/3/b/dataset_c3beeeb3-40ac-4c43-9595-d0b5f7d44fc7.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:08,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:08,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:01:08,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:08,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:08,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:09,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:14,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wjbtf with k8s id: gxy-wjbtf succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:01:14,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 34: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:01:22,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 34 finished
galaxy.model.metadata DEBUG 2025-03-14 01:01:22,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 53
galaxy.model.metadata DEBUG 2025-03-14 01:01:22,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 54
galaxy.util WARNING 2025-03-14 01:01:22,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/3/b/dataset_c3beeeb3-40ac-4c43-9595-d0b5f7d44fc7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/3/b/dataset_c3beeeb3-40ac-4c43-9595-d0b5f7d44fc7.dat'
galaxy.jobs INFO 2025-03-14 01:01:22,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 34 in /galaxy/server/database/jobs_directory/000/34
galaxy.jobs DEBUG 2025-03-14 01:01:22,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 34 executed (130.000 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:22,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:01:23,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 35, 36
tpv.core.entities DEBUG 2025-03-14 01:01:23,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:01:23,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:01:23,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:01:23,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:01:23,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Working directory for job is: /galaxy/server/database/jobs_directory/000/35
galaxy.jobs.runners DEBUG 2025-03-14 01:01:23,866 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [35] queued (38.271 ms)
galaxy.jobs.handler INFO 2025-03-14 01:01:23,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:23,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 35
tpv.core.entities DEBUG 2025-03-14 01:01:23,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:01:23,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:01:23,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:01:23,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:01:23,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Working directory for job is: /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-03-14 01:01:23,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [36] queued (43.551 ms)
galaxy.jobs.handler INFO 2025-03-14 01:01:23,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:23,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 36
galaxy.jobs DEBUG 2025-03-14 01:01:23,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [35] prepared (79.322 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:01:23,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/35/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/35/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/35/configs/tmpuq0ht_i5']
galaxy.jobs.runners DEBUG 2025-03-14 01:01:23,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (35) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/35/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/35/galaxy_35.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:24,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:01:24,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [36] prepared (69.503 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:24,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:01:24,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/36/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/36/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/36/configs/tmp_v93o7su']
galaxy.jobs.runners DEBUG 2025-03-14 01:01:24,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (36) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/36/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/36/galaxy_36.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:24,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:24,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:24,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:24,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:33,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8xdpn with k8s id: gxy-8xdpn succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:01:33,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 36: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:34,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kjq96 with k8s id: gxy-kjq96 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:01:34,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 35: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:01:40,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 36 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:01:40,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'input_reverse.fastqsanger.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/36/working/gxupload_0', 'object_id': 56}]}]}]
galaxy.jobs INFO 2025-03-14 01:01:40,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs DEBUG 2025-03-14 01:01:40,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 36 executed (174.159 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:40,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 01:01:41,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 35 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:01:41,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'input_forward.fastqsanger.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/35/working/gxupload_0', 'object_id': 55}]}]}]
galaxy.jobs INFO 2025-03-14 01:01:41,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 35 in /galaxy/server/database/jobs_directory/000/35
galaxy.jobs DEBUG 2025-03-14 01:01:41,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 35 executed (152.000 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:41,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:01:43,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 37
tpv.core.entities DEBUG 2025-03-14 01:01:43,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/.*, abstract=False, cores=4, mem=min(max(int(input_size * 6), 3.7), 58), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:01:43,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:01:43,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:01:43,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:01:43,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.runners DEBUG 2025-03-14 01:01:43,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (40.336 ms)
galaxy.jobs.handler INFO 2025-03-14 01:01:43,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:43,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 37
galaxy.jobs DEBUG 2025-03-14 01:01:43,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [37] prepared (62.928 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:43,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:01:43,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:43,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:01:43,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [fastp -v > /galaxy/server/database/jobs_directory/000/37/outputs/COMMAND_VERSION 2>&1;
ln -sf '/galaxy/server/database/objects/4/8/4/dataset_484b8359-ea14-4199-ae90-5f271154db0b.dat' 'Unnamed Collection.fastqsanger.gz' && ln -sf '/galaxy/server/database/objects/a/6/a/dataset_a6ad9248-3558-41c3-a96f-a26eef56aa0a.dat' 'Unnamed Collection_R2.fastqsanger.gz' &&   fastp  --thread ${GALAXY_SLOTS:-1} --report_title 'fastp report for Unnamed Collection.fastqsanger.gz'  -i 'Unnamed Collection.fastqsanger.gz'   -I 'Unnamed Collection_R2.fastqsanger.gz' -o first.fastqsanger.gz -O second.fastqsanger.gz                                                && mv first.fastqsanger.gz '/galaxy/server/database/objects/9/7/2/dataset_9725c254-6202-42a8-a7c8-9f5d7d8827ce.dat' && mv second.fastqsanger.gz '/galaxy/server/database/objects/8/0/b/dataset_80b44ea6-57fb-4bf4-91dd-b85a8176b619.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:01:43,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/37/working/fastp.html" -a -f "/galaxy/server/database/objects/a/9/8/dataset_a9802ff2-ff07-4029-82d0-73ed86fbafd0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/37/working/fastp.html" "/galaxy/server/database/objects/a/9/8/dataset_a9802ff2-ff07-4029-82d0-73ed86fbafd0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:43,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:43,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:01:43,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3: fastp:0.24.0
galaxy.tool_util.deps.containers INFO 2025-03-14 01:01:43,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/fastp:0.24.0--heae3180_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:43,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:44,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:48,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4chr5 with k8s id: gxy-4chr5 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:01:48,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 37: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:01:55,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 37 finished
galaxy.model.metadata DEBUG 2025-03-14 01:01:55,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 58
galaxy.model.metadata DEBUG 2025-03-14 01:01:55,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 57
galaxy.model.metadata DEBUG 2025-03-14 01:01:55,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 59
galaxy.util WARNING 2025-03-14 01:01:55,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/0/b/dataset_80b44ea6-57fb-4bf4-91dd-b85a8176b619.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/0/b/dataset_80b44ea6-57fb-4bf4-91dd-b85a8176b619.dat'
galaxy.util WARNING 2025-03-14 01:01:55,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/9/8/dataset_a9802ff2-ff07-4029-82d0-73ed86fbafd0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/9/8/dataset_a9802ff2-ff07-4029-82d0-73ed86fbafd0.dat'
galaxy.util WARNING 2025-03-14 01:01:55,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/7/2/dataset_9725c254-6202-42a8-a7c8-9f5d7d8827ce.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/7/2/dataset_9725c254-6202-42a8-a7c8-9f5d7d8827ce.dat'
galaxy.jobs INFO 2025-03-14 01:01:55,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 37 in /galaxy/server/database/jobs_directory/000/37
galaxy.jobs DEBUG 2025-03-14 01:01:55,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 37 executed (144.078 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:55,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:01:58,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38, 40, 39
tpv.core.entities DEBUG 2025-03-14 01:01:58,671 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:01:58,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:01:58,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:01:58,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:01:58,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-03-14 01:01:58,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (30.677 ms)
galaxy.jobs.handler INFO 2025-03-14 01:01:58,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:58,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 38
tpv.core.entities DEBUG 2025-03-14 01:01:58,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:01:58,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:01:58,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:01:58,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:01:58,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-03-14 01:01:58,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (42.819 ms)
galaxy.jobs.handler INFO 2025-03-14 01:01:58,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:58,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 40
tpv.core.entities DEBUG 2025-03-14 01:01:58,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:01:58,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:01:58,785 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:01:58,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:01:58,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [38] prepared (87.949 ms)
galaxy.jobs DEBUG 2025-03-14 01:01:58,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.command_factory INFO 2025-03-14 01:01:58,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/38/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/38/configs/tmpmucyzknu']
galaxy.jobs.runners DEBUG 2025-03-14 01:01:58,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (57.668 ms)
galaxy.jobs.handler INFO 2025-03-14 01:01:58,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:58,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 39
galaxy.jobs.runners DEBUG 2025-03-14 01:01:58,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:58,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:01:58,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [40] prepared (96.166 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:58,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:01:58,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/40/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/40/configs/tmp9kqg4dty']
galaxy.jobs.runners DEBUG 2025-03-14 01:01:58,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:58,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:01:58,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [39] prepared (90.905 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:58,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:01:58,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/39/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/39/configs/tmpp2dbgzjg']
galaxy.jobs.runners DEBUG 2025-03-14 01:01:58,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:59,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:59,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:59,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:59,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:01:59,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:09,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q5rbs with k8s id: gxy-q5rbs succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:09,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-k6z8h with k8s id: gxy-k6z8h succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:09,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hvckg with k8s id: gxy-hvckg succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:02:09,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 38: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:02:09,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 40: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:02:10,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 39: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:02:21,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 38 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:02:21,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_a.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/38/working/data_fetch_upload_5d2goq_u', 'object_id': 60}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:02:21,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 40 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:02:21,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_c.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/40/working/data_fetch_upload_fqgovxpq', 'object_id': 62}]}]}]
galaxy.jobs INFO 2025-03-14 01:02:21,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 38 in /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-03-14 01:02:21,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 39 finished
galaxy.jobs INFO 2025-03-14 01:02:21,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 40 in /galaxy/server/database/jobs_directory/000/40
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:02:21,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_b.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/39/working/data_fetch_upload_wofi4iec', 'object_id': 61}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:02:21,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 38 executed (209.024 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:21,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-14 01:02:21,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 39 in /galaxy/server/database/jobs_directory/000/39
galaxy.jobs DEBUG 2025-03-14 01:02:21,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 40 executed (204.098 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:21,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:02:21,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 39 executed (156.461 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:21,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:02:22,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-03-14 01:02:22,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:02:22,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:02:22,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:02:22,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:02:22,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-03-14 01:02:22,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (35.558 ms)
galaxy.jobs.handler INFO 2025-03-14 01:02:22,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:22,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 41
galaxy.jobs DEBUG 2025-03-14 01:02:22,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [41] prepared (57.904 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:02:22,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:02:22,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:02:22,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:02:22,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/41/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -O sam -o 0.sam '/galaxy/server/database/objects/e/6/e/dataset_e6ec96e4-3a9e-4e26-bc48-d700b9bf6a72.dat' &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -O sam -o 1.sam '/galaxy/server/database/objects/2/6/b/dataset_26b326b9-a842-439e-9bf2-cff4af3f3d64.dat' &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -O sam -o 2.sam '/galaxy/server/database/objects/f/d/9/dataset_fd9b3815-dcd6-492e-875a-a89459d71c83.dat' &&   samtools merge -@ $addthreads -s 1 -f   /galaxy/server/database/objects/b/3/d/dataset_b3dc8f3a-786e-45df-b548-fc1b1e1f034a.dat 0.sam 1.sam 2.sam]
galaxy.jobs.runners DEBUG 2025-03-14 01:02:22,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:22,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:02:22,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:02:22,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:02:22,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:22,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:23,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:36,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vpjss with k8s id: gxy-vpjss succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:02:36,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:02:43,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 41 finished
galaxy.model.metadata DEBUG 2025-03-14 01:02:43,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 63
galaxy.jobs INFO 2025-03-14 01:02:43,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-03-14 01:02:43,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 41 executed (139.324 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:43,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:02:44,873 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43, 42, 44
tpv.core.entities DEBUG 2025-03-14 01:02:44,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:02:44,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:02:44,903 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:02:44,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:02:44,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-03-14 01:02:44,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (33.858 ms)
galaxy.jobs.handler INFO 2025-03-14 01:02:44,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:44,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 42
tpv.core.entities DEBUG 2025-03-14 01:02:44,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:02:44,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:02:44,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:02:44,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:02:44,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-03-14 01:02:45,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (44.322 ms)
galaxy.jobs.handler INFO 2025-03-14 01:02:45,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 43
tpv.core.entities DEBUG 2025-03-14 01:02:45,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:02:45,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:02:45,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:02:45,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:02:45,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [42] prepared (91.020 ms)
galaxy.jobs DEBUG 2025-03-14 01:02:45,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.command_factory INFO 2025-03-14 01:02:45,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/42/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/42/configs/tmpu9v6jq3y']
galaxy.jobs.runners DEBUG 2025-03-14 01:02:45,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (62.934 ms)
galaxy.jobs.handler INFO 2025-03-14 01:02:45,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners DEBUG 2025-03-14 01:02:45,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 44
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:02:45,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [43] prepared (99.133 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:02:45,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/43/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/43/configs/tmp9cmom7q7']
galaxy.jobs.runners DEBUG 2025-03-14 01:02:45,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:02:45,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [44] prepared (90.919 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:02:45,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/44/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/44/configs/tmp15n91hqb']
galaxy.jobs.runners DEBUG 2025-03-14 01:02:45,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:45,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:46,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:46,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:46,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:55,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q6xh4 with k8s id: gxy-q6xh4 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:55,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d69wj with k8s id: gxy-d69wj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:02:55,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t6g8c with k8s id: gxy-t6g8c succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:02:55,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:02:55,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:02:55,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:03:07,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 42 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:03:07,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 44 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:03:07,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_a.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/42/working/gxupload_0', 'object_id': 64}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:03:07,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_c.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/44/working/gxupload_0', 'object_id': 66}]}]}]
galaxy.jobs INFO 2025-03-14 01:03:07,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs INFO 2025-03-14 01:03:07,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-03-14 01:03:07,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 42 executed (168.872 ms)
galaxy.jobs.runners DEBUG 2025-03-14 01:03:07,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 43 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:07,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:03:07,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_b.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/43/working/gxupload_0', 'object_id': 65}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:03:07,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 44 executed (202.357 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:07,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-14 01:03:07,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-03-14 01:03:07,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 43 executed (139.418 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:07,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:03:08,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-03-14 01:03:08,791 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:03:08,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:03:08,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:03:08,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:03:08,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-03-14 01:03:08,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (38.903 ms)
galaxy.jobs.handler INFO 2025-03-14 01:03:08,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:08,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-03-14 01:03:08,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [45] prepared (65.138 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:08,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:03:08,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:08,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:03:08,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/45/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   ln -s '/galaxy/server/database/objects/c/c/3/dataset_cc37d0b0-7acb-4ce7-8b34-5397a6f1ca24.dat' 0.sam &&  ln -s '/galaxy/server/database/objects/d/a/d/dataset_dad19df1-0f99-429e-bca2-69778e4c98f8.dat' 1.sam &&  ln -s '/galaxy/server/database/objects/0/4/c/dataset_04c1c094-fd2e-4585-9aa5-56748ddd84af.dat' 2.sam &&   samtools merge -@ $addthreads -s 1 -f   /galaxy/server/database/objects/0/c/f/dataset_0cf19fad-3cb1-46bf-9544-1869e49e7cd1.dat 0.sam 1.sam 2.sam]
galaxy.jobs.runners DEBUG 2025-03-14 01:03:08,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:08,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:08,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:03:08,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:09,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:09,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:09,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:13,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6wdbr with k8s id: gxy-6wdbr succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:03:14,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:03:21,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 45 finished
galaxy.model.metadata DEBUG 2025-03-14 01:03:21,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 67
galaxy.jobs INFO 2025-03-14 01:03:21,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-03-14 01:03:21,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 45 executed (132.198 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:21,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:03:22,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-03-14 01:03:22,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:03:22,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:03:22,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:03:22,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:03:22,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-03-14 01:03:22,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (30.994 ms)
galaxy.jobs.handler INFO 2025-03-14 01:03:22,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:22,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-03-14 01:03:22,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [46] prepared (57.575 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:03:22,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/46/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/46/configs/tmpln3yv753']
galaxy.jobs.runners DEBUG 2025-03-14 01:03:22,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:22,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:22,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:22,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:33,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hshsc with k8s id: gxy-hshsc succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:03:33,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:03:40,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 46 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:03:40,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_b.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/46/working/gxupload_0', 'object_id': 68}]}]}]
galaxy.jobs INFO 2025-03-14 01:03:40,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-03-14 01:03:40,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 46 executed (164.033 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:40,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:03:41,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-03-14 01:03:41,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:03:41,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:03:41,481 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:03:41,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:03:41,508 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-03-14 01:03:41,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (35.549 ms)
galaxy.jobs.handler INFO 2025-03-14 01:03:41,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:41,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 47
galaxy.jobs DEBUG 2025-03-14 01:03:41,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [47] prepared (46.002 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:41,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:03:41,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:41,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:03:41,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/47/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   ln -s '/galaxy/server/database/objects/f/c/d/dataset_fcda643f-add0-4125-96d5-ebbf8d56ed28.dat' 0.sam &&   samtools merge -@ $addthreads -s 1 -f   /galaxy/server/database/objects/8/b/f/dataset_8bf86e19-c775-424c-a4a6-e3bc0a23d3b8.dat 0.sam]
galaxy.jobs.runners DEBUG 2025-03-14 01:03:41,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:41,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:41,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:03:41,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:03:41,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:41,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:42,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:46,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p456z with k8s id: gxy-p456z succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:03:46,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:03:53,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 47 finished
galaxy.model.metadata DEBUG 2025-03-14 01:03:53,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 69
galaxy.jobs INFO 2025-03-14 01:03:53,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs DEBUG 2025-03-14 01:03:53,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 47 executed (118.837 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:53,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:03:54,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 48, 49
tpv.core.entities DEBUG 2025-03-14 01:03:54,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:03:54,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:03:54,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:03:54,785 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:03:54,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-03-14 01:03:54,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (32.140 ms)
galaxy.jobs.handler INFO 2025-03-14 01:03:54,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:54,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 48
tpv.core.entities DEBUG 2025-03-14 01:03:54,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:03:54,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:03:54,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:03:54,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:03:54,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-03-14 01:03:54,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (40.634 ms)
galaxy.jobs.handler INFO 2025-03-14 01:03:54,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:54,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-03-14 01:03:54,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [48] prepared (69.863 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:03:54,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/48/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/48/configs/tmpyv3z884b']
galaxy.jobs.runners DEBUG 2025-03-14 01:03:54,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:54,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:03:54,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [49] prepared (75.786 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:54,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:03:54,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/49/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/49/configs/tmp6bgh_j9a']
galaxy.jobs.runners DEBUG 2025-03-14 01:03:54,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:55,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:55,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:55,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:03:55,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:05,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qtt5j with k8s id: gxy-qtt5j succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:05,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kgzcb with k8s id: gxy-kgzcb succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:04:05,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:04:05,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:04:13,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 48 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:04:13,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 49 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:04:13,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_a.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/48/working/gxupload_0', 'object_id': 70}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:04:13,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_b.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/49/working/gxupload_0', 'object_id': 71}]}]}]
galaxy.jobs INFO 2025-03-14 01:04:13,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs INFO 2025-03-14 01:04:13,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs DEBUG 2025-03-14 01:04:13,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 48 executed (165.435 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:13,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:04:13,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 49 executed (159.789 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:13,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:04:14,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-03-14 01:04:14,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:04:14,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:04:14,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:04:14,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:04:14,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-03-14 01:04:14,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (40.823 ms)
galaxy.jobs.handler INFO 2025-03-14 01:04:14,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:14,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-03-14 01:04:14,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [50] prepared (59.823 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:14,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:04:14,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:14,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:04:14,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/50/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   ln -s '/galaxy/server/database/objects/a/7/7/dataset_a77ca667-fb54-418c-a207-39d52719aea1.dat' 0.sam &&  ln -s '/galaxy/server/database/objects/9/6/e/dataset_96e3905a-f78a-47d0-b050-23fa2d13b02e.dat' 1.sam &&   samtools merge -@ $addthreads -s 1 -f -c -p /galaxy/server/database/objects/a/0/a/dataset_a0afd352-b2db-4a1e-bfa8-76e335dcdf4b.dat 0.sam 1.sam]
galaxy.jobs.runners DEBUG 2025-03-14 01:04:14,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:14,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:14,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:04:14,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:14,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:14,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:14,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:18,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qx6jx with k8s id: gxy-qx6jx succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:04:19,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:04:26,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 50 finished
galaxy.model.metadata DEBUG 2025-03-14 01:04:26,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 72
galaxy.jobs INFO 2025-03-14 01:04:26,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-03-14 01:04:26,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 50 executed (120.031 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:26,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:04:27,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52, 51
tpv.core.entities DEBUG 2025-03-14 01:04:27,560 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:04:27,560 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:04:27,563 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:04:27,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:04:27,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-03-14 01:04:27,592 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (28.865 ms)
galaxy.jobs.handler INFO 2025-03-14 01:04:27,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:27,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 51
tpv.core.entities DEBUG 2025-03-14 01:04:27,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:04:27,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:04:27,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:04:27,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:04:27,647 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-03-14 01:04:27,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (40.216 ms)
galaxy.jobs.handler INFO 2025-03-14 01:04:27,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:27,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-03-14 01:04:27,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [51] prepared (76.235 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:04:27,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/51/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/51/configs/tmp6cwdq1sa']
galaxy.jobs.runners DEBUG 2025-03-14 01:04:27,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:27,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:04:27,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [52] prepared (79.388 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:27,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:04:27,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/52/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/52/configs/tmp1fgxwj9x']
galaxy.jobs.runners DEBUG 2025-03-14 01:04:27,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:27,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:27,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:29,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:29,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:38,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-54c9b with k8s id: gxy-54c9b succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:38,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kxlgj with k8s id: gxy-kxlgj succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:04:38,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:04:38,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:04:45,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 52 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:04:45,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 51 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:04:45,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_b_regex.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/52/working/data_fetch_upload_18v_9gfq', 'object_id': 74}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:04:45,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'test_input_1_a_regex.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/51/working/data_fetch_upload_rsh26ixf', 'object_id': 73}]}]}]
galaxy.jobs INFO 2025-03-14 01:04:46,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs INFO 2025-03-14 01:04:46,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs DEBUG 2025-03-14 01:04:46,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 52 executed (157.650 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:46,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:04:46,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 51 executed (162.802 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:46,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:04:47,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 53
tpv.core.entities DEBUG 2025-03-14 01:04:47,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:04:47,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:04:47,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:04:47,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:04:47,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-03-14 01:04:47,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (37.231 ms)
galaxy.jobs.handler INFO 2025-03-14 01:04:47,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:47,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 53
galaxy.jobs DEBUG 2025-03-14 01:04:47,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [53] prepared (39.721 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:47,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:04:47,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:47,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:04:47,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/53/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -O sam -o 0.sam '/galaxy/server/database/objects/a/b/1/dataset_ab1201d0-f700-4bcd-8e09-356bb65eb8eb.dat' &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -O sam -o 1.sam '/galaxy/server/database/objects/3/3/9/dataset_339ed8a0-fe68-433e-a390-c58f2494044e.dat' &&   samtools merge -@ $addthreads -s 1 -f   /galaxy/server/database/objects/2/d/7/dataset_2d72ecef-4949-42a4-9f9f-94a4bf9d52a4.dat 0.sam 1.sam]
galaxy.jobs.runners DEBUG 2025-03-14 01:04:47,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:47,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:47,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:04:47,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-14 01:04:47,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:47,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:48,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:51,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b48vq with k8s id: gxy-b48vq succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:04:51,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:04:58,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 53 finished
galaxy.model.metadata DEBUG 2025-03-14 01:04:58,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 75
galaxy.jobs INFO 2025-03-14 01:04:58,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs DEBUG 2025-03-14 01:04:58,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 53 executed (114.808 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:04:58,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:05:01,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54
tpv.core.entities DEBUG 2025-03-14 01:05:01,457 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:05:01,457 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:05:01,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:05:01,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:05:01,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-03-14 01:05:01,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (29.023 ms)
galaxy.jobs.handler INFO 2025-03-14 01:05:01,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:01,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 54
galaxy.jobs DEBUG 2025-03-14 01:05:01,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [54] prepared (59.349 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:05:01,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/54/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/54/configs/tmpa8yqodte']
galaxy.jobs.runners DEBUG 2025-03-14 01:05:01,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:01,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:01,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:02,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-svsr4 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-svsr4.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:05:11,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-svsr4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-svsr4": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-svsr4) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-svsr4) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-svsr4) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-svsr4) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-svsr4.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 54 (gxy-svsr4)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-svsr4 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:11,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54/gxy-svsr4) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-14 01:05:12,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 55
tpv.core.entities DEBUG 2025-03-14 01:05:12,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:05:12,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:05:12,697 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:05:12,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:05:12,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-03-14 01:05:12,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (34.965 ms)
galaxy.jobs.handler INFO 2025-03-14 01:05:12,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:12,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 55
galaxy.jobs DEBUG 2025-03-14 01:05:12,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [55] prepared (60.264 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:05:12,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/55/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/55/configs/tmpwekh_s26']
galaxy.jobs.runners DEBUG 2025-03-14 01:05:12,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:12,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:12,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:13,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:22,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ktsgs with k8s id: gxy-ktsgs succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:05:23,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:05:29,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 55 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:05:29,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam_to_sam_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/55/working/gxupload_0', 'object_id': 77}]}]}]
galaxy.jobs INFO 2025-03-14 01:05:30,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-03-14 01:05:30,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 55 executed (154.137 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:30,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:05:31,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56
tpv.core.entities DEBUG 2025-03-14 01:05:31,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:05:31,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:05:31,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:05:31,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:05:31,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-03-14 01:05:31,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (35.351 ms)
galaxy.jobs.handler INFO 2025-03-14 01:05:31,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:31,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 56
galaxy.jobs DEBUG 2025-03-14 01:05:31,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [56] prepared (43.609 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:05:31,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:05:31,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bam_to_sam/bam_to_sam/2.0.4: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-03-14 01:05:31,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:05:31,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/56/outputs/COMMAND_VERSION 2>&1;
samtools view -o '/galaxy/server/database/objects/d/f/a/dataset_dfa80620-c60e-406e-b1dc-cd071a5948b8.dat' -H '/galaxy/server/database/objects/6/6/8/dataset_6687949d-a964-4ccf-a00a-d3a7496313b2.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:05:31,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:31,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:05:31,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:05:31,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bam_to_sam/bam_to_sam/2.0.4: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-03-14 01:05:31,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:31,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:31,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:39,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cwj9w with k8s id: gxy-cwj9w succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:05:39,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 56: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:05:46,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 56 finished
galaxy.model.metadata DEBUG 2025-03-14 01:05:46,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.jobs INFO 2025-03-14 01:05:46,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 56 in /galaxy/server/database/jobs_directory/000/56
galaxy.jobs DEBUG 2025-03-14 01:05:46,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 56 executed (119.122 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:46,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:05:48,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 57
tpv.core.entities DEBUG 2025-03-14 01:05:48,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:05:48,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:05:48,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:05:48,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:05:48,506 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-03-14 01:05:48,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (34.436 ms)
galaxy.jobs.handler INFO 2025-03-14 01:05:48,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:48,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 57
galaxy.jobs DEBUG 2025-03-14 01:05:48,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [57] prepared (66.221 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:05:48,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/57/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/57/configs/tmpqr2q5p8w']
galaxy.jobs.runners DEBUG 2025-03-14 01:05:48,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:48,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:48,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:49,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:05:59,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-slcq5 with k8s id: gxy-slcq5 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:05:59,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 57: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:06:06,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 57 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:06:06,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam_to_sam_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/57/working/gxupload_0', 'object_id': 79}]}]}]
galaxy.jobs INFO 2025-03-14 01:06:07,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 57 in /galaxy/server/database/jobs_directory/000/57
galaxy.jobs DEBUG 2025-03-14 01:06:07,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 57 executed (147.075 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:07,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:06:07,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58
tpv.core.entities DEBUG 2025-03-14 01:06:07,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:06:07,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:06:07,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:06:07,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:06:07,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-03-14 01:06:07,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (31.642 ms)
galaxy.jobs.handler INFO 2025-03-14 01:06:07,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:07,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-03-14 01:06:07,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [58] prepared (51.563 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:07,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:06:07,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bam_to_sam/bam_to_sam/2.0.4: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:08,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:06:08,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/58/outputs/COMMAND_VERSION 2>&1;
samtools view -o '/galaxy/server/database/objects/8/9/4/dataset_8945217a-407d-463f-bb32-7c831e5038e8.dat'  '/galaxy/server/database/objects/d/4/6/dataset_d465f220-29b4-4b40-929d-5a8714e781e6.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:06:08,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:08,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:08,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:06:08,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bam_to_sam/bam_to_sam/2.0.4: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:08,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:08,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:08,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gkr95 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gkr95.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:06:12,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-gkr95

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-gkr95": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (58/gxy-gkr95) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (58/gxy-gkr95) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (58/gxy-gkr95) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (58/gxy-gkr95) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gkr95.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:12,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 58 (gxy-gkr95)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:13,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-gkr95 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:13,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:13,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (58/gxy-gkr95) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-14 01:06:15,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-03-14 01:06:15,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:06:15,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:06:15,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:06:15,095 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:06:15,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-03-14 01:06:15,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (40.358 ms)
galaxy.jobs.handler INFO 2025-03-14 01:06:15,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:15,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-03-14 01:06:15,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [59] prepared (74.397 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:06:15,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/59/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/59/configs/tmph2uxdhv3']
galaxy.jobs.runners DEBUG 2025-03-14 01:06:15,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:15,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:15,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:16,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:25,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-86vm9 with k8s id: gxy-86vm9 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:06:25,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:06:32,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 59 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:06:32,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'plugin1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/59/working/data_fetch_upload_y0_mx9i_', 'object_id': 81}]}]}]
galaxy.jobs INFO 2025-03-14 01:06:32,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-03-14 01:06:32,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 59 executed (132.871 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:32,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:06:33,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-03-14 01:06:33,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:06:33,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:06:33,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:06:33,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:06:33,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-03-14 01:06:33,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (35.630 ms)
galaxy.jobs.handler INFO 2025-03-14 01:06:33,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:33,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-03-14 01:06:33,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [60] prepared (67.280 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:33,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:06:33,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_missing2ref/bcftools_plugin_missing2ref/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:33,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:06:33,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/60/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/a/b/5/dataset_ab5d2f0c-b376-429d-a490-03d0ae251398.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools plugin missing2ref                 --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz   > '/galaxy/server/database/objects/1/8/1/dataset_1816f91e-8205-4866-9538-5c73e3d72b8f.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:06:33,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:33,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:33,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:06:33,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_missing2ref/bcftools_plugin_missing2ref/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:06:33,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:33,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:34,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:48,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hlxr7 with k8s id: gxy-hlxr7 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:06:48,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:06:56,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 60 finished
galaxy.model.metadata DEBUG 2025-03-14 01:06:56,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 82
galaxy.jobs INFO 2025-03-14 01:06:56,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-03-14 01:06:56,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 60 executed (121.755 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:56,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:06:58,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63, 61, 62
tpv.core.entities DEBUG 2025-03-14 01:06:59,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:06:59,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:06:59,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:06:59,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:06:59,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-03-14 01:06:59,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (36.201 ms)
galaxy.jobs.handler INFO 2025-03-14 01:06:59,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 61
tpv.core.entities DEBUG 2025-03-14 01:06:59,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:06:59,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:06:59,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:06:59,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:06:59,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-03-14 01:06:59,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (45.714 ms)
galaxy.jobs.handler INFO 2025-03-14 01:06:59,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 62
tpv.core.entities DEBUG 2025-03-14 01:06:59,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:06:59,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:06:59,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:06:59,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:06:59,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [61] prepared (88.611 ms)
galaxy.jobs DEBUG 2025-03-14 01:06:59,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.command_factory INFO 2025-03-14 01:06:59,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/61/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/61/configs/tmp0b57sio4']
galaxy.jobs.runners DEBUG 2025-03-14 01:06:59,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (58.412 ms)
galaxy.jobs.handler INFO 2025-03-14 01:06:59,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 63
galaxy.jobs.runners DEBUG 2025-03-14 01:06:59,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:06:59,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [62] prepared (92.097 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:06:59,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/62/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/62/configs/tmpg0dfifbl']
galaxy.jobs.runners DEBUG 2025-03-14 01:06:59,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:06:59,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [63] prepared (74.399 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:06:59,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/63/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/63/configs/tmpfc7asvo4']
galaxy.jobs.runners DEBUG 2025-03-14 01:06:59,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:06:59,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ntg97 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-ntg97.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:07:09,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-ntg97

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-ntg97": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pb2jq failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (62/gxy-ntg97) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (62/gxy-ntg97) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (62/gxy-ntg97) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (62/gxy-ntg97) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-ntg97.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 62 (gxy-ntg97)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-ntg97 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-pb2jq.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:07:09,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-pb2jq

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-pb2jq": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (62/gxy-ntg97) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-pb2jq) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-pb2jq) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-pb2jq) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-pb2jq) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-pb2jq.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 63 (gxy-pb2jq)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-pb2jq to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:09,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-pb2jq) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:10,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6nkdc with k8s id: gxy-6nkdc succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:07:10,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 61: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:07:17,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 61 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:07:17,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/61/working/data_fetch_upload_pdp0khy2', 'object_id': 83}]}]}]
galaxy.jobs INFO 2025-03-14 01:07:17,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 61 in /galaxy/server/database/jobs_directory/000/61
galaxy.jobs DEBUG 2025-03-14 01:07:17,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 61 executed (137.382 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:17,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:07:18,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65, 64
tpv.core.entities DEBUG 2025-03-14 01:07:18,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:07:18,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:07:18,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:07:18,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:07:18,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-03-14 01:07:18,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (35.552 ms)
galaxy.jobs.handler INFO 2025-03-14 01:07:18,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:18,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 64
tpv.core.entities DEBUG 2025-03-14 01:07:18,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:07:18,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:07:18,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:07:18,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:07:18,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-03-14 01:07:18,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (50.328 ms)
galaxy.jobs.handler INFO 2025-03-14 01:07:18,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:18,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-03-14 01:07:18,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [64] prepared (82.951 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:07:18,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/64/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/64/configs/tmpd_d2uw7t']
galaxy.jobs.runners DEBUG 2025-03-14 01:07:18,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:07:18,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [65] prepared (67.486 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:18,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:18,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:07:18,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/65/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/65/configs/tmpl0h_jfq4']
galaxy.jobs.runners DEBUG 2025-03-14 01:07:18,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:18,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:18,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:19,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:19,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:28,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bsdv4 with k8s id: gxy-bsdv4 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:28,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hn8ld with k8s id: gxy-hn8ld succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:07:28,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:07:28,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:07:36,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 64 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:07:36,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 65 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:07:36,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.2.a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/64/working/data_fetch_upload_e8raw377', 'object_id': 86}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:07:36,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.2.b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/65/working/data_fetch_upload_knbhxc2c', 'object_id': 87}]}]}]
galaxy.jobs INFO 2025-03-14 01:07:36,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.jobs INFO 2025-03-14 01:07:36,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-03-14 01:07:36,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 64 executed (164.331 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:36,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:07:36,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 65 executed (162.578 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:36,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:07:37,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-03-14 01:07:37,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:07:37,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:07:37,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:07:37,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:07:37,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-03-14 01:07:37,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (37.566 ms)
galaxy.jobs.handler INFO 2025-03-14 01:07:37,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:37,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-03-14 01:07:37,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [66] prepared (61.324 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:07:37,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:07:37,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:07:37,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:07:37,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/66/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/2/1/f/dataset_21f20cf6-f5dd-42e4-9776-fd10fc6808d7.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/4/9/b/dataset_49b4d2b0-eddd-49cf-b436-f23861286de0.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools merge    --force-samples   --merge "none"            --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/d/e/a/dataset_dea875b1-16a2-41a0-9463-92508b476213.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:07:37,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:37,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:07:37,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:07:37,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:07:37,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:37,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:37,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:41,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rhzrz with k8s id: gxy-rhzrz succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:07:42,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:07:49,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 66 finished
galaxy.model.metadata DEBUG 2025-03-14 01:07:49,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 88
galaxy.jobs INFO 2025-03-14 01:07:49,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.jobs DEBUG 2025-03-14 01:07:49,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 66 executed (110.691 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:49,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:07:50,508 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67, 68
tpv.core.entities DEBUG 2025-03-14 01:07:50,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:07:50,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:07:50,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:07:50,554 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:07:50,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-03-14 01:07:50,581 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (39.313 ms)
galaxy.jobs.handler INFO 2025-03-14 01:07:50,584 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:50,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 67
tpv.core.entities DEBUG 2025-03-14 01:07:50,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:07:50,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:07:50,596 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:07:50,611 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:07:50,633 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-03-14 01:07:50,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (42.580 ms)
galaxy.jobs.handler INFO 2025-03-14 01:07:50,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:50,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-03-14 01:07:50,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [67] prepared (78.202 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:07:50,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/67/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/67/configs/tmp2j9tj0_s']
galaxy.jobs.runners DEBUG 2025-03-14 01:07:50,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:07:50,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [68] prepared (69.107 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:50,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:50,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:07:50,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/68/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/68/configs/tmp_8y5ytk2']
galaxy.jobs.runners DEBUG 2025-03-14 01:07:50,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:50,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:50,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:50,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:07:51,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:00,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2dtvx with k8s id: gxy-2dtvx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:00,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-k79cd with k8s id: gxy-k79cd succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:08:00,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:08:00,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 68: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:08:08,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 68 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:08:08,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.2.b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/68/working/data_fetch_upload_gd39_61_', 'object_id': 90}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:08:08,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 67 finished
galaxy.jobs INFO 2025-03-14 01:08:08,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 68 in /galaxy/server/database/jobs_directory/000/68
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:08:08,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.2.a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/67/working/data_fetch_upload_43m5neih', 'object_id': 89}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:08:08,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 68 executed (155.944 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:08,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-14 01:08:08,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs DEBUG 2025-03-14 01:08:08,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 67 executed (133.935 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:08,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:08:09,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69
tpv.core.entities DEBUG 2025-03-14 01:08:09,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:08:09,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:08:09,042 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:08:09,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:08:09,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-03-14 01:08:09,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (40.881 ms)
galaxy.jobs.handler INFO 2025-03-14 01:08:09,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:09,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 69
galaxy.jobs DEBUG 2025-03-14 01:08:09,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [69] prepared (49.458 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:09,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:08:09,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:09,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:08:09,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/69/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/6/2/dataset_b62b223f-74a8-41dd-a94e-35d885023a9f.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/8/0/a/dataset_80a1f0e9-f7f4-4aa5-9b6d-8b94540101fe.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools merge    --force-samples   --merge "both"            --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/c/8/9/dataset_c893a703-ca4c-4755-80d7-6c2b3b5c4c2a.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:08:09,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:09,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:09,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:08:09,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:09,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:09,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:09,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:13,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jwfgw with k8s id: gxy-jwfgw succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:08:13,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:08:21,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 69 finished
galaxy.model.metadata DEBUG 2025-03-14 01:08:21,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 91
galaxy.jobs INFO 2025-03-14 01:08:21,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs DEBUG 2025-03-14 01:08:21,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 69 executed (117.917 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:21,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:08:22,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 70
tpv.core.entities DEBUG 2025-03-14 01:08:22,351 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:08:22,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:08:22,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:08:22,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:08:22,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-03-14 01:08:22,387 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (32.565 ms)
galaxy.jobs.handler INFO 2025-03-14 01:08:22,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:22,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 70
galaxy.jobs DEBUG 2025-03-14 01:08:22,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [70] prepared (67.520 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:08:22,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/70/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/70/configs/tmpdjm_oqub']
galaxy.jobs.runners DEBUG 2025-03-14 01:08:22,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:22,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:22,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:22,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-14 01:08:23,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71
tpv.core.entities DEBUG 2025-03-14 01:08:23,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:08:23,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:08:23,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:08:23,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:08:23,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-03-14 01:08:23,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (31.537 ms)
galaxy.jobs.handler INFO 2025-03-14 01:08:23,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:23,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-03-14 01:08:23,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [71] prepared (73.125 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:08:23,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/71/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/71/configs/tmplrx5bvkm']
galaxy.jobs.runners DEBUG 2025-03-14 01:08:23,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:23,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:23,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:23,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-96g2g failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-96g2g.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:08:32,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-96g2g

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-96g2g": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (70/gxy-96g2g) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (70/gxy-96g2g) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (70/gxy-96g2g) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (70/gxy-96g2g) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-96g2g.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 70 (gxy-96g2g)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-96g2g to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:32,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (70/gxy-96g2g) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:33,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rckhq with k8s id: gxy-rckhq succeeded
galaxy.jobs.handler DEBUG 2025-03-14 01:08:33,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72, 73
tpv.core.entities DEBUG 2025-03-14 01:08:33,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:08:33,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:08:33,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:08:33,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs.runners DEBUG 2025-03-14 01:08:33,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 71: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs DEBUG 2025-03-14 01:08:33,685 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-03-14 01:08:33,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (44.087 ms)
galaxy.jobs.handler INFO 2025-03-14 01:08:33,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:33,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 72
tpv.core.entities DEBUG 2025-03-14 01:08:33,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:08:33,711 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:08:33,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:08:33,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:08:33,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-03-14 01:08:33,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (45.101 ms)
galaxy.jobs.handler INFO 2025-03-14 01:08:33,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:33,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-03-14 01:08:33,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [72] prepared (80.658 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:08:33,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/72/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/72/configs/tmp18ojgmlp']
galaxy.jobs.runners DEBUG 2025-03-14 01:08:33,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:33,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:08:33,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [73] prepared (69.815 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:33,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:08:33,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/73/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/73/configs/tmpz3p9_vi9']
galaxy.jobs.runners DEBUG 2025-03-14 01:08:33,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:33,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:33,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:34,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:34,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-14 01:08:41,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 71 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:08:41,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.2.b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/71/working/data_fetch_upload_1ey16qni', 'object_id': 93}]}]}]
galaxy.jobs INFO 2025-03-14 01:08:41,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 71 in /galaxy/server/database/jobs_directory/000/71
galaxy.jobs DEBUG 2025-03-14 01:08:41,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 71 executed (130.072 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:41,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:43,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zpsr9 with k8s id: gxy-zpsr9 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:43,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r6vzf with k8s id: gxy-r6vzf succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:08:44,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:08:44,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:08:51,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 73 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:08:51,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 72 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:08:51,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.3.b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/73/working/data_fetch_upload_fj9xiuvj', 'object_id': 95}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:08:51,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.3.a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/72/working/data_fetch_upload_139wuo3h', 'object_id': 94}]}]}]
galaxy.jobs INFO 2025-03-14 01:08:51,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs INFO 2025-03-14 01:08:51,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs DEBUG 2025-03-14 01:08:52,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 73 executed (165.849 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:52,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:08:52,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 72 executed (155.737 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:52,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:08:53,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-03-14 01:08:53,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:08:53,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:08:53,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:08:53,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:08:53,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-03-14 01:08:53,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (39.960 ms)
galaxy.jobs.handler INFO 2025-03-14 01:08:53,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:53,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-03-14 01:08:53,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [74] prepared (53.841 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:53,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:08:53,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:53,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:08:53,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/74/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/d/a/6/dataset_da635998-f231-4e35-8dd8-6aa0a7dd281d.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/e/2/9/dataset_e2936a94-8f4f-48eb-9281-2b107c21d1e5.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools merge    --force-samples  --info-rules "TR:sum,TA:sum,TG:sum"            --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/3/4/c/dataset_34c6325a-f2b0-45c7-b454-43dd9352ce5b.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:08:53,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:53,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:53,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:08:53,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:08:53,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:53,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:53,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:08:58,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9s4gx with k8s id: gxy-9s4gx succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:08:58,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 74: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:09:05,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 74 finished
galaxy.model.metadata DEBUG 2025-03-14 01:09:05,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 96
galaxy.jobs INFO 2025-03-14 01:09:05,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 74 in /galaxy/server/database/jobs_directory/000/74
galaxy.jobs DEBUG 2025-03-14 01:09:05,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 74 executed (123.010 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:05,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:09:07,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76, 75
tpv.core.entities DEBUG 2025-03-14 01:09:07,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:09:07,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:09:07,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:09:07,506 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:09:07,522 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-03-14 01:09:07,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (37.267 ms)
galaxy.jobs.handler INFO 2025-03-14 01:09:07,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:07,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 75
tpv.core.entities DEBUG 2025-03-14 01:09:07,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:09:07,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:09:07,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:09:07,570 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:09:07,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-03-14 01:09:07,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (49.904 ms)
galaxy.jobs.handler INFO 2025-03-14 01:09:07,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:07,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-03-14 01:09:07,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [75] prepared (81.661 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:09:07,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/75/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/75/configs/tmpo_u2iosd']
galaxy.jobs.runners DEBUG 2025-03-14 01:09:07,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:07,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:09:07,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [76] prepared (72.230 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:07,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:09:07,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/76/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/76/configs/tmpew_5gtw4']
galaxy.jobs.runners DEBUG 2025-03-14 01:09:07,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:07,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:07,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:08,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:08,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:17,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lptjs with k8s id: gxy-lptjs succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:17,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7lwc5 with k8s id: gxy-7lwc5 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:09:17,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:09:17,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:09:25,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 76 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:09:25,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 75 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:09:25,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.4.b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/76/working/data_fetch_upload_b8eai3_6', 'object_id': 98}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:09:25,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.4.a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/75/working/data_fetch_upload_93a3bhef', 'object_id': 97}]}]}]
galaxy.jobs INFO 2025-03-14 01:09:25,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.jobs INFO 2025-03-14 01:09:25,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-03-14 01:09:25,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 76 executed (160.769 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:25,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:09:25,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 75 executed (152.196 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:25,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:09:26,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-03-14 01:09:26,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:09:26,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:09:26,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:09:26,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:09:27,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-03-14 01:09:27,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (34.383 ms)
galaxy.jobs.handler INFO 2025-03-14 01:09:27,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:27,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-03-14 01:09:27,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [77] prepared (56.489 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:27,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:09:27,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:27,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:09:27,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/77/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/1/f/7/dataset_1f79db03-22d6-454a-b5ca-6ac895f14d61.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/a/f/2/dataset_af2d782e-4b24-47ed-a97f-3e5363dd7d20.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools merge    --force-samples   --merge "id" --no-index           --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/a/c/f/dataset_acfa5e37-302b-45bc-bf34-f1a4b54694a2.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:09:27,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:27,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:27,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:09:27,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:27,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:27,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:28,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:32,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rfhqm with k8s id: gxy-rfhqm succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:09:33,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:09:40,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 77 finished
galaxy.model.metadata DEBUG 2025-03-14 01:09:40,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 99
galaxy.jobs INFO 2025-03-14 01:09:40,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-03-14 01:09:40,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 77 executed (123.111 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:40,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:09:41,257 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79, 78
tpv.core.entities DEBUG 2025-03-14 01:09:41,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:09:41,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:09:41,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:09:41,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:09:41,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-03-14 01:09:41,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (33.468 ms)
galaxy.jobs.handler INFO 2025-03-14 01:09:41,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:41,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 78
tpv.core.entities DEBUG 2025-03-14 01:09:41,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:09:41,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:09:41,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:09:41,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:09:41,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-03-14 01:09:41,387 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (43.725 ms)
galaxy.jobs.handler INFO 2025-03-14 01:09:41,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:41,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-03-14 01:09:41,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [78] prepared (74.670 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:09:41,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/78/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/78/configs/tmpkty_tprx']
galaxy.jobs.runners DEBUG 2025-03-14 01:09:41,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:09:41,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [79] prepared (70.712 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:41,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:41,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:09:41,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/79/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/79/configs/tmprc6jsvwi']
galaxy.jobs.runners DEBUG 2025-03-14 01:09:41,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:41,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:41,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:41,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:42,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:51,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6xt8d with k8s id: gxy-6xt8d succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:51,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-znp8t with k8s id: gxy-znp8t succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:09:51,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:09:51,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:09:58,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 78 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:09:58,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.4.a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/78/working/data_fetch_upload_x94d4w0b', 'object_id': 100}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:09:58,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 79 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:09:59,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'merge.4.b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/79/working/data_fetch_upload_erzk2i0v', 'object_id': 101}]}]}]
galaxy.jobs INFO 2025-03-14 01:09:59,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.jobs INFO 2025-03-14 01:09:59,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-03-14 01:09:59,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 78 executed (167.653 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:59,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:09:59,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 79 executed (158.434 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:59,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:09:59,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 80
tpv.core.entities DEBUG 2025-03-14 01:09:59,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:09:59,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:09:59,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:09:59,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:09:59,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-03-14 01:09:59,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (34.263 ms)
galaxy.jobs.handler INFO 2025-03-14 01:09:59,785 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:59,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 80
galaxy.jobs DEBUG 2025-03-14 01:09:59,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [80] prepared (53.401 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:59,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:09:59,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:59,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:09:59,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/80/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/b/8/dataset_3b84cddf-b1de-4d2a-b727-13e4f2a81588.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/a/6/f/dataset_a6feeffa-893e-4004-9ebf-085588f4e57c.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools merge    --force-samples   --merge "id"       --regions-overlap 1      --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/5/9/8/dataset_598a5f2e-7847-492e-95c1-2a330b123d55.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:09:59,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:59,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:59,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:09:59,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_merge/bcftools_merge/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-14 01:09:59,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:09:59,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:00,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:05,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n5trw with k8s id: gxy-n5trw succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:10:05,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:10:12,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 80 finished
galaxy.model.metadata DEBUG 2025-03-14 01:10:13,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 102
galaxy.jobs INFO 2025-03-14 01:10:13,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-03-14 01:10:13,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 80 executed (133.539 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:13,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:10:15,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81
tpv.core.entities DEBUG 2025-03-14 01:10:15,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:10:15,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:10:15,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:10:15,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:10:15,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-03-14 01:10:15,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (33.099 ms)
galaxy.jobs.handler INFO 2025-03-14 01:10:15,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:15,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-03-14 01:10:15,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [81] prepared (58.602 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:10:15,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/81/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/81/configs/tmpml5p133d']
galaxy.jobs.runners DEBUG 2025-03-14 01:10:15,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:15,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:15,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:15,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-14 01:10:16,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-03-14 01:10:16,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:10:16,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:10:16,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:10:16,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:10:16,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-03-14 01:10:16,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (32.761 ms)
galaxy.jobs.handler INFO 2025-03-14 01:10:16,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:16,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-03-14 01:10:16,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [82] prepared (66.097 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:10:16,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/82/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/82/configs/tmpv24n4_39']
galaxy.jobs.runners DEBUG 2025-03-14 01:10:16,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:16,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:16,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:16,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v5xj2 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-v5xj2.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:10:24,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-v5xj2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-v5xj2": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (81/gxy-v5xj2) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (81/gxy-v5xj2) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (81/gxy-v5xj2) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (81/gxy-v5xj2) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-v5xj2.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 81 (gxy-v5xj2)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-v5xj2 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:24,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:25,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (81/gxy-v5xj2) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:25,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lfpxv with k8s id: gxy-lfpxv succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:10:26,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 82: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-14 01:10:27,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83
tpv.core.entities DEBUG 2025-03-14 01:10:27,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:10:27,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:10:27,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:10:27,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:10:27,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-03-14 01:10:27,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (33.448 ms)
galaxy.jobs.handler INFO 2025-03-14 01:10:27,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:27,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 83
galaxy.jobs DEBUG 2025-03-14 01:10:27,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [83] prepared (66.014 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:10:27,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/83/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/83/configs/tmpra78gjtf']
galaxy.jobs.runners DEBUG 2025-03-14 01:10:27,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:27,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:27,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:29,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-14 01:10:33,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 82 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:10:33,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcfannotategenotypes-input2.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/82/working/data_fetch_upload_m5mj66m2', 'object_id': 104}]}]}]
galaxy.jobs INFO 2025-03-14 01:10:33,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 82 in /galaxy/server/database/jobs_directory/000/82
galaxy.jobs DEBUG 2025-03-14 01:10:33,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 82 executed (131.037 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:33,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:38,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c9rfv with k8s id: gxy-c9rfv succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:10:38,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:10:45,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 83 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:10:45,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcfsort-test1-input.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/83/working/data_fetch_upload_0mx16jdx', 'object_id': 105}]}]}]
galaxy.jobs INFO 2025-03-14 01:10:45,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.jobs DEBUG 2025-03-14 01:10:45,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 83 executed (151.684 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:45,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:10:46,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 84
tpv.core.entities DEBUG 2025-03-14 01:10:46,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:10:46,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:10:46,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:10:46,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:10:46,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-03-14 01:10:46,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (34.211 ms)
galaxy.jobs.handler INFO 2025-03-14 01:10:46,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:46,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-03-14 01:10:46,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [84] prepared (52.088 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:10:46,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:10:46,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfsort/vcfsort/1.0.0_rc3+galaxy0: coreutils:8.31
galaxy.tool_util.deps.containers INFO 2025-03-14 01:10:47,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/coreutils:8.31--h14c3975_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:10:47,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [(grep ^"#" "/galaxy/server/database/objects/f/8/8/dataset_f88e22a1-e09c-4fb6-8caf-e4c011a38ad0.dat" && grep -v ^"#" "/galaxy/server/database/objects/f/8/8/dataset_f88e22a1-e09c-4fb6-8caf-e4c011a38ad0.dat" | LC_ALL=C sort -k1,1 -k2,2n -V) > '/galaxy/server/database/objects/6/0/4/dataset_604fe4df-2642-42d8-81a8-bd6e667a36eb.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:10:47,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:47,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:10:47,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:10:47,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfsort/vcfsort/1.0.0_rc3+galaxy0: coreutils:8.31
galaxy.tool_util.deps.containers INFO 2025-03-14 01:10:47,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/coreutils:8.31--h14c3975_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:47,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:48,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:10:59,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sd66v with k8s id: gxy-sd66v succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:10:59,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:11:07,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 84 finished
galaxy.model.metadata DEBUG 2025-03-14 01:11:07,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 106
galaxy.jobs INFO 2025-03-14 01:11:07,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-03-14 01:11:07,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 84 executed (107.254 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:07,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:11:09,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-03-14 01:11:09,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:11:09,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:11:09,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:11:09,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:11:09,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-03-14 01:11:09,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (34.068 ms)
galaxy.jobs.handler INFO 2025-03-14 01:11:09,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:09,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-03-14 01:11:09,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [85] prepared (77.444 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:11:09,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/85/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/85/configs/tmpabub2liz']
galaxy.jobs.runners DEBUG 2025-03-14 01:11:09,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:09,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:09,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:09,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:19,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qcm4f with k8s id: gxy-qcm4f succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:11:20,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:11:27,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 85 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:11:27,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/85/working/gxupload_0', 'object_id': 107}]}]}]
galaxy.jobs INFO 2025-03-14 01:11:27,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-03-14 01:11:27,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 85 executed (356.429 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:27,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:11:28,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 86
tpv.core.entities DEBUG 2025-03-14 01:11:28,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_events/poretools_events/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:11:28,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:11:28,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:11:28,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:11:28,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-03-14 01:11:28,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (36.902 ms)
galaxy.jobs.handler INFO 2025-03-14 01:11:28,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:28,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 86
galaxy.jobs DEBUG 2025-03-14 01:11:28,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [86] prepared (42.333 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:11:28,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:11:28,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_events/poretools_events/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:11:29,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:11:29,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [poretools events  '/galaxy/server/database/objects/0/6/1/dataset_0610f76e-2909-400a-8427-5f23e32ba9e6.dat' > '/galaxy/server/database/objects/0/6/c/dataset_06c5ca55-f48b-49f7-8f7c-e256c283cc6a.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:11:29,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:29,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:11:29,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:11:29,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_events/poretools_events/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:11:29,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:29,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:11:29,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:22,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hvd8g failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:22,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:23,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job: gxy-hvd8g failed due to an unknown exit code from the tool.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:23,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 01:12:23,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:12:30,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 86 finished
galaxy.tool_util.output_checker INFO 2025-03-14 01:12:30,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job error detected, failing job. Reasons are [{'type': 'exit_code', 'desc': 'Fatal error: Exit code 1 ()', 'exit_code': 1, 'code_desc': '', 'error_level': 3}, {'type': 'regex', 'stream': 'stderr', 'desc': 'Fatal error: ', 'code_desc': '', 'match': 'Error:', 'error_level': 3}]
galaxy.jobs DEBUG 2025-03-14 01:12:30,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (86) setting dataset 108 state to ERROR
galaxy.jobs INFO 2025-03-14 01:12:30,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.jobs DEBUG 2025-03-14 01:12:30,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 86 executed (96.194 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:30,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:12:32,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87
tpv.core.entities DEBUG 2025-03-14 01:12:32,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:12:32,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:12:32,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:12:32,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:12:32,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-03-14 01:12:32,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (36.877 ms)
galaxy.jobs.handler INFO 2025-03-14 01:12:32,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:32,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-03-14 01:12:32,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [87] prepared (69.483 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:12:32,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/87/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/87/configs/tmp23h7u99f']
galaxy.jobs.runners DEBUG 2025-03-14 01:12:32,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:32,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:32,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:33,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:43,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbp64 with k8s id: gxy-vbp64 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:12:43,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:12:50,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 87 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:12:50,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/87/working/gxupload_0', 'object_id': 109}]}]}]
galaxy.jobs INFO 2025-03-14 01:12:50,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs DEBUG 2025-03-14 01:12:50,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 87 executed (263.870 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:50,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:12:51,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-03-14 01:12:51,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_events/poretools_events/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:12:51,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:12:51,481 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:12:51,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:12:51,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-03-14 01:12:51,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (31.594 ms)
galaxy.jobs.handler INFO 2025-03-14 01:12:51,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:51,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-03-14 01:12:51,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [88] prepared (37.846 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:12:51,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:12:51,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_events/poretools_events/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:12:51,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:12:51,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [poretools events --pre-basecalled '/galaxy/server/database/objects/f/e/f/dataset_fef04a60-843d-4000-9955-32b5c42a193c.dat' > '/galaxy/server/database/objects/4/f/7/dataset_4f7c7c79-31f1-4a0c-aabe-0d51e76de5e5.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:12:51,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:51,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:12:51,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:12:51,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_events/poretools_events/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:12:51,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:51,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:52,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:57,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bnnbl failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:57,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:57,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job: gxy-bnnbl failed due to an unknown exit code from the tool.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:12:57,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 01:12:58,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:13:05,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 88 finished
galaxy.tool_util.output_checker INFO 2025-03-14 01:13:05,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job error detected, failing job. Reasons are [{'type': 'exit_code', 'desc': 'Fatal error: Exit code 1 ()', 'exit_code': 1, 'code_desc': '', 'error_level': 3}, {'type': 'regex', 'stream': 'stderr', 'desc': 'Fatal error: ', 'code_desc': '', 'match': 'Error:', 'error_level': 3}]
galaxy.jobs DEBUG 2025-03-14 01:13:05,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (88) setting dataset 110 state to ERROR
galaxy.jobs INFO 2025-03-14 01:13:05,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-03-14 01:13:05,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 88 executed (99.910 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:05,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:13:08,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 89
tpv.core.entities DEBUG 2025-03-14 01:13:08,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:13:08,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:13:08,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:13:08,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:13:09,012 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-03-14 01:13:09,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (29.864 ms)
galaxy.jobs.handler INFO 2025-03-14 01:13:09,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:09,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 89
galaxy.jobs DEBUG 2025-03-14 01:13:09,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [89] prepared (60.073 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:13:09,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/89/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/89/configs/tmpp536e7ev']
galaxy.jobs.runners DEBUG 2025-03-14 01:13:09,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:09,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:09,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:10,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:19,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-68ts7 with k8s id: gxy-68ts7 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:13:19,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:13:26,804 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 89 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:13:26,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'multiBamSummary_result2b.npz', 'dbkey': '?', 'ext': 'deeptools_coverage_matrix', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded deeptools_coverage_matrix file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/89/working/gxupload_0', 'object_id': 111}]}]}]
galaxy.jobs INFO 2025-03-14 01:13:26,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-03-14 01:13:26,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 89 executed (125.489 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:26,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:13:27,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90
tpv.core.entities DEBUG 2025-03-14 01:13:27,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_pca/deeptools_plot_pca/.*, abstract=False, cores=1, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:13:27,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:13:27,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:13:27,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:13:27,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-03-14 01:13:27,549 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (27.572 ms)
galaxy.jobs.handler INFO 2025-03-14 01:13:27,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:27,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-03-14 01:13:27,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [90] prepared (42.142 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:13:27,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:13:27,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_pca/deeptools_plot_pca/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-14 01:13:28,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:13:28,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [plotPCA --version > /galaxy/server/database/jobs_directory/000/90/outputs/COMMAND_VERSION 2>&1;
plotPCA --corData '/galaxy/server/database/objects/5/7/e/dataset_57efee3f-f0a9-4ada-b736-89042c1f9c63.dat' --plotTitle 'Test Plot' --plotFile '/galaxy/server/database/objects/d/5/8/dataset_d5847093-d40c-4708-9294-22b699457af6.dat' --plotFileFormat 'png'  --plotWidth '10.0' --plotHeight '10.0'   --PCs 1 2 --ntop 1000]
galaxy.jobs.runners DEBUG 2025-03-14 01:13:28,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:28,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:13:28,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:13:28,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_pca/deeptools_plot_pca/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-14 01:13:28,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:28,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:13:29,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:07,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w79dr with k8s id: gxy-w79dr succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:14:07,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:14:14,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 90 finished
galaxy.model.metadata DEBUG 2025-03-14 01:14:14,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 112
galaxy.util WARNING 2025-03-14 01:14:14,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/5/8/dataset_d5847093-d40c-4708-9294-22b699457af6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/5/8/dataset_d5847093-d40c-4708-9294-22b699457af6.dat'
galaxy.jobs INFO 2025-03-14 01:14:14,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-03-14 01:14:14,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 90 executed (76.333 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:14,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:14:16,403 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-03-14 01:14:16,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:14:16,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:14:16,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:14:16,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:14:16,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-03-14 01:14:16,458 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (28.681 ms)
galaxy.jobs.handler INFO 2025-03-14 01:14:16,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:16,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-03-14 01:14:16,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [91] prepared (72.653 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:14:16,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/91/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/91/configs/tmpj1r8lvg2']
galaxy.jobs.runners DEBUG 2025-03-14 01:14:16,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:16,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:16,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:17,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:26,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ln9pv with k8s id: gxy-ln9pv succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:14:26,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:14:33,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 91 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:14:33,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'multiBamSummary_result2b.npz', 'dbkey': '?', 'ext': 'deeptools_coverage_matrix', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded deeptools_coverage_matrix file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/91/working/gxupload_0', 'object_id': 113}]}]}]
galaxy.jobs INFO 2025-03-14 01:14:33,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-03-14 01:14:33,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 91 executed (114.220 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:33,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:14:34,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92
tpv.core.entities DEBUG 2025-03-14 01:14:34,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_pca/deeptools_plot_pca/.*, abstract=False, cores=1, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:14:34,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:14:34,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:14:34,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:14:34,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-03-14 01:14:34,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (45.768 ms)
galaxy.jobs.handler INFO 2025-03-14 01:14:34,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:34,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 92
galaxy.jobs DEBUG 2025-03-14 01:14:34,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [92] prepared (49.658 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:14:34,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:14:34,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_pca/deeptools_plot_pca/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-14 01:14:34,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:14:34,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [plotPCA --version > /galaxy/server/database/jobs_directory/000/92/outputs/COMMAND_VERSION 2>&1;
plotPCA --corData '/galaxy/server/database/objects/b/c/4/dataset_bc45ad87-1ec4-4eab-bdf4-f9b1ea336fab.dat' --plotTitle 'Test Plot' --plotFile '/galaxy/server/database/objects/5/3/9/dataset_539a25dc-3ea8-4d78-abcd-5adc558231cf.dat' --plotFileFormat 'png' --outFileNameData '/galaxy/server/database/objects/b/b/b/dataset_bbb01297-4fef-457d-b58e-a874d8beb27c.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:14:34,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:35,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:14:35,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:14:35,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_pca/deeptools_plot_pca/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-14 01:14:35,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:35,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:35,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:43,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rnbmp with k8s id: gxy-rnbmp succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:14:43,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:14:51,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 92 finished
galaxy.model.metadata DEBUG 2025-03-14 01:14:51,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 114
galaxy.model.metadata DEBUG 2025-03-14 01:14:51,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 115
galaxy.util WARNING 2025-03-14 01:14:51,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/3/9/dataset_539a25dc-3ea8-4d78-abcd-5adc558231cf.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/3/9/dataset_539a25dc-3ea8-4d78-abcd-5adc558231cf.dat'
galaxy.util WARNING 2025-03-14 01:14:51,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/b/b/dataset_bbb01297-4fef-457d-b58e-a874d8beb27c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/b/b/dataset_bbb01297-4fef-457d-b58e-a874d8beb27c.dat'
galaxy.jobs INFO 2025-03-14 01:14:51,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs DEBUG 2025-03-14 01:14:51,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 92 executed (122.306 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:51,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:14:54,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94, 93
tpv.core.entities DEBUG 2025-03-14 01:14:54,234 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:14:54,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:14:54,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:14:54,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:14:54,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-03-14 01:14:54,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (45.182 ms)
galaxy.jobs.handler INFO 2025-03-14 01:14:54,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:54,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 93
tpv.core.entities DEBUG 2025-03-14 01:14:54,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:14:54,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:14:54,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:14:54,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:14:54,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-03-14 01:14:54,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (60.838 ms)
galaxy.jobs.handler INFO 2025-03-14 01:14:54,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:54,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-03-14 01:14:54,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [93] prepared (95.656 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:14:54,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/93/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/93/configs/tmpa9qch39l']
galaxy.jobs.runners DEBUG 2025-03-14 01:14:54,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:54,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:14:54,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [94] prepared (80.553 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:54,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:14:54,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/94/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/94/configs/tmphkazbyia']
galaxy.jobs.runners DEBUG 2025-03-14 01:14:54,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:54,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:54,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:55,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:14:55,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xl9dk failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xl9dk.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:15:03,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-xl9dk

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-xl9dk": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94/gxy-xl9dk) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94/gxy-xl9dk) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94/gxy-xl9dk) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94/gxy-xl9dk) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xl9dk.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 94 (gxy-xl9dk)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-xl9dk to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:03,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:04,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94/gxy-xl9dk) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:04,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8v6nv with k8s id: gxy-8v6nv succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:15:05,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:15:12,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 93 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:15:12,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/93/working/gxupload_0', 'object_id': 116}]}]}]
galaxy.jobs INFO 2025-03-14 01:15:12,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs DEBUG 2025-03-14 01:15:12,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 93 executed (148.566 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:12,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:15:13,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95, 96
tpv.core.entities DEBUG 2025-03-14 01:15:13,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:15:13,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:15:13,777 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:15:13,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:15:13,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-03-14 01:15:13,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (34.570 ms)
galaxy.jobs.handler INFO 2025-03-14 01:15:13,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:13,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 95
tpv.core.entities DEBUG 2025-03-14 01:15:13,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:15:13,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:15:13,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:15:13,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:15:13,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-03-14 01:15:13,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (54.863 ms)
galaxy.jobs.handler INFO 2025-03-14 01:15:13,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:13,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 96
galaxy.jobs DEBUG 2025-03-14 01:15:13,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [95] prepared (92.601 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:15:13,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/95/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/95/configs/tmpu1ex60ns']
galaxy.jobs.runners DEBUG 2025-03-14 01:15:13,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:13,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:15:13,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [96] prepared (85.558 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:14,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:15:14,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/96/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/96/configs/tmp3vt7q_0z']
galaxy.jobs.runners DEBUG 2025-03-14 01:15:14,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:14,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:14,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:15,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:15,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:24,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qjkjj with k8s id: gxy-qjkjj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:24,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2jhv8 with k8s id: gxy-2jhv8 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:15:24,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:15:24,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:15:32,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 96 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:15:32,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 95 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:15:32,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pBR322.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/96/working/data_fetch_upload_8ku88m68', 'object_id': 119}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:15:32,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/95/working/gxupload_0', 'object_id': 118}]}]}]
galaxy.jobs INFO 2025-03-14 01:15:32,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs INFO 2025-03-14 01:15:32,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-03-14 01:15:32,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 96 executed (168.355 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:32,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:15:32,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 95 executed (178.682 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:32,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:15:33,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97
tpv.core.entities DEBUG 2025-03-14 01:15:33,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/.*, abstract=False, cores=4, mem=16, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:15:33,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:15:33,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:15:33,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:15:33,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-03-14 01:15:33,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (46.310 ms)
galaxy.jobs.handler INFO 2025-03-14 01:15:33,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:33,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-03-14 01:15:33,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [97] prepared (55.673 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:15:33,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:15:33,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-14 01:15:33,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:15:33,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/97/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/e/5/c/dataset_e5c1db7c-10e0-4f1a-96d2-44ccffb7ac68.dat' reference.fa && lofreq faidx reference.fa 2>&1 || echo "Error running samtools faidx for indexing fasta reference for lofreq" >&2 &&  lofreq viterbi --ref 'reference.fa' --keepflags --defqual 2 --out tmp.bam '/galaxy/server/database/objects/9/1/2/dataset_9124e0de-4f8c-4fdb-a841-651ca98b6c0e.dat' &&  samtools sort --no-PG -T "${TMPDIR:-.}" -@ ${GALAXY_SLOTS:-1} -O BAM -o '/galaxy/server/database/objects/8/6/3/dataset_8638ba41-5a67-4794-913c-fbc15136cf2d.dat' tmp.bam]
galaxy.jobs.runners DEBUG 2025-03-14 01:15:33,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:33,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:15:33,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:15:33,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-14 01:15:33,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:33,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:34,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:15:59,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qxc9c with k8s id: gxy-qxc9c succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:16:00,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:16:08,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 97 finished
galaxy.model.metadata DEBUG 2025-03-14 01:16:08,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 120
galaxy.jobs INFO 2025-03-14 01:16:08,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs DEBUG 2025-03-14 01:16:08,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 97 executed (145.611 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:08,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:16:12,228 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98, 99
tpv.core.entities DEBUG 2025-03-14 01:16:12,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:16:12,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:16:12,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:16:12,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:16:12,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-03-14 01:16:12,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (40.751 ms)
galaxy.jobs.handler INFO 2025-03-14 01:16:12,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:12,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 98
tpv.core.entities DEBUG 2025-03-14 01:16:12,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:16:12,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:16:12,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:16:12,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:16:12,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-03-14 01:16:12,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (54.978 ms)
galaxy.jobs.handler INFO 2025-03-14 01:16:12,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:12,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 99
galaxy.jobs DEBUG 2025-03-14 01:16:12,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [98] prepared (101.009 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:16:12,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/98/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/98/configs/tmplezqejo6']
galaxy.jobs.runners DEBUG 2025-03-14 01:16:12,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:16:12,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [99] prepared (86.762 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:12,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:12,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:16:12,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/99/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/99/configs/tmp0524hpej']
galaxy.jobs.runners DEBUG 2025-03-14 01:16:12,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:12,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:12,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:14,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:14,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:23,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f9wcn with k8s id: gxy-f9wcn succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:16:23,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:24,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fn2ld with k8s id: gxy-fn2ld succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:16:24,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:16:33,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 99 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:16:33,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'PRV.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/99/working/data_fetch_upload_4fw5grok', 'object_id': 122}]}]}]
galaxy.jobs INFO 2025-03-14 01:16:33,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs DEBUG 2025-03-14 01:16:33,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 99 executed (181.715 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:33,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 01:16:33,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 98 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:16:34,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Z52_a.masked.sorted.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/98/working/gxupload_0', 'object_id': 121}]}]}]
galaxy.jobs INFO 2025-03-14 01:16:34,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs DEBUG 2025-03-14 01:16:34,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 98 executed (233.311 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:34,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:16:34,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100
tpv.core.entities DEBUG 2025-03-14 01:16:35,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/.*, abstract=False, cores=1, mem=30.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:16:35,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:16:35,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:16:35,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:16:35,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-03-14 01:16:35,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (60.543 ms)
galaxy.jobs.handler INFO 2025-03-14 01:16:35,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:35,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-03-14 01:16:35,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [100] prepared (78.565 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:16:35,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:16:35,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:16:35,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:16:35,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [ivar version | grep version > /galaxy/server/database/jobs_directory/000/100/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/4/f/d/dataset_4fdbcae1-c344-4251-ab05-73b1a3dcc5ac.dat' ref.fa && ln -s '/galaxy/server/database/objects/3/3/4/dataset_334aaa28-201c-45b1-a924-ea2e129c8b8c.dat' sorted.bam && samtools mpileup -A -d 0 --reference ref.fa -B -Q 0 sorted.bam | ivar variants -p variants -q 20 -t 0.03]
galaxy.jobs.runners DEBUG 2025-03-14 01:16:35,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/100/working/./variants.tsv" -a -f "/galaxy/server/database/objects/4/d/c/dataset_4dca1358-650f-4cc0-9e52-c78727237ad1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/100/working/./variants.tsv" "/galaxy/server/database/objects/4/d/c/dataset_4dca1358-650f-4cc0-9e52-c78727237ad1.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:35,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:16:35,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:16:35,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:16:35,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:35,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:36,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:16:58,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5bgms with k8s id: gxy-5bgms succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:16:59,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:17:07,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 100 finished
galaxy.model.metadata DEBUG 2025-03-14 01:17:07,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 123
galaxy.util WARNING 2025-03-14 01:17:07,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/d/c/dataset_4dca1358-650f-4cc0-9e52-c78727237ad1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/d/c/dataset_4dca1358-650f-4cc0-9e52-c78727237ad1.dat'
galaxy.jobs INFO 2025-03-14 01:17:07,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-03-14 01:17:07,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 100 executed (155.587 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:07,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:17:09,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 102, 101
tpv.core.entities DEBUG 2025-03-14 01:17:09,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:17:09,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:17:09,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:17:09,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:17:09,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-03-14 01:17:09,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (43.347 ms)
galaxy.jobs.handler INFO 2025-03-14 01:17:09,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:09,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 101
tpv.core.entities DEBUG 2025-03-14 01:17:09,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:17:09,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:17:09,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:17:09,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:17:09,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-03-14 01:17:09,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (54.964 ms)
galaxy.jobs.handler INFO 2025-03-14 01:17:09,998 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:10,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 102
galaxy.jobs DEBUG 2025-03-14 01:17:10,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [101] prepared (107.364 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:17:10,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/101/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/101/configs/tmp7ztjlz6j']
galaxy.jobs.runners DEBUG 2025-03-14 01:17:10,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:17:10,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [102] prepared (92.372 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:10,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:10,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:17:10,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/102/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/102/configs/tmp7pzpe2x2']
galaxy.jobs.runners DEBUG 2025-03-14 01:17:10,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:10,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:10,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:11,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:11,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:21,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xb82s with k8s id: gxy-xb82s succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:21,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qkvwf with k8s id: gxy-qkvwf succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:17:21,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:17:21,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 102: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:17:29,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 101 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:17:29,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Z52_a.masked.sorted.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/101/working/gxupload_0', 'object_id': 124}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:17:29,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 102 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:17:29,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'PRV.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/102/working/data_fetch_upload_2vxy1d1j', 'object_id': 125}]}]}]
galaxy.jobs INFO 2025-03-14 01:17:29,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs INFO 2025-03-14 01:17:29,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 102 in /galaxy/server/database/jobs_directory/000/102
galaxy.jobs DEBUG 2025-03-14 01:17:29,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 101 executed (238.402 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:29,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:17:29,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 102 executed (145.299 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:29,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:17:30,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 103
tpv.core.entities DEBUG 2025-03-14 01:17:30,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/.*, abstract=False, cores=1, mem=30.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:17:30,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:17:30,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:17:30,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:17:30,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-03-14 01:17:30,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (37.549 ms)
galaxy.jobs.handler INFO 2025-03-14 01:17:30,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:30,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 103
galaxy.jobs DEBUG 2025-03-14 01:17:30,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [103] prepared (53.614 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:17:30,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:17:30,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:17:30,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:17:30,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [ivar version | grep version > /galaxy/server/database/jobs_directory/000/103/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/e/e/b/dataset_eeb7261c-33af-4dae-bd2b-437f7e3694f1.dat' ref.fa && ln -s '/galaxy/server/database/objects/0/9/0/dataset_09066acd-dcb4-4604-b9b9-669b70027dc0.dat' sorted.bam && samtools mpileup -A -d 0 --reference ref.fa -B -Q 0 sorted.bam | ivar variants -p variants -q 20 -t 0.03 && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/abbc8041e8ec/ivar_variants/ivar_variants_to_vcf.py'  variants.tsv variants.vcf]
galaxy.jobs.runners DEBUG 2025-03-14 01:17:30,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/103/working/./variants.vcf" -a -f "/galaxy/server/database/objects/d/2/a/dataset_d2a56cfc-01a4-49d6-be5e-ab73389b8e52.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/103/working/./variants.vcf" "/galaxy/server/database/objects/d/2/a/dataset_d2a56cfc-01a4-49d6-be5e-ab73389b8e52.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:30,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:17:30,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:17:30,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:17:30,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:30,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:31,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:35,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zbsd2 with k8s id: gxy-zbsd2 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:17:35,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:17:43,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 103 finished
galaxy.model.metadata DEBUG 2025-03-14 01:17:43,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 126
galaxy.util WARNING 2025-03-14 01:17:43,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/2/a/dataset_d2a56cfc-01a4-49d6-be5e-ab73389b8e52.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/2/a/dataset_d2a56cfc-01a4-49d6-be5e-ab73389b8e52.dat'
galaxy.jobs INFO 2025-03-14 01:17:43,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs DEBUG 2025-03-14 01:17:44,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 103 executed (146.307 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:44,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:17:45,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 104, 105
tpv.core.entities DEBUG 2025-03-14 01:17:45,975 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:17:45,975 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:17:45,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:17:45,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:17:46,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Working directory for job is: /galaxy/server/database/jobs_directory/000/104
galaxy.jobs.runners DEBUG 2025-03-14 01:17:46,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [104] queued (41.806 ms)
galaxy.jobs.handler INFO 2025-03-14 01:17:46,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 104
tpv.core.entities DEBUG 2025-03-14 01:17:46,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:17:46,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:17:46,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:17:46,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:17:46,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Working directory for job is: /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-03-14 01:17:46,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [105] queued (51.766 ms)
galaxy.jobs.handler INFO 2025-03-14 01:17:46,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 105
galaxy.jobs DEBUG 2025-03-14 01:17:46,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [104] prepared (99.111 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:17:46,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/104/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/104/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/104/configs/tmptdfog37s']
galaxy.jobs.runners DEBUG 2025-03-14 01:17:46,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (104) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/104/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/104/galaxy_104.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:17:46,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [105] prepared (91.594 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:17:46,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/105/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/105/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/105/configs/tmpxy_vfw8q']
galaxy.jobs.runners DEBUG 2025-03-14 01:17:46,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (105) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/105/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/105/galaxy_105.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:46,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:57,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4zzpp with k8s id: gxy-4zzpp succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:17:57,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pnmkx with k8s id: gxy-pnmkx succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:17:57,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 104: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:17:57,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 105: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:18:05,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 104 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:18:05,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 105 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:18:05,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Z52_a.masked.sorted.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/104/working/gxupload_0', 'object_id': 127}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:18:05,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'PRV.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/105/working/data_fetch_upload_khjipll2', 'object_id': 128}]}]}]
galaxy.jobs INFO 2025-03-14 01:18:06,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 104 in /galaxy/server/database/jobs_directory/000/104
galaxy.jobs INFO 2025-03-14 01:18:06,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 105 in /galaxy/server/database/jobs_directory/000/105
galaxy.jobs DEBUG 2025-03-14 01:18:06,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 104 executed (260.402 ms)
galaxy.jobs DEBUG 2025-03-14 01:18:06,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 105 executed (223.959 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:06,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:06,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:18:06,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 106
tpv.core.entities DEBUG 2025-03-14 01:18:06,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/.*, abstract=False, cores=1, mem=30.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:18:06,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:18:06,666 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:18:06,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:18:06,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Working directory for job is: /galaxy/server/database/jobs_directory/000/106
galaxy.jobs.runners DEBUG 2025-03-14 01:18:06,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [106] queued (48.749 ms)
galaxy.jobs.handler INFO 2025-03-14 01:18:06,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:06,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 106
galaxy.jobs DEBUG 2025-03-14 01:18:06,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [106] prepared (58.420 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:06,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:18:06,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:06,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:18:06,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/106/tool_script.sh] for tool command [ivar version | grep version > /galaxy/server/database/jobs_directory/000/106/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/3/f/c/dataset_3fcaefa7-bd9d-4279-ba5b-26dc1892c03e.dat' ref.fa && ln -s '/galaxy/server/database/objects/b/3/a/dataset_b3a368cd-d827-49b6-82b5-97d62ad124f6.dat' sorted.bam && samtools mpileup -A -d 0 --reference ref.fa -B -Q 0 sorted.bam | ivar variants -p variants -q 20 -t 0.03 && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/abbc8041e8ec/ivar_variants/ivar_variants_to_vcf.py' --pass_only variants.tsv variants.vcf]
galaxy.jobs.runners DEBUG 2025-03-14 01:18:06,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (106) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/106/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/106/galaxy_106.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/106/working/./variants.tsv" -a -f "/galaxy/server/database/objects/2/c/0/dataset_2c0c72a9-6faa-47c9-b5bf-9d08367870d2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/106/working/./variants.tsv" "/galaxy/server/database/objects/2/c/0/dataset_2c0c72a9-6faa-47c9-b5bf-9d08367870d2.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/106/working/./variants.vcf" -a -f "/galaxy/server/database/objects/7/3/b/dataset_73b037e5-6f4d-4323-9496-d7870af01f03.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/106/working/./variants.vcf" "/galaxy/server/database/objects/7/3/b/dataset_73b037e5-6f4d-4323-9496-d7870af01f03.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:06,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:06,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:18:06,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:06,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:06,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:07,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:12,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-29hl6 with k8s id: gxy-29hl6 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:18:12,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 106: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:18:20,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 106 finished
galaxy.model.metadata DEBUG 2025-03-14 01:18:20,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 129
galaxy.model.metadata DEBUG 2025-03-14 01:18:20,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 130
galaxy.util WARNING 2025-03-14 01:18:20,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/c/0/dataset_2c0c72a9-6faa-47c9-b5bf-9d08367870d2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/c/0/dataset_2c0c72a9-6faa-47c9-b5bf-9d08367870d2.dat'
galaxy.util WARNING 2025-03-14 01:18:20,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/3/b/dataset_73b037e5-6f4d-4323-9496-d7870af01f03.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/3/b/dataset_73b037e5-6f4d-4323-9496-d7870af01f03.dat'
galaxy.jobs INFO 2025-03-14 01:18:20,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 106 in /galaxy/server/database/jobs_directory/000/106
galaxy.jobs DEBUG 2025-03-14 01:18:20,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 106 executed (177.111 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:20,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:18:23,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 109, 108, 107
tpv.core.entities DEBUG 2025-03-14 01:18:23,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:18:23,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:18:23,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:18:23,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:18:23,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Working directory for job is: /galaxy/server/database/jobs_directory/000/107
galaxy.jobs.runners DEBUG 2025-03-14 01:18:23,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [107] queued (37.859 ms)
galaxy.jobs.handler INFO 2025-03-14 01:18:23,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 107
tpv.core.entities DEBUG 2025-03-14 01:18:23,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:18:23,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:18:23,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:18:23,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:18:23,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Working directory for job is: /galaxy/server/database/jobs_directory/000/108
galaxy.jobs.runners DEBUG 2025-03-14 01:18:23,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [108] queued (49.793 ms)
galaxy.jobs.handler INFO 2025-03-14 01:18:23,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 108
tpv.core.entities DEBUG 2025-03-14 01:18:23,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:18:23,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:18:23,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:18:23,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [107] prepared (98.117 ms)
galaxy.jobs DEBUG 2025-03-14 01:18:23,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:18:23,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Working directory for job is: /galaxy/server/database/jobs_directory/000/109
galaxy.jobs.command_factory INFO 2025-03-14 01:18:23,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/107/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/107/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/107/configs/tmpwj6z5qoy']
galaxy.jobs.runners DEBUG 2025-03-14 01:18:23,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [109] queued (55.335 ms)
galaxy.jobs.handler INFO 2025-03-14 01:18:23,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Job dispatched
galaxy.jobs.runners DEBUG 2025-03-14 01:18:23,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (107) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/107/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/107/galaxy_107.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 109
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:18:23,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [108] prepared (106.227 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:18:23,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/108/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/108/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/108/configs/tmp8zibvq9v']
galaxy.jobs.runners DEBUG 2025-03-14 01:18:23,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (108) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/108/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/108/galaxy_108.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:18:23,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [109] prepared (85.993 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:18:23,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/109/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/109/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/109/configs/tmpsiv76ov1']
galaxy.jobs.runners DEBUG 2025-03-14 01:18:23,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (109) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/109/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/109/galaxy_109.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:23,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:33,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qrwjr with k8s id: gxy-qrwjr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:33,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cvsv7 with k8s id: gxy-cvsv7 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:18:33,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 108: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:18:33,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 109: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:34,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2wp9m with k8s id: gxy-2wp9m succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:18:34,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 107: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:18:44,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 109 finished
galaxy.jobs.runners DEBUG 2025-03-14 01:18:44,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 108 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:18:44,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'PRV.gff', 'dbkey': '?', 'ext': 'gff3', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gff3 file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/109/working/data_fetch_upload_2lirr14j', 'object_id': 133}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:18:44,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'PRV.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/108/working/data_fetch_upload_d3_kwqtp', 'object_id': 132}]}]}]
galaxy.jobs INFO 2025-03-14 01:18:44,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 109 in /galaxy/server/database/jobs_directory/000/109
galaxy.jobs INFO 2025-03-14 01:18:44,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 108 in /galaxy/server/database/jobs_directory/000/108
galaxy.jobs DEBUG 2025-03-14 01:18:44,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 109 executed (154.968 ms)
galaxy.jobs DEBUG 2025-03-14 01:18:44,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 108 executed (135.264 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:44,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:44,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 01:18:45,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 107 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:18:45,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Z52_a.masked.sorted.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/107/working/gxupload_0', 'object_id': 131}]}]}]
galaxy.jobs INFO 2025-03-14 01:18:45,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 107 in /galaxy/server/database/jobs_directory/000/107
galaxy.jobs DEBUG 2025-03-14 01:18:45,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 107 executed (212.500 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:45,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:18:47,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 110
tpv.core.entities DEBUG 2025-03-14 01:18:47,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/.*, abstract=False, cores=1, mem=30.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:18:47,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:18:47,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:18:47,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:18:47,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Working directory for job is: /galaxy/server/database/jobs_directory/000/110
galaxy.jobs.runners DEBUG 2025-03-14 01:18:47,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [110] queued (36.572 ms)
galaxy.jobs.handler INFO 2025-03-14 01:18:47,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:47,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 110
galaxy.jobs DEBUG 2025-03-14 01:18:47,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [110] prepared (58.626 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:47,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:18:47,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:47,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:18:47,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/110/tool_script.sh] for tool command [ivar version | grep version > /galaxy/server/database/jobs_directory/000/110/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/c/4/6/dataset_c465b93f-c36a-4db9-bc36-0f1892b4e0db.dat' ref.fa && ln -s '/galaxy/server/database/objects/9/0/0/dataset_90077dcd-0655-4ee4-b4c0-236986777296.dat' sorted.bam && samtools mpileup -A -d 0 --reference ref.fa -B -Q 0 sorted.bam | ivar variants -p variants -q 20 -t 0.03 -r ref.fa -g '/galaxy/server/database/objects/1/6/2/dataset_16295c52-9da0-4e40-a2b3-e15d4266d373.dat']
galaxy.jobs.runners DEBUG 2025-03-14 01:18:47,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (110) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/110/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/110/galaxy_110.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/110/working/./variants.tsv" -a -f "/galaxy/server/database/objects/f/c/3/dataset_fc36c6ff-1aef-43a0-9258-a06703a218e3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/110/working/./variants.tsv" "/galaxy/server/database/objects/f/c/3/dataset_fc36c6ff-1aef-43a0-9258-a06703a218e3.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:47,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:47,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:18:47,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:18:47,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:47,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:48,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:18:53,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s72hj with k8s id: gxy-s72hj succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:18:53,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 110: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:19:01,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 110 finished
galaxy.model.metadata DEBUG 2025-03-14 01:19:01,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.util WARNING 2025-03-14 01:19:01,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/c/3/dataset_fc36c6ff-1aef-43a0-9258-a06703a218e3.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/c/3/dataset_fc36c6ff-1aef-43a0-9258-a06703a218e3.dat'
galaxy.jobs INFO 2025-03-14 01:19:01,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 110 in /galaxy/server/database/jobs_directory/000/110
galaxy.jobs DEBUG 2025-03-14 01:19:01,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 110 executed (128.441 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:01,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:19:03,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 112, 113, 111
tpv.core.entities DEBUG 2025-03-14 01:19:03,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:19:03,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:19:03,582 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:19:03,595 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:19:03,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Working directory for job is: /galaxy/server/database/jobs_directory/000/111
galaxy.jobs.runners DEBUG 2025-03-14 01:19:03,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [111] queued (42.883 ms)
galaxy.jobs.handler INFO 2025-03-14 01:19:03,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 111
tpv.core.entities DEBUG 2025-03-14 01:19:03,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:19:03,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:19:03,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:19:03,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:19:03,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Working directory for job is: /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-03-14 01:19:03,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [112] queued (42.774 ms)
galaxy.jobs.handler INFO 2025-03-14 01:19:03,690 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 112
tpv.core.entities DEBUG 2025-03-14 01:19:03,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:19:03,711 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:19:03,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:19:03,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [111] prepared (85.589 ms)
galaxy.jobs DEBUG 2025-03-14 01:19:03,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:19:03,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Working directory for job is: /galaxy/server/database/jobs_directory/000/113
galaxy.jobs.command_factory INFO 2025-03-14 01:19:03,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/111/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/111/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/111/configs/tmpoy63_t3l']
galaxy.jobs.runners DEBUG 2025-03-14 01:19:03,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [113] queued (51.280 ms)
galaxy.jobs.handler INFO 2025-03-14 01:19:03,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 113
galaxy.jobs.runners DEBUG 2025-03-14 01:19:03,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (111) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/111/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/111/galaxy_111.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:19:03,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [112] prepared (99.049 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:19:03,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/112/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/112/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/112/configs/tmp8pi_bxhi']
galaxy.jobs.runners DEBUG 2025-03-14 01:19:03,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (112) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/112/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/112/galaxy_112.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:19:03,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [113] prepared (85.305 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:19:03,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/113/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/113/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/113/configs/tmp1_0mpjkq']
galaxy.jobs.runners DEBUG 2025-03-14 01:19:03,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (113) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/113/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/113/galaxy_113.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:03,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:04,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:04,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:04,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:14,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-df2b4 with k8s id: gxy-df2b4 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:14,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z7cxn with k8s id: gxy-z7cxn succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:14,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-k67wp with k8s id: gxy-k67wp succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:19:14,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 111: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:19:14,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 112: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:19:14,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 113: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:19:26,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 112 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:19:26,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'PRV.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/112/working/data_fetch_upload_24mxn88o', 'object_id': 136}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:19:26,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 113 finished
galaxy.jobs INFO 2025-03-14 01:19:26,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:19:26,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'PRV.gff', 'dbkey': '?', 'ext': 'gff3', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gff3 file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/113/working/data_fetch_upload_w9r1dauy', 'object_id': 137}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:19:26,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 111 finished
galaxy.jobs DEBUG 2025-03-14 01:19:26,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 112 executed (263.941 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:26,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-14 01:19:26,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 113 in /galaxy/server/database/jobs_directory/000/113
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:19:26,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Z52_a.masked.sorted.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/111/working/gxupload_0', 'object_id': 135}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:19:27,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 113 executed (330.463 ms)
galaxy.jobs INFO 2025-03-14 01:19:27,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.jobs DEBUG 2025-03-14 01:19:27,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 111 executed (301.822 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:27,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:27,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:19:28,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 114
tpv.core.entities DEBUG 2025-03-14 01:19:28,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/.*, abstract=False, cores=1, mem=30.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:19:28,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:19:28,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:19:28,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:19:28,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Working directory for job is: /galaxy/server/database/jobs_directory/000/114
galaxy.jobs.runners DEBUG 2025-03-14 01:19:28,563 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [114] queued (64.366 ms)
galaxy.jobs.handler INFO 2025-03-14 01:19:28,567 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:28,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 114
galaxy.jobs DEBUG 2025-03-14 01:19:28,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [114] prepared (79.639 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:19:28,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:19:28,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:19:28,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:19:28,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/114/tool_script.sh] for tool command [ivar version | grep version > /galaxy/server/database/jobs_directory/000/114/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/8/d/5/dataset_8d55cd61-dd21-440a-98e7-80090ae4c7fa.dat' ref.fa && ln -s '/galaxy/server/database/objects/0/d/e/dataset_0dead43d-2e0d-409b-9669-d221f7f5c8c6.dat' sorted.bam && samtools mpileup -A -d 0 --reference ref.fa -B -Q 0 sorted.bam | ivar variants -p variants -q 20 -t 0.03 -r ref.fa -g '/galaxy/server/database/objects/7/f/4/dataset_7f4b965c-a834-4005-ab17-b40cf6900ccb.dat' && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/abbc8041e8ec/ivar_variants/ivar_variants_to_vcf.py' --pass_only variants.tsv variants.vcf]
galaxy.jobs.runners DEBUG 2025-03-14 01:19:28,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/114/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/114/galaxy_114.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/114/working/./variants.tsv" -a -f "/galaxy/server/database/objects/f/1/4/dataset_f14ac1d4-d728-4c93-9f05-538dbc6b65e6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/114/working/./variants.tsv" "/galaxy/server/database/objects/f/1/4/dataset_f14ac1d4-d728-4c93-9f05-538dbc6b65e6.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/114/working/./variants.vcf" -a -f "/galaxy/server/database/objects/3/4/e/dataset_34ed08e1-4dd7-494c-b6bf-e4728b09c111.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/114/working/./variants.vcf" "/galaxy/server/database/objects/3/4/e/dataset_34ed08e1-4dd7-494c-b6bf-e4728b09c111.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:28,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:19:28,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:19:28,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.3+galaxy1: mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1
galaxy.tool_util.deps.containers INFO 2025-03-14 01:19:28,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-63799f2237acb361cf94ed0b24a3ed6e5b57284b:7c7fc8fa267667f2e08ffd4247f8f49bf77baed1-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:28,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:29,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:33,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6sqxs failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:33,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:33,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job: gxy-6sqxs failed due to an unknown exit code from the tool.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:33,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 01:19:33,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 114: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:19:40,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 114 finished
galaxy.tool_util.output_checker INFO 2025-03-14 01:19:41,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job error detected, failing job. Reasons are [{'type': 'exit_code', 'desc': 'Fatal error: Exit code 139 ()', 'exit_code': 139, 'code_desc': '', 'error_level': 3}]
galaxy.jobs DEBUG 2025-03-14 01:19:41,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114) setting dataset 138 state to ERROR
galaxy.jobs DEBUG 2025-03-14 01:19:41,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114) setting dataset 139 state to ERROR
galaxy.util WARNING 2025-03-14 01:19:41,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/1/4/dataset_f14ac1d4-d728-4c93-9f05-538dbc6b65e6.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/1/4/dataset_f14ac1d4-d728-4c93-9f05-538dbc6b65e6.dat'
galaxy.jobs INFO 2025-03-14 01:19:41,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 114 in /galaxy/server/database/jobs_directory/000/114
galaxy.jobs DEBUG 2025-03-14 01:19:41,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 114 executed (124.024 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:41,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:19:43,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 115
tpv.core.entities DEBUG 2025-03-14 01:19:43,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:19:43,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:19:43,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:19:43,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:19:43,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Working directory for job is: /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-03-14 01:19:43,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [115] queued (38.178 ms)
galaxy.jobs.handler INFO 2025-03-14 01:19:43,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:43,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 115
galaxy.jobs DEBUG 2025-03-14 01:19:44,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [115] prepared (66.298 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:19:44,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/115/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/115/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/115/configs/tmpbu_jza7g']
galaxy.jobs.runners DEBUG 2025-03-14 01:19:44,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (115) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/115/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/115/galaxy_115.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:44,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:44,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:44,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:19:53,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m996x with k8s id: gxy-m996x succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:19:53,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 115: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:20:01,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 115 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:20:01,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'input.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/115/working/data_fetch_upload_sfk2qflx', 'object_id': 140}]}]}]
galaxy.jobs INFO 2025-03-14 01:20:01,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs DEBUG 2025-03-14 01:20:01,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 115 executed (294.795 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:01,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:20:02,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 116
tpv.core.entities DEBUG 2025-03-14 01:20:02,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:20:02,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:20:02,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:20:02,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:20:02,457 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Working directory for job is: /galaxy/server/database/jobs_directory/000/116
galaxy.jobs.runners DEBUG 2025-03-14 01:20:02,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [116] queued (40.240 ms)
galaxy.jobs.handler INFO 2025-03-14 01:20:02,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:02,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 116
galaxy.jobs DEBUG 2025-03-14 01:20:02,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [116] prepared (79.764 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:20:02,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:20:02,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:20:02,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:20:02,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/116/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", ggplot2 version" $(R --vanilla --slave -e "library(ggplot2); cat(sessionInfo()\$otherPkgs\$ggplot2\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", ggrepel version" $(R --vanilla --slave -e "library(ggrepel); cat(sessionInfo()\$otherPkgs\$ggrepel\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", dplyr version" $(R --vanilla --slave -e "library(dplyr); cat(sessionInfo()\$otherPkgs\$dplyr\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/116/outputs/COMMAND_VERSION 2>&1;
Rscript '/galaxy/server/database/jobs_directory/000/116/configs/tmp72nt5ing']
galaxy.jobs.runners DEBUG 2025-03-14 01:20:02,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (116) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/116/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/116/galaxy_116.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/116/working/volcano_plot.pdf" -a -f "/galaxy/server/database/objects/c/f/7/dataset_cf7a6bbf-ac1b-452f-82cf-5e70cb917214.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/116/working/volcano_plot.pdf" "/galaxy/server/database/objects/c/f/7/dataset_cf7a6bbf-ac1b-452f-82cf-5e70cb917214.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:02,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:20:02,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:20:02,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:20:02,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:02,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:03,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:50,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4jbxz with k8s id: gxy-4jbxz succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:20:50,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 116: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:20:57,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 116 finished
galaxy.model.metadata DEBUG 2025-03-14 01:20:57,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.util WARNING 2025-03-14 01:20:57,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/f/7/dataset_cf7a6bbf-ac1b-452f-82cf-5e70cb917214.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/f/7/dataset_cf7a6bbf-ac1b-452f-82cf-5e70cb917214.dat'
galaxy.jobs INFO 2025-03-14 01:20:57,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 116 in /galaxy/server/database/jobs_directory/000/116
galaxy.jobs DEBUG 2025-03-14 01:20:57,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 116 executed (89.631 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:58,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:20:59,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 117, 118
tpv.core.entities DEBUG 2025-03-14 01:20:59,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:20:59,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:20:59,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:20:59,559 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:20:59,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Working directory for job is: /galaxy/server/database/jobs_directory/000/117
galaxy.jobs.runners DEBUG 2025-03-14 01:20:59,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [117] queued (30.084 ms)
galaxy.jobs.handler INFO 2025-03-14 01:20:59,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:59,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 117
tpv.core.entities DEBUG 2025-03-14 01:20:59,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:20:59,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:20:59,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:20:59,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:20:59,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Working directory for job is: /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-03-14 01:20:59,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [118] queued (48.106 ms)
galaxy.jobs.handler INFO 2025-03-14 01:20:59,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:59,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 118
galaxy.jobs DEBUG 2025-03-14 01:20:59,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [117] prepared (79.806 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:20:59,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/117/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/117/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/117/configs/tmpap2h5iz2']
galaxy.jobs.runners DEBUG 2025-03-14 01:20:59,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (117) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/117/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/117/galaxy_117.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-14 01:20:59,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [118] prepared (69.781 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:59,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:59,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:20:59,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/118/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/118/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/118/configs/tmpqpqra_sm']
galaxy.jobs.runners DEBUG 2025-03-14 01:20:59,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (118) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/118/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/118/galaxy_118.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:59,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:20:59,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:00,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:00,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:09,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-snwj8 with k8s id: gxy-snwj8 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:09,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cqbb8 with k8s id: gxy-cqbb8 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:21:09,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 117: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:21:09,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 118: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:21:17,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 118 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:21:17,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'labels.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/118/working/data_fetch_upload_qmtxe1dv', 'object_id': 143}]}]}]
galaxy.jobs INFO 2025-03-14 01:21:17,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 118 in /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-03-14 01:21:17,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 117 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:21:17,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'input.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/117/working/data_fetch_upload_6gzwn5ub', 'object_id': 142}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:21:17,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 118 executed (151.335 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:17,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-14 01:21:17,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 117 in /galaxy/server/database/jobs_directory/000/117
galaxy.jobs DEBUG 2025-03-14 01:21:17,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 117 executed (325.816 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:18,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:21:19,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 119
tpv.core.entities DEBUG 2025-03-14 01:21:19,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:21:19,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:21:19,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:21:19,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:21:19,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Working directory for job is: /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners DEBUG 2025-03-14 01:21:19,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [119] queued (47.451 ms)
galaxy.jobs.handler INFO 2025-03-14 01:21:19,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:19,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 119
galaxy.jobs DEBUG 2025-03-14 01:21:19,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [119] prepared (68.203 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:21:19,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:21:19,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:21:19,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:21:19,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/119/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", ggplot2 version" $(R --vanilla --slave -e "library(ggplot2); cat(sessionInfo()\$otherPkgs\$ggplot2\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", ggrepel version" $(R --vanilla --slave -e "library(ggrepel); cat(sessionInfo()\$otherPkgs\$ggrepel\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", dplyr version" $(R --vanilla --slave -e "library(dplyr); cat(sessionInfo()\$otherPkgs\$dplyr\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/119/outputs/COMMAND_VERSION 2>&1;
Rscript '/galaxy/server/database/jobs_directory/000/119/configs/tmpsdy_xwg5']
galaxy.jobs.runners DEBUG 2025-03-14 01:21:19,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (119) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/119/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/119/galaxy_119.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/119/working/volcano_plot.pdf" -a -f "/galaxy/server/database/objects/3/6/2/dataset_362075c0-8c53-41aa-93d2-db98a72e1bf2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/119/working/volcano_plot.pdf" "/galaxy/server/database/objects/3/6/2/dataset_362075c0-8c53-41aa-93d2-db98a72e1bf2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:19,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:21:19,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:21:19,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:21:19,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:19,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:19,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:27,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wwb77 with k8s id: gxy-wwb77 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:21:28,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:21:35,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 119 finished
galaxy.model.metadata DEBUG 2025-03-14 01:21:35,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 144
galaxy.util WARNING 2025-03-14 01:21:35,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/6/2/dataset_362075c0-8c53-41aa-93d2-db98a72e1bf2.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/6/2/dataset_362075c0-8c53-41aa-93d2-db98a72e1bf2.dat'
galaxy.jobs INFO 2025-03-14 01:21:35,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs DEBUG 2025-03-14 01:21:35,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 119 executed (80.543 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:35,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:21:37,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 121, 120
tpv.core.entities DEBUG 2025-03-14 01:21:37,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:21:37,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:21:37,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:21:37,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:21:37,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Working directory for job is: /galaxy/server/database/jobs_directory/000/120
galaxy.jobs.runners DEBUG 2025-03-14 01:21:37,583 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [120] queued (29.583 ms)
galaxy.jobs.handler INFO 2025-03-14 01:21:37,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:37,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 120
tpv.core.entities DEBUG 2025-03-14 01:21:37,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:21:37,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:21:37,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:21:37,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:21:37,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Working directory for job is: /galaxy/server/database/jobs_directory/000/121
galaxy.jobs.runners DEBUG 2025-03-14 01:21:37,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [121] queued (48.618 ms)
galaxy.jobs.handler INFO 2025-03-14 01:21:37,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:37,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 121
galaxy.jobs DEBUG 2025-03-14 01:21:37,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [120] prepared (83.368 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:21:37,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/120/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/120/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/120/configs/tmpprsgsfq_']
galaxy.jobs.runners DEBUG 2025-03-14 01:21:37,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (120) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/120/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/120/galaxy_120.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:37,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:21:37,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [121] prepared (77.150 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:37,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:21:37,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/121/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/121/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/121/configs/tmpwnsd8vgr']
galaxy.jobs.runners DEBUG 2025-03-14 01:21:37,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (121) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/121/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/121/galaxy_121.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:37,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:37,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:38,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:38,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-frwvl failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-frwvl.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-14 01:21:47,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-14-00-36-1/jobs/gxy-frwvl

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-frwvl": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120/gxy-frwvl) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120/gxy-frwvl) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120/gxy-frwvl) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120/gxy-frwvl) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-frwvl.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 120 (gxy-frwvl)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-l2pl8 with k8s id: gxy-l2pl8 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-frwvl to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-14 01:21:47,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 121: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:47,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120/gxy-frwvl) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-14 01:21:48,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 122, 123
tpv.core.entities DEBUG 2025-03-14 01:21:48,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:21:48,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:21:48,875 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:21:48,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:21:48,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Working directory for job is: /galaxy/server/database/jobs_directory/000/122
galaxy.jobs.runners DEBUG 2025-03-14 01:21:48,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [122] queued (37.097 ms)
galaxy.jobs.handler INFO 2025-03-14 01:21:48,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:48,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 122
tpv.core.entities DEBUG 2025-03-14 01:21:48,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:21:48,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:21:48,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:21:48,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:21:48,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Working directory for job is: /galaxy/server/database/jobs_directory/000/123
galaxy.jobs.runners DEBUG 2025-03-14 01:21:48,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [123] queued (44.389 ms)
galaxy.jobs.handler INFO 2025-03-14 01:21:48,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:48,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 123
galaxy.jobs DEBUG 2025-03-14 01:21:49,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [122] prepared (79.308 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:21:49,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/122/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/122/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/122/configs/tmpwgavzqwu']
galaxy.jobs.runners DEBUG 2025-03-14 01:21:49,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (122) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/122/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/122/galaxy_122.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:49,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:21:49,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [123] prepared (77.749 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:49,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:21:49,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/123/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/123/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/123/configs/tmpe4ehsor0']
galaxy.jobs.runners DEBUG 2025-03-14 01:21:49,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (123) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/123/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/123/galaxy_123.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:49,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:49,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:49,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:49,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-14 01:21:54,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 121 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:21:55,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'labels.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/121/working/data_fetch_upload_mio6jcqu', 'object_id': 146}]}]}]
galaxy.jobs INFO 2025-03-14 01:21:55,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 121 in /galaxy/server/database/jobs_directory/000/121
galaxy.jobs DEBUG 2025-03-14 01:21:55,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 121 executed (130.234 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:55,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:59,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qrksj with k8s id: gxy-qrksj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:21:59,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m7vkg with k8s id: gxy-m7vkg succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:21:59,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 122: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:21:59,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 123: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:22:06,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 122 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:22:06,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'category.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/122/working/data_fetch_upload_c4yxmvsj', 'object_id': 147}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:22:06,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 123 finished
galaxy.jobs INFO 2025-03-14 01:22:06,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 122 in /galaxy/server/database/jobs_directory/000/122
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:22:06,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'labels.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/123/working/data_fetch_upload_znf2g2w6', 'object_id': 148}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:22:06,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 122 executed (142.981 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:06,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-14 01:22:06,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 123 in /galaxy/server/database/jobs_directory/000/123
galaxy.jobs DEBUG 2025-03-14 01:22:06,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 123 executed (127.603 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:06,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:22:07,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 124
tpv.core.entities DEBUG 2025-03-14 01:22:07,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:22:07,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:22:07,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:22:07,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:22:07,463 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Working directory for job is: /galaxy/server/database/jobs_directory/000/124
galaxy.jobs.runners DEBUG 2025-03-14 01:22:07,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [124] queued (35.940 ms)
galaxy.jobs.handler INFO 2025-03-14 01:22:07,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:07,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 124
galaxy.jobs DEBUG 2025-03-14 01:22:07,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [124] prepared (52.982 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:07,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:22:07,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:07,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:22:08,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/124/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", ggplot2 version" $(R --vanilla --slave -e "library(ggplot2); cat(sessionInfo()\$otherPkgs\$ggplot2\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", ggrepel version" $(R --vanilla --slave -e "library(ggrepel); cat(sessionInfo()\$otherPkgs\$ggrepel\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", dplyr version" $(R --vanilla --slave -e "library(dplyr); cat(sessionInfo()\$otherPkgs\$dplyr\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/124/outputs/COMMAND_VERSION 2>&1;
Rscript '/galaxy/server/database/jobs_directory/000/124/configs/tmpgkigi6cz']
galaxy.jobs.runners DEBUG 2025-03-14 01:22:08,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (124) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/124/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/124/galaxy_124.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/124/working/volcano_plot.pdf" -a -f "/galaxy/server/database/objects/c/a/b/dataset_cab636b8-a8cd-4c42-a18d-23459fd88be6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/124/working/volcano_plot.pdf" "/galaxy/server/database/objects/c/a/b/dataset_cab636b8-a8cd-4c42-a18d-23459fd88be6.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:08,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:08,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:22:08,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:08,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:08,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:09,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:16,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c27qc with k8s id: gxy-c27qc succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:22:16,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 124: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:22:23,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 124 finished
galaxy.model.metadata DEBUG 2025-03-14 01:22:24,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 149
galaxy.util WARNING 2025-03-14 01:22:24,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/a/b/dataset_cab636b8-a8cd-4c42-a18d-23459fd88be6.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/a/b/dataset_cab636b8-a8cd-4c42-a18d-23459fd88be6.dat'
galaxy.jobs INFO 2025-03-14 01:22:24,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 124 in /galaxy/server/database/jobs_directory/000/124
galaxy.jobs DEBUG 2025-03-14 01:22:24,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 124 executed (92.203 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:24,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:22:25,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 126, 125
tpv.core.entities DEBUG 2025-03-14 01:22:25,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:22:25,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:22:25,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:22:25,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:22:25,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Working directory for job is: /galaxy/server/database/jobs_directory/000/125
galaxy.jobs.runners DEBUG 2025-03-14 01:22:25,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [125] queued (28.287 ms)
galaxy.jobs.handler INFO 2025-03-14 01:22:25,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:25,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 125
tpv.core.entities DEBUG 2025-03-14 01:22:25,862 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:22:25,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:22:25,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:22:25,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:22:25,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Working directory for job is: /galaxy/server/database/jobs_directory/000/126
galaxy.jobs.runners DEBUG 2025-03-14 01:22:25,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [126] queued (45.805 ms)
galaxy.jobs.handler INFO 2025-03-14 01:22:25,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:25,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 126
galaxy.jobs DEBUG 2025-03-14 01:22:25,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [125] prepared (81.524 ms)
galaxy.jobs.command_factory INFO 2025-03-14 01:22:25,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/125/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/125/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/125/configs/tmpeb68whws']
galaxy.jobs.runners DEBUG 2025-03-14 01:22:25,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (125) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/125/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/125/galaxy_125.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:25,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:22:25,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [126] prepared (71.309 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:26,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-14 01:22:26,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/126/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/126/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/126/configs/tmpv7upoi3y']
galaxy.jobs.runners DEBUG 2025-03-14 01:22:26,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (126) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/126/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/126/galaxy_126.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:26,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:26,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:26,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:26,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:35,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7ns59 with k8s id: gxy-7ns59 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:35,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6bdz7 with k8s id: gxy-6bdz7 succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:22:36,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 125: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:22:36,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 126: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:22:43,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 125 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:22:43,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'category.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/125/working/data_fetch_upload_w8p88zav', 'object_id': 150}]}]}]
galaxy.jobs.runners DEBUG 2025-03-14 01:22:43,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 126 finished
galaxy.jobs INFO 2025-03-14 01:22:43,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 125 in /galaxy/server/database/jobs_directory/000/125
galaxy.tool_util.provided_metadata DEBUG 2025-03-14 01:22:43,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'labels.tab', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/126/working/data_fetch_upload_kcpm_t4b', 'object_id': 151}]}]}]
galaxy.jobs DEBUG 2025-03-14 01:22:44,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 125 executed (151.388 ms)
galaxy.jobs INFO 2025-03-14 01:22:44,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 126 in /galaxy/server/database/jobs_directory/000/126
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:44,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-14 01:22:44,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 126 executed (124.628 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:44,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-14 01:22:44,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 127
tpv.core.entities DEBUG 2025-03-14 01:22:44,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-14 01:22:44,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-14 01:22:44,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-14 01:22:44,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-14 01:22:44,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Working directory for job is: /galaxy/server/database/jobs_directory/000/127
galaxy.jobs.runners DEBUG 2025-03-14 01:22:44,314 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [127] queued (34.859 ms)
galaxy.jobs.handler INFO 2025-03-14 01:22:44,316 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:44,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 127
galaxy.jobs DEBUG 2025-03-14 01:22:44,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [127] prepared (63.715 ms)
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:44,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:22:44,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:44,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-14 01:22:44,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/127/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", ggplot2 version" $(R --vanilla --slave -e "library(ggplot2); cat(sessionInfo()\$otherPkgs\$ggplot2\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", ggrepel version" $(R --vanilla --slave -e "library(ggrepel); cat(sessionInfo()\$otherPkgs\$ggrepel\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", dplyr version" $(R --vanilla --slave -e "library(dplyr); cat(sessionInfo()\$otherPkgs\$dplyr\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/127/outputs/COMMAND_VERSION 2>&1;
Rscript '/galaxy/server/database/jobs_directory/000/127/configs/tmpmp4j5es5']
galaxy.jobs.runners DEBUG 2025-03-14 01:22:44,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (127) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/127/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/127/galaxy_127.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/127/working/volcano_plot.pdf" -a -f "/galaxy/server/database/objects/4/f/a/dataset_4fa69962-25c4-449b-b10c-b2bbee9bc5e7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/127/working/volcano_plot.pdf" "/galaxy/server/database/objects/4/f/a/dataset_4fa69962-25c4-449b-b10c-b2bbee9bc5e7.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:44,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:44,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-14 01:22:44,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7: mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c
galaxy.tool_util.deps.containers INFO 2025-03-14 01:22:44,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-a1e116d3536834ecbbada3c89e7eb04341cbc496:4825d9b0ce09150fdee454e83ca2f24e2f7e952c-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:44,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:45,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:22:53,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bkllc with k8s id: gxy-bkllc succeeded
galaxy.jobs.runners DEBUG 2025-03-14 01:22:53,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 127: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-14 01:23:00,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 127 finished
galaxy.model.metadata DEBUG 2025-03-14 01:23:00,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 152
galaxy.util WARNING 2025-03-14 01:23:00,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/f/a/dataset_4fa69962-25c4-449b-b10c-b2bbee9bc5e7.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/f/a/dataset_4fa69962-25c4-449b-b10c-b2bbee9bc5e7.dat'
galaxy.jobs INFO 2025-03-14 01:23:00,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 127 in /galaxy/server/database/jobs_directory/000/127
galaxy.jobs DEBUG 2025-03-14 01:23:00,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 127 executed (126.773 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-14 01:23:00,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
