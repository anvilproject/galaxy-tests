galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:21,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ngmtc with k8s id: gxy-ngmtc succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:58:21,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 4: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 00:58:28,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 4 finished
galaxy.model.metadata DEBUG 2025-04-26 00:58:28,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 6
galaxy.model.metadata DEBUG 2025-04-26 00:58:28,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 5
galaxy.model.metadata DEBUG 2025-04-26 00:58:28,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 4
galaxy.util WARNING 2025-04-26 00:58:28,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/7/f/dataset_e7f97945-40bd-4d84-bf33-a2dd6a2ae007.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/7/f/dataset_e7f97945-40bd-4d84-bf33-a2dd6a2ae007.dat'
galaxy.jobs INFO 2025-04-26 00:58:28,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 4 in /galaxy/server/database/jobs_directory/000/4
galaxy.jobs DEBUG 2025-04-26 00:58:29,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 4 executed (118.002 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:29,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 4 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 00:58:31,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 5
tpv.core.entities DEBUG 2025-04-26 00:58:31,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:58:31,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:58:31,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:58:31,458 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:58:31,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Working directory for job is: /galaxy/server/database/jobs_directory/000/5
galaxy.jobs.runners DEBUG 2025-04-26 00:58:31,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [5] queued (26.610 ms)
galaxy.jobs.handler INFO 2025-04-26 00:58:31,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:31,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 5
galaxy.jobs DEBUG 2025-04-26 00:58:31,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [5] prepared (56.610 ms)
galaxy.jobs.command_factory INFO 2025-04-26 00:58:31,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/5/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/5/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/5/configs/tmp9m207ohj']
galaxy.jobs.runners DEBUG 2025-04-26 00:58:31,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (5) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/5/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/5/galaxy_5.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:31,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:31,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:32,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:41,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hzrjd with k8s id: gxy-hzrjd succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:58:41,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 5: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 00:58:49,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 5 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 00:58:49,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/5/working/gxupload_0', 'object_id': 7}]}]}]
galaxy.jobs INFO 2025-04-26 00:58:49,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 5 in /galaxy/server/database/jobs_directory/000/5
galaxy.jobs DEBUG 2025-04-26 00:58:49,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 5 executed (113.126 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:49,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 00:58:49,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 6
tpv.core.entities DEBUG 2025-04-26 00:58:49,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:58:49,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:58:49,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:58:49,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:58:49,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Working directory for job is: /galaxy/server/database/jobs_directory/000/6
galaxy.jobs.runners DEBUG 2025-04-26 00:58:49,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [6] queued (26.878 ms)
galaxy.jobs.handler INFO 2025-04-26 00:58:49,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:49,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 6
galaxy.jobs DEBUG 2025-04-26 00:58:50,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [6] prepared (46.070 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 00:58:50,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 00:58:50,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 00:58:50,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 00:58:50,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/6/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/5/9/2/dataset_59235d5c-27b3-4c7c-b890-f684c11b6676.dat' fastq_in.'fastqsanger.gz' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar SE -threads ${GALAXY_SLOTS:-6} fastq_in.'fastqsanger.gz' fastq_out.'fastqsanger.gz' SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out.'fastqsanger.gz' '/galaxy/server/database/objects/9/f/4/dataset_9f415990-661b-47ba-bbae-0d80cbc6494e.dat']
galaxy.jobs.runners DEBUG 2025-04-26 00:58:50,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (6) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/6/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/6/galaxy_6.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:50,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 00:58:50,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 00:58:50,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 00:58:50,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:50,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:50,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:58:54,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wgstt with k8s id: gxy-wgstt succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:58:54,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 6: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 00:59:01,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 6 finished
galaxy.model.metadata DEBUG 2025-04-26 00:59:02,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 8
galaxy.util WARNING 2025-04-26 00:59:02,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/f/4/dataset_9f415990-661b-47ba-bbae-0d80cbc6494e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/f/4/dataset_9f415990-661b-47ba-bbae-0d80cbc6494e.dat'
galaxy.jobs INFO 2025-04-26 00:59:02,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 6 in /galaxy/server/database/jobs_directory/000/6
galaxy.jobs DEBUG 2025-04-26 00:59:02,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 6 executed (105.500 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:02,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 00:59:03,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 7
tpv.core.entities DEBUG 2025-04-26 00:59:03,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:59:03,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:59:03,177 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:59:03,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:59:03,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Working directory for job is: /galaxy/server/database/jobs_directory/000/7
galaxy.jobs.runners DEBUG 2025-04-26 00:59:03,209 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [7] queued (32.032 ms)
galaxy.jobs.handler INFO 2025-04-26 00:59:03,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:03,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 7
galaxy.jobs DEBUG 2025-04-26 00:59:03,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [7] prepared (55.967 ms)
galaxy.jobs.command_factory INFO 2025-04-26 00:59:03,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/7/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/7/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/7/configs/tmpre9iicao']
galaxy.jobs.runners DEBUG 2025-04-26 00:59:03,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (7) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/7/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/7/galaxy_7.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:03,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:03,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:03,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-04-26 00:59:04,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 8
tpv.core.entities DEBUG 2025-04-26 00:59:04,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:59:04,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:59:04,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:59:04,257 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:59:04,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Working directory for job is: /galaxy/server/database/jobs_directory/000/8
galaxy.jobs.runners DEBUG 2025-04-26 00:59:04,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [8] queued (31.395 ms)
galaxy.jobs.handler INFO 2025-04-26 00:59:04,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:04,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 8
galaxy.jobs DEBUG 2025-04-26 00:59:04,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [8] prepared (69.705 ms)
galaxy.jobs.command_factory INFO 2025-04-26 00:59:04,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/8/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/8/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/8/configs/tmpvixkwy0y']
galaxy.jobs.runners DEBUG 2025-04-26 00:59:04,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (8) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/8/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/8/galaxy_8.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:04,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:04,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:04,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:13,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5m2fm with k8s id: gxy-5m2fm succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:59:13,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 7: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:14,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-spfws with k8s id: gxy-spfws succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:59:14,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 8: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 00:59:20,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 7 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 00:59:20,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/7/working/gxupload_0', 'object_id': 9}]}]}]
galaxy.jobs INFO 2025-04-26 00:59:20,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 7 in /galaxy/server/database/jobs_directory/000/7
galaxy.jobs DEBUG 2025-04-26 00:59:20,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 7 executed (119.091 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:20,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-26 00:59:21,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 8 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 00:59:21,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/8/working/gxupload_0', 'object_id': 10}]}]}]
galaxy.jobs INFO 2025-04-26 00:59:21,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 8 in /galaxy/server/database/jobs_directory/000/8
galaxy.jobs DEBUG 2025-04-26 00:59:21,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 8 executed (120.799 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:21,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 00:59:22,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 9
tpv.core.entities DEBUG 2025-04-26 00:59:22,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:59:22,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:59:22,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:59:22,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:59:22,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Working directory for job is: /galaxy/server/database/jobs_directory/000/9
galaxy.jobs.runners DEBUG 2025-04-26 00:59:22,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [9] queued (42.342 ms)
galaxy.jobs.handler INFO 2025-04-26 00:59:22,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:22,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 9
galaxy.jobs DEBUG 2025-04-26 00:59:22,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [9] prepared (56.119 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 00:59:22,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 00:59:22,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 00:59:22,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 00:59:22,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/9/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/f/a/7/dataset_fa7d501e-51ca-4be5-bffe-dd1139af786d.dat' fastq_r1.'fastqsanger.gz' && ln -s '/galaxy/server/database/objects/e/3/3/dataset_e33f7ba1-4f24-4767-9251-ec3c7c745556.dat' fastq_r2.'fastqsanger.gz' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar PE -threads ${GALAXY_SLOTS:-6} fastq_r1.'fastqsanger.gz' fastq_r2.'fastqsanger.gz' fastq_out_r1_paired.'fastqsanger.gz' fastq_out_r1_unpaired.'fastqsanger.gz' fastq_out_r2_paired.'fastqsanger.gz' fastq_out_r2_unpaired.'fastqsanger.gz' SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out_r1_paired.'fastqsanger.gz' '/galaxy/server/database/objects/6/7/e/dataset_67e8025e-7fa7-4f33-8e7e-4b53991250d6.dat' && mv fastq_out_r1_unpaired.'fastqsanger.gz' '/galaxy/server/database/objects/2/1/d/dataset_21d2cf4c-06a8-4d02-8ccb-374b19acec73.dat' && mv fastq_out_r2_paired.'fastqsanger.gz' '/galaxy/server/database/objects/e/0/5/dataset_e058a610-f684-4da0-8ef1-324be50f5a9b.dat' && mv fastq_out_r2_unpaired.'fastqsanger.gz' '/galaxy/server/database/objects/5/8/0/dataset_580de600-837b-4559-a72d-5bb91237220e.dat']
galaxy.jobs.runners DEBUG 2025-04-26 00:59:22,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (9) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/9/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/9/galaxy_9.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:22,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 00:59:22,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 00:59:22,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 00:59:22,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:22,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:23,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:27,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2m8hp with k8s id: gxy-2m8hp succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:59:27,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 9: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 00:59:34,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 9 finished
galaxy.model.metadata DEBUG 2025-04-26 00:59:34,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 14
galaxy.model.metadata DEBUG 2025-04-26 00:59:34,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 13
galaxy.model.metadata DEBUG 2025-04-26 00:59:34,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 12
galaxy.model.metadata DEBUG 2025-04-26 00:59:34,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 11
galaxy.util WARNING 2025-04-26 00:59:34,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/8/0/dataset_580de600-837b-4559-a72d-5bb91237220e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/8/0/dataset_580de600-837b-4559-a72d-5bb91237220e.dat'
galaxy.util WARNING 2025-04-26 00:59:34,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/1/d/dataset_21d2cf4c-06a8-4d02-8ccb-374b19acec73.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/1/d/dataset_21d2cf4c-06a8-4d02-8ccb-374b19acec73.dat'
galaxy.util WARNING 2025-04-26 00:59:34,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/0/5/dataset_e058a610-f684-4da0-8ef1-324be50f5a9b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/0/5/dataset_e058a610-f684-4da0-8ef1-324be50f5a9b.dat'
galaxy.util WARNING 2025-04-26 00:59:34,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/7/e/dataset_67e8025e-7fa7-4f33-8e7e-4b53991250d6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/7/e/dataset_67e8025e-7fa7-4f33-8e7e-4b53991250d6.dat'
galaxy.jobs INFO 2025-04-26 00:59:34,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 9 in /galaxy/server/database/jobs_directory/000/9
galaxy.jobs DEBUG 2025-04-26 00:59:34,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 9 executed (137.087 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:34,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 00:59:36,897 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 10
tpv.core.entities DEBUG 2025-04-26 00:59:36,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:59:36,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:59:36,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:59:36,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:59:36,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Working directory for job is: /galaxy/server/database/jobs_directory/000/10
galaxy.jobs.runners DEBUG 2025-04-26 00:59:36,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [10] queued (31.591 ms)
galaxy.jobs.handler INFO 2025-04-26 00:59:36,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:36,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 10
galaxy.jobs DEBUG 2025-04-26 00:59:37,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [10] prepared (59.346 ms)
galaxy.jobs.command_factory INFO 2025-04-26 00:59:37,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/10/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/10/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/10/configs/tmpt4vizaj7']
galaxy.jobs.runners DEBUG 2025-04-26 00:59:37,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (10) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/10/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/10/galaxy_10.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:37,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:37,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:37,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-04-26 00:59:37,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 11
tpv.core.entities DEBUG 2025-04-26 00:59:37,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:59:37,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:59:37,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:59:37,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:59:38,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Working directory for job is: /galaxy/server/database/jobs_directory/000/11
galaxy.jobs.runners DEBUG 2025-04-26 00:59:38,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [11] queued (34.609 ms)
galaxy.jobs.handler INFO 2025-04-26 00:59:38,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:38,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 11
galaxy.jobs DEBUG 2025-04-26 00:59:38,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [11] prepared (60.467 ms)
galaxy.jobs.command_factory INFO 2025-04-26 00:59:38,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/11/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/11/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/11/configs/tmp6ho4xrci']
galaxy.jobs.runners DEBUG 2025-04-26 00:59:38,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (11) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/11/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/11/galaxy_11.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:38,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:38,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:38,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n7vx6 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-n7vx6.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 00:59:47,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-n7vx6

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-n7vx6": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10/gxy-n7vx6) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10/gxy-n7vx6) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10/gxy-n7vx6) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10/gxy-n7vx6) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-n7vx6.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 10 (gxy-n7vx6)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-n7vx6 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:47,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10/gxy-n7vx6) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:48,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mtzjq with k8s id: gxy-mtzjq succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:59:48,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 11: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-04-26 00:59:49,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 12, 13
tpv.core.entities DEBUG 2025-04-26 00:59:49,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:59:49,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:59:49,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:59:49,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:59:49,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Working directory for job is: /galaxy/server/database/jobs_directory/000/12
galaxy.jobs.runners DEBUG 2025-04-26 00:59:49,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [12] queued (37.394 ms)
galaxy.jobs.handler INFO 2025-04-26 00:59:49,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:49,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 12
tpv.core.entities DEBUG 2025-04-26 00:59:49,288 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 00:59:49,288 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 00:59:49,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 00:59:49,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 00:59:49,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Working directory for job is: /galaxy/server/database/jobs_directory/000/13
galaxy.jobs.runners DEBUG 2025-04-26 00:59:49,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [13] queued (42.126 ms)
galaxy.jobs.handler INFO 2025-04-26 00:59:49,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:49,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 13
galaxy.jobs DEBUG 2025-04-26 00:59:49,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [12] prepared (75.607 ms)
galaxy.jobs.command_factory INFO 2025-04-26 00:59:49,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/12/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/12/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/12/configs/tmpnil5lmq3']
galaxy.jobs.runners DEBUG 2025-04-26 00:59:49,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (12) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/12/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/12/galaxy_12.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:49,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 00:59:49,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [13] prepared (75.095 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:49,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 00:59:49,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/13/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/13/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/13/configs/tmp56n16onj']
galaxy.jobs.runners DEBUG 2025-04-26 00:59:49,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (13) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/13/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/13/galaxy_13.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:49,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:49,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:49,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:50,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-04-26 00:59:56,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 11 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 00:59:56,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/11/working/data_fetch_upload_5ibu7we6', 'object_id': 16}]}]}]
galaxy.jobs INFO 2025-04-26 00:59:56,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 11 in /galaxy/server/database/jobs_directory/000/11
galaxy.jobs DEBUG 2025-04-26 00:59:56,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 11 executed (116.127 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:56,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 00:59:59,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dxn6m with k8s id: gxy-dxn6m succeeded
galaxy.jobs.runners DEBUG 2025-04-26 00:59:59,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 12: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:00,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-98zg5 with k8s id: gxy-98zg5 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:00:00,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 13: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:00:07,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 12 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:00:07,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastqillumina', 'dbkey': '?', 'ext': 'fastqillumina', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqillumina file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/12/working/data_fetch_upload_er4titep', 'object_id': 17}]}]}]
galaxy.jobs INFO 2025-04-26 01:00:07,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 12 in /galaxy/server/database/jobs_directory/000/12
galaxy.jobs DEBUG 2025-04-26 01:00:07,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 12 executed (125.430 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:07,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-26 01:00:08,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 13 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:00:08,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastqillumina', 'dbkey': '?', 'ext': 'fastqillumina', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqillumina file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/13/working/data_fetch_upload_vqh4lv4l', 'object_id': 18}]}]}]
galaxy.jobs INFO 2025-04-26 01:00:08,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 13 in /galaxy/server/database/jobs_directory/000/13
galaxy.jobs DEBUG 2025-04-26 01:00:08,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 13 executed (122.410 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:08,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:00:08,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 14
tpv.core.entities DEBUG 2025-04-26 01:00:08,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:00:08,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:00:08,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:00:08,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:00:08,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Working directory for job is: /galaxy/server/database/jobs_directory/000/14
galaxy.jobs.runners DEBUG 2025-04-26 01:00:08,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [14] queued (47.457 ms)
galaxy.jobs.handler INFO 2025-04-26 01:00:08,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:08,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 14
galaxy.jobs DEBUG 2025-04-26 01:00:08,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [14] prepared (66.013 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:08,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:00:08,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:08,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:00:08,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/14/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/a/9/e/dataset_a9eee98a-571d-4189-b98c-bb7eb2588b7a.dat' fastq_r1.'fastqillumina' && ln -s '/galaxy/server/database/objects/0/b/0/dataset_0b041af7-8c9f-4c91-b327-b27310359c82.dat' fastq_r2.'fastqillumina' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar PE -threads ${GALAXY_SLOTS:-6} fastq_r1.'fastqillumina' fastq_r2.'fastqillumina' fastq_out_r1_paired.'fastqillumina' fastq_out_r1_unpaired.'fastqillumina' fastq_out_r2_paired.'fastqillumina' fastq_out_r2_unpaired.'fastqillumina' SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out_r1_paired.'fastqillumina' '/galaxy/server/database/objects/8/c/5/dataset_8c576b73-1a2c-4f61-ac83-88f55252f3ee.dat' && mv fastq_out_r1_unpaired.'fastqillumina' '/galaxy/server/database/objects/4/c/c/dataset_4cce0f0f-1fac-47fc-9d78-d3278e78c988.dat' && mv fastq_out_r2_paired.'fastqillumina' '/galaxy/server/database/objects/c/0/f/dataset_c0f99f42-11dc-41b5-8b83-c1b09f36caff.dat' && mv fastq_out_r2_unpaired.'fastqillumina' '/galaxy/server/database/objects/a/e/6/dataset_ae6b29fe-ced4-4bdd-8b76-96961b4f03d5.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:00:08,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (14) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/14/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/14/galaxy_14.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:08,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:08,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:00:08,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:08,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:08,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:09,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:14,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4vttc with k8s id: gxy-4vttc succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:00:14,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 14: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:00:21,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 14 finished
galaxy.model.metadata DEBUG 2025-04-26 01:00:21,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 22
galaxy.model.metadata DEBUG 2025-04-26 01:00:21,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 21
galaxy.model.metadata DEBUG 2025-04-26 01:00:21,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 20
galaxy.model.metadata DEBUG 2025-04-26 01:00:21,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 19
galaxy.util WARNING 2025-04-26 01:00:21,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/e/6/dataset_ae6b29fe-ced4-4bdd-8b76-96961b4f03d5.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/e/6/dataset_ae6b29fe-ced4-4bdd-8b76-96961b4f03d5.dat'
galaxy.util WARNING 2025-04-26 01:00:21,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/c/c/dataset_4cce0f0f-1fac-47fc-9d78-d3278e78c988.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/c/c/dataset_4cce0f0f-1fac-47fc-9d78-d3278e78c988.dat'
galaxy.util WARNING 2025-04-26 01:00:21,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/0/f/dataset_c0f99f42-11dc-41b5-8b83-c1b09f36caff.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/0/f/dataset_c0f99f42-11dc-41b5-8b83-c1b09f36caff.dat'
galaxy.util WARNING 2025-04-26 01:00:21,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/c/5/dataset_8c576b73-1a2c-4f61-ac83-88f55252f3ee.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/c/5/dataset_8c576b73-1a2c-4f61-ac83-88f55252f3ee.dat'
galaxy.jobs INFO 2025-04-26 01:00:21,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 14 in /galaxy/server/database/jobs_directory/000/14
galaxy.jobs DEBUG 2025-04-26 01:00:21,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 14 executed (182.735 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:21,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:00:25,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 15, 16
tpv.core.entities DEBUG 2025-04-26 01:00:25,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:00:25,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:00:25,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:00:25,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:00:25,094 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Working directory for job is: /galaxy/server/database/jobs_directory/000/15
galaxy.jobs.runners DEBUG 2025-04-26 01:00:25,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [15] queued (34.024 ms)
galaxy.jobs.handler INFO 2025-04-26 01:00:25,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:25,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 15
tpv.core.entities DEBUG 2025-04-26 01:00:25,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:00:25,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:00:25,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:00:25,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:00:25,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Working directory for job is: /galaxy/server/database/jobs_directory/000/16
galaxy.jobs.runners DEBUG 2025-04-26 01:00:25,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [16] queued (42.552 ms)
galaxy.jobs.handler INFO 2025-04-26 01:00:25,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:25,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 16
galaxy.jobs DEBUG 2025-04-26 01:00:25,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [15] prepared (78.706 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:00:25,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/15/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/15/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/15/configs/tmp7eqbaogs']
galaxy.jobs.runners DEBUG 2025-04-26 01:00:25,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (15) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/15/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/15/galaxy_15.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-26 01:00:25,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [16] prepared (67.089 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:25,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:25,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:00:25,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/16/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/16/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/16/configs/tmpf00o2xd7']
galaxy.jobs.runners DEBUG 2025-04-26 01:00:25,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (16) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/16/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/16/galaxy_16.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:25,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:25,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:26,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:26,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:34,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gqhz5 with k8s id: gxy-gqhz5 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:34,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gdz9g with k8s id: gxy-gdz9g succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:00:34,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 15: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:00:35,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 16: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:00:42,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 16 finished
galaxy.jobs.runners DEBUG 2025-04-26 01:00:42,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 15 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:00:42,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastqsolexa', 'dbkey': '?', 'ext': 'fastqsolexa', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsolexa file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/16/working/data_fetch_upload_4mk0n7w8', 'object_id': 24}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:00:42,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastqsolexa', 'dbkey': '?', 'ext': 'fastqsolexa', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsolexa file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/15/working/data_fetch_upload_wb24x3ez', 'object_id': 23}]}]}]
galaxy.jobs INFO 2025-04-26 01:00:42,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 15 in /galaxy/server/database/jobs_directory/000/15
galaxy.jobs INFO 2025-04-26 01:00:42,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 16 in /galaxy/server/database/jobs_directory/000/16
galaxy.jobs DEBUG 2025-04-26 01:00:42,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 15 executed (141.003 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:42,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:00:42,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 16 executed (164.205 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:42,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:00:43,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 17
tpv.core.entities DEBUG 2025-04-26 01:00:43,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:00:43,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:00:43,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:00:43,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:00:43,583 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Working directory for job is: /galaxy/server/database/jobs_directory/000/17
galaxy.jobs.runners DEBUG 2025-04-26 01:00:43,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [17] queued (49.734 ms)
galaxy.jobs.handler INFO 2025-04-26 01:00:43,596 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:43,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 17
galaxy.jobs DEBUG 2025-04-26 01:00:43,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [17] prepared (70.878 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:43,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:00:43,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:43,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:00:43,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/17/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/a/e/2/dataset_ae2a6bb7-9289-4426-94a7-fd2ff41793a4.dat' fastq_r1.'fastqsolexa' && ln -s '/galaxy/server/database/objects/d/c/a/dataset_dcabadd0-bcd9-494f-832c-703c78cec360.dat' fastq_r2.'fastqsolexa' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar PE -threads ${GALAXY_SLOTS:-6} fastq_r1.'fastqsolexa' fastq_r2.'fastqsolexa' fastq_out_r1_paired.'fastqsolexa' fastq_out_r1_unpaired.'fastqsolexa' fastq_out_r2_paired.'fastqsolexa' fastq_out_r2_unpaired.'fastqsolexa' SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out_r1_paired.'fastqsolexa' '/galaxy/server/database/objects/c/1/a/dataset_c1ab8726-7b8d-468f-83ec-4d6d2fb1f1a3.dat' && mv fastq_out_r1_unpaired.'fastqsolexa' '/galaxy/server/database/objects/d/d/5/dataset_dd5c4c33-afce-44b7-9460-fac729e78706.dat' && mv fastq_out_r2_paired.'fastqsolexa' '/galaxy/server/database/objects/5/9/3/dataset_5935ec6d-d973-42eb-8e85-2480d5c915f4.dat' && mv fastq_out_r2_unpaired.'fastqsolexa' '/galaxy/server/database/objects/1/f/0/dataset_1f078cd4-42c4-43ee-940d-7f7161edb5a3.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:00:43,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (17) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/17/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/17/galaxy_17.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:43,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:43,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:00:43,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:00:43,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:43,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:44,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:49,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-prx4m with k8s id: gxy-prx4m succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:00:49,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 17: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:00:56,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 17 finished
galaxy.model.metadata DEBUG 2025-04-26 01:00:56,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 25
galaxy.model.metadata DEBUG 2025-04-26 01:00:56,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 26
galaxy.model.metadata DEBUG 2025-04-26 01:00:56,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 27
galaxy.model.metadata DEBUG 2025-04-26 01:00:56,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 28
galaxy.util WARNING 2025-04-26 01:00:56,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/1/a/dataset_c1ab8726-7b8d-468f-83ec-4d6d2fb1f1a3.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/1/a/dataset_c1ab8726-7b8d-468f-83ec-4d6d2fb1f1a3.dat'
galaxy.util WARNING 2025-04-26 01:00:56,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/9/3/dataset_5935ec6d-d973-42eb-8e85-2480d5c915f4.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/9/3/dataset_5935ec6d-d973-42eb-8e85-2480d5c915f4.dat'
galaxy.util WARNING 2025-04-26 01:00:56,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/d/5/dataset_dd5c4c33-afce-44b7-9460-fac729e78706.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/d/5/dataset_dd5c4c33-afce-44b7-9460-fac729e78706.dat'
galaxy.util WARNING 2025-04-26 01:00:56,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/f/0/dataset_1f078cd4-42c4-43ee-940d-7f7161edb5a3.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/f/0/dataset_1f078cd4-42c4-43ee-940d-7f7161edb5a3.dat'
galaxy.jobs INFO 2025-04-26 01:00:56,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 17 in /galaxy/server/database/jobs_directory/000/17
galaxy.jobs DEBUG 2025-04-26 01:00:56,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 17 executed (189.024 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:56,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:00:58,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 18
tpv.core.entities DEBUG 2025-04-26 01:00:58,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:00:58,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:00:58,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:00:58,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:00:58,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Working directory for job is: /galaxy/server/database/jobs_directory/000/18
galaxy.jobs.runners DEBUG 2025-04-26 01:00:58,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [18] queued (29.382 ms)
galaxy.jobs.handler INFO 2025-04-26 01:00:58,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:58,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 18
galaxy.jobs DEBUG 2025-04-26 01:00:58,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [18] prepared (54.880 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:00:58,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/18/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/18/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/18/configs/tmp50y42kuj']
galaxy.jobs.runners DEBUG 2025-04-26 01:00:58,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (18) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/18/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/18/galaxy_18.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:58,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:00:59,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:00,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:08,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nrbv2 with k8s id: gxy-nrbv2 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:01:08,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 18: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:01:15,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 18 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:01:15,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/18/working/data_fetch_upload_val18lnd', 'object_id': 29}]}]}]
galaxy.jobs INFO 2025-04-26 01:01:15,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 18 in /galaxy/server/database/jobs_directory/000/18
galaxy.jobs DEBUG 2025-04-26 01:01:15,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 18 executed (112.109 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:15,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:01:16,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 19
tpv.core.entities DEBUG 2025-04-26 01:01:16,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:01:16,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:01:16,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:01:16,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:01:16,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Working directory for job is: /galaxy/server/database/jobs_directory/000/19
galaxy.jobs.runners DEBUG 2025-04-26 01:01:16,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [19] queued (33.783 ms)
galaxy.jobs.handler INFO 2025-04-26 01:01:16,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:16,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 19
galaxy.jobs DEBUG 2025-04-26 01:01:16,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [19] prepared (52.413 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:16,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:01:16,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:16,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:01:16,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/19/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/d/f/b/dataset_dfbc9d99-716d-424b-8abe-e76e985489f8.dat' fastq_in.'fastqsanger' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar SE -threads ${GALAXY_SLOTS:-6} fastq_in.'fastqsanger' fastq_out.'fastqsanger' CROP:10 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out.'fastqsanger' '/galaxy/server/database/objects/2/9/4/dataset_294b03c8-2fdf-4290-95bb-f2ce564812bf.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:01:16,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (19) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/19/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/19/galaxy_19.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:16,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:16,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:01:16,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:16,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:16,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:17,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:21,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zrzt9 with k8s id: gxy-zrzt9 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:01:21,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 19: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:01:28,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 19 finished
galaxy.model.metadata DEBUG 2025-04-26 01:01:28,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 30
galaxy.util WARNING 2025-04-26 01:01:28,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/9/4/dataset_294b03c8-2fdf-4290-95bb-f2ce564812bf.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/9/4/dataset_294b03c8-2fdf-4290-95bb-f2ce564812bf.dat'
galaxy.jobs INFO 2025-04-26 01:01:28,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 19 in /galaxy/server/database/jobs_directory/000/19
galaxy.jobs DEBUG 2025-04-26 01:01:28,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 19 executed (101.688 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:28,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:01:30,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 20, 21
tpv.core.entities DEBUG 2025-04-26 01:01:30,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:01:30,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:01:30,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:01:30,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:01:30,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Working directory for job is: /galaxy/server/database/jobs_directory/000/20
galaxy.jobs.runners DEBUG 2025-04-26 01:01:30,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [20] queued (25.544 ms)
galaxy.jobs.handler INFO 2025-04-26 01:01:30,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:30,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 20
tpv.core.entities DEBUG 2025-04-26 01:01:30,580 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:01:30,580 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:01:30,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:01:30,599 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:01:30,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Working directory for job is: /galaxy/server/database/jobs_directory/000/21
galaxy.jobs.runners DEBUG 2025-04-26 01:01:30,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [21] queued (38.292 ms)
galaxy.jobs.handler INFO 2025-04-26 01:01:30,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:30,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 21
galaxy.jobs DEBUG 2025-04-26 01:01:30,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [20] prepared (65.904 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:01:30,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/20/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/20/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/20/configs/tmpgo_h9omh']
galaxy.jobs.runners DEBUG 2025-04-26 01:01:30,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (20) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/20/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/20/galaxy_20.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:30,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:01:30,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [21] prepared (63.694 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:30,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:01:30,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/21/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/21/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/21/configs/tmpggjblsc3']
galaxy.jobs.runners DEBUG 2025-04-26 01:01:30,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (21) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/21/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/21/galaxy_21.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:30,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:30,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:36,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:36,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:45,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vdj25 with k8s id: gxy-vdj25 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:01:45,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 20: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:46,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5fjsl with k8s id: gxy-5fjsl succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:01:46,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 21: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:01:52,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 20 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:01:52,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/20/working/data_fetch_upload_6q7qh1qk', 'object_id': 31}]}]}]
galaxy.jobs INFO 2025-04-26 01:01:52,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 20 in /galaxy/server/database/jobs_directory/000/20
galaxy.jobs DEBUG 2025-04-26 01:01:52,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 20 executed (116.531 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:52,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-26 01:01:53,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 21 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:01:53,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/21/working/data_fetch_upload_83xg_vsh', 'object_id': 32}]}]}]
galaxy.jobs INFO 2025-04-26 01:01:53,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 21 in /galaxy/server/database/jobs_directory/000/21
galaxy.jobs DEBUG 2025-04-26 01:01:53,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 21 executed (105.417 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:53,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:01:55,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 22
tpv.core.entities DEBUG 2025-04-26 01:01:55,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:01:55,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:01:55,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:01:55,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:01:55,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Working directory for job is: /galaxy/server/database/jobs_directory/000/22
galaxy.jobs.runners DEBUG 2025-04-26 01:01:55,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [22] queued (46.030 ms)
galaxy.jobs.handler INFO 2025-04-26 01:01:55,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:55,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 22
galaxy.tools.evaluation INFO 2025-04-26 01:01:55,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for forward with /galaxy/server/database/objects/a/5/b/dataset_a5b60445-d43b-4912-b273-aa37046eacfa.dat
galaxy.tools.evaluation INFO 2025-04-26 01:01:55,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for reverse with /galaxy/server/database/objects/5/1/3/dataset_513eacae-b24c-4969-83ff-0ed4d90f30ff.dat
galaxy.tools.evaluation INFO 2025-04-26 01:01:55,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for forward with /galaxy/server/database/objects/6/e/6/dataset_6e68d00a-b1c8-4e4d-804c-ef71c5ad4fa3.dat
galaxy.tools.evaluation INFO 2025-04-26 01:01:55,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for reverse with /galaxy/server/database/objects/7/9/9/dataset_79957017-0d64-4197-be47-a83c1e94050b.dat
galaxy.jobs DEBUG 2025-04-26 01:01:55,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [22] prepared (92.384 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:55,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:01:55,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:55,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:01:55,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/22/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/b/1/8/dataset_b188dbee-2c9d-4203-a824-8f51667c32f4.dat' fastq_r1.'fastqsanger' && ln -s '/galaxy/server/database/objects/b/c/b/dataset_bcb32344-c52d-460a-a052-3db065cedef5.dat' fastq_r2.'fastqsanger' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar PE -threads ${GALAXY_SLOTS:-6} fastq_r1.'fastqsanger' fastq_r2.'fastqsanger' fastq_out_r1_paired.'fastqsanger' fastq_out_r1_unpaired.'fastqsanger' fastq_out_r2_paired.'fastqsanger' fastq_out_r2_unpaired.'fastqsanger' SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out_r1_paired.'fastqsanger' '/galaxy/server/database/objects/6/e/6/dataset_6e68d00a-b1c8-4e4d-804c-ef71c5ad4fa3.dat' && mv fastq_out_r1_unpaired.'fastqsanger' '/galaxy/server/database/objects/a/5/b/dataset_a5b60445-d43b-4912-b273-aa37046eacfa.dat' && mv fastq_out_r2_paired.'fastqsanger' '/galaxy/server/database/objects/7/9/9/dataset_79957017-0d64-4197-be47-a83c1e94050b.dat' && mv fastq_out_r2_unpaired.'fastqsanger' '/galaxy/server/database/objects/5/1/3/dataset_513eacae-b24c-4969-83ff-0ed4d90f30ff.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:01:55,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (22) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/22/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/22/galaxy_22.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:55,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:55,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:01:55,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:01:55,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:55,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:01:56,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:01,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9n8k4 with k8s id: gxy-9n8k4 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:02:01,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 22: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:02:08,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 22 finished
galaxy.model.metadata DEBUG 2025-04-26 01:02:08,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 33
galaxy.model.metadata DEBUG 2025-04-26 01:02:08,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 34
galaxy.model.metadata DEBUG 2025-04-26 01:02:08,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 35
galaxy.model.metadata DEBUG 2025-04-26 01:02:08,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 36
galaxy.util WARNING 2025-04-26 01:02:08,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/e/6/dataset_6e68d00a-b1c8-4e4d-804c-ef71c5ad4fa3.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/e/6/dataset_6e68d00a-b1c8-4e4d-804c-ef71c5ad4fa3.dat'
galaxy.util WARNING 2025-04-26 01:02:08,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/9/9/dataset_79957017-0d64-4197-be47-a83c1e94050b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/9/9/dataset_79957017-0d64-4197-be47-a83c1e94050b.dat'
galaxy.util WARNING 2025-04-26 01:02:08,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/5/b/dataset_a5b60445-d43b-4912-b273-aa37046eacfa.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/5/b/dataset_a5b60445-d43b-4912-b273-aa37046eacfa.dat'
galaxy.util WARNING 2025-04-26 01:02:08,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/1/3/dataset_513eacae-b24c-4969-83ff-0ed4d90f30ff.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/1/3/dataset_513eacae-b24c-4969-83ff-0ed4d90f30ff.dat'
galaxy.jobs INFO 2025-04-26 01:02:08,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 22 in /galaxy/server/database/jobs_directory/000/22
galaxy.jobs DEBUG 2025-04-26 01:02:08,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 22 executed (178.850 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:08,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:02:10,587 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 23, 24
tpv.core.entities DEBUG 2025-04-26 01:02:10,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:02:10,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:02:10,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:02:10,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:02:10,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Working directory for job is: /galaxy/server/database/jobs_directory/000/23
galaxy.jobs.runners DEBUG 2025-04-26 01:02:10,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [23] queued (30.150 ms)
galaxy.jobs.handler INFO 2025-04-26 01:02:10,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:10,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 23
tpv.core.entities DEBUG 2025-04-26 01:02:10,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:02:10,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:02:10,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:02:10,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:02:10,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Working directory for job is: /galaxy/server/database/jobs_directory/000/24
galaxy.jobs.runners DEBUG 2025-04-26 01:02:10,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [24] queued (46.178 ms)
galaxy.jobs.handler INFO 2025-04-26 01:02:10,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:10,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 24
galaxy.jobs DEBUG 2025-04-26 01:02:10,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [23] prepared (77.750 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:02:10,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/23/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/23/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/23/configs/tmpgrpz8uti']
galaxy.jobs.runners DEBUG 2025-04-26 01:02:10,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (23) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/23/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/23/galaxy_23.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-26 01:02:10,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [24] prepared (64.366 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:10,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:10,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:02:10,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/24/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/24/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/24/configs/tmpqyjfqeuo']
galaxy.jobs.runners DEBUG 2025-04-26 01:02:10,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (24) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/24/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/24/galaxy_24.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:10,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:10,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:11,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:11,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:20,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ksfgm with k8s id: gxy-ksfgm succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:20,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jxbcz with k8s id: gxy-jxbcz succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:02:20,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 23: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:02:20,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 24: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:02:28,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 23 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:02:28,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/23/working/gxupload_0', 'object_id': 37}]}]}]
galaxy.jobs INFO 2025-04-26 01:02:28,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 23 in /galaxy/server/database/jobs_directory/000/23
galaxy.jobs.runners DEBUG 2025-04-26 01:02:28,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 24 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:02:28,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/24/working/gxupload_0', 'object_id': 38}]}]}]
galaxy.jobs DEBUG 2025-04-26 01:02:28,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 23 executed (144.299 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:28,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-04-26 01:02:28,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 24 in /galaxy/server/database/jobs_directory/000/24
galaxy.jobs DEBUG 2025-04-26 01:02:28,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 24 executed (128.973 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:28,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:02:29,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 25
tpv.core.entities DEBUG 2025-04-26 01:02:29,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:02:29,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:02:29,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:02:29,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:02:29,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Working directory for job is: /galaxy/server/database/jobs_directory/000/25
galaxy.jobs.runners DEBUG 2025-04-26 01:02:29,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [25] queued (45.882 ms)
galaxy.jobs.handler INFO 2025-04-26 01:02:29,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:29,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 25
galaxy.tools.evaluation INFO 2025-04-26 01:02:29,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for forward with /galaxy/server/database/objects/f/c/5/dataset_fc5a9e22-086b-468e-b5df-eba808246272.dat
galaxy.tools.evaluation INFO 2025-04-26 01:02:29,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for reverse with /galaxy/server/database/objects/f/6/a/dataset_f6a55ca6-2c4d-4efb-9a61-d21a5eca80d2.dat
galaxy.tools.evaluation INFO 2025-04-26 01:02:29,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for forward with /galaxy/server/database/objects/a/a/1/dataset_aa16fb74-c879-46eb-964a-ff703130cd7f.dat
galaxy.tools.evaluation INFO 2025-04-26 01:02:29,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Updating param_dict for reverse with /galaxy/server/database/objects/3/e/6/dataset_3e64ec8c-350a-486f-96a5-078f2cfc9160.dat
galaxy.jobs DEBUG 2025-04-26 01:02:29,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [25] prepared (78.115 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:02:29,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:02:29,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:02:29,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:02:29,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/25/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/b/e/4/dataset_be40a387-63ac-4106-8213-937246d84971.dat' fastq_r1.'fastqsanger.gz' && ln -s '/galaxy/server/database/objects/6/4/e/dataset_64e7d372-d659-4986-8510-2d193b485e2b.dat' fastq_r2.'fastqsanger.gz' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar PE -threads ${GALAXY_SLOTS:-6} fastq_r1.'fastqsanger.gz' fastq_r2.'fastqsanger.gz' fastq_out_r1_paired.'fastqsanger.gz' fastq_out_r1_unpaired.'fastqsanger.gz' fastq_out_r2_paired.'fastqsanger.gz' fastq_out_r2_unpaired.'fastqsanger.gz' SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out_r1_paired.'fastqsanger.gz' '/galaxy/server/database/objects/a/a/1/dataset_aa16fb74-c879-46eb-964a-ff703130cd7f.dat' && mv fastq_out_r1_unpaired.'fastqsanger.gz' '/galaxy/server/database/objects/f/c/5/dataset_fc5a9e22-086b-468e-b5df-eba808246272.dat' && mv fastq_out_r2_paired.'fastqsanger.gz' '/galaxy/server/database/objects/3/e/6/dataset_3e64ec8c-350a-486f-96a5-078f2cfc9160.dat' && mv fastq_out_r2_unpaired.'fastqsanger.gz' '/galaxy/server/database/objects/f/6/a/dataset_f6a55ca6-2c4d-4efb-9a61-d21a5eca80d2.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:02:29,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (25) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/25/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/25/galaxy_25.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:29,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:02:29,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:02:29,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:02:29,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:29,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:29,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:33,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pdsqt with k8s id: gxy-pdsqt succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:02:34,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 25: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:02:41,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 25 finished
galaxy.model.metadata DEBUG 2025-04-26 01:02:41,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 39
galaxy.model.metadata DEBUG 2025-04-26 01:02:41,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 40
galaxy.model.metadata DEBUG 2025-04-26 01:02:41,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 41
galaxy.model.metadata DEBUG 2025-04-26 01:02:41,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 42
galaxy.util WARNING 2025-04-26 01:02:41,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/a/1/dataset_aa16fb74-c879-46eb-964a-ff703130cd7f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/a/1/dataset_aa16fb74-c879-46eb-964a-ff703130cd7f.dat'
galaxy.util WARNING 2025-04-26 01:02:41,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/e/6/dataset_3e64ec8c-350a-486f-96a5-078f2cfc9160.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/e/6/dataset_3e64ec8c-350a-486f-96a5-078f2cfc9160.dat'
galaxy.util WARNING 2025-04-26 01:02:41,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/c/5/dataset_fc5a9e22-086b-468e-b5df-eba808246272.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/c/5/dataset_fc5a9e22-086b-468e-b5df-eba808246272.dat'
galaxy.util WARNING 2025-04-26 01:02:41,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/6/a/dataset_f6a55ca6-2c4d-4efb-9a61-d21a5eca80d2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/6/a/dataset_f6a55ca6-2c4d-4efb-9a61-d21a5eca80d2.dat'
galaxy.jobs INFO 2025-04-26 01:02:41,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 25 in /galaxy/server/database/jobs_directory/000/25
galaxy.jobs DEBUG 2025-04-26 01:02:41,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 25 executed (144.994 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:41,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:02:43,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 26
tpv.core.entities DEBUG 2025-04-26 01:02:43,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:02:43,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:02:43,457 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:02:43,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:02:43,481 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Working directory for job is: /galaxy/server/database/jobs_directory/000/26
galaxy.jobs.runners DEBUG 2025-04-26 01:02:43,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [26] queued (31.121 ms)
galaxy.jobs.handler INFO 2025-04-26 01:02:43,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:43,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 26
galaxy.jobs DEBUG 2025-04-26 01:02:43,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [26] prepared (58.006 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:02:43,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/26/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/26/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/26/configs/tmp3gotp7na']
galaxy.jobs.runners DEBUG 2025-04-26 01:02:43,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/26/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/26/galaxy_26.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:43,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:43,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:43,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4wdwq failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-4wdwq.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:02:53,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-4wdwq

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-4wdwq": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26/gxy-4wdwq) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26/gxy-4wdwq) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26/gxy-4wdwq) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26/gxy-4wdwq) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-4wdwq.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 26 (gxy-4wdwq)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-4wdwq to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:53,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26/gxy-4wdwq) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-26 01:02:54,664 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 27
tpv.core.entities DEBUG 2025-04-26 01:02:54,690 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:02:54,691 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:02:54,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:02:54,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:02:54,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Working directory for job is: /galaxy/server/database/jobs_directory/000/27
galaxy.jobs.runners DEBUG 2025-04-26 01:02:54,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [27] queued (29.676 ms)
galaxy.jobs.handler INFO 2025-04-26 01:02:54,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:54,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 27
galaxy.jobs DEBUG 2025-04-26 01:02:54,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [27] prepared (54.645 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:02:54,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/27/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/27/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/27/configs/tmpautzv1ci']
galaxy.jobs.runners DEBUG 2025-04-26 01:02:54,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (27) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/27/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/27/galaxy_27.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:54,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:54,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:02:55,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:04,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-chnkb with k8s id: gxy-chnkb succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:03:04,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 27: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:03:11,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 27 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:03:11,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/27/working/data_fetch_upload_vydgg2sd', 'object_id': 44}]}]}]
galaxy.jobs INFO 2025-04-26 01:03:11,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 27 in /galaxy/server/database/jobs_directory/000/27
galaxy.jobs DEBUG 2025-04-26 01:03:11,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 27 executed (106.667 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:11,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:03:12,005 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 28
tpv.core.entities DEBUG 2025-04-26 01:03:12,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:03:12,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:03:12,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:03:12,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:03:12,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Working directory for job is: /galaxy/server/database/jobs_directory/000/28
galaxy.jobs.runners DEBUG 2025-04-26 01:03:12,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [28] queued (31.889 ms)
galaxy.jobs.handler INFO 2025-04-26 01:03:12,070 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:12,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 28
galaxy.jobs DEBUG 2025-04-26 01:03:12,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [28] prepared (52.912 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:12,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:03:12,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:12,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:03:12,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/28/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/0/7/9/dataset_07919350-8955-4b9f-ac4c-53fb6b3e8a1e.dat' fastq_in.'fastqsanger' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar SE -threads ${GALAXY_SLOTS:-6} fastq_in.'fastqsanger' fastq_out.'fastqsanger' MAXINFO:75:0.8 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out.'fastqsanger' '/galaxy/server/database/objects/7/e/3/dataset_7e32fb88-fde5-4477-872c-becdfb01996c.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:03:12,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (28) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/28/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/28/galaxy_28.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:12,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:12,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:03:12,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:12,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:12,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:12,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:16,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wm68r failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:16,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:16,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-wm68r.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:16,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:03:17,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-wm68r

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-wm68r": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (28/gxy-wm68r) tool_stdout: Picked up _JAVA_OPTIONS: -Xmx12G -Xms1G
TrimmomaticSE: Started with arguments:
 -threads 6 fastq_in.fastqsanger fastq_out.fastqsanger MAXINFO:75:0.8
Quality encoding detected as phred33
Input Reads: 10 Surviving: 10 (100.00%) Dropped: 0 (0.00%)
TrimmomaticSE: Completed successfully

galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (28/gxy-wm68r) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (28/gxy-wm68r) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (28/gxy-wm68r) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-wm68r.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 28 (gxy-wm68r)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-wm68r to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:17,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (28/gxy-wm68r) Terminated at user's request
galaxy.util WARNING 2025-04-26 01:03:17,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/e/3/dataset_7e32fb88-fde5-4477-872c-becdfb01996c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/e/3/dataset_7e32fb88-fde5-4477-872c-becdfb01996c.dat'
galaxy.jobs.handler DEBUG 2025-04-26 01:03:19,177 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 29, 30
tpv.core.entities DEBUG 2025-04-26 01:03:19,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:03:19,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:03:19,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:03:19,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:03:19,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Working directory for job is: /galaxy/server/database/jobs_directory/000/29
galaxy.jobs.runners DEBUG 2025-04-26 01:03:19,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [29] queued (27.235 ms)
galaxy.jobs.handler INFO 2025-04-26 01:03:19,233 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:19,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 29
tpv.core.entities DEBUG 2025-04-26 01:03:19,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:03:19,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:03:19,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:03:19,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:03:19,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Working directory for job is: /galaxy/server/database/jobs_directory/000/30
galaxy.jobs.runners DEBUG 2025-04-26 01:03:19,289 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [30] queued (40.582 ms)
galaxy.jobs.handler INFO 2025-04-26 01:03:19,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:19,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 30
galaxy.jobs DEBUG 2025-04-26 01:03:19,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [29] prepared (67.673 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:03:19,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/29/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/29/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/29/configs/tmptqtl2pwh']
galaxy.jobs.runners DEBUG 2025-04-26 01:03:19,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (29) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/29/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/29/galaxy_29.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:19,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:03:19,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [30] prepared (71.980 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:19,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:03:19,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/30/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/30/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/30/configs/tmpsywwj_xq']
galaxy.jobs.runners DEBUG 2025-04-26 01:03:19,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (30) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/30/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/30/galaxy_30.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:19,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:19,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:20,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:20,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:29,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-stj4m with k8s id: gxy-stj4m succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:29,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ztxfk with k8s id: gxy-ztxfk succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:03:29,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 29: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:03:29,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 30: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:03:37,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 30 finished
galaxy.jobs.runners DEBUG 2025-04-26 01:03:37,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 29 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:03:37,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/30/working/data_fetch_upload_qfepx4ks', 'object_id': 47}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:03:37,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/29/working/data_fetch_upload_chkgw223', 'object_id': 46}]}]}]
galaxy.jobs INFO 2025-04-26 01:03:37,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 30 in /galaxy/server/database/jobs_directory/000/30
galaxy.jobs INFO 2025-04-26 01:03:37,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 29 in /galaxy/server/database/jobs_directory/000/29
galaxy.jobs DEBUG 2025-04-26 01:03:37,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 30 executed (142.061 ms)
galaxy.jobs DEBUG 2025-04-26 01:03:37,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 29 executed (132.702 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:37,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:37,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:03:37,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 31
tpv.core.entities DEBUG 2025-04-26 01:03:37,660 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:03:37,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:03:37,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:03:37,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:03:37,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Working directory for job is: /galaxy/server/database/jobs_directory/000/31
galaxy.jobs.runners DEBUG 2025-04-26 01:03:37,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [31] queued (48.241 ms)
galaxy.jobs.handler INFO 2025-04-26 01:03:37,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:37,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 31
galaxy.jobs DEBUG 2025-04-26 01:03:37,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [31] prepared (65.034 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:37,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:03:37,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:37,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:03:37,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/31/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/a/c/3/dataset_ac3413f4-0bad-4a7d-88e3-5880eb8ce331.dat' fastq_r1.'fastqsanger' && ln -s '/galaxy/server/database/objects/b/d/9/dataset_bd93d516-2fe0-4f53-883b-9a7a5a1b6e0d.dat' fastq_r2.'fastqsanger' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar PE -threads ${GALAXY_SLOTS:-6} fastq_r1.'fastqsanger' fastq_r2.'fastqsanger' fastq_out_r1_paired.'fastqsanger' fastq_out_r1_unpaired.'fastqsanger' fastq_out_r2_paired.'fastqsanger' fastq_out_r2_unpaired.'fastqsanger' ILLUMINACLIP:$TRIMMOMATIC_ADAPTERS_PATH/TruSeq2-PE.fa:2:30:10:8:true SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out_r1_paired.'fastqsanger' '/galaxy/server/database/objects/6/8/5/dataset_685f9a35-af5b-495e-a986-3cf572a61da8.dat' && mv fastq_out_r1_unpaired.'fastqsanger' '/galaxy/server/database/objects/c/a/c/dataset_caca95a5-7d28-439e-be77-2ceb394a2c67.dat' && mv fastq_out_r2_paired.'fastqsanger' '/galaxy/server/database/objects/7/9/f/dataset_79fb9c22-1dc2-4773-b1a0-30085703ec26.dat' && mv fastq_out_r2_unpaired.'fastqsanger' '/galaxy/server/database/objects/3/0/f/dataset_30fe0185-f30e-4890-aac6-8c76dea466f0.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:03:37,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (31) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/31/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/31/galaxy_31.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:37,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:37,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:03:37,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:03:37,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:37,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:38,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:42,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bw2t5 with k8s id: gxy-bw2t5 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:03:42,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 31: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:03:49,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 31 finished
galaxy.model.metadata DEBUG 2025-04-26 01:03:49,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 48
galaxy.model.metadata DEBUG 2025-04-26 01:03:49,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 49
galaxy.model.metadata DEBUG 2025-04-26 01:03:49,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 50
galaxy.model.metadata DEBUG 2025-04-26 01:03:49,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 51
galaxy.util WARNING 2025-04-26 01:03:49,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/8/5/dataset_685f9a35-af5b-495e-a986-3cf572a61da8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/8/5/dataset_685f9a35-af5b-495e-a986-3cf572a61da8.dat'
galaxy.util WARNING 2025-04-26 01:03:49,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/9/f/dataset_79fb9c22-1dc2-4773-b1a0-30085703ec26.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/9/f/dataset_79fb9c22-1dc2-4773-b1a0-30085703ec26.dat'
galaxy.util WARNING 2025-04-26 01:03:49,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/a/c/dataset_caca95a5-7d28-439e-be77-2ceb394a2c67.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/a/c/dataset_caca95a5-7d28-439e-be77-2ceb394a2c67.dat'
galaxy.util WARNING 2025-04-26 01:03:49,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/0/f/dataset_30fe0185-f30e-4890-aac6-8c76dea466f0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/0/f/dataset_30fe0185-f30e-4890-aac6-8c76dea466f0.dat'
galaxy.jobs INFO 2025-04-26 01:03:49,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 31 in /galaxy/server/database/jobs_directory/000/31
galaxy.jobs DEBUG 2025-04-26 01:03:49,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 31 executed (182.076 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:49,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:03:52,966 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 33, 32
tpv.core.entities DEBUG 2025-04-26 01:03:52,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:03:52,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:03:52,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:03:53,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:03:53,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Working directory for job is: /galaxy/server/database/jobs_directory/000/33
galaxy.jobs.runners DEBUG 2025-04-26 01:03:53,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [33] queued (33.478 ms)
galaxy.jobs.handler INFO 2025-04-26 01:03:53,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 33
tpv.core.entities DEBUG 2025-04-26 01:03:53,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:03:53,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:03:53,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:03:53,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:03:53,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Working directory for job is: /galaxy/server/database/jobs_directory/000/32
galaxy.jobs.runners DEBUG 2025-04-26 01:03:53,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [32] queued (40.073 ms)
galaxy.jobs.handler INFO 2025-04-26 01:03:53,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 32
galaxy.jobs DEBUG 2025-04-26 01:03:53,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [33] prepared (74.803 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:03:53,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/33/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/33/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/33/configs/tmpe4ccs6ei']
galaxy.jobs.runners DEBUG 2025-04-26 01:03:53,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (33) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/33/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/33/galaxy_33.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:03:53,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [32] prepared (70.047 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:03:53,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/32/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/32/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/32/configs/tmpcolosr_7']
galaxy.jobs.runners DEBUG 2025-04-26 01:03:53,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (32) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/32/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/32/galaxy_32.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:03:53,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:02,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sqtvj with k8s id: gxy-sqtvj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:02,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6dzmg with k8s id: gxy-6dzmg succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:04:02,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 33: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:04:03,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 32: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:04:10,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 32 finished
galaxy.jobs.runners DEBUG 2025-04-26 01:04:10,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 33 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:04:10,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/32/working/data_fetch_upload_2quga9f6', 'object_id': 52}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:04:10,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R2.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/33/working/data_fetch_upload_viv84i39', 'object_id': 53}]}]}]
galaxy.jobs INFO 2025-04-26 01:04:10,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 32 in /galaxy/server/database/jobs_directory/000/32
galaxy.jobs INFO 2025-04-26 01:04:10,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 33 in /galaxy/server/database/jobs_directory/000/33
galaxy.jobs DEBUG 2025-04-26 01:04:10,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 32 executed (141.145 ms)
galaxy.jobs DEBUG 2025-04-26 01:04:10,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 33 executed (143.380 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:10,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:10,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:04:11,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 34
tpv.core.entities DEBUG 2025-04-26 01:04:11,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:04:11,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:04:11,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:04:11,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:04:11,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Working directory for job is: /galaxy/server/database/jobs_directory/000/34
galaxy.jobs.runners DEBUG 2025-04-26 01:04:11,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [34] queued (40.358 ms)
galaxy.jobs.handler INFO 2025-04-26 01:04:11,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:11,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 34
galaxy.jobs DEBUG 2025-04-26 01:04:11,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [34] prepared (59.633 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:11,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:04:11,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:11,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:04:11,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/34/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/0/6/c/dataset_06c1d762-d02a-4cfe-a7d4-088713280d00.dat' fastq_r1.'fastqsanger' && ln -s '/galaxy/server/database/objects/c/f/e/dataset_cfe12348-5634-4c49-aa1d-3eeea791b11d.dat' fastq_r2.'fastqsanger' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar PE -threads ${GALAXY_SLOTS:-6} fastq_r1.'fastqsanger' fastq_r2.'fastqsanger' fastq_out_r1_paired.'fastqsanger' fastq_out_r1_unpaired.'fastqsanger' fastq_out_r2_paired.'fastqsanger' fastq_out_r2_unpaired.'fastqsanger' ILLUMINACLIP:/galaxy/server/database/jobs_directory/000/34/configs/tmp834o_f66:2:30:10:8:true SLIDINGWINDOW:4:20 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out_r1_paired.'fastqsanger' '/galaxy/server/database/objects/5/b/c/dataset_5bc0bc5a-c637-4abb-84ef-05ad52ce6013.dat' && mv fastq_out_r1_unpaired.'fastqsanger' '/galaxy/server/database/objects/f/2/3/dataset_f23c8e43-014f-481b-b01f-4249cb24c920.dat' && mv fastq_out_r2_paired.'fastqsanger' '/galaxy/server/database/objects/2/f/5/dataset_2f5e981d-d25c-4955-86fb-e5aa084ef5e4.dat' && mv fastq_out_r2_unpaired.'fastqsanger' '/galaxy/server/database/objects/8/c/3/dataset_8c31d393-0ed3-46f0-a749-3e78a8270f4e.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:04:11,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (34) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/34/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/34/galaxy_34.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:11,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:11,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:04:11,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:11,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:11,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:11,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:15,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bd6kr with k8s id: gxy-bd6kr succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:04:16,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 34: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:04:23,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 34 finished
galaxy.model.metadata DEBUG 2025-04-26 01:04:23,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 54
galaxy.model.metadata DEBUG 2025-04-26 01:04:23,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 55
galaxy.model.metadata DEBUG 2025-04-26 01:04:23,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 56
galaxy.model.metadata DEBUG 2025-04-26 01:04:23,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 57
galaxy.util WARNING 2025-04-26 01:04:23,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/b/c/dataset_5bc0bc5a-c637-4abb-84ef-05ad52ce6013.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/b/c/dataset_5bc0bc5a-c637-4abb-84ef-05ad52ce6013.dat'
galaxy.util WARNING 2025-04-26 01:04:23,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/f/5/dataset_2f5e981d-d25c-4955-86fb-e5aa084ef5e4.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/f/5/dataset_2f5e981d-d25c-4955-86fb-e5aa084ef5e4.dat'
galaxy.util WARNING 2025-04-26 01:04:23,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/2/3/dataset_f23c8e43-014f-481b-b01f-4249cb24c920.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/2/3/dataset_f23c8e43-014f-481b-b01f-4249cb24c920.dat'
galaxy.util WARNING 2025-04-26 01:04:23,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/c/3/dataset_8c31d393-0ed3-46f0-a749-3e78a8270f4e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/c/3/dataset_8c31d393-0ed3-46f0-a749-3e78a8270f4e.dat'
galaxy.jobs INFO 2025-04-26 01:04:23,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 34 in /galaxy/server/database/jobs_directory/000/34
galaxy.jobs DEBUG 2025-04-26 01:04:23,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 34 executed (186.993 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:23,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:04:25,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 35
tpv.core.entities DEBUG 2025-04-26 01:04:25,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:04:25,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:04:25,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:04:25,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:04:25,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Working directory for job is: /galaxy/server/database/jobs_directory/000/35
galaxy.jobs.runners DEBUG 2025-04-26 01:04:25,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [35] queued (32.804 ms)
galaxy.jobs.handler INFO 2025-04-26 01:04:25,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:25,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 35
galaxy.jobs DEBUG 2025-04-26 01:04:25,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [35] prepared (54.760 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:04:25,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/35/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/35/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/35/configs/tmpzoyix_yd']
galaxy.jobs.runners DEBUG 2025-04-26 01:04:25,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (35) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/35/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/35/galaxy_35.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:25,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:25,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:27,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:36,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9hcl4 with k8s id: gxy-9hcl4 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:04:36,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 35: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:04:43,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 35 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:04:43,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Illumina_SG_R1.fastq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/35/working/data_fetch_upload_ztcp3my7', 'object_id': 58}]}]}]
galaxy.jobs INFO 2025-04-26 01:04:43,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 35 in /galaxy/server/database/jobs_directory/000/35
galaxy.jobs DEBUG 2025-04-26 01:04:43,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 35 executed (122.525 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:43,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:04:44,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 36
tpv.core.entities DEBUG 2025-04-26 01:04:44,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/.*, abstract=False, cores=6, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[{'name': '_JAVA_OPTIONS', 'value': '-Xmx{int(mem)}G -Xms1G'}], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:04:44,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:04:44,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:04:44,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:04:44,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Working directory for job is: /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-04-26 01:04:44,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [36] queued (37.149 ms)
galaxy.jobs.handler INFO 2025-04-26 01:04:44,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:44,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 36
galaxy.jobs DEBUG 2025-04-26 01:04:44,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [36] prepared (56.695 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:44,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:04:44,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:44,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:04:44,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/36/tool_script.sh] for tool command [if [ -z "$TRIMMOMATIC_JAR_PATH" ]; then export TRIMMOMATIC_JAR_PATH=$(dirname $(readlink -e $(which trimmomatic))); fi && if [ -z "$TRIMMOMATIC_ADAPTERS_PATH" ]; then export TRIMMOMATIC_ADAPTERS_PATH=$(dirname $(readlink -e $(which trimmomatic)))/adapters; fi && ln -s '/galaxy/server/database/objects/4/7/9/dataset_479c5e54-e559-4a2e-b240-bc70bd00892b.dat' fastq_in.'fastqsanger' && java ${_JAVA_OPTIONS:--Xmx8G} -jar $TRIMMOMATIC_JAR_PATH/trimmomatic.jar SE -threads ${GALAXY_SLOTS:-6} fastq_in.'fastqsanger' fastq_out.'fastqsanger' SLIDINGWINDOW:4:20 -trimlog trimlog -phred33 2>&1 | tee trimmomatic.log && if [ -z "$(tail -1 trimmomatic.log | grep "Completed successfully")" ]; then echo "Trimmomatic did not finish successfully" >&2 ; exit 1 ; fi && mv fastq_out.'fastqsanger' '/galaxy/server/database/objects/8/6/8/dataset_8686e0a5-9d35-4142-bb7f-897bfab8513d.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:04:44,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (36) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/36/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/36/galaxy_36.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/36/working/trimlog" -a -f "/galaxy/server/database/objects/f/4/1/dataset_f411c49d-92c1-4219-96ee-29df974d48af.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/36/working/trimlog" "/galaxy/server/database/objects/f/4/1/dataset_f411c49d-92c1-4219-96ee-29df974d48af.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/36/working/trimmomatic.log" -a -f "/galaxy/server/database/objects/e/4/0/dataset_e40a6880-8188-4b46-a983-b7a6e0949936.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/36/working/trimmomatic.log" "/galaxy/server/database/objects/e/4/0/dataset_e40a6880-8188-4b46-a983-b7a6e0949936.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:44,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:44,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:04:44,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/pjbriggs/trimmomatic/trimmomatic/0.39+galaxy2: mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc
galaxy.tool_util.deps.containers INFO 2025-04-26 01:04:44,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e1a6ade46756da3bd8ceef31e6d2e7645f383a87:cc0d0a81b925169f595fa865742dc6e1e836f3cc-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:44,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:45,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:48,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wv4xz with k8s id: gxy-wv4xz succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:04:48,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 36: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:04:55,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 36 finished
galaxy.model.metadata DEBUG 2025-04-26 01:04:55,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 59
galaxy.model.metadata DEBUG 2025-04-26 01:04:55,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 60
galaxy.model.metadata DEBUG 2025-04-26 01:04:55,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 61
galaxy.util WARNING 2025-04-26 01:04:55,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/6/8/dataset_8686e0a5-9d35-4142-bb7f-897bfab8513d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/6/8/dataset_8686e0a5-9d35-4142-bb7f-897bfab8513d.dat'
galaxy.jobs INFO 2025-04-26 01:04:55,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs DEBUG 2025-04-26 01:04:55,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 36 executed (146.636 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:55,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:04:58,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 37
tpv.core.entities DEBUG 2025-04-26 01:04:58,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:04:58,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:04:58,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:04:58,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:04:58,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.runners DEBUG 2025-04-26 01:04:58,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (33.573 ms)
galaxy.jobs.handler INFO 2025-04-26 01:04:58,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:58,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 37
galaxy.jobs DEBUG 2025-04-26 01:04:58,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [37] prepared (72.139 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:04:58,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/37/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/37/configs/tmp2ce16m9d']
galaxy.jobs.runners DEBUG 2025-04-26 01:04:58,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:58,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:58,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:04:59,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:08,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qlgnz with k8s id: gxy-qlgnz succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:05:08,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 37: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:05:15,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 37 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:05:15,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '2_isize_overflow.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/37/working/data_fetch_upload_mc751p5t', 'object_id': 62}]}]}]
galaxy.jobs INFO 2025-04-26 01:05:15,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 37 in /galaxy/server/database/jobs_directory/000/37
galaxy.jobs DEBUG 2025-04-26 01:05:15,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 37 executed (113.050 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:15,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:05:16,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38
tpv.core.entities DEBUG 2025-04-26 01:05:16,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:05:16,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:05:16,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:05:16,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:05:16,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-04-26 01:05:16,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (33.697 ms)
galaxy.jobs.handler INFO 2025-04-26 01:05:16,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:16,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 38
galaxy.tools.wrappers WARNING 2025-04-26 01:05:16,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Datatype class not found for extension 'qnamed_input_sorted.bam', which is used as parameter of 'is_of_type()' method
galaxy.jobs DEBUG 2025-04-26 01:05:16,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [38] prepared (45.147 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:05:16,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:05:16,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:05:17,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:05:17,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/38/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -n -O BAM -o namesorted.bam '/galaxy/server/database/objects/a/4/4/dataset_a44fb28d-5178-492c-ad57-d82b6f24c8d0.dat' &&  samtools fixmate -@ $addthreads     -O BAM namesorted.bam '/galaxy/server/database/objects/0/3/b/dataset_03b8ed57-ff0d-4f73-8716-e37b937801c6.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:05:17,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:17,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:05:17,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:05:17,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:05:17,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:17,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:17,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:26,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hqrw9 with k8s id: gxy-hqrw9 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:05:26,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 38: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:05:33,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 38 finished
galaxy.model.metadata DEBUG 2025-04-26 01:05:33,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 63
galaxy.jobs INFO 2025-04-26 01:05:33,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 38 in /galaxy/server/database/jobs_directory/000/38
galaxy.jobs DEBUG 2025-04-26 01:05:33,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 38 executed (100.121 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:33,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:05:35,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 39
tpv.core.entities DEBUG 2025-04-26 01:05:35,193 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:05:35,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:05:35,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:05:35,208 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:05:35,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.runners DEBUG 2025-04-26 01:05:35,228 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (30.191 ms)
galaxy.jobs.handler INFO 2025-04-26 01:05:35,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:35,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 39
galaxy.jobs DEBUG 2025-04-26 01:05:35,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [39] prepared (56.749 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:05:35,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/39/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/39/configs/tmpens0wm7_']
galaxy.jobs.runners DEBUG 2025-04-26 01:05:35,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:35,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:35,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:35,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d2r25 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-d2r25.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:05:44,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-d2r25

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-d2r25": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (39/gxy-d2r25) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (39/gxy-d2r25) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (39/gxy-d2r25) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (39/gxy-d2r25) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-d2r25.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 39 (gxy-d2r25)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-d2r25 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:44,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (39/gxy-d2r25) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-26 01:05:46,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 40
tpv.core.entities DEBUG 2025-04-26 01:05:46,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:05:46,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:05:46,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:05:46,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:05:46,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-04-26 01:05:46,465 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (30.167 ms)
galaxy.jobs.handler INFO 2025-04-26 01:05:46,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:46,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 40
galaxy.jobs DEBUG 2025-04-26 01:05:46,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [40] prepared (53.946 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:05:46,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/40/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/40/configs/tmp_xuzfekr']
galaxy.jobs.runners DEBUG 2025-04-26 01:05:46,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:46,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:46,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:48,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:05:57,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sdb6f with k8s id: gxy-sdb6f succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:05:57,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 40: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:06:04,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 40 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:06:04,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '4_reverse_read_pp_equal.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/40/working/data_fetch_upload_vp92nwie', 'object_id': 65}]}]}]
galaxy.jobs INFO 2025-04-26 01:06:04,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 40 in /galaxy/server/database/jobs_directory/000/40
galaxy.jobs DEBUG 2025-04-26 01:06:04,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 40 executed (128.981 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:04,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:06:05,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-04-26 01:06:05,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:06:05,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:06:05,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:06:05,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:06:05,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-04-26 01:06:05,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (33.052 ms)
galaxy.jobs.handler INFO 2025-04-26 01:06:05,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:05,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 41
galaxy.tools.wrappers WARNING 2025-04-26 01:06:05,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Datatype class not found for extension 'qnamed_input_sorted.bam', which is used as parameter of 'is_of_type()' method
galaxy.jobs DEBUG 2025-04-26 01:06:05,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [41] prepared (37.853 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:05,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:06:05,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:05,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:06:05,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/41/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -n -O BAM -o namesorted.bam '/galaxy/server/database/objects/f/4/c/dataset_f4c664cc-60ea-413e-a0a7-be5dd4d372b7.dat' &&  samtools fixmate -@ $addthreads     -O BAM namesorted.bam '/galaxy/server/database/objects/a/3/3/dataset_a33d6dc9-7e73-477d-a0c2-2fbfb5dc7893.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:06:05,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:05,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:05,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:06:05,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:06,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:06,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:06,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:10,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xmf99 with k8s id: gxy-xmf99 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:06:10,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:06:17,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 41 finished
galaxy.model.metadata DEBUG 2025-04-26 01:06:17,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 66
galaxy.jobs INFO 2025-04-26 01:06:17,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-04-26 01:06:17,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 41 executed (95.714 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:17,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:06:19,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 42
tpv.core.entities DEBUG 2025-04-26 01:06:19,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:06:19,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:06:19,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:06:19,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:06:19,169 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-04-26 01:06:19,177 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (35.945 ms)
galaxy.jobs.handler INFO 2025-04-26 01:06:19,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:19,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 42
galaxy.jobs DEBUG 2025-04-26 01:06:19,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [42] prepared (58.816 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:06:19,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/42/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/42/configs/tmp3jius0jq']
galaxy.jobs.runners DEBUG 2025-04-26 01:06:19,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:19,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:19,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:20,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:28,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5fmbc with k8s id: gxy-5fmbc succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:06:28,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:06:35,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 42 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:06:35,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '5_ct.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/42/working/data_fetch_upload_g0hwpql2', 'object_id': 67}]}]}]
galaxy.jobs INFO 2025-04-26 01:06:35,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs DEBUG 2025-04-26 01:06:35,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 42 executed (105.913 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:35,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:06:36,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43
tpv.core.entities DEBUG 2025-04-26 01:06:36,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:06:36,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:06:36,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:06:36,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:06:36,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-04-26 01:06:36,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (31.443 ms)
galaxy.jobs.handler INFO 2025-04-26 01:06:36,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:36,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 43
galaxy.tools.wrappers WARNING 2025-04-26 01:06:36,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Datatype class not found for extension 'qnamed_input_sorted.bam', which is used as parameter of 'is_of_type()' method
galaxy.jobs DEBUG 2025-04-26 01:06:36,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [43] prepared (36.341 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:36,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:06:36,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:36,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:06:36,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/43/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -n -O BAM -o namesorted.bam '/galaxy/server/database/objects/9/5/2/dataset_95282fae-8a05-47c6-875f-c2496e9937a7.dat' &&  samtools fixmate -@ $addthreads   -c  -O BAM namesorted.bam '/galaxy/server/database/objects/8/9/f/dataset_89ff096a-765c-46d2-b673-2cb72fc90297.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:06:36,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:36,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:36,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:06:36,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:06:36,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:36,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:37,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:40,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-k2h6z with k8s id: gxy-k2h6z succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:06:40,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:06:47,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 43 finished
galaxy.model.metadata DEBUG 2025-04-26 01:06:47,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 68
galaxy.jobs INFO 2025-04-26 01:06:47,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-04-26 01:06:47,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 43 executed (96.766 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:47,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:06:48,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 44
tpv.core.entities DEBUG 2025-04-26 01:06:48,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:06:48,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:06:48,755 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:06:48,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:06:48,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.runners DEBUG 2025-04-26 01:06:48,785 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (29.485 ms)
galaxy.jobs.handler INFO 2025-04-26 01:06:48,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:48,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 44
galaxy.jobs DEBUG 2025-04-26 01:06:48,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [44] prepared (67.613 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:06:48,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/44/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/44/configs/tmptk4rru5d']
galaxy.jobs.runners DEBUG 2025-04-26 01:06:48,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:48,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:48,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:49,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:06:58,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hg468 with k8s id: gxy-hg468 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:06:58,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:07:05,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 44 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:07:06,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '6_ct_replace.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/44/working/data_fetch_upload_w3s2h1wr', 'object_id': 69}]}]}]
galaxy.jobs INFO 2025-04-26 01:07:06,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-04-26 01:07:06,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 44 executed (117.101 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:06,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:07:07,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-04-26 01:07:07,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:07:07,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:07:07,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:07:07,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:07:07,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-04-26 01:07:07,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (31.027 ms)
galaxy.jobs.handler INFO 2025-04-26 01:07:07,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:07,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 45
galaxy.tools.wrappers WARNING 2025-04-26 01:07:07,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Datatype class not found for extension 'qnamed_input_sorted.bam', which is used as parameter of 'is_of_type()' method
galaxy.jobs DEBUG 2025-04-26 01:07:07,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [45] prepared (40.961 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:07,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:07:07,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:07,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:07:07,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/45/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -n -O BAM -o namesorted.bam '/galaxy/server/database/objects/c/0/c/dataset_c0c9ff75-2e99-4ded-9052-b46929ecf044.dat' &&  samtools fixmate -@ $addthreads   -c  -O BAM namesorted.bam '/galaxy/server/database/objects/7/a/0/dataset_7a0339a6-2af6-4e33-ae8f-d9f416b1a4b6.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:07:07,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:07,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:07,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:07:07,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:07,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:07,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:07,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:11,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d78jp with k8s id: gxy-d78jp succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:07:11,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:07:18,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 45 finished
galaxy.model.metadata DEBUG 2025-04-26 01:07:18,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 70
galaxy.jobs INFO 2025-04-26 01:07:18,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-04-26 01:07:19,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 45 executed (89.399 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:19,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:07:20,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-04-26 01:07:20,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:07:20,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:07:20,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:07:20,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:07:20,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-04-26 01:07:20,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (28.987 ms)
galaxy.jobs.handler INFO 2025-04-26 01:07:20,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:20,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-04-26 01:07:20,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [46] prepared (48.648 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:07:20,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/46/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/46/configs/tmpt8qnndni']
galaxy.jobs.runners DEBUG 2025-04-26 01:07:20,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:20,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:20,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:20,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:29,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vqdlr with k8s id: gxy-vqdlr succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:07:30,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:07:37,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 46 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:07:37,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': '7_two_read_mapped.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/46/working/data_fetch_upload_mxomfr4u', 'object_id': 71}]}]}]
galaxy.jobs INFO 2025-04-26 01:07:37,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-04-26 01:07:37,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 46 executed (100.001 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:37,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:07:37,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-04-26 01:07:37,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:07:37,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:07:37,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:07:37,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:07:37,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-04-26 01:07:37,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (32.072 ms)
galaxy.jobs.handler INFO 2025-04-26 01:07:37,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:37,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 47
galaxy.tools.wrappers WARNING 2025-04-26 01:07:37,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Datatype class not found for extension 'qnamed_input_sorted.bam', which is used as parameter of 'is_of_type()' method
galaxy.jobs DEBUG 2025-04-26 01:07:37,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [47] prepared (38.158 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:37,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:07:37,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:38,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:07:38,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/47/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -m ${GALAXY_MEMORY_MB:-768}M -T sorttemp -n -O BAM -o namesorted.bam '/galaxy/server/database/objects/6/1/2/dataset_6121a092-1538-4835-b745-7c2f69985b78.dat' &&  samtools fixmate -@ $addthreads     -O BAM namesorted.bam '/galaxy/server/database/objects/8/a/c/dataset_8ac57d3a-bd4e-4ed5-be10-1f381be8106e.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:07:38,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:38,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:38,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:07:38,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fixmate/samtools_fixmate/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-04-26 01:07:38,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:38,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:38,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:43,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t7fpb with k8s id: gxy-t7fpb succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:07:43,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:07:50,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 47 finished
galaxy.model.metadata DEBUG 2025-04-26 01:07:50,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 72
galaxy.jobs INFO 2025-04-26 01:07:50,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs DEBUG 2025-04-26 01:07:50,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 47 executed (93.976 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:50,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:07:53,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 49, 48
tpv.core.entities DEBUG 2025-04-26 01:07:53,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:07:53,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:07:53,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:07:53,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:07:53,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-04-26 01:07:53,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (32.326 ms)
galaxy.jobs.handler INFO 2025-04-26 01:07:53,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:53,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 48
tpv.core.entities DEBUG 2025-04-26 01:07:53,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:07:53,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:07:53,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:07:53,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:07:53,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-04-26 01:07:53,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (42.746 ms)
galaxy.jobs.handler INFO 2025-04-26 01:07:53,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:53,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-04-26 01:07:53,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [48] prepared (73.041 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:07:53,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/48/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/48/configs/tmp8wvvun9u']
galaxy.jobs.runners DEBUG 2025-04-26 01:07:53,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:53,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:07:53,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [49] prepared (71.541 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:53,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:07:53,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/49/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/49/configs/tmpk795cxio']
galaxy.jobs.runners DEBUG 2025-04-26 01:07:53,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:53,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:53,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:54,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:07:54,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:03,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qlhpx with k8s id: gxy-qlhpx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:03,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q4tz2 with k8s id: gxy-q4tz2 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:08:03,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:08:03,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:08:10,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 48 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:08:10,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sam_to_bam_in1.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/48/working/data_fetch_upload_6f2c5i8h', 'object_id': 73}]}]}]
galaxy.jobs.runners DEBUG 2025-04-26 01:08:10,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 49 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:08:10,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'chr_m.fasta', 'dbkey': 'equCab2', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/49/working/data_fetch_upload_mne1l2sj', 'object_id': 74}]}]}]
galaxy.jobs INFO 2025-04-26 01:08:10,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs INFO 2025-04-26 01:08:10,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs DEBUG 2025-04-26 01:08:10,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 48 executed (136.277 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:10,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:08:10,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 49 executed (133.977 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:10,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:08:11,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-04-26 01:08:11,548 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:08:11,548 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:08:11,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:08:11,564 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:08:11,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-04-26 01:08:11,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (33.954 ms)
galaxy.jobs.handler INFO 2025-04-26 01:08:11,589 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:11,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-04-26 01:08:11,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [50] prepared (47.767 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:11,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:08:11,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:11,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:08:11,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/50/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&      reffa="reference.fa" && ln -s '/galaxy/server/database/objects/7/0/d/dataset_70db0707-2d20-4c94-aa91-870c415ece81.dat' $reffa && samtools faidx $reffa && reffai=$reffa.fai &&   samtools view -b -@ $addthreads -t "$reffai" '/galaxy/server/database/objects/a/a/9/dataset_aa956aac-3925-4f44-8eb7-29eb3b645882.dat' |  samtools sort -O bam -@ $addthreads -m $addmemory"M" -o '/galaxy/server/database/objects/4/9/5/dataset_495f3b2b-fff7-4cd7-82ba-c2af62699644.dat' -T "${TMPDIR:-.}"]
galaxy.jobs.runners DEBUG 2025-04-26 01:08:11,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:11,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:11,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:08:11,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:11,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:11,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:12,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:18,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jxqg4 with k8s id: gxy-jxqg4 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:08:18,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:08:25,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 50 finished
galaxy.model.metadata DEBUG 2025-04-26 01:08:25,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 75
galaxy.jobs INFO 2025-04-26 01:08:26,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-04-26 01:08:26,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 50 executed (114.515 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:26,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:08:27,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52, 51
tpv.core.entities DEBUG 2025-04-26 01:08:27,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:08:27,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:08:27,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:08:27,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:08:27,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-04-26 01:08:27,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (27.970 ms)
galaxy.jobs.handler INFO 2025-04-26 01:08:27,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:27,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 51
tpv.core.entities DEBUG 2025-04-26 01:08:27,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:08:27,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:08:27,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:08:27,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:08:27,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-04-26 01:08:27,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (52.508 ms)
galaxy.jobs.handler INFO 2025-04-26 01:08:27,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:27,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-04-26 01:08:28,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [51] prepared (83.346 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:08:28,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/51/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/51/configs/tmpdmpd6g84']
galaxy.jobs.runners DEBUG 2025-04-26 01:08:28,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-26 01:08:28,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [52] prepared (67.840 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:28,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:28,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:08:28,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/52/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/52/configs/tmpivr_ph7n']
galaxy.jobs.runners DEBUG 2025-04-26 01:08:28,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:28,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:28,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:28,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:28,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:38,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nw8w5 with k8s id: gxy-nw8w5 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:38,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6m869 with k8s id: gxy-6m869 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:08:38,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:08:38,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:08:45,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 51 finished
galaxy.jobs.runners DEBUG 2025-04-26 01:08:45,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 52 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:08:45,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sam_to_bam_noheader_in2.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/51/working/data_fetch_upload_t8je0o56', 'object_id': 76}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:08:45,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'chr_m.fasta', 'dbkey': 'equCab2', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/52/working/data_fetch_upload_n72kin5o', 'object_id': 77}]}]}]
galaxy.jobs INFO 2025-04-26 01:08:45,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs INFO 2025-04-26 01:08:45,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-04-26 01:08:45,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 51 executed (132.936 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:45,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:08:45,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 52 executed (135.408 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:46,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:08:46,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 53
tpv.core.entities DEBUG 2025-04-26 01:08:46,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:08:46,371 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:08:46,374 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:08:46,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:08:46,399 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-04-26 01:08:46,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (33.818 ms)
galaxy.jobs.handler INFO 2025-04-26 01:08:46,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:46,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 53
galaxy.jobs DEBUG 2025-04-26 01:08:46,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [53] prepared (43.519 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:46,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:08:46,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:46,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:08:46,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/53/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&      reffa="reference.fa" && ln -s '/galaxy/server/database/objects/f/3/8/dataset_f38582d7-bf30-4c67-8af8-f8dcd38e1c16.dat' $reffa && samtools faidx $reffa && reffai=$reffa.fai &&   samtools view -b -@ $addthreads -t "$reffai" '/galaxy/server/database/objects/7/2/2/dataset_722fd159-942b-4b70-ab19-03b2dce0f559.dat' |  samtools sort -O bam -@ $addthreads -m $addmemory"M" -o '/galaxy/server/database/objects/1/4/3/dataset_143c852e-6f3b-472a-b1cb-20362698ca2f.dat' -T "${TMPDIR:-.}"]
galaxy.jobs.runners DEBUG 2025-04-26 01:08:46,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:46,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:46,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:08:46,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:08:46,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:46,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:47,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:51,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8vvx4 with k8s id: gxy-8vvx4 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:08:51,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:08:58,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 53 finished
galaxy.model.metadata DEBUG 2025-04-26 01:08:58,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.jobs INFO 2025-04-26 01:08:58,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs DEBUG 2025-04-26 01:08:58,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 53 executed (96.300 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:08:58,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:09:00,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54, 55
tpv.core.entities DEBUG 2025-04-26 01:09:00,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:09:00,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:09:00,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:09:00,670 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:09:00,682 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-04-26 01:09:00,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (27.609 ms)
galaxy.jobs.handler INFO 2025-04-26 01:09:00,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:00,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 54
tpv.core.entities DEBUG 2025-04-26 01:09:00,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:09:00,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:09:00,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:09:00,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:09:00,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-04-26 01:09:00,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (43.283 ms)
galaxy.jobs.handler INFO 2025-04-26 01:09:00,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:00,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 55
galaxy.jobs DEBUG 2025-04-26 01:09:00,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [54] prepared (72.933 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:09:00,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/54/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/54/configs/tmp9y4xkzev']
galaxy.jobs.runners DEBUG 2025-04-26 01:09:00,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-26 01:09:00,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [55] prepared (60.936 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:00,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:00,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:09:00,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/55/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/55/configs/tmpa8wrbizy']
galaxy.jobs.runners DEBUG 2025-04-26 01:09:00,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:00,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:00,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:01,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:01,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:10,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-js4th with k8s id: gxy-js4th succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:10,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w8txt with k8s id: gxy-w8txt succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:09:10,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 54: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:09:10,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:09:18,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 55 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:09:18,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'chr_m.bgzipped_fasta.gz', 'dbkey': 'equCab2', 'ext': 'fasta.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/55/working/gxupload_0', 'object_id': 80}]}]}]
galaxy.jobs.runners DEBUG 2025-04-26 01:09:18,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 54 finished
galaxy.jobs INFO 2025-04-26 01:09:18,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:09:18,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sam_to_bam_noheader_in2.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/54/working/data_fetch_upload_ag68zgwr', 'object_id': 79}]}]}]
galaxy.jobs DEBUG 2025-04-26 01:09:18,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 55 executed (133.033 ms)
galaxy.jobs INFO 2025-04-26 01:09:18,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 54 in /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:18,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:09:18,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 54 executed (120.747 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:18,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:09:19,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56
tpv.core.entities DEBUG 2025-04-26 01:09:19,208 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:09:19,208 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:09:19,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:09:19,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:09:19,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-04-26 01:09:19,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (30.682 ms)
galaxy.jobs.handler INFO 2025-04-26 01:09:19,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:19,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 56
galaxy.jobs DEBUG 2025-04-26 01:09:19,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [56] prepared (48.425 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:19,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:09:19,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:19,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:09:19,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/56/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&      reffa="reference.fa.gz" && ln -s '/galaxy/server/database/objects/0/7/f/dataset_07f6c62c-02c6-4ac8-8de4-4c4189c4080e.dat' $reffa && { samtools faidx $reffa || { echo "Failed to index compressed reference. Trying decompressed ..." 1>&2 && gzip -dc $reffa > reference.fa && reffa="reference.fa" && samtools faidx $reffa; } } && reffai=$reffa.fai &&   samtools view -b -@ $addthreads -t "$reffai" '/galaxy/server/database/objects/b/b/4/dataset_bb4d2094-91b8-4c8e-8d79-fa727b4053fd.dat' |  samtools sort -O bam -@ $addthreads -m $addmemory"M" -o '/galaxy/server/database/objects/6/a/c/dataset_6ac0ee92-7cc6-41a9-b346-b428b30eb2c9.dat' -T "${TMPDIR:-.}"]
galaxy.jobs.runners DEBUG 2025-04-26 01:09:19,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:19,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:19,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:09:19,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:19,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:19,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:19,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tdzhz failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-tdzhz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:09:22,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-tdzhz

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-tdzhz": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-tdzhz) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-tdzhz) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-tdzhz) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-tdzhz) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-tdzhz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 56 (gxy-tdzhz)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-tdzhz to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:22,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:23,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-tdzhz) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-26 01:09:24,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58, 57
tpv.core.entities DEBUG 2025-04-26 01:09:24,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:09:24,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:09:24,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:09:24,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:09:24,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-04-26 01:09:24,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (36.309 ms)
galaxy.jobs.handler INFO 2025-04-26 01:09:24,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:24,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 57
tpv.core.entities DEBUG 2025-04-26 01:09:24,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:09:24,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:09:24,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:09:24,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:09:24,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-04-26 01:09:24,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (38.407 ms)
galaxy.jobs.handler INFO 2025-04-26 01:09:24,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:24,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-04-26 01:09:24,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [57] prepared (65.297 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:09:24,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/57/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/57/configs/tmp9h20bozo']
galaxy.jobs.runners DEBUG 2025-04-26 01:09:24,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:24,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:09:24,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [58] prepared (67.595 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:24,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:09:24,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/58/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/58/configs/tmpp2vu33wo']
galaxy.jobs.runners DEBUG 2025-04-26 01:09:24,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:24,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:24,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:24,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:25,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:34,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s5pbd with k8s id: gxy-s5pbd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:34,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dfkhc with k8s id: gxy-dfkhc succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:09:34,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 57: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:09:34,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:09:41,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 57 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:09:41,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sam_to_bam_noheader_in2.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/57/working/data_fetch_upload_y9kr1r4z', 'object_id': 82}]}]}]
galaxy.jobs INFO 2025-04-26 01:09:41,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 57 in /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-04-26 01:09:41,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 58 finished
galaxy.jobs DEBUG 2025-04-26 01:09:41,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 57 executed (130.409 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:41,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:09:41,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'chr_m.fasta.gz', 'dbkey': 'equCab2', 'ext': 'fasta.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/58/working/gxupload_0', 'object_id': 83}]}]}]
galaxy.jobs INFO 2025-04-26 01:09:41,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.jobs DEBUG 2025-04-26 01:09:41,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 58 executed (130.050 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:41,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:09:42,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-04-26 01:09:42,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:09:42,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:09:42,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:09:42,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:09:42,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-04-26 01:09:42,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (31.272 ms)
galaxy.jobs.handler INFO 2025-04-26 01:09:42,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:42,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-04-26 01:09:42,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [59] prepared (45.476 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:42,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:09:42,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:42,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:09:42,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/59/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&      reffa="reference.fa.gz" && ln -s '/galaxy/server/database/objects/1/3/3/dataset_133172db-6c6d-41ca-91a7-9edcf84e23cb.dat' $reffa && { samtools faidx $reffa || { echo "Failed to index compressed reference. Trying decompressed ..." 1>&2 && gzip -dc $reffa > reference.fa && reffa="reference.fa" && samtools faidx $reffa; } } && reffai=$reffa.fai &&   samtools view -b -@ $addthreads -t "$reffai" '/galaxy/server/database/objects/7/1/1/dataset_7114c94c-53b9-4433-a7e1-c51b0ea9e408.dat' |  samtools sort -O bam -@ $addthreads -m $addmemory"M" -o '/galaxy/server/database/objects/a/b/e/dataset_abe075f2-dc0b-4b93-b47b-6bd6b741a34d.dat' -T "${TMPDIR:-.}"]
galaxy.jobs.runners DEBUG 2025-04-26 01:09:42,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:42,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:42,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:09:42,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/sam_to_bam/sam_to_bam/2.1.2: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-26 01:09:43,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:43,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:43,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:47,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4p7zw with k8s id: gxy-4p7zw succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:09:47,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:09:54,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 59 finished
galaxy.model.metadata DEBUG 2025-04-26 01:09:54,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 84
galaxy.jobs INFO 2025-04-26 01:09:55,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-04-26 01:09:55,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 59 executed (136.413 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:55,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:09:58,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-04-26 01:09:58,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:09:58,104 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:09:58,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:09:58,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:09:58,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-04-26 01:09:58,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (26.169 ms)
galaxy.jobs.handler INFO 2025-04-26 01:09:58,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:58,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-04-26 01:09:58,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [60] prepared (58.496 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:09:58,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/60/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/60/configs/tmphypem15g']
galaxy.jobs.runners DEBUG 2025-04-26 01:09:58,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:58,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:58,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:09:58,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:07,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7dgq6 with k8s id: gxy-7dgq6 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:10:08,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:10:15,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 60 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:10:15,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'plugin1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/60/working/data_fetch_upload_wxngbqwo', 'object_id': 85}]}]}]
galaxy.jobs INFO 2025-04-26 01:10:15,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-04-26 01:10:15,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 60 executed (134.419 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:15,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:10:16,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-04-26 01:10:16,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:10:16,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:10:16,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:10:16,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:10:16,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-04-26 01:10:16,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (29.360 ms)
galaxy.jobs.handler INFO 2025-04-26 01:10:16,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:16,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-04-26 01:10:16,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [61] prepared (67.138 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:16,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:10:16,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_setgt/bcftools_plugin_setgt/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:16,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:10:16,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/61/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/4/b/dataset_34bb62e1-01d7-4530-91c2-2e4fb68c48eb.dat' > input.vcf.gz && bcftools index input.vcf.gz &&           bcftools plugin setGT             --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz   -- --target-gt '.' --new-gt '0'      > '/galaxy/server/database/objects/a/f/3/dataset_af316601-a570-4a43-ba53-50117bfa5320.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:10:16,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:16,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:16,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:10:16,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_setgt/bcftools_plugin_setgt/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:16,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:16,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:17,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lpb6g failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lpb6g.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:10:28,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-lpb6g

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-lpb6g": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (61/gxy-lpb6g) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (61/gxy-lpb6g) tool_stderr: Filled 44 alleles

galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (61/gxy-lpb6g) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (61/gxy-lpb6g) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lpb6g.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 61 (gxy-lpb6g)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-lpb6g to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:28,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (61/gxy-lpb6g) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-26 01:10:29,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-04-26 01:10:29,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:10:29,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:10:29,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:10:29,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:10:29,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-04-26 01:10:29,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (35.861 ms)
galaxy.jobs.handler INFO 2025-04-26 01:10:29,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:29,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-04-26 01:10:29,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [62] prepared (67.829 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:10:29,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/62/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/62/configs/tmppb87ez86']
galaxy.jobs.runners DEBUG 2025-04-26 01:10:29,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:29,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:29,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:30,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:39,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-675r9 with k8s id: gxy-675r9 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:10:39,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:10:47,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 62 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:10:47,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'plugin1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/62/working/data_fetch_upload_evpbuhl9', 'object_id': 87}]}]}]
galaxy.jobs INFO 2025-04-26 01:10:47,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.jobs DEBUG 2025-04-26 01:10:47,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 62 executed (134.962 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:47,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:10:48,104 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-04-26 01:10:48,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:10:48,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:10:48,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:10:48,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:10:48,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-04-26 01:10:48,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (30.183 ms)
galaxy.jobs.handler INFO 2025-04-26 01:10:48,163 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:48,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-04-26 01:10:48,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [63] prepared (46.609 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:48,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:10:48,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_setgt/bcftools_plugin_setgt/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:48,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:10:48,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/63/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/7/b/3/dataset_7b32e902-9098-4034-90c0-ec81990da3f0.dat' > input.vcf.gz && bcftools index input.vcf.gz &&           bcftools plugin setGT             --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz   -- --target-gt 'q' --new-gt '.'     --exclude 'FMT/GQ>20'  > '/galaxy/server/database/objects/e/a/8/dataset_ea83f292-4781-4c68-a60c-a55a449b03b6.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:10:48,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:48,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:48,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:10:48,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_setgt/bcftools_plugin_setgt/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:10:48,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:48,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:48,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:10:53,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5ktjg with k8s id: gxy-5ktjg succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:10:54,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 63: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:11:00,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 63 finished
galaxy.model.metadata DEBUG 2025-04-26 01:11:01,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 88
galaxy.jobs INFO 2025-04-26 01:11:01,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 63 in /galaxy/server/database/jobs_directory/000/63
galaxy.jobs DEBUG 2025-04-26 01:11:01,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 63 executed (111.332 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:01,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:11:03,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-04-26 01:11:03,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:11:03,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:11:03,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:11:03,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:11:03,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-04-26 01:11:03,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (33.388 ms)
galaxy.jobs.handler INFO 2025-04-26 01:11:03,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:03,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-04-26 01:11:03,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [64] prepared (64.688 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:11:03,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/64/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/64/configs/tmp4va54vv2']
galaxy.jobs.runners DEBUG 2025-04-26 01:11:03,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:03,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:03,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:03,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:13,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jkp67 with k8s id: gxy-jkp67 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:11:13,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:11:20,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 64 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:11:20,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/64/working/data_fetch_upload_xv9qijw5', 'object_id': 89}]}]}]
galaxy.jobs INFO 2025-04-26 01:11:20,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.jobs DEBUG 2025-04-26 01:11:20,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 64 executed (258.795 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:20,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:11:21,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65
tpv.core.entities DEBUG 2025-04-26 01:11:21,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:11:21,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:11:21,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:11:21,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:11:21,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-04-26 01:11:21,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (39.330 ms)
galaxy.jobs.handler INFO 2025-04-26 01:11:21,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:21,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-04-26 01:11:21,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [65] prepared (80.096 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:21,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:11:21,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:21,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:11:22,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/65/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/4/2/dataset_b4299024-1eef-4058-b5aa-c861852107e6.dat' > input.vcf.gz && bcftools index input.vcf.gz &&        bcftools call  -m --prior 0.0011                  --variants-only    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/b/f/5/dataset_bf57027b-8185-4462-9c01-d8963f5f8682.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:11:22,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:22,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:22,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:11:22,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:22,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:22,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:23,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:27,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mzzjn with k8s id: gxy-mzzjn succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:11:27,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:11:34,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 65 finished
galaxy.model.metadata DEBUG 2025-04-26 01:11:34,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 90
galaxy.jobs INFO 2025-04-26 01:11:34,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-04-26 01:11:34,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 65 executed (99.306 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:34,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:11:36,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-04-26 01:11:36,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:11:36,177 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:11:36,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:11:36,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:11:36,208 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-04-26 01:11:36,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (34.066 ms)
galaxy.jobs.handler INFO 2025-04-26 01:11:36,219 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:36,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-04-26 01:11:36,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [66] prepared (64.610 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:11:36,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/66/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/66/configs/tmpqbpfs28b']
galaxy.jobs.runners DEBUG 2025-04-26 01:11:36,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:36,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:36,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:37,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:46,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8hpn9 with k8s id: gxy-8hpn9 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:11:46,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:11:54,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 66 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:11:54,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/66/working/data_fetch_upload__2evscyf', 'object_id': 91}]}]}]
galaxy.jobs INFO 2025-04-26 01:11:54,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.jobs DEBUG 2025-04-26 01:11:54,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 66 executed (249.247 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:54,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:11:54,595 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67
tpv.core.entities DEBUG 2025-04-26 01:11:54,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:11:54,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:11:54,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:11:54,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:11:54,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-04-26 01:11:54,666 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (38.048 ms)
galaxy.jobs.handler INFO 2025-04-26 01:11:54,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:54,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 67
galaxy.jobs DEBUG 2025-04-26 01:11:54,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [67] prepared (41.918 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:54,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:11:54,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:54,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:11:54,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/67/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/d/9/5/dataset_d95efebd-6c3c-4959-92b2-594f3c35dcda.dat' > input.vcf.gz && bcftools index input.vcf.gz &&        bcftools call  -m --gvcf 0 --prior 0.0011                      --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/3/4/7/dataset_3475ded7-198a-4930-b97b-9009d301a42c.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:11:54,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:54,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:54,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:11:54,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:11:54,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:54,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:55,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:11:59,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7qd2d with k8s id: gxy-7qd2d succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:11:59,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:12:07,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 67 finished
galaxy.model.metadata DEBUG 2025-04-26 01:12:07,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 92
galaxy.jobs INFO 2025-04-26 01:12:07,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs DEBUG 2025-04-26 01:12:07,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 67 executed (103.959 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:07,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:12:08,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 68, 69, 70
tpv.core.entities DEBUG 2025-04-26 01:12:08,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:08,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:08,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:08,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:08,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-04-26 01:12:08,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (32.827 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:08,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:08,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 68
tpv.core.entities DEBUG 2025-04-26 01:12:08,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:08,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:08,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:09,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:09,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-04-26 01:12:09,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (38.571 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:09,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 69
tpv.core.entities DEBUG 2025-04-26 01:12:09,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:09,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:09,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:09,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [68] prepared (79.881 ms)
galaxy.jobs DEBUG 2025-04-26 01:12:09,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:09,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.command_factory INFO 2025-04-26 01:12:09,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/68/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/68/configs/tmp6phkbgth']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:09,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (54.954 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:09,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 70
galaxy.jobs.runners DEBUG 2025-04-26 01:12:09,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:12:09,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [69] prepared (96.200 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:12:09,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/69/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/69/configs/tmpxyy06wn_']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:09,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-26 01:12:09,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [70] prepared (74.332 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:12:09,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/70/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/70/configs/tmpdwmbv3fc']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:09,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:09,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s7pnx failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-s7pnx.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:12:18,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-s7pnx

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-s7pnx": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68/gxy-s7pnx) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68/gxy-s7pnx) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68/gxy-s7pnx) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68/gxy-s7pnx) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-s7pnx.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 68 (gxy-s7pnx)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-s7pnx to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:18,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68/gxy-s7pnx) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:19,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zg4jq with k8s id: gxy-zg4jq succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:19,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rgr78 with k8s id: gxy-rgr78 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:12:19,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:12:19,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-04-26 01:12:20,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72, 71
tpv.core.entities DEBUG 2025-04-26 01:12:20,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:20,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:20,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:20,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:20,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-04-26 01:12:20,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (36.136 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:20,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:20,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 71
tpv.core.entities DEBUG 2025-04-26 01:12:20,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:20,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:20,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:20,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:20,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-04-26 01:12:20,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (43.360 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:20,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:20,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 72
galaxy.jobs DEBUG 2025-04-26 01:12:20,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [71] prepared (110.124 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:12:20,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/71/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/71/configs/tmpgxwjm_yv']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:20,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:20,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:12:20,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [72] prepared (99.949 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:20,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:12:20,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/72/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/72/configs/tmpnubrqjwh']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:20,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:20,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:20,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:21,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:21,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-04-26 01:12:27,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 69 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:12:27,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.samples', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/69/working/data_fetch_upload_crhbhncd', 'object_id': 94}]}]}]
galaxy.jobs INFO 2025-04-26 01:12:27,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-04-26 01:12:28,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 70 finished
galaxy.jobs DEBUG 2025-04-26 01:12:28,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 69 executed (139.095 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:28,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:12:28,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ploidy', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/70/working/data_fetch_upload_gt8nd2if', 'object_id': 95}]}]}]
galaxy.jobs INFO 2025-04-26 01:12:28,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs DEBUG 2025-04-26 01:12:28,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 70 executed (124.726 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:28,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:31,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h9plx with k8s id: gxy-h9plx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:31,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ng5m5 with k8s id: gxy-ng5m5 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:12:31,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 71: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:12:31,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:12:38,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 72 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:12:38,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ploidy', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/72/working/data_fetch_upload_65fkk_2y', 'object_id': 97}]}]}]
galaxy.jobs.runners DEBUG 2025-04-26 01:12:38,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 71 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:12:38,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.X.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/71/working/data_fetch_upload_ci75xaeq', 'object_id': 96}]}]}]
galaxy.jobs INFO 2025-04-26 01:12:38,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs INFO 2025-04-26 01:12:39,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 71 in /galaxy/server/database/jobs_directory/000/71
galaxy.jobs DEBUG 2025-04-26 01:12:39,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 72 executed (274.551 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:39,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:12:39,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 71 executed (285.494 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:39,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:12:39,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 73
tpv.core.entities DEBUG 2025-04-26 01:12:39,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:39,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:39,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:39,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:39,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-04-26 01:12:39,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (39.211 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:39,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:39,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-04-26 01:12:39,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [73] prepared (52.797 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:12:39,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:12:39,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:12:40,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:12:40,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/73/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/d/7/1/dataset_d710bdef-1635-46e0-8f74-3065157fd92a.dat' > input.vcf.gz && bcftools index input.vcf.gz &&        bcftools call  -c --pval-threshold 0.5          --ploidy-file '/galaxy/server/database/objects/4/8/4/dataset_4840aa56-9ec7-4b67-a67b-c21525028265.dat'         --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/2/9/0/dataset_290a00a1-a111-4c8a-8af1-15229ecc2610.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:40,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:40,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:12:40,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:12:40,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:12:40,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:40,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:41,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:45,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lwsrd with k8s id: gxy-lwsrd succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:12:45,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:12:52,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 73 finished
galaxy.model.metadata DEBUG 2025-04-26 01:12:52,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 98
galaxy.jobs INFO 2025-04-26 01:12:52,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs DEBUG 2025-04-26 01:12:52,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 73 executed (98.969 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:52,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:12:54,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-04-26 01:12:54,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:54,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:54,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:54,177 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:54,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-04-26 01:12:54,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (28.373 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:54,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:54,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-04-26 01:12:54,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [74] prepared (57.405 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:12:54,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/74/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/74/configs/tmpt8vra0t_']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:54,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:54,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:54,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:12:55,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 75
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:55,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
tpv.core.entities DEBUG 2025-04-26 01:12:55,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:12:55,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:12:55,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:12:55,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:12:55,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-04-26 01:12:55,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (37.189 ms)
galaxy.jobs.handler INFO 2025-04-26 01:12:55,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:55,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 75
galaxy.jobs DEBUG 2025-04-26 01:12:55,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [75] prepared (68.730 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:12:55,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/75/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/75/configs/tmpdb71y3b8']
galaxy.jobs.runners DEBUG 2025-04-26 01:12:55,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:55,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:55,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:12:56,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v7gxx failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-v7gxx.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:13:03,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-v7gxx

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-v7gxx": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74/gxy-v7gxx) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74/gxy-v7gxx) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74/gxy-v7gxx) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74/gxy-v7gxx) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-v7gxx.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 74 (gxy-v7gxx)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-v7gxx to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:03,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74/gxy-v7gxx) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:04,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t92zs with k8s id: gxy-t92zs succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:13:04,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-04-26 01:13:05,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76
tpv.core.entities DEBUG 2025-04-26 01:13:05,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:13:05,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:13:05,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:13:05,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:13:05,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-04-26 01:13:05,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (37.189 ms)
galaxy.jobs.handler INFO 2025-04-26 01:13:05,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:05,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-04-26 01:13:05,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [76] prepared (65.061 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:13:05,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/76/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/76/configs/tmp5elx6lek']
galaxy.jobs.runners DEBUG 2025-04-26 01:13:05,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:05,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:05,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:06,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-04-26 01:13:11,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 75 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:13:11,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ploidy', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/75/working/data_fetch_upload_uwgw0cyf', 'object_id': 100}]}]}]
galaxy.jobs INFO 2025-04-26 01:13:11,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-04-26 01:13:11,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 75 executed (119.953 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:11,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:14,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fg7qc with k8s id: gxy-fg7qc succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:13:14,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:13:21,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 76 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:13:21,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/76/working/data_fetch_upload_nyc8z7vm', 'object_id': 101}]}]}]
galaxy.jobs INFO 2025-04-26 01:13:22,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.jobs DEBUG 2025-04-26 01:13:22,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 76 executed (299.110 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:22,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:13:22,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-04-26 01:13:22,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:13:22,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:13:22,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:13:22,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:13:22,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-04-26 01:13:22,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (42.093 ms)
galaxy.jobs.handler INFO 2025-04-26 01:13:22,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:22,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-04-26 01:13:22,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [77] prepared (40.753 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:22,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:13:22,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:22,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:13:22,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/77/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/0/f/5/dataset_0f565402-b1ed-4505-b44d-0a2ab2640923.dat' > input.vcf.gz && bcftools index input.vcf.gz &&        bcftools call  -m --prior 0.0011        --regions-overlap 1           --variants-only    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/4/0/c/dataset_40c7609a-fdde-4794-aca9-f30b144130d9.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:13:22,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:22,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:22,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:13:22,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:23,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:23,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:23,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:27,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p8tm8 with k8s id: gxy-p8tm8 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:13:28,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:13:35,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 77 finished
galaxy.model.metadata DEBUG 2025-04-26 01:13:35,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 102
galaxy.jobs INFO 2025-04-26 01:13:35,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-04-26 01:13:35,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 77 executed (102.746 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:35,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:13:37,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 78
tpv.core.entities DEBUG 2025-04-26 01:13:37,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:13:37,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:13:37,153 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:13:37,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:13:37,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-04-26 01:13:37,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (28.210 ms)
galaxy.jobs.handler INFO 2025-04-26 01:13:37,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:37,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 78
galaxy.jobs DEBUG 2025-04-26 01:13:37,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [78] prepared (56.407 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:13:37,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/78/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/78/configs/tmprij8sm31']
galaxy.jobs.runners DEBUG 2025-04-26 01:13:37,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:37,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:37,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:37,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:47,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tp2cx with k8s id: gxy-tp2cx succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:13:47,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:13:54,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 78 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:13:54,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.AD.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/78/working/data_fetch_upload_a5zncryu', 'object_id': 103}]}]}]
galaxy.jobs INFO 2025-04-26 01:13:54,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.jobs DEBUG 2025-04-26 01:13:54,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 78 executed (276.146 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:54,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:13:55,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79
tpv.core.entities DEBUG 2025-04-26 01:13:55,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:13:55,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:13:55,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:13:55,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:13:55,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-04-26 01:13:55,559 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (39.333 ms)
galaxy.jobs.handler INFO 2025-04-26 01:13:55,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:55,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-04-26 01:13:55,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [79] prepared (54.668 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:55,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:13:55,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:55,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:13:55,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/79/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/7/0/4/dataset_704dcbce-1807-4a9c-990d-3b1205329452.dat' > input.vcf.gz && bcftools index input.vcf.gz &&        bcftools call  -m --prior 0.0011               --group-samples -       --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/7/7/f/dataset_77f94e0f-6cb4-47ae-ba25-a1c91d52f847.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:13:55,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:55,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:55,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:13:55,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:13:55,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:55,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:13:56,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:00,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nqw4x with k8s id: gxy-nqw4x succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:14:00,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:14:07,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 79 finished
galaxy.model.metadata DEBUG 2025-04-26 01:14:07,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 104
galaxy.jobs INFO 2025-04-26 01:14:07,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-04-26 01:14:07,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 79 executed (98.230 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:07,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:14:09,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81, 80
tpv.core.entities DEBUG 2025-04-26 01:14:09,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:14:09,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:14:09,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:14:09,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:14:09,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-04-26 01:14:09,855 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (37.736 ms)
galaxy.jobs.handler INFO 2025-04-26 01:14:09,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:09,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 80
tpv.core.entities DEBUG 2025-04-26 01:14:09,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:14:09,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:14:09,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:14:09,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:14:09,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-04-26 01:14:09,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (42.043 ms)
galaxy.jobs.handler INFO 2025-04-26 01:14:09,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:09,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-04-26 01:14:09,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [80] prepared (71.290 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:14:09,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/80/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/80/configs/tmp31q7cn8i']
galaxy.jobs.runners DEBUG 2025-04-26 01:14:09,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:09,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:14:10,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [81] prepared (68.888 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:10,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:14:10,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/81/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/81/configs/tmploye1qh_']
galaxy.jobs.runners DEBUG 2025-04-26 01:14:10,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:10,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:10,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:10,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:10,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:19,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gqzsh with k8s id: gxy-gqzsh succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:14:19,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:20,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vfwv9 with k8s id: gxy-vfwv9 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:14:20,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:14:27,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 80 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:14:27,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.AD.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/80/working/data_fetch_upload_0l3w4dha', 'object_id': 105}]}]}]
galaxy.jobs INFO 2025-04-26 01:14:27,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-04-26 01:14:27,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 80 executed (273.871 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:27,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-26 01:14:28,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 81 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:14:28,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'targets_file.tab', 'dbkey': '?', 'ext': 'mothur.sabund', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded mothur.sabund file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/81/working/data_fetch_upload_7frnt9jc', 'object_id': 106}]}]}]
galaxy.jobs INFO 2025-04-26 01:14:28,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-04-26 01:14:28,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 81 executed (129.814 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:28,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:14:29,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-04-26 01:14:29,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:14:29,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:14:29,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:14:29,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:14:29,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-04-26 01:14:29,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (33.207 ms)
galaxy.jobs.handler INFO 2025-04-26 01:14:29,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:29,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-04-26 01:14:29,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [82] prepared (50.425 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:29,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:14:29,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:29,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:14:29,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/82/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/f/0/dataset_3f03ba2d-ff6b-4b7c-b8bf-9b443c1cc6fe.dat' > input.vcf.gz && bcftools index input.vcf.gz &&        bcftools call  -m --prior 0.0011   --targets-file "/galaxy/server/database/objects/4/b/c/dataset_4bc4bdca-998b-4295-85b0-e18910d2467d.dat"                    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/f/3/4/dataset_f341f890-8c17-4027-a61f-da01c8f9be86.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:14:29,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:29,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:29,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:14:29,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_call/bcftools_call/1.15.1+galaxy5: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:29,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:29,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:29,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jqmb7 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jqmb7.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:14:33,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-jqmb7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-jqmb7": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-jqmb7) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-jqmb7) tool_stderr: Note: none of --samples-file, --ploidy or --ploidy-file given, assuming all sites are diploid

galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-jqmb7) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-jqmb7) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jqmb7.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 82 (gxy-jqmb7)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-jqmb7 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:33,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:34,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-jqmb7) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-26 01:14:36,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83
tpv.core.entities DEBUG 2025-04-26 01:14:36,522 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:14:36,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:14:36,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:14:36,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:14:36,557 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-04-26 01:14:36,565 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (38.294 ms)
galaxy.jobs.handler INFO 2025-04-26 01:14:36,567 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:36,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 83
galaxy.jobs DEBUG 2025-04-26 01:14:36,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [83] prepared (60.997 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:14:36,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/83/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/83/configs/tmp9ez4few3']
galaxy.jobs.runners DEBUG 2025-04-26 01:14:36,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:36,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:36,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:36,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:46,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mq2w4 with k8s id: gxy-mq2w4 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:14:46,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:14:53,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 83 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:14:53,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/83/working/data_fetch_upload_xahpuvyf', 'object_id': 108}]}]}]
galaxy.jobs INFO 2025-04-26 01:14:53,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.jobs DEBUG 2025-04-26 01:14:53,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 83 executed (108.099 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:53,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:14:53,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 84
tpv.core.entities DEBUG 2025-04-26 01:14:53,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:14:53,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:14:53,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:14:53,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:14:53,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-04-26 01:14:53,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (29.130 ms)
galaxy.jobs.handler INFO 2025-04-26 01:14:53,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:53,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-04-26 01:14:53,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [84] prepared (39.649 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:53,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:14:53,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbreakcreatemulti/vcfbreakcreatemulti/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:54,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:14:54,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [vcfbreakmulti '/galaxy/server/database/objects/8/d/6/dataset_8d6b388b-3555-47a0-a34c-fbeba7d1181f.dat' > '/galaxy/server/database/objects/e/6/b/dataset_e6bbf44f-dfd3-4539-be13-ecea8c0c0742.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:14:54,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:54,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:54,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:14:54,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbreakcreatemulti/vcfbreakcreatemulti/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-26 01:14:54,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:54,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:14:55,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:08,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-x2r27 with k8s id: gxy-x2r27 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:15:08,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:15:15,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 84 finished
galaxy.model.metadata DEBUG 2025-04-26 01:15:15,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 109
galaxy.jobs INFO 2025-04-26 01:15:15,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-04-26 01:15:15,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 84 executed (93.811 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:15,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:15:17,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-04-26 01:15:17,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:15:17,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:15:17,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:15:17,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:15:17,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-04-26 01:15:17,427 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (32.311 ms)
galaxy.jobs.handler INFO 2025-04-26 01:15:17,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:17,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-04-26 01:15:17,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [85] prepared (56.621 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:15:17,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/85/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/85/configs/tmpa2z02otn']
galaxy.jobs.runners DEBUG 2025-04-26 01:15:17,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:17,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:17,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:18,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:27,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zb7rr with k8s id: gxy-zb7rr succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:15:27,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:15:34,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 85 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:15:34,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcfbreakcreatemulti-test2-input.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/85/working/data_fetch_upload_gc_2ih4j', 'object_id': 110}]}]}]
galaxy.jobs INFO 2025-04-26 01:15:34,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-04-26 01:15:34,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 85 executed (128.352 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:35,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:15:35,747 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 86
tpv.core.entities DEBUG 2025-04-26 01:15:35,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:15:35,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:15:35,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:15:35,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:15:35,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-04-26 01:15:35,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (33.281 ms)
galaxy.jobs.handler INFO 2025-04-26 01:15:35,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:35,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 86
galaxy.jobs DEBUG 2025-04-26 01:15:35,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [86] prepared (37.097 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:15:35,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:15:35,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbreakcreatemulti/vcfbreakcreatemulti/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-26 01:15:35,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:15:35,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [vcfcreatemulti '/galaxy/server/database/objects/0/a/6/dataset_0a6d78ea-366c-40b1-a90c-4f2f91e57643.dat' > '/galaxy/server/database/objects/5/6/e/dataset_56e16fd3-763a-4875-a075-c89bcc976564.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:15:35,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:35,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:15:35,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:15:35,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbreakcreatemulti/vcfbreakcreatemulti/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-26 01:15:35,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:35,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:36,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:40,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qwhcp with k8s id: gxy-qwhcp succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:15:40,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:15:47,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 86 finished
galaxy.model.metadata DEBUG 2025-04-26 01:15:47,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 111
galaxy.jobs INFO 2025-04-26 01:15:47,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.jobs DEBUG 2025-04-26 01:15:48,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 86 executed (101.111 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:48,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:15:51,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87
tpv.core.entities DEBUG 2025-04-26 01:15:51,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:15:51,104 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:15:51,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:15:51,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:15:51,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-04-26 01:15:51,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (30.988 ms)
galaxy.jobs.handler INFO 2025-04-26 01:15:51,140 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:51,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-04-26 01:15:51,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [87] prepared (58.561 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:15:51,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/87/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/87/configs/tmp3qk1x_my']
galaxy.jobs.runners DEBUG 2025-04-26 01:15:51,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:51,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:51,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:15:51,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:00,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gk72w with k8s id: gxy-gk72w succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:16:00,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:16:07,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 87 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:16:08,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/87/working/data_fetch_upload_87s7bq1v', 'object_id': 112}]}]}]
galaxy.jobs INFO 2025-04-26 01:16:08,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs DEBUG 2025-04-26 01:16:08,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 87 executed (119.232 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:08,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:16:09,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-04-26 01:16:09,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:16:09,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:16:09,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:16:09,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:16:09,508 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-04-26 01:16:09,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (26.575 ms)
galaxy.jobs.handler INFO 2025-04-26 01:16:09,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:09,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-04-26 01:16:09,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [88] prepared (40.365 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:09,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:16:09,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfselectsamples/vcfselectsamples/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:09,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:16:09,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [vcfkeepsamples '/galaxy/server/database/objects/3/4/0/dataset_3409b533-ef52-4ec4-a309-d52b8645c5fe.dat' "NA00001" "NA00003" > '/galaxy/server/database/objects/9/d/8/dataset_9d877eba-aba9-49cc-8f87-453dc35598e5.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:16:09,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:09,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:09,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:16:09,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfselectsamples/vcfselectsamples/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:09,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:09,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:09,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:13,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sb9tf with k8s id: gxy-sb9tf succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:16:14,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:16:20,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 88 finished
galaxy.model.metadata DEBUG 2025-04-26 01:16:20,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 113
galaxy.jobs INFO 2025-04-26 01:16:21,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-04-26 01:16:21,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 88 executed (96.403 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:21,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:16:23,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 89
tpv.core.entities DEBUG 2025-04-26 01:16:23,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:16:23,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:16:23,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:16:23,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:16:23,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-04-26 01:16:23,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (33.133 ms)
galaxy.jobs.handler INFO 2025-04-26 01:16:23,897 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:23,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 89
galaxy.jobs DEBUG 2025-04-26 01:16:23,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [89] prepared (59.247 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:16:23,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/89/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/89/configs/tmpi91mgau1']
galaxy.jobs.runners DEBUG 2025-04-26 01:16:23,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:24,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:24,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:24,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:34,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-km755 with k8s id: gxy-km755 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:16:34,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:16:41,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 89 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:16:41,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/89/working/gxupload_0', 'object_id': 114}]}]}]
galaxy.jobs INFO 2025-04-26 01:16:41,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-04-26 01:16:41,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 89 executed (253.115 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:41,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:16:42,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90
tpv.core.entities DEBUG 2025-04-26 01:16:42,219 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_occupancy/poretools_occupancy/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:16:42,219 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:16:42,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:16:42,234 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:16:42,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-04-26 01:16:42,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (36.725 ms)
galaxy.jobs.handler INFO 2025-04-26 01:16:42,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:42,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-04-26 01:16:42,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [90] prepared (46.804 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:42,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:16:42,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_occupancy/poretools_occupancy/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:42,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:16:42,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools occupancy '/galaxy/server/database/objects/a/9/4/dataset_a949a968-574d-4915-b4ea-6151a32190ad.dat' --saveas occupancy.png --plot-type read_count && mv occupancy.png '/galaxy/server/database/objects/5/f/2/dataset_5f2a7754-69a1-414a-873e-0f72fe284fb1.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:16:42,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:42,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:42,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:16:42,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_occupancy/poretools_occupancy/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-26 01:16:42,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:42,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:16:43,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:27,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wzjx4 with k8s id: gxy-wzjx4 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:17:27,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:17:34,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 90 finished
galaxy.model.metadata DEBUG 2025-04-26 01:17:34,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 115
galaxy.util WARNING 2025-04-26 01:17:34,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/f/2/dataset_5f2a7754-69a1-414a-873e-0f72fe284fb1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/f/2/dataset_5f2a7754-69a1-414a-873e-0f72fe284fb1.dat'
galaxy.jobs INFO 2025-04-26 01:17:34,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-04-26 01:17:34,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 90 executed (80.033 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:34,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:17:36,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-04-26 01:17:36,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:17:36,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:17:36,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:17:36,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:17:36,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-04-26 01:17:36,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (30.044 ms)
galaxy.jobs.handler INFO 2025-04-26 01:17:36,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:36,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-04-26 01:17:36,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [91] prepared (52.253 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:17:36,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/91/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/91/configs/tmputnncaln']
galaxy.jobs.runners DEBUG 2025-04-26 01:17:36,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:36,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:36,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:37,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:46,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-txdgl with k8s id: gxy-txdgl succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:17:46,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:17:53,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 91 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:17:53,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/91/working/gxupload_0', 'object_id': 116}]}]}]
galaxy.jobs INFO 2025-04-26 01:17:53,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-04-26 01:17:53,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 91 executed (277.969 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:53,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:17:54,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92
tpv.core.entities DEBUG 2025-04-26 01:17:54,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_occupancy/poretools_occupancy/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:17:54,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:17:54,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:17:54,589 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:17:54,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-04-26 01:17:54,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (34.338 ms)
galaxy.jobs.handler INFO 2025-04-26 01:17:54,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:54,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 92
galaxy.jobs DEBUG 2025-04-26 01:17:54,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [92] prepared (34.252 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:17:54,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:17:54,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_occupancy/poretools_occupancy/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-26 01:17:54,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:17:54,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools occupancy '/galaxy/server/database/objects/1/5/4/dataset_154abb85-22fe-4d5a-aee6-581ff9432ade.dat' --saveas occupancy.pdf --plot-type total_bp && mv occupancy.pdf '/galaxy/server/database/objects/4/7/7/dataset_47714af3-f56d-494e-b458-93027841464b.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:17:54,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:54,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:17:54,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:17:54,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_occupancy/poretools_occupancy/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-26 01:17:54,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:54,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:17:55,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jsxmm failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jsxmm.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92/gxy-jsxmm) tool_stdout: channel_number	start_time	duration
128	1457127626	1
128	1457129549	4
126	1457148908	5
126	1457129771	6

galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92/gxy-jsxmm) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92/gxy-jsxmm) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92/gxy-jsxmm) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jsxmm.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 92 (gxy-jsxmm)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Could not find job with id gxy-jsxmm to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:01,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92/gxy-jsxmm) Terminated at user's request
galaxy.util WARNING 2025-04-26 01:18:01,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/7/7/dataset_47714af3-f56d-494e-b458-93027841464b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/7/7/dataset_47714af3-f56d-494e-b458-93027841464b.dat'
galaxy.jobs.handler DEBUG 2025-04-26 01:18:03,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 93
tpv.core.entities DEBUG 2025-04-26 01:18:03,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:18:03,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:18:03,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:18:03,803 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:18:03,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-04-26 01:18:03,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (32.830 ms)
galaxy.jobs.handler INFO 2025-04-26 01:18:03,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:03,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 93
galaxy.jobs DEBUG 2025-04-26 01:18:03,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [93] prepared (59.134 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:18:03,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/93/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/93/configs/tmpyc1st0br']
galaxy.jobs.runners DEBUG 2025-04-26 01:18:03,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:03,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:03,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:04,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:13,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7nrxj with k8s id: gxy-7nrxj succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:18:13,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:18:20,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 93 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:18:20,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix_result1.gz', 'dbkey': '?', 'ext': 'deeptools_compute_matrix_archive', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded deeptools_compute_matrix_archive file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/93/working/gxupload_0', 'object_id': 118}]}]}]
galaxy.jobs INFO 2025-04-26 01:18:20,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs DEBUG 2025-04-26 01:18:20,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 93 executed (99.411 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:20,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:18:22,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94
tpv.core.entities DEBUG 2025-04-26 01:18:22,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_heatmap/deeptools_plot_heatmap/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:18:22,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:18:22,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:18:22,163 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:18:22,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-04-26 01:18:22,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (44.136 ms)
galaxy.jobs.handler INFO 2025-04-26 01:18:22,200 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:22,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-04-26 01:18:22,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [94] prepared (80.310 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:18:22,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:18:22,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_heatmap/deeptools_plot_heatmap/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-26 01:18:22,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:18:22,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [plotHeatmap --version > /galaxy/server/database/jobs_directory/000/94/outputs/COMMAND_VERSION 2>&1;
plotHeatmap --matrixFile '/galaxy/server/database/objects/0/7/0/dataset_0702c9ba-030b-4a24-a59c-5979362ed52d.dat' --outFileName '/galaxy/server/database/objects/d/e/e/dataset_deec2a59-ee60-46c4-b157-5303a4300a62.dat'  --plotFileFormat 'png']
galaxy.jobs.runners DEBUG 2025-04-26 01:18:22,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:22,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:18:22,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:18:22,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_heatmap/deeptools_plot_heatmap/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-26 01:18:22,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:22,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:22,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:48,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hljdl with k8s id: gxy-hljdl succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:18:48,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:18:55,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 94 finished
galaxy.model.metadata DEBUG 2025-04-26 01:18:55,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 119
galaxy.util WARNING 2025-04-26 01:18:55,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/e/e/dataset_deec2a59-ee60-46c4-b157-5303a4300a62.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/e/e/dataset_deec2a59-ee60-46c4-b157-5303a4300a62.dat'
galaxy.jobs INFO 2025-04-26 01:18:55,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.jobs DEBUG 2025-04-26 01:18:55,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 94 executed (84.457 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:55,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:18:56,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95
tpv.core.entities DEBUG 2025-04-26 01:18:56,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:18:56,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:18:56,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:18:56,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:18:56,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-04-26 01:18:56,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (26.678 ms)
galaxy.jobs.handler INFO 2025-04-26 01:18:56,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:56,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 95
galaxy.jobs DEBUG 2025-04-26 01:18:56,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [95] prepared (59.337 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:18:56,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/95/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/95/configs/tmps3n_bv45']
galaxy.jobs.runners DEBUG 2025-04-26 01:18:56,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:56,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:56,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:18:57,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:07,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gwp4k with k8s id: gxy-gwp4k succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:19:07,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:19:14,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 95 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:19:14,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix_result1.gz', 'dbkey': '?', 'ext': 'deeptools_compute_matrix_archive', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded deeptools_compute_matrix_archive file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/95/working/gxupload_0', 'object_id': 120}]}]}]
galaxy.jobs INFO 2025-04-26 01:19:14,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-04-26 01:19:14,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 95 executed (101.460 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:14,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:19:16,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 96
tpv.core.entities DEBUG 2025-04-26 01:19:16,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_heatmap/deeptools_plot_heatmap/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:19:16,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:19:16,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:19:16,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:19:16,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-04-26 01:19:16,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (37.216 ms)
galaxy.jobs.handler INFO 2025-04-26 01:19:16,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:16,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 96
galaxy.jobs DEBUG 2025-04-26 01:19:16,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [96] prepared (39.034 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:19:16,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:19:16,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_heatmap/deeptools_plot_heatmap/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-26 01:19:16,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:19:16,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [plotHeatmap --version > /galaxy/server/database/jobs_directory/000/96/outputs/COMMAND_VERSION 2>&1;
plotHeatmap --matrixFile '/galaxy/server/database/objects/d/0/1/dataset_d010c191-5261-4973-9c58-f835dcc5ec28.dat' --outFileName '/galaxy/server/database/objects/4/f/7/dataset_4f7bd32e-90eb-4085-9dee-c142bdf4a885.dat'  --plotFileFormat 'png'  --sortRegions 'descend'   --sortUsing 'mean'  --averageTypeSummaryPlot 'mean'  --plotType 'lines'  --missingDataColor 'black'   --alpha '1.0'    --xAxisLabel 'distance from TSS (bp)' --yAxisLabel 'genes'  --heatmapWidth 7.5 --heatmapHeight 25.0  --whatToShow 'heatmap and colorbar'  --startLabel 'TSS' --endLabel 'TES'  --refPointLabel 'TSS'     --legendLocation 'best'  --labelRotation '0']
galaxy.jobs.runners DEBUG 2025-04-26 01:19:16,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:16,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:19:16,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:19:16,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_heatmap/deeptools_plot_heatmap/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-26 01:19:16,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:16,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:17,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:24,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-45qkn with k8s id: gxy-45qkn succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:19:24,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:19:31,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 96 finished
galaxy.model.metadata DEBUG 2025-04-26 01:19:31,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 121
galaxy.util WARNING 2025-04-26 01:19:31,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/f/7/dataset_4f7bd32e-90eb-4085-9dee-c142bdf4a885.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/f/7/dataset_4f7bd32e-90eb-4085-9dee-c142bdf4a885.dat'
galaxy.jobs INFO 2025-04-26 01:19:31,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs DEBUG 2025-04-26 01:19:31,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 96 executed (76.215 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:31,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:19:34,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97
tpv.core.entities DEBUG 2025-04-26 01:19:34,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:19:34,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:19:34,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:19:34,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:19:34,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-04-26 01:19:34,682 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (33.605 ms)
galaxy.jobs.handler INFO 2025-04-26 01:19:34,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:34,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-04-26 01:19:34,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [97] prepared (74.294 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:19:34,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/97/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/97/configs/tmpx4nft3sr']
galaxy.jobs.runners DEBUG 2025-04-26 01:19:34,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:34,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:34,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:35,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nrqf8 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-nrqf8.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:19:43,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-nrqf8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-nrqf8": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97/gxy-nrqf8) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97/gxy-nrqf8) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97/gxy-nrqf8) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97/gxy-nrqf8) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-nrqf8.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 97 (gxy-nrqf8)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-nrqf8 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:43,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:44,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97/gxy-nrqf8) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-26 01:19:44,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98
tpv.core.entities DEBUG 2025-04-26 01:19:44,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:19:44,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:19:44,875 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:19:44,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:19:44,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-04-26 01:19:44,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (32.768 ms)
galaxy.jobs.handler INFO 2025-04-26 01:19:44,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:44,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 98
galaxy.jobs DEBUG 2025-04-26 01:19:44,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [98] prepared (56.485 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:19:44,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/98/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/98/configs/tmptobl0f3x']
galaxy.jobs.runners DEBUG 2025-04-26 01:19:45,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:45,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:45,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:45,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:19:54,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p7v7q with k8s id: gxy-p7v7q succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:19:54,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:20:01,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 98 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:20:01,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeMatrix_result1.gz', 'dbkey': '?', 'ext': 'deeptools_compute_matrix_archive', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded deeptools_compute_matrix_archive file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/98/working/gxupload_0', 'object_id': 123}]}]}]
galaxy.jobs INFO 2025-04-26 01:20:01,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs DEBUG 2025-04-26 01:20:01,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 98 executed (121.141 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:01,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:20:02,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 99
tpv.core.entities DEBUG 2025-04-26 01:20:02,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_profile/deeptools_plot_profile/.*, abstract=False, cores=1, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:20:02,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:20:02,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:20:02,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:20:02,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-04-26 01:20:02,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (42.526 ms)
galaxy.jobs.handler INFO 2025-04-26 01:20:02,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:02,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 99
galaxy.jobs DEBUG 2025-04-26 01:20:02,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [99] prepared (66.624 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:02,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:20:02,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_profile/deeptools_plot_profile/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:02,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:20:02,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [plotProfile --version > /galaxy/server/database/jobs_directory/000/99/outputs/COMMAND_VERSION 2>&1;
plotProfile --matrixFile "/galaxy/server/database/objects/6/6/5/dataset_665a93eb-110e-46b9-96f4-858d51dbccb8.dat" --outFileName "/galaxy/server/database/objects/6/c/7/dataset_6c754f82-fe1d-4025-92da-aa02b7e14ac8.dat"  --plotFileFormat 'png'  --startLabel 'TSS' --endLabel 'TES'  --averageType 'mean' --plotHeight 5 --plotWidth 11 --plotType lines     --legendLocation 'best'  --labelRotation '0'     --outFileNameData '/galaxy/server/database/objects/c/4/b/dataset_c4baaeef-b068-41df-87de-772a1ac14a4a.dat']
galaxy.jobs.runners DEBUG 2025-04-26 01:20:02,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:02,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:02,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:20:02,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_plot_profile/deeptools_plot_profile/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:02,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:02,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:03,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:09,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xswz2 with k8s id: gxy-xswz2 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:20:09,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:20:16,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 99 finished
galaxy.model.metadata DEBUG 2025-04-26 01:20:16,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 124
galaxy.model.metadata DEBUG 2025-04-26 01:20:16,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 125
galaxy.util WARNING 2025-04-26 01:20:16,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/c/7/dataset_6c754f82-fe1d-4025-92da-aa02b7e14ac8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/c/7/dataset_6c754f82-fe1d-4025-92da-aa02b7e14ac8.dat'
galaxy.util WARNING 2025-04-26 01:20:16,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/4/b/dataset_c4baaeef-b068-41df-87de-772a1ac14a4a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/4/b/dataset_c4baaeef-b068-41df-87de-772a1ac14a4a.dat'
galaxy.jobs INFO 2025-04-26 01:20:16,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs DEBUG 2025-04-26 01:20:16,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 99 executed (93.506 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:16,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:20:19,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100
tpv.core.entities DEBUG 2025-04-26 01:20:19,591 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:20:19,591 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:20:19,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:20:19,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:20:19,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-04-26 01:20:19,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (24.249 ms)
galaxy.jobs.handler INFO 2025-04-26 01:20:19,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:19,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-04-26 01:20:19,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [100] prepared (54.051 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:20:19,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/100/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/100/configs/tmphd6tasgb']
galaxy.jobs.runners DEBUG 2025-04-26 01:20:19,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:19,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:19,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:20,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:29,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nsssk with k8s id: gxy-nsssk succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:20:29,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:20:36,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 100 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:20:36,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'call-out2.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/100/working/data_fetch_upload_h3hz7u04', 'object_id': 126}]}]}]
galaxy.jobs INFO 2025-04-26 01:20:36,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-04-26 01:20:36,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 100 executed (133.849 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:36,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:20:37,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 101
tpv.core.entities DEBUG 2025-04-26 01:20:37,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/.*, abstract=False, cores=1, mem=15.2, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:20:37,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:20:37,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:20:37,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:20:37,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-04-26 01:20:37,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (32.088 ms)
galaxy.jobs.handler INFO 2025-04-26 01:20:37,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:37,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 101
galaxy.jobs DEBUG 2025-04-26 01:20:38,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [101] prepared (47.766 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:38,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:20:38,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:38,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:20:38,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/101/outputs/COMMAND_VERSION 2>&1;
lofreq filter -i /galaxy/server/database/objects/d/a/2/dataset_da21ed17-a7f2-423c-aa3d-2acff751b2aa.dat --no-defaults --verbose   -Q 38 -K 20 -v 10 -V 0 -a 0.0 -A 0.0 -b fdr -c 0.001   -o filtered.vcf]
galaxy.jobs.runners DEBUG 2025-04-26 01:20:38,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/101/working/filtered.vcf" -a -f "/galaxy/server/database/objects/4/f/8/dataset_4f8d205b-5687-4681-aef1-74defa3c2a2a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/101/working/filtered.vcf" "/galaxy/server/database/objects/4/f/8/dataset_4f8d205b-5687-4681-aef1-74defa3c2a2a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:38,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:38,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:20:38,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-26 01:20:38,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:38,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:38,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:49,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ndqrm with k8s id: gxy-ndqrm succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:20:49,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:20:56,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 101 finished
galaxy.model.metadata DEBUG 2025-04-26 01:20:57,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 127
galaxy.util WARNING 2025-04-26 01:20:57,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/f/8/dataset_4f8d205b-5687-4681-aef1-74defa3c2a2a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/f/8/dataset_4f8d205b-5687-4681-aef1-74defa3c2a2a.dat'
galaxy.jobs INFO 2025-04-26 01:20:57,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-04-26 01:20:57,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 101 executed (94.363 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:57,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:20:58,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 102
tpv.core.entities DEBUG 2025-04-26 01:20:58,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:20:58,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:20:58,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:20:58,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:20:58,365 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-04-26 01:20:58,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (32.726 ms)
galaxy.jobs.handler INFO 2025-04-26 01:20:58,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:58,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 102
galaxy.jobs DEBUG 2025-04-26 01:20:58,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [102] prepared (59.663 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:20:58,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/102/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/102/configs/tmpi5qxg3df']
galaxy.jobs.runners DEBUG 2025-04-26 01:20:58,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:58,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:58,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:20:58,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:08,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-skgnq with k8s id: gxy-skgnq succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:21:08,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 102: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:21:15,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 102 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:21:15,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'call-out2.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/102/working/data_fetch_upload_60696sv1', 'object_id': 128}]}]}]
galaxy.jobs INFO 2025-04-26 01:21:15,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 102 in /galaxy/server/database/jobs_directory/000/102
galaxy.jobs DEBUG 2025-04-26 01:21:15,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 102 executed (117.440 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:15,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:21:15,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 103
tpv.core.entities DEBUG 2025-04-26 01:21:15,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/.*, abstract=False, cores=1, mem=15.2, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:21:15,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:21:15,683 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:21:15,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:21:15,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-04-26 01:21:15,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (30.929 ms)
galaxy.jobs.handler INFO 2025-04-26 01:21:15,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:15,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 103
galaxy.jobs DEBUG 2025-04-26 01:21:15,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [103] prepared (33.062 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:15,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:21:15,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:15,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:21:15,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/103/outputs/COMMAND_VERSION 2>&1;
lofreq filter -i /galaxy/server/database/objects/e/d/b/dataset_edbae2f1-fae5-4054-ab4f-5a10617609c2.dat --no-defaults --verbose  --only-snvs -q bonf -r 0.01 -s 66 -v 10 -V 0 -a 0.0 -A 0.0 -b fdr -c 0.001   -o filtered.vcf]
galaxy.jobs.runners DEBUG 2025-04-26 01:21:15,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/103/working/filtered.vcf" -a -f "/galaxy/server/database/objects/3/4/d/dataset_34d3531c-738f-4416-aa73-bab164fae94c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/103/working/filtered.vcf" "/galaxy/server/database/objects/3/4/d/dataset_34d3531c-738f-4416-aa73-bab164fae94c.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:15,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:15,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:21:15,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:15,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:15,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:16,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:19,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vhvkh with k8s id: gxy-vhvkh succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:21:19,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:21:26,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 103 finished
galaxy.model.metadata DEBUG 2025-04-26 01:21:27,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 129
galaxy.util WARNING 2025-04-26 01:21:27,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/4/d/dataset_34d3531c-738f-4416-aa73-bab164fae94c.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/4/d/dataset_34d3531c-738f-4416-aa73-bab164fae94c.dat'
galaxy.jobs INFO 2025-04-26 01:21:27,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs DEBUG 2025-04-26 01:21:27,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 103 executed (99.221 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:27,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:21:28,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 104
tpv.core.entities DEBUG 2025-04-26 01:21:28,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:21:28,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:21:28,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:21:28,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:21:28,975 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Working directory for job is: /galaxy/server/database/jobs_directory/000/104
galaxy.jobs.runners DEBUG 2025-04-26 01:21:28,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [104] queued (26.665 ms)
galaxy.jobs.handler INFO 2025-04-26 01:21:28,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:28,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 104
galaxy.jobs DEBUG 2025-04-26 01:21:29,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [104] prepared (51.732 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:21:29,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/104/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/104/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/104/configs/tmpa94wcq62']
galaxy.jobs.runners DEBUG 2025-04-26 01:21:29,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (104) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/104/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/104/galaxy_104.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:29,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:29,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:29,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:39,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rrp89 with k8s id: gxy-rrp89 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:21:39,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 104: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:21:46,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 104 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:21:46,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'call-out2.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/104/working/data_fetch_upload_0t2b51qm', 'object_id': 130}]}]}]
galaxy.jobs INFO 2025-04-26 01:21:46,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 104 in /galaxy/server/database/jobs_directory/000/104
galaxy.jobs DEBUG 2025-04-26 01:21:46,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 104 executed (126.934 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:46,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:21:47,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 105
tpv.core.entities DEBUG 2025-04-26 01:21:47,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/.*, abstract=False, cores=1, mem=15.2, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:21:47,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:21:47,316 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:21:47,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:21:47,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Working directory for job is: /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-04-26 01:21:47,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [105] queued (33.279 ms)
galaxy.jobs.handler INFO 2025-04-26 01:21:47,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:47,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 105
galaxy.jobs DEBUG 2025-04-26 01:21:47,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [105] prepared (37.747 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:47,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:21:47,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:47,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:21:47,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/105/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/105/outputs/COMMAND_VERSION 2>&1;
lofreq filter -i /galaxy/server/database/objects/3/3/a/dataset_33a5fc70-3c0c-4374-960a-4fd0871ec57a.dat --no-defaults --verbose --print-all  -Q 38 -K 20 -v 10 -V 0 -a 0.0 -A 0.0 -b fdr -c 0.001   -o filtered.vcf]
galaxy.jobs.runners DEBUG 2025-04-26 01:21:47,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (105) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/105/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/105/galaxy_105.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/105/working/filtered.vcf" -a -f "/galaxy/server/database/objects/a/8/0/dataset_a80ebb48-c75a-4ec9-ac73-778916b63bf8.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/105/working/filtered.vcf" "/galaxy/server/database/objects/a/8/0/dataset_a80ebb48-c75a-4ec9-ac73-778916b63bf8.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:47,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:47,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:21:47,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-04-26 01:21:47,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:47,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:48,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:21:53,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n7kvr with k8s id: gxy-n7kvr succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:21:53,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 105: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:22:00,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 105 finished
galaxy.model.metadata DEBUG 2025-04-26 01:22:00,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 131
galaxy.util WARNING 2025-04-26 01:22:00,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/8/0/dataset_a80ebb48-c75a-4ec9-ac73-778916b63bf8.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/8/0/dataset_a80ebb48-c75a-4ec9-ac73-778916b63bf8.dat'
galaxy.jobs INFO 2025-04-26 01:22:00,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 105 in /galaxy/server/database/jobs_directory/000/105
galaxy.jobs DEBUG 2025-04-26 01:22:00,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 105 executed (103.298 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:00,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:22:02,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 106
tpv.core.entities DEBUG 2025-04-26 01:22:02,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:22:02,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:22:02,627 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:22:02,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:22:02,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Working directory for job is: /galaxy/server/database/jobs_directory/000/106
galaxy.jobs.runners DEBUG 2025-04-26 01:22:02,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [106] queued (27.740 ms)
galaxy.jobs.handler INFO 2025-04-26 01:22:02,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:02,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 106
galaxy.jobs DEBUG 2025-04-26 01:22:02,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [106] prepared (50.638 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:22:02,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/106/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/106/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/106/configs/tmpipyfp8js']
galaxy.jobs.runners DEBUG 2025-04-26 01:22:02,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (106) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/106/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/106/galaxy_106.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:02,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:02,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:03,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:12,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-94jf5 with k8s id: gxy-94jf5 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:22:12,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 106: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:22:19,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 106 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:22:19,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/106/working/data_fetch_upload_erne2fgv', 'object_id': 132}]}]}]
galaxy.jobs INFO 2025-04-26 01:22:19,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 106 in /galaxy/server/database/jobs_directory/000/106
galaxy.jobs DEBUG 2025-04-26 01:22:19,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 106 executed (114.774 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:19,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:22:20,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 107
tpv.core.entities DEBUG 2025-04-26 01:22:20,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/.*, abstract=False, cores=8, mem=70, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:22:20,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:22:20,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:22:20,968 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:22:20,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Working directory for job is: /galaxy/server/database/jobs_directory/000/107
galaxy.jobs.runners DEBUG 2025-04-26 01:22:20,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [107] queued (32.086 ms)
galaxy.jobs.handler INFO 2025-04-26 01:22:20,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:20,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 107
galaxy.jobs DEBUG 2025-04-26 01:22:21,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [107] prepared (74.499 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:22:21,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:22:21,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:22:21,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:22:21,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/107/tool_script.sh] for tool command [mkdir ./index && mkdir ./output && salmon index -i ./index --kmerLen '31' --gencode --transcripts '/galaxy/server/database/objects/7/6/d/dataset_76d74906-8e64-4975-ab77-23c15c0612bc.dat' &&    ln -s /galaxy/server/database/objects/7/6/d/dataset_76d74906-8e64-4975-ab77-23c15c0612bc.dat ./single.fasta &&  salmon quant --index './index' --libType U --unmatedReads ./single.fasta --threads "${GALAXY_SLOTS:-4}"     --writeMappings=./output/samout.sam         --incompatPrior '0.0'       --biasSpeedSamp '5' --fldMax '1000' --fldMean '250' --fldSD '25' --forgettingFactor '0.65'  --maxReadOcc '100'     --numBiasSamples '2000000' --numAuxModelSamples '5000000' --numPreAuxModelSamples '5000'  --numGibbsSamples '0'  --numBootstraps '0'  --thinningFactor '16'  --sigDigits '3' --vbPrior '1e-05'   -o ./output   && samtools sort -@ ${GALAXY_SLOTS} --output-fmt=BAM -o ./output/bamout.bam ./output/samout.sam]
galaxy.jobs.runners DEBUG 2025-04-26 01:22:21,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (107) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/107/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/107/galaxy_107.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/107/working/output/quant.sf" -a -f "/galaxy/server/database/objects/3/4/5/dataset_34593621-9ed4-45a9-9f5e-bd2386fa168a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/107/working/output/quant.sf" "/galaxy/server/database/objects/3/4/5/dataset_34593621-9ed4-45a9-9f5e-bd2386fa168a.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/107/working/output/bamout.bam" -a -f "/galaxy/server/database/objects/7/a/a/dataset_7aac2c53-02ae-43ab-a144-e3448613eeee.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/107/working/output/bamout.bam" "/galaxy/server/database/objects/7/a/a/dataset_7aac2c53-02ae-43ab-a144-e3448613eeee.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:21,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:22:21,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:22:21,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:22:21,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:21,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:21,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:42,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8dvrg with k8s id: gxy-8dvrg succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:22:43,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 107: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:22:49,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 107 finished
galaxy.model.metadata DEBUG 2025-04-26 01:22:50,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 133
galaxy.model.metadata DEBUG 2025-04-26 01:22:50,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.util WARNING 2025-04-26 01:22:50,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/4/5/dataset_34593621-9ed4-45a9-9f5e-bd2386fa168a.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/4/5/dataset_34593621-9ed4-45a9-9f5e-bd2386fa168a.dat'
galaxy.util WARNING 2025-04-26 01:22:50,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/a/a/dataset_7aac2c53-02ae-43ab-a144-e3448613eeee.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/a/a/dataset_7aac2c53-02ae-43ab-a144-e3448613eeee.dat'
galaxy.jobs INFO 2025-04-26 01:22:50,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 107 in /galaxy/server/database/jobs_directory/000/107
galaxy.jobs DEBUG 2025-04-26 01:22:50,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 107 executed (103.279 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:50,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:22:51,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 108
tpv.core.entities DEBUG 2025-04-26 01:22:51,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:22:51,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:22:51,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:22:51,535 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:22:51,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Working directory for job is: /galaxy/server/database/jobs_directory/000/108
galaxy.jobs.runners DEBUG 2025-04-26 01:22:51,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [108] queued (27.975 ms)
galaxy.jobs.handler INFO 2025-04-26 01:22:51,554 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:51,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 108
galaxy.jobs DEBUG 2025-04-26 01:22:51,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [108] prepared (54.855 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:22:51,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/108/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/108/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/108/configs/tmpc_xa9cnv']
galaxy.jobs.runners DEBUG 2025-04-26 01:22:51,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (108) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/108/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/108/galaxy_108.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:51,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:51,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:22:51,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:01,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xhg6g with k8s id: gxy-xhg6g succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:23:01,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 108: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:23:08,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 108 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:23:08,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/108/working/data_fetch_upload_gq5qg_rg', 'object_id': 135}]}]}]
galaxy.jobs INFO 2025-04-26 01:23:08,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 108 in /galaxy/server/database/jobs_directory/000/108
galaxy.jobs DEBUG 2025-04-26 01:23:08,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 108 executed (107.595 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:08,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:23:08,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 109
tpv.core.entities DEBUG 2025-04-26 01:23:08,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/.*, abstract=False, cores=8, mem=70, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:23:08,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:23:08,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:23:08,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:23:08,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Working directory for job is: /galaxy/server/database/jobs_directory/000/109
galaxy.jobs.runners DEBUG 2025-04-26 01:23:08,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [109] queued (34.253 ms)
galaxy.jobs.handler INFO 2025-04-26 01:23:08,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:08,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 109
galaxy.jobs DEBUG 2025-04-26 01:23:08,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [109] prepared (40.882 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:08,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:23:08,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:09,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:23:09,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/109/tool_script.sh] for tool command [mkdir ./index && mkdir ./output && salmon index -i ./index --kmerLen '31' --gencode --transcripts '/galaxy/server/database/objects/0/8/0/dataset_0800d75d-51c2-485f-981a-b5c2827b080e.dat' &&    ln -s /galaxy/server/database/objects/0/8/0/dataset_0800d75d-51c2-485f-981a-b5c2827b080e.dat ./single.fasta &&  salmon quant --index './index' --libType U --unmatedReads ./single.fasta --threads "${GALAXY_SLOTS:-4}"              --incompatPrior '0.0'       --biasSpeedSamp '5' --fldMax '1000' --fldMean '250' --fldSD '25' --forgettingFactor '0.65'  --maxReadOcc '100'     --numBiasSamples '2000000' --numAuxModelSamples '5000000' --numPreAuxModelSamples '5000'  --numGibbsSamples '0'  --numBootstraps '0'  --thinningFactor '16'  --sigDigits '3' --vbPrior '1e-05'   -o ./output]
galaxy.jobs.runners DEBUG 2025-04-26 01:23:09,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (109) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/109/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/109/galaxy_109.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/109/working/output/quant.sf" -a -f "/galaxy/server/database/objects/0/0/a/dataset_00a0ab91-d419-405d-9b63-3fccf1258240.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/109/working/output/quant.sf" "/galaxy/server/database/objects/0/0/a/dataset_00a0ab91-d419-405d-9b63-3fccf1258240.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:09,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:09,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:23:09,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:09,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:09,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:10,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:15,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6hf4z with k8s id: gxy-6hf4z succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:23:15,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 109: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:23:22,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 109 finished
galaxy.model.metadata DEBUG 2025-04-26 01:23:22,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 136
galaxy.util WARNING 2025-04-26 01:23:22,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/0/a/dataset_00a0ab91-d419-405d-9b63-3fccf1258240.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/0/a/dataset_00a0ab91-d419-405d-9b63-3fccf1258240.dat'
galaxy.jobs INFO 2025-04-26 01:23:22,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 109 in /galaxy/server/database/jobs_directory/000/109
galaxy.jobs DEBUG 2025-04-26 01:23:22,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 109 executed (79.108 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:22,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:23:24,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 110
tpv.core.entities DEBUG 2025-04-26 01:23:24,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:23:24,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:23:24,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:23:24,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:23:24,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Working directory for job is: /galaxy/server/database/jobs_directory/000/110
galaxy.jobs.runners DEBUG 2025-04-26 01:23:24,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [110] queued (28.275 ms)
galaxy.jobs.handler INFO 2025-04-26 01:23:24,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:24,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 110
galaxy.jobs DEBUG 2025-04-26 01:23:24,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [110] prepared (53.818 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:23:24,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/110/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/110/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/110/configs/tmp6t84q8gx']
galaxy.jobs.runners DEBUG 2025-04-26 01:23:24,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (110) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/110/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/110/galaxy_110.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:24,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:24,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:25,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:33,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7kdlm with k8s id: gxy-7kdlm succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:23:33,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 110: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:23:40,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 110 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:23:40,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/110/working/data_fetch_upload_ks6apat8', 'object_id': 137}]}]}]
galaxy.jobs INFO 2025-04-26 01:23:40,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 110 in /galaxy/server/database/jobs_directory/000/110
galaxy.jobs DEBUG 2025-04-26 01:23:40,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 110 executed (107.430 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:40,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:23:41,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 111
tpv.core.entities DEBUG 2025-04-26 01:23:41,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/.*, abstract=False, cores=8, mem=70, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:23:41,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:23:41,524 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:23:41,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:23:41,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Working directory for job is: /galaxy/server/database/jobs_directory/000/111
galaxy.jobs.runners DEBUG 2025-04-26 01:23:41,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [111] queued (31.385 ms)
galaxy.jobs.handler INFO 2025-04-26 01:23:41,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:41,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 111
galaxy.jobs DEBUG 2025-04-26 01:23:41,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [111] prepared (46.597 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:41,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:23:41,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:41,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:23:41,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/111/tool_script.sh] for tool command [mkdir ./index && mkdir ./output && salmon index -i ./index --kmerLen '31' --gencode --transcripts '/galaxy/server/database/objects/e/a/8/dataset_ea88d775-fc9b-4034-bbb3-49cfa1024043.dat' &&    ln -s /galaxy/server/database/objects/e/a/8/dataset_ea88d775-fc9b-4034-bbb3-49cfa1024043.dat ./single.fasta &&  salmon quant --index './index' --libType U --unmatedReads ./single.fasta --threads "${GALAXY_SLOTS:-4}"              --incompatPrior '0.0'       --biasSpeedSamp '5' --fldMax '1000' --fldMean '250' --fldSD '25' --forgettingFactor '0.65'  --maxReadOcc '100'     --numBiasSamples '2000000' --numAuxModelSamples '5000000' --numPreAuxModelSamples '5000'  --numGibbsSamples '0'  --numBootstraps '0'  --thinningFactor '16'  --sigDigits '3' --vbPrior '1e-05'   -o ./output]
galaxy.jobs.runners DEBUG 2025-04-26 01:23:41,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (111) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/111/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/111/galaxy_111.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/111/working/output/quant.sf" -a -f "/galaxy/server/database/objects/0/a/7/dataset_0a7f4b10-c5a3-4c7f-94f8-f2c45f6355a2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/111/working/output/quant.sf" "/galaxy/server/database/objects/0/a/7/dataset_0a7f4b10-c5a3-4c7f-94f8-f2c45f6355a2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:41,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:41,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:23:41,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:23:41,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:41,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:42,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:47,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kpqm2 with k8s id: gxy-kpqm2 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:23:47,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 111: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:23:54,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 111 finished
galaxy.model.metadata DEBUG 2025-04-26 01:23:54,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 138
galaxy.util WARNING 2025-04-26 01:23:54,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/a/7/dataset_0a7f4b10-c5a3-4c7f-94f8-f2c45f6355a2.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/a/7/dataset_0a7f4b10-c5a3-4c7f-94f8-f2c45f6355a2.dat'
galaxy.jobs INFO 2025-04-26 01:23:54,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.jobs DEBUG 2025-04-26 01:23:54,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 111 executed (84.264 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:54,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:23:56,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 112, 113
tpv.core.entities DEBUG 2025-04-26 01:23:56,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:23:56,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:23:56,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:23:56,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:23:56,862 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Working directory for job is: /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-04-26 01:23:56,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [112] queued (32.716 ms)
galaxy.jobs.handler INFO 2025-04-26 01:23:56,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:56,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 112
tpv.core.entities DEBUG 2025-04-26 01:23:56,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:23:56,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:23:56,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:23:56,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:23:56,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Working directory for job is: /galaxy/server/database/jobs_directory/000/113
galaxy.jobs.runners DEBUG 2025-04-26 01:23:56,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [113] queued (41.535 ms)
galaxy.jobs.handler INFO 2025-04-26 01:23:56,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:56,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 113
galaxy.jobs DEBUG 2025-04-26 01:23:56,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [112] prepared (71.491 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:23:56,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/112/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/112/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/112/configs/tmp2sool44v']
galaxy.jobs.runners DEBUG 2025-04-26 01:23:56,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (112) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/112/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/112/galaxy_112.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:57,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:23:57,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [113] prepared (70.800 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:57,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:23:57,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/113/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/113/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/113/configs/tmpy5gf9qdm']
galaxy.jobs.runners DEBUG 2025-04-26 01:23:57,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (113) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/113/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/113/galaxy_113.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:57,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:57,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:57,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:23:57,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:06,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-58lmg with k8s id: gxy-58lmg succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:24:06,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 113: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:07,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bdtd9 with k8s id: gxy-bdtd9 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:24:07,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 112: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:24:13,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 113 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:24:13,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/113/working/data_fetch_upload_x0n0_l9_', 'object_id': 140}]}]}]
galaxy.jobs INFO 2025-04-26 01:24:13,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 113 in /galaxy/server/database/jobs_directory/000/113
galaxy.jobs DEBUG 2025-04-26 01:24:13,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 113 executed (110.526 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:13,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-26 01:24:14,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 112 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:24:14,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'salmonbam.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/112/working/gxupload_0', 'object_id': 139}]}]}]
galaxy.jobs INFO 2025-04-26 01:24:14,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.jobs DEBUG 2025-04-26 01:24:14,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 112 executed (127.673 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:14,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:24:15,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 114
tpv.core.entities DEBUG 2025-04-26 01:24:15,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/.*, abstract=False, cores=8, mem=70, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:24:15,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:24:15,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:24:15,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:24:15,298 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Working directory for job is: /galaxy/server/database/jobs_directory/000/114
galaxy.jobs.runners DEBUG 2025-04-26 01:24:15,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [114] queued (36.483 ms)
galaxy.jobs.handler INFO 2025-04-26 01:24:15,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:15,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 114
galaxy.jobs DEBUG 2025-04-26 01:24:15,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [114] prepared (52.883 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:15,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:24:15,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:15,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:24:15,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/114/tool_script.sh] for tool command [salmon quant -t '/galaxy/server/database/objects/4/1/4/dataset_414cd2b0-a05d-40db-af56-c29b0a0cfdce.dat' -l 'U' -a '/galaxy/server/database/objects/8/f/b/dataset_8fb32d83-7950-43e5-a297-3226445f195b.dat' --threads "${GALAXY_SLOTS:-4}"  --noErrorModel --numErrorBins '5' --sampleOut --sampleUnaligned         --incompatPrior '0.0'       --biasSpeedSamp '5' --fldMax '1000' --fldMean '250' --fldSD '25' --forgettingFactor '0.65'  --maxReadOcc '100'     --numBiasSamples '2000000' --numAuxModelSamples '5000000' --numPreAuxModelSamples '5000'  --numGibbsSamples '0'  --numBootstraps '0'  --thinningFactor '16'  --sigDigits '3' --vbPrior '1e-05'   -o ./output   && samtools sort -@ ${GALAXY_SLOTS} --output-fmt=BAM -o ./output/bamout.bam ./output/postSample.bam]
galaxy.jobs.runners DEBUG 2025-04-26 01:24:15,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (114) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/114/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/114/galaxy_114.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/114/working/output/quant.sf" -a -f "/galaxy/server/database/objects/b/e/a/dataset_bea6efd5-e358-49d4-b961-6a2674e28b49.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/114/working/output/quant.sf" "/galaxy/server/database/objects/b/e/a/dataset_bea6efd5-e358-49d4-b961-6a2674e28b49.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/114/working/output/bamout.bam" -a -f "/galaxy/server/database/objects/e/7/1/dataset_e7180bfc-fe2f-4bef-b192-d37d5396deeb.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/114/working/output/bamout.bam" "/galaxy/server/database/objects/e/7/1/dataset_e7180bfc-fe2f-4bef-b192-d37d5396deeb.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:15,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:15,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:24:15,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:15,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:15,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:16,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:21,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lf9nv with k8s id: gxy-lf9nv succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:24:21,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 114: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:24:28,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 114 finished
galaxy.model.metadata DEBUG 2025-04-26 01:24:28,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.model.metadata DEBUG 2025-04-26 01:24:28,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 142
galaxy.util WARNING 2025-04-26 01:24:28,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/e/a/dataset_bea6efd5-e358-49d4-b961-6a2674e28b49.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/e/a/dataset_bea6efd5-e358-49d4-b961-6a2674e28b49.dat'
galaxy.util WARNING 2025-04-26 01:24:28,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/7/1/dataset_e7180bfc-fe2f-4bef-b192-d37d5396deeb.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/7/1/dataset_e7180bfc-fe2f-4bef-b192-d37d5396deeb.dat'
galaxy.jobs INFO 2025-04-26 01:24:28,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 114 in /galaxy/server/database/jobs_directory/000/114
galaxy.jobs DEBUG 2025-04-26 01:24:28,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 114 executed (91.856 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:28,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:24:30,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 115
tpv.core.entities DEBUG 2025-04-26 01:24:30,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:24:30,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:24:30,582 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:24:30,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:24:30,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Working directory for job is: /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-04-26 01:24:30,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [115] queued (30.715 ms)
galaxy.jobs.handler INFO 2025-04-26 01:24:30,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:30,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 115
galaxy.jobs DEBUG 2025-04-26 01:24:30,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [115] prepared (58.056 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:24:30,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/115/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/115/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/115/configs/tmp3rj1exs_']
galaxy.jobs.runners DEBUG 2025-04-26 01:24:30,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (115) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/115/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/115/galaxy_115.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:30,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:30,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:31,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:41,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-grrdl with k8s id: gxy-grrdl succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:24:41,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 115: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:24:48,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 115 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:24:49,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/115/working/data_fetch_upload_uy9v5o2u', 'object_id': 143}]}]}]
galaxy.jobs INFO 2025-04-26 01:24:49,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs DEBUG 2025-04-26 01:24:49,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 115 executed (111.141 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:49,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:24:49,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 116
tpv.core.entities DEBUG 2025-04-26 01:24:49,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/.*, abstract=False, cores=8, mem=70, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:24:49,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:24:49,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:24:49,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:24:49,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Working directory for job is: /galaxy/server/database/jobs_directory/000/116
galaxy.jobs.runners DEBUG 2025-04-26 01:24:50,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [116] queued (38.854 ms)
galaxy.jobs.handler INFO 2025-04-26 01:24:50,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:50,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 116
galaxy.jobs DEBUG 2025-04-26 01:24:50,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [116] prepared (51.167 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:50,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:24:50,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:50,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:24:50,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/116/tool_script.sh] for tool command [mkdir ./index && mkdir ./output && salmon index -i ./index --kmerLen '31' --gencode --transcripts '/galaxy/server/database/objects/4/9/9/dataset_49946f9a-d424-4f2e-9db7-4c43e395bd3f.dat' &&    ln -s /galaxy/server/database/objects/4/9/9/dataset_49946f9a-d424-4f2e-9db7-4c43e395bd3f.dat ./single.fasta &&  salmon quant --index './index' --libType U --unmatedReads ./single.fasta --threads "${GALAXY_SLOTS:-4}"  --validateMappings --minScoreFraction '0.65' --ma '2' --mp '4' --go '5' --ge '3'    --allowDovetail --recoverOrphans          --incompatPrior '0.0'       --biasSpeedSamp '5' --fldMax '1000' --fldMean '250' --fldSD '25' --forgettingFactor '0.65'  --maxReadOcc '100'     --numBiasSamples '2000000' --numAuxModelSamples '5000000' --numPreAuxModelSamples '5000'  --numGibbsSamples '0'  --numBootstraps '0'  --thinningFactor '16'  --sigDigits '3' --vbPrior '1e-05'   -o ./output]
galaxy.jobs.runners DEBUG 2025-04-26 01:24:50,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/116/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/116/galaxy_116.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/116/working/output/quant.sf" -a -f "/galaxy/server/database/objects/3/c/3/dataset_3c36c7b9-e8e2-49f1-9aa6-c11feaf01681.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/116/working/output/quant.sf" "/galaxy/server/database/objects/3/c/3/dataset_3c36c7b9-e8e2-49f1-9aa6-c11feaf01681.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:50,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:50,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:24:50,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:24:50,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:50,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:50,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kl6w5 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-kl6w5.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:24:54,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-kl6w5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-kl6w5": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116/gxy-kl6w5) tool_stdout: Threads = 2
Vertex length = 31
Hash functions = 5
Filter size = 1048576
Capacity = 2
Files: 
./index/ref_k31_fixed.fa
--------------------------------------------------------------------------------
Round 0, 0:1048576
Pass	Filling	Filtering
1	0	0	
2	0	0
True junctions count = 14
False junctions count = 17
Hash table size = 31
Candidate marks count = 54
--------------------------------------------------------------------------------
Reallocating bifurcations time: 0
True marks count: 51
Edges construction time: 0
--------------------------------------------------------------------------------
Distinct junctions = 14

for info, total work write each  : 2.331    total work inram from level 3 : 4.322  total work raw : 25.000 
Bitarray          105024  bits (100.00 %)   (array + ranks )
final hash             0  bits (0.00 %) (nb in final hash 0)

galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116/gxy-kl6w5) tool_stderr: Version Info: ### PLEASE UPGRADE SALMON ###
### A newer version of salmon with important bug fixes and improvements is available. ####
###
The newest version, available at https://github.com/COMBINE-lab/salmon/releases
contains new features, improvements, and bug fixes; please upgrade at your
earliest convenience.
###
Sign up for the salmon mailing list to hear about new versions, features and updates at:
https://oceangenomics.com/subscribe
###
[2025-04-26 01:24:51.038] [jLog] [warning] The salmon index is being built without any decoy sequences.  It is recommended that decoy sequence (either computed auxiliary decoy sequence or the genome of the organism) be provided during indexing. Further details can be found at https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode.
[2025-04-26 01:24:51.038] [jLog] [info] building index
out : ./index
[2025-04-26 01:24:51.039] [puff::index::jointLog] [info] Running fixFasta

[Step 1 of 4] : counting k-mers

[2025-04-26 01:24:51.046] [puff::index::jointLog] [info] Replaced 0 non-ATCG nucleotides
[2025-04-26 01:24:51.046] [puff::index::jointLog] [info] Clipped poly-A tails from 0 transcripts
wrote 15 cleaned references
[2025-04-26 01:24:51.060] [puff::index::jointLog] [info] Filter size not provided; estimating from number of distinct k-mers
[2025-04-26 01:24:51.062] [puff::index::jointLog] [info] ntHll estimated 54811 distinct k-mers, setting filter size to 2^20
allowedIn: 12
Max Junction ID: 70
seen.size():569 kmerInfo.size():71
approximateContigTotalLength: 17465
counters for complex kmers:
(prec>1 & succ>1)=0 | (succ>1 & isStart)=0 | (prec>1 & isEnd)=0 | (isStart & isEnd)=0
contig count: 24 element count: 19592 complex nodes: 0
# of ones in rank vector: 23
[2025-04-26 01:24:51.120] [puff::index::jointLog] [info] Starting the Pufferfish indexing by reading the GFA binary file.
[2025-04-26 01:24:51.120] [puff::index::jointLog] [info] Setting the index/BinaryGfa directory ./index
size = 19592
-----------------------------------------
| Loading contigs | Time = 690.4 us
-----------------------------------------
size = 19592
-----------------------------------------
| Loading contig boundaries | Time = 565.89 us
-----------------------------------------
Number of ones: 23
Number of ones per inventory item: 512
Inventory entries filled: 1
23
[2025-04-26 01:24:51.122] [puff::index::jointLog] [info] Done wrapping the rank vector with a rank9sel structure.
[2025-04-26 01:24:51.122] [puff::index::jointLog] [info] contig count for validation: 23
[2025-04-26 01:24:51.122] [puff::index::jointLog] [info] Total # of Contigs : 23
[2025-04-26 01:24:51.122] [puff::index::jointLog] [info] Total # of numerical Contigs : 23
[2025-04-26 01:24:51.122] [puff::index::jointLog] [info] Total # of contig vec entries: 36
[2025-04-26 01:24:51.122] [puff::index::jointLog] [info] bits per offset entry 6
[2025-04-26 01:24:51.122] [puff::index::jointLog] [info] Done constructing the contig vector. 24
[2025-04-26 01:24:51.134] [puff::index::jointLog] [info] # segments = 23
[2025-04-26 01:24:51.134] [puff::index::jointLog] [info] total length = 19592
[2025-04-26 01:24:51.134] [puff::index::jointLog] [info] Reading the reference files ...
[2025-04-26 01:24:51.144] [puff::index::jointLog] [info] positional integer width = 15
[2025-04-26 01:24:51.144] [puff::index::jointLog] [info] seqSize = 19592
[2025-04-26 01:24:51.144] [puff::index::jointLog] [info] rankSize = 19592
[2025-04-26 01:24:51.144] [puff::index::jointLog] [info] edgeVecSize = 0
[2025-04-26 01:24:51.144] [puff::index::jointLog] [info] num keys = 18902
[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  2.32 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  4.65 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  4.65 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  4.65 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  4.65 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  4.65 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  4.65 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  4.65 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  6.97 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  9.3  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  11.6 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  13.9 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  16.3 %   elap
..
ec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  88.3 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  90.7 %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec[Building BooPHF]  100  %   elapsed:   0 min 0  sec   remaining:   0 min 0  sec
[2025-04-26 01:24:51.213] [puff::index::jointLog] [info] mphf size = 0.0125198 MB
[2025-04-26 01:24:51.213] [puff::index::jointLog] [info] chunk size = 9796
[2025-04-26 01:24:51.213] [puff::index::jointLog] [info] chunk 0 = [0, 9796)
[2025-04-26 01:24:51.213] [puff::index::jointLog] [info] chunk 1 = [9796, 19562)
[2025-04-26 01:24:51.214] [puff::index::jointLog] [info] finished populating pos vector
[2025-04-26 01:24:51.214] [puff::index::jointLog] [info] writing index components
[2025-04-26 01:24:51.227] [puff::index::jointLog] [info] finished writing dense pufferfish index
[2025-04-26 01:24:51.231] [jLog] [info] done building index
Version Info: ### PLEASE UPGRADE SALMON ###
### A newer version of salmon with important bug fixes and improvements is available. ####
###
The newest version, available at https://github.com/COMBINE-lab/salmon/releases
contains new features, improvements, and bug fixes; please upgrade at your
earliest convenience.
###
Sign up for the salmon mailing list to hear about new versions, features and updates at:
https://oceangenomics.com/subscribe
###
### salmon (selective-alignment-based) v1.3.0
### [ program ] => salmon 
### [ command ] => quant 
### [ index ] => { ./index }
### [ libType ] => { U }
### [ unmatedReads ] => { ./single.fasta }
### [ threads ] => { 8 }
### [ validateMappings ] => { }
### [ minScoreFraction ] => { 0.65 }
### [ ma ] => { 2 }
### [ mp ] => { 4 }
### [ go ] => { 5 }
### [ ge ] => { 3 }
### [ allowDovetail ] => { }
### [ recoverOrphans ] => { }
### [ incompatPrior ] => { 0.0 }
### [ biasSpeedSamp ] => { 5 }
### [ fldMax ] => { 1000 }
### [ fldMean ] => { 250 }
### [ fldSD ] => { 25 }
### [ forgettingFactor ] => { 0.65 }
### [ maxReadOcc ] => { 100 }
### [ numBiasSamples ] => { 2000000 }
### [ numAuxModelSamples ] => { 5000000 }
### [ numPreAuxModelSamples ] => { 5000 }
### [ numGibbsSamples ] => { 0 }
### [ numBootstraps ] => { 0 }
### [ thinningFactor ] => { 16 }
### [ sigDigits ] => { 3 }
### [ vbPrior ] => { 1e-05 }
### [ output ] => { ./output }
Logs will be written to ./output/logs
[2025-04-26 01:24:51.308] [jointLog] [info] setting maxHashResizeThreads to 8
[2025-04-26 01:24:51.308] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.
[2025-04-26 01:24:51.308] [jointLog] [warning] You set the mismatch penalty as 4, but it should be negative.  It is being negated to -4.
[2025-04-26 01:24:51.308] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.
[2025-04-26 01:24:51.308] [jointLog] [info] parsing read library format
[2025-04-26 01:24:51.308] [jointLog] [info] There is 1 library.
-----------------------------------------
| Loading contig table | Time = 681.95 us
-----------------------------------------
size = 24
-----------------------------------------
| Loading contig offsets | Time = 1.4991 ms
-----------------------------------------
-----------------------------------------
| Loading reference lengths | Time = 632.12 us
-----------------------------------------
-----------------------------------------
| Loading mphf table | Time = 432.13 us
-----------------------------------------
size = 19592
Number of ones: 23
Number of ones per inventory item: 512
Inventory entries filled: 1
-----------------------------------------
| Loading contig boundaries | Time = 791.98 us
-----------------------------------------
size = 19592
-----------------------------------------
| Loading sequence | Time = 948.79 us
-----------------------------------------
size = 18902
-----------------------------------------
| Loading positions | Time = 1.1113 ms
-----------------------------------------
size = 28562
-----------------------------------------
| Loading reference sequence | Time = 629.51 us
-----------------------------------------
-----------------------------------------
| Loading reference accumulative lengths | Time = 377.06 us
-----------------------------------------
[2025-04-26 01:24:51.375] [jointLog] [info] Loading pufferfish index
[2025-04-26 01:24:51.377] [jointLog] [info] Loading dense pufferfish index.
[2025-04-26 01:24:51.386] [jointLog] [info] done
[2025-04-26 01:24:51.386] [jointLog] [info] Index contained 15 targets
[2025-04-26 01:24:51.386] [jointLog] [info] Number of decoys : 0












[2025-04-26 01:24:51.437] [jointLog] [info] Computed 15 rich equivalence classes for further processing
[2025-04-26 01:24:51.437] [jointLog] [info] Counted 15 total reads in the equivalence classes 
[2025-04-26 01:24:51.438] [jointLog] [info] Number of orphans recovered using orphan rescue : 0
[2025-04-26 01:24:51.438] [jointLog] [info] Number of mappings discarded because of alignment score : 6
[2025-04-26 01:24:51.438] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 0
[2025-04-26 01:24:51.438] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 0
[2025-04-26 01:24:51.438] [jointLog] [warning] Only 15 fragments were mapped, but the number of burn-in fragments was set to 5000000.
The effective lengths have been computed using the observed mappings.

[2025-04-26 01:24:51.438] [jointLog] [info] Mapping rate = 100%

[2025-04-26 01:24:51.438] [jointLog] [info] finished quantifyLibrary()
[2025-04-26 01:24:51.443] [jointLog] [info] Starting optimizer
[2025-04-26 01:24:51.443] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate
[2025-04-26 01:24:51.443] [jointLog] [info] iteration = 0 | max rel diff. = 0
[2025-04-26 01:24:51.445] [jointLog] [info] iteration = 100 | max rel diff. = 0
[2025-04-26 01:24:51.445] [jointLog] [info] Finished optimizer
[2025-04-26 01:24:51.445] [jointLog] [info] writing output 

[2025-04-26 01:24:51.450] [jointLog] [warning] NOTE: Read Lib [[ ./single.fasta ]] :

Detected a *potential* strand bias > 1% in an unstranded protocol check the file: ./output/lib_format_counts.json for details


galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116/gxy-kl6w5) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116/gxy-kl6w5) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-kl6w5.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 116 (gxy-kl6w5)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-kl6w5 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:54,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:55,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116/gxy-kl6w5) Terminated at user's request
galaxy.util WARNING 2025-04-26 01:24:55,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/c/3/dataset_3c36c7b9-e8e2-49f1-9aa6-c11feaf01681.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/c/3/dataset_3c36c7b9-e8e2-49f1-9aa6-c11feaf01681.dat'
galaxy.jobs.handler DEBUG 2025-04-26 01:24:56,097 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 117
tpv.core.entities DEBUG 2025-04-26 01:24:56,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:24:56,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:24:56,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:24:56,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:24:56,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Working directory for job is: /galaxy/server/database/jobs_directory/000/117
galaxy.jobs.runners DEBUG 2025-04-26 01:24:56,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [117] queued (27.882 ms)
galaxy.jobs.handler INFO 2025-04-26 01:24:56,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:56,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 117
galaxy.jobs DEBUG 2025-04-26 01:24:56,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [117] prepared (55.407 ms)
galaxy.jobs.command_factory INFO 2025-04-26 01:24:56,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/117/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/117/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/117/configs/tmpt5hlpnsv']
galaxy.jobs.runners DEBUG 2025-04-26 01:24:56,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (117) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/117/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/117/galaxy_117.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:56,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:56,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:24:56,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:05,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z9fj8 with k8s id: gxy-z9fj8 succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:25:05,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 117: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:25:12,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 117 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:25:12,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/117/working/data_fetch_upload_qcy03qmr', 'object_id': 145}]}]}]
galaxy.jobs INFO 2025-04-26 01:25:12,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 117 in /galaxy/server/database/jobs_directory/000/117
galaxy.jobs DEBUG 2025-04-26 01:25:12,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 117 executed (158.801 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:12,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:25:13,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 118
tpv.core.entities DEBUG 2025-04-26 01:25:13,464 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/.*, abstract=False, cores=8, mem=70, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:25:13,464 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:25:13,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:25:13,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:25:13,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Working directory for job is: /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-04-26 01:25:13,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [118] queued (30.453 ms)
galaxy.jobs.handler INFO 2025-04-26 01:25:13,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:13,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 118
galaxy.jobs DEBUG 2025-04-26 01:25:13,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [118] prepared (40.128 ms)
galaxy.tool_util.deps.containers INFO 2025-04-26 01:25:13,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:25:13,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:25:13,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-26 01:25:13,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/118/tool_script.sh] for tool command [mkdir ./index && mkdir ./output && salmon index -i ./index --kmerLen '31' --gencode --transcripts '/galaxy/server/database/objects/f/7/a/dataset_f7a13d48-a3c2-4ad1-b905-9fbc39f50ee3.dat' &&    ln -s /galaxy/server/database/objects/f/7/a/dataset_f7a13d48-a3c2-4ad1-b905-9fbc39f50ee3.dat ./single.fasta &&  salmon quant --index './index' --libType U --unmatedReads ./single.fasta --threads "${GALAXY_SLOTS:-4}"            --seqBias --gcBias --incompatPrior '0.0'     --dumpEq  --minAssignedFrags '10' --biasSpeedSamp '5' --fldMax '1000' --fldMean '250' --fldSD '25' --forgettingFactor '0.65' --initUniform --maxReadOcc '100'     --numBiasSamples '2000000' --numAuxModelSamples '5000000' --numPreAuxModelSamples '5000' --useEM --numGibbsSamples '0' --noGammaDraw --numBootstraps '0'  --thinningFactor '16'  --sigDigits '3' --vbPrior '1e-05'   -o ./output]
galaxy.jobs.runners DEBUG 2025-04-26 01:25:13,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (118) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/118/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/118/galaxy_118.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/118/working/output/quant.sf" -a -f "/galaxy/server/database/objects/b/4/5/dataset_b453e95a-5c08-43b2-bb01-c647bcca6cb9.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/118/working/output/quant.sf" "/galaxy/server/database/objects/b/4/5/dataset_b453e95a-5c08-43b2-bb01-c647bcca6cb9.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:13,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-26 01:25:13,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-26 01:25:13,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/salmon/salmon/1.3.0+galaxy1: mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09
galaxy.tool_util.deps.containers INFO 2025-04-26 01:25:13,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-0fd299cadb7a80e2cc704b5d903ccc54893c512d:377fcafe3b6e7ef703094f8f47a64d081622ee09-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:13,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:14,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:18,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f8kdd with k8s id: gxy-f8kdd succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:25:18,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 118: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:25:25,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 118 finished
galaxy.model.metadata DEBUG 2025-04-26 01:25:25,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 146
galaxy.util WARNING 2025-04-26 01:25:25,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/4/5/dataset_b453e95a-5c08-43b2-bb01-c647bcca6cb9.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/4/5/dataset_b453e95a-5c08-43b2-bb01-c647bcca6cb9.dat'
galaxy.jobs INFO 2025-04-26 01:25:25,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 118 in /galaxy/server/database/jobs_directory/000/118
galaxy.jobs DEBUG 2025-04-26 01:25:25,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 118 executed (75.719 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:25,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-26 01:25:28,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 121, 120, 119
tpv.core.entities DEBUG 2025-04-26 01:25:28,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:25:28,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:25:28,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:25:28,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:25:28,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Working directory for job is: /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners DEBUG 2025-04-26 01:25:28,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [119] queued (32.917 ms)
galaxy.jobs.handler INFO 2025-04-26 01:25:28,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:28,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 119
tpv.core.entities DEBUG 2025-04-26 01:25:28,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:25:28,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:25:28,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:25:28,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:25:28,862 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Working directory for job is: /galaxy/server/database/jobs_directory/000/120
galaxy.jobs.runners DEBUG 2025-04-26 01:25:28,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [120] queued (42.843 ms)
galaxy.jobs.handler INFO 2025-04-26 01:25:28,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:28,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 120
tpv.core.entities DEBUG 2025-04-26 01:25:28,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-26 01:25:28,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-26 01:25:28,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-26 01:25:28,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [119] prepared (85.751 ms)
galaxy.jobs DEBUG 2025-04-26 01:25:28,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-26 01:25:28,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Working directory for job is: /galaxy/server/database/jobs_directory/000/121
galaxy.jobs.command_factory INFO 2025-04-26 01:25:28,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/119/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/119/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/119/configs/tmpx76ojueu']
galaxy.jobs.runners DEBUG 2025-04-26 01:25:28,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [121] queued (50.344 ms)
galaxy.jobs.handler INFO 2025-04-26 01:25:28,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Job dispatched
galaxy.jobs.runners DEBUG 2025-04-26 01:25:28,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (119) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/119/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/119/galaxy_119.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:28,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 121
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:28,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:25:28,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [120] prepared (87.239 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:28,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:25:28,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/120/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/120/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/120/configs/tmplm3ta42c']
galaxy.jobs.runners DEBUG 2025-04-26 01:25:29,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/120/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/120/galaxy_120.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:29,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-26 01:25:29,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [121] prepared (72.371 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:29,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-26 01:25:29,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/121/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/121/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/121/configs/tmpoq66_zm_']
galaxy.jobs.runners DEBUG 2025-04-26 01:25:29,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (121) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/121/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/121/galaxy_121.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:29,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:29,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:29,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:29,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:29,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4t7tz with k8s id: gxy-4t7tz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zwnjh failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-zwnjh.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-26 01:25:38,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 120: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes ERROR 2025-04-26 01:25:38,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-26-00-38-1/jobs/gxy-zwnjh

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-zwnjh": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (121/gxy-zwnjh) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (121/gxy-zwnjh) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (121/gxy-zwnjh) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (121/gxy-zwnjh) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-zwnjh.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 121 (gxy-zwnjh)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-zwnjh to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:38,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (121/gxy-zwnjh) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:39,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bkjqt with k8s id: gxy-bkjqt succeeded
galaxy.jobs.runners DEBUG 2025-04-26 01:25:39,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-26 01:25:45,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 120 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:25:45,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'human_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/120/working/data_fetch_upload_i_shg4if', 'object_id': 148}]}]}]
galaxy.jobs INFO 2025-04-26 01:25:45,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 120 in /galaxy/server/database/jobs_directory/000/120
galaxy.jobs DEBUG 2025-04-26 01:25:45,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 120 executed (104.393 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:45,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-26 01:25:46,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 119 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-26 01:25:46,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'nucmer.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/119/working/data_fetch_upload_fch84qgb', 'object_id': 147}]}]}]
galaxy.jobs INFO 2025-04-26 01:25:46,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs DEBUG 2025-04-26 01:25:46,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 119 executed (93.704 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-26 01:25:46,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
