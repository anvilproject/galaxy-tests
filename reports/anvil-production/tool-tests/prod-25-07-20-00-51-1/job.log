galaxy.jobs.runners DEBUG 2025-07-20 01:37:15,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 118 finished
galaxy.model.metadata DEBUG 2025-07-20 01:37:15,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 131
galaxy.jobs INFO 2025-07-20 01:37:15,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 118 in /galaxy/server/database/jobs_directory/000/118
galaxy.jobs DEBUG 2025-07-20 01:37:15,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 118 executed (119.173 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:15,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:37:16,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 119
tpv.core.entities DEBUG 2025-07-20 01:37:16,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:37:16,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:37:16,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:37:16,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:37:16,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:37:16,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Working directory for job is: /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners DEBUG 2025-07-20 01:37:16,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [119] queued (34.005 ms)
galaxy.jobs.handler INFO 2025-07-20 01:37:16,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:16,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 119
galaxy.jobs DEBUG 2025-07-20 01:37:17,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [119] prepared (63.186 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:37:17,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/119/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/119/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/119/configs/tmprxnfti2c']
galaxy.jobs.runners DEBUG 2025-07-20 01:37:17,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (119) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/119/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/119/galaxy_119.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:17,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:17,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:17,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-20 01:37:17,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 120
tpv.core.entities DEBUG 2025-07-20 01:37:18,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:37:18,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:37:18,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:37:18,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:37:18,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:37:18,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Working directory for job is: /galaxy/server/database/jobs_directory/000/120
galaxy.jobs.runners DEBUG 2025-07-20 01:37:18,067 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [120] queued (46.721 ms)
galaxy.jobs.handler INFO 2025-07-20 01:37:18,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:18,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 120
galaxy.jobs DEBUG 2025-07-20 01:37:18,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [120] prepared (84.973 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:37:18,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/120/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/120/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/120/configs/tmpf51vzzf6']
galaxy.jobs.runners DEBUG 2025-07-20 01:37:18,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (120) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/120/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/120/galaxy_120.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:18,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:18,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:18,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:28,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c5wjt with k8s id: gxy-c5wjt succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:28,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p9lfk with k8s id: gxy-p9lfk succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:37:28,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:37:28,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 120: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:37:36,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 120 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:37:36,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/120/working/data_fetch_upload_9lunvi4i', 'object_id': 133}]}]}]
galaxy.jobs.runners DEBUG 2025-07-20 01:37:36,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 119 finished
galaxy.jobs INFO 2025-07-20 01:37:36,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 120 in /galaxy/server/database/jobs_directory/000/120
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:37:36,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/119/working/gxupload_0', 'object_id': 132}]}]}]
galaxy.jobs DEBUG 2025-07-20 01:37:37,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 120 executed (192.291 ms)
galaxy.jobs INFO 2025-07-20 01:37:37,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:37,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:37:37,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 119 executed (181.004 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:37,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:37:37,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 121
tpv.core.entities DEBUG 2025-07-20 01:37:37,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:37:37,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:37:37,506 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:37:37,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:37:37,522 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:37:37,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Working directory for job is: /galaxy/server/database/jobs_directory/000/121
galaxy.jobs.runners DEBUG 2025-07-20 01:37:37,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [121] queued (45.464 ms)
galaxy.jobs.handler INFO 2025-07-20 01:37:37,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:37,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 121
galaxy.jobs DEBUG 2025-07-20 01:37:37,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [121] prepared (70.963 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:37:37,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:37:37,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-20 01:37:37,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:37:37,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/121/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/121/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/7/0/8/dataset_7089a970-5c26-4817-9ea3-5eabbb859c63.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/9/8/6/dataset_9860d386-7065-4062-b1e2-bf167544b1a4.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/d/0/1/metadata_d012e8ae-873b-4df2-a912-0188525cfc77.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 0 --skip-coverage 0 --limit-coverage 400    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/2/5/6/dataset_256625e4-3072-41e9-ac34-e484df51ff00.dat']
galaxy.jobs.runners DEBUG 2025-07-20 01:37:37,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (121) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/121/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/121/galaxy_121.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:37,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:37:37,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:37:37,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-20 01:37:37,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:37,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:38,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:42,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qrlrd with k8s id: gxy-qrlrd succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:37:42,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 121: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:37:50,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 121 finished
galaxy.model.metadata DEBUG 2025-07-20 01:37:50,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.jobs INFO 2025-07-20 01:37:50,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 121 in /galaxy/server/database/jobs_directory/000/121
galaxy.jobs DEBUG 2025-07-20 01:37:50,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 121 executed (110.574 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:50,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:37:51,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 122, 123
tpv.core.entities DEBUG 2025-07-20 01:37:51,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:37:51,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:37:51,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:37:51,855 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:37:51,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:37:51,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Working directory for job is: /galaxy/server/database/jobs_directory/000/122
galaxy.jobs.runners DEBUG 2025-07-20 01:37:51,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [122] queued (34.752 ms)
galaxy.jobs.handler INFO 2025-07-20 01:37:51,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:51,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 122
tpv.core.entities DEBUG 2025-07-20 01:37:51,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:37:51,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:37:51,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:37:51,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:37:51,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:37:51,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Working directory for job is: /galaxy/server/database/jobs_directory/000/123
galaxy.jobs.runners DEBUG 2025-07-20 01:37:51,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [123] queued (44.262 ms)
galaxy.jobs.handler INFO 2025-07-20 01:37:51,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:51,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 123
galaxy.jobs DEBUG 2025-07-20 01:37:51,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [122] prepared (83.365 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:37:52,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/122/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/122/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/122/configs/tmp0t1zmsqq']
galaxy.jobs.runners DEBUG 2025-07-20 01:37:52,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (122) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/122/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/122/galaxy_122.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-20 01:37:52,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [123] prepared (74.710 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:52,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:52,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:37:52,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/123/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/123/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/123/configs/tmpgaecnseg']
galaxy.jobs.runners DEBUG 2025-07-20 01:37:52,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (123) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/123/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/123/galaxy_123.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:52,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:52,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:52,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:37:52,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:03,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8rcgd with k8s id: gxy-8rcgd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:03,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fjxfd with k8s id: gxy-fjxfd succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:38:03,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 122: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:38:03,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 123: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:38:11,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 122 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:38:11,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/122/working/gxupload_0', 'object_id': 135}]}]}]
galaxy.jobs.runners DEBUG 2025-07-20 01:38:11,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 123 finished
galaxy.jobs INFO 2025-07-20 01:38:11,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 122 in /galaxy/server/database/jobs_directory/000/122
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:38:11,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/123/working/data_fetch_upload_xp9d81n0', 'object_id': 136}]}]}]
galaxy.jobs DEBUG 2025-07-20 01:38:11,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 122 executed (185.660 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:11,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-20 01:38:11,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 123 in /galaxy/server/database/jobs_directory/000/123
galaxy.jobs DEBUG 2025-07-20 01:38:11,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 123 executed (131.917 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:11,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:38:12,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 124
tpv.core.entities DEBUG 2025-07-20 01:38:12,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:38:12,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:38:12,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:38:12,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:38:12,557 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:38:12,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Working directory for job is: /galaxy/server/database/jobs_directory/000/124
galaxy.jobs.runners DEBUG 2025-07-20 01:38:12,584 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [124] queued (40.306 ms)
galaxy.jobs.handler INFO 2025-07-20 01:38:12,587 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:12,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 124
galaxy.jobs DEBUG 2025-07-20 01:38:12,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [124] prepared (60.822 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:12,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:38:12,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:12,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:38:12,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/124/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/124/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/0/0/e/dataset_00e93595-7c83-43be-9823-08bc0efdb890.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/4/1/a/dataset_41a4daae-4f2e-4c9b-827c-023a431e9cb1.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/b/1/f/metadata_b1f0a548-0615-4b2b-a0f6-ada554f02006.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 0 --skip-coverage 100 --limit-coverage 0    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/3/9/1/dataset_39130488-2dc6-438b-844e-9275337f5686.dat']
galaxy.jobs.runners DEBUG 2025-07-20 01:38:12,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (124) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/124/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/124/galaxy_124.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:12,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:12,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:38:12,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:12,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:12,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:13,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:18,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-74bdb with k8s id: gxy-74bdb succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:38:18,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 124: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:38:26,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 124 finished
galaxy.model.metadata DEBUG 2025-07-20 01:38:26,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 137
galaxy.jobs INFO 2025-07-20 01:38:26,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 124 in /galaxy/server/database/jobs_directory/000/124
galaxy.jobs DEBUG 2025-07-20 01:38:26,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 124 executed (130.647 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:26,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:38:27,878 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 126, 125
tpv.core.entities DEBUG 2025-07-20 01:38:27,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:38:27,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:38:27,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:38:27,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:38:27,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:38:27,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Working directory for job is: /galaxy/server/database/jobs_directory/000/125
galaxy.jobs.runners DEBUG 2025-07-20 01:38:27,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [125] queued (35.060 ms)
galaxy.jobs.handler INFO 2025-07-20 01:38:27,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:27,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 125
tpv.core.entities DEBUG 2025-07-20 01:38:27,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:38:27,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:38:27,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:38:27,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:38:27,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:38:28,010 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Working directory for job is: /galaxy/server/database/jobs_directory/000/126
galaxy.jobs.runners DEBUG 2025-07-20 01:38:28,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [126] queued (45.216 ms)
galaxy.jobs.handler INFO 2025-07-20 01:38:28,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:28,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 126
galaxy.jobs DEBUG 2025-07-20 01:38:28,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [125] prepared (86.116 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:38:28,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/125/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/125/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/125/configs/tmpbmr4j6wa']
galaxy.jobs.runners DEBUG 2025-07-20 01:38:28,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (125) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/125/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/125/galaxy_125.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:28,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:38:28,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [126] prepared (84.279 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:28,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:38:28,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/126/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/126/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/126/configs/tmpyhysqzav']
galaxy.jobs.runners DEBUG 2025-07-20 01:38:28,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (126) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/126/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/126/galaxy_126.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:28,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:28,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:28,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:28,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:38,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-757v8 with k8s id: gxy-757v8 succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:38:38,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 126: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:39,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2hbmx with k8s id: gxy-2hbmx succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:38:39,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 125: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:38:46,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 126 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:38:46,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/126/working/data_fetch_upload_nxpjbod8', 'object_id': 139}]}]}]
galaxy.jobs INFO 2025-07-20 01:38:46,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 126 in /galaxy/server/database/jobs_directory/000/126
galaxy.jobs DEBUG 2025-07-20 01:38:46,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 126 executed (147.539 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:46,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-20 01:38:47,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 125 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:38:47,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.cram', 'dbkey': '?', 'ext': 'cram', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded cram file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/125/working/gxupload_0', 'object_id': 138}]}]}]
galaxy.jobs INFO 2025-07-20 01:38:47,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 125 in /galaxy/server/database/jobs_directory/000/125
galaxy.jobs DEBUG 2025-07-20 01:38:47,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 125 executed (169.129 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:47,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:38:48,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 127
tpv.core.entities DEBUG 2025-07-20 01:38:48,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:38:48,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:38:48,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:38:48,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:38:48,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:38:48,591 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Working directory for job is: /galaxy/server/database/jobs_directory/000/127
galaxy.jobs.runners DEBUG 2025-07-20 01:38:48,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [127] queued (41.828 ms)
galaxy.jobs.handler INFO 2025-07-20 01:38:48,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:48,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 127
galaxy.jobs DEBUG 2025-07-20 01:38:48,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [127] prepared (63.388 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:48,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:38:48,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:48,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:38:48,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/127/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/127/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/b/f/5/dataset_bf5f8d58-e0d8-422a-946a-2ddf65fbdc01.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/a/4/3/dataset_a4359ef0-602d-4cfe-8220-85549cb2c498.dat' 'b_0.cram' && ln -s -f '/galaxy/server/database/objects/_metadata_files/a/a/3/metadata_aa3eacd1-5b74-4ba4-be4b-251be7ff43fd.dat' 'b_0.cram.crai' &&   samtools view -H b_0.cram| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.cram' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/d/8/a/dataset_d8a89e04-beba-47d2-be08-05ef63208f90.dat']
galaxy.jobs.runners DEBUG 2025-07-20 01:38:48,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (127) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/127/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/127/galaxy_127.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:48,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:48,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:38:48,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-20 01:38:48,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:48,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:49,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:38:53,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-x5zh7 with k8s id: gxy-x5zh7 succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:38:53,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 127: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:39:01,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 127 finished
galaxy.model.metadata DEBUG 2025-07-20 01:39:01,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 140
galaxy.jobs INFO 2025-07-20 01:39:01,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 127 in /galaxy/server/database/jobs_directory/000/127
galaxy.jobs DEBUG 2025-07-20 01:39:01,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 127 executed (134.903 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:01,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:39:04,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 130, 129, 128
tpv.core.entities DEBUG 2025-07-20 01:39:04,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:39:04,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:39:04,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:39:04,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:39:04,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:39:04,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Working directory for job is: /galaxy/server/database/jobs_directory/000/128
galaxy.jobs.runners DEBUG 2025-07-20 01:39:04,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [128] queued (40.789 ms)
galaxy.jobs.handler INFO 2025-07-20 01:39:04,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:04,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 128
tpv.core.entities DEBUG 2025-07-20 01:39:05,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:39:05,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:39:05,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:39:05,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:39:05,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:39:05,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Working directory for job is: /galaxy/server/database/jobs_directory/000/129
galaxy.jobs.runners DEBUG 2025-07-20 01:39:05,065 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [129] queued (51.219 ms)
galaxy.jobs.handler INFO 2025-07-20 01:39:05,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 129
tpv.core.entities DEBUG 2025-07-20 01:39:05,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:39:05,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:39:05,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:39:05,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:39:05,113 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:39:05,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [128] prepared (102.597 ms)
galaxy.jobs DEBUG 2025-07-20 01:39:05,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Working directory for job is: /galaxy/server/database/jobs_directory/000/130
galaxy.jobs.command_factory INFO 2025-07-20 01:39:05,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/128/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/128/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/128/configs/tmpkqyt8bep']
galaxy.jobs.runners DEBUG 2025-07-20 01:39:05,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [130] queued (65.615 ms)
galaxy.jobs.handler INFO 2025-07-20 01:39:05,159 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 130
galaxy.jobs.runners DEBUG 2025-07-20 01:39:05,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (128) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/128/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/128/galaxy_128.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:39:05,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [129] prepared (115.155 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:39:05,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/129/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/129/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/129/configs/tmplx23dsx0']
galaxy.jobs.runners DEBUG 2025-07-20 01:39:05,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (129) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/129/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/129/galaxy_129.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:39:05,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [130] prepared (93.141 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:39:05,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/130/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/130/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/130/configs/tmp0wjaws1h']
galaxy.jobs.runners DEBUG 2025-07-20 01:39:05,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (130) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/130/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/130/galaxy_130.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:05,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:16,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lgjnx with k8s id: gxy-lgjnx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:16,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gw9th with k8s id: gxy-gw9th succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:16,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mch6s with k8s id: gxy-mch6s succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:39:16,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 128: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:39:17,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 129: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:39:17,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 130: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:39:29,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 130 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:39:29,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/130/working/data_fetch_upload_xcz_avlo', 'object_id': 143}]}]}]
galaxy.jobs.runners DEBUG 2025-07-20 01:39:29,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 128 finished
galaxy.jobs.runners DEBUG 2025-07-20 01:39:29,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 129 finished
galaxy.jobs INFO 2025-07-20 01:39:29,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 130 in /galaxy/server/database/jobs_directory/000/130
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:39:29,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/128/working/data_fetch_upload_606zbb5u', 'object_id': 141}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:39:29,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/129/working/data_fetch_upload_j502a3v3', 'object_id': 142}]}]}]
galaxy.jobs INFO 2025-07-20 01:39:29,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 128 in /galaxy/server/database/jobs_directory/000/128
galaxy.jobs DEBUG 2025-07-20 01:39:29,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 130 executed (207.499 ms)
galaxy.jobs INFO 2025-07-20 01:39:29,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 129 in /galaxy/server/database/jobs_directory/000/129
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:29,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:39:29,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 128 executed (161.263 ms)
galaxy.jobs DEBUG 2025-07-20 01:39:29,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 129 executed (154.454 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:29,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:29,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:39:30,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 131
tpv.core.entities DEBUG 2025-07-20 01:39:30,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:39:30,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:39:30,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:39:30,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:39:30,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:39:30,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Working directory for job is: /galaxy/server/database/jobs_directory/000/131
galaxy.jobs.runners DEBUG 2025-07-20 01:39:30,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [131] queued (46.040 ms)
galaxy.jobs.handler INFO 2025-07-20 01:39:30,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:30,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 131
galaxy.jobs DEBUG 2025-07-20 01:39:31,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [131] prepared (72.812 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:39:31,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:39:31,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:39:31,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:39:31,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/131/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/2/7/7/dataset_277a05b1-877a-4808-964a-ac3b0e1f4fc5.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/0/7/4/dataset_074dd023-673f-4548-9b58-e7e15f8d84e2.dat' '/galaxy/server/database/objects/e/d/8/dataset_ed8fcd52-c37c-49dd-b862-9a782bab51e6.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-20 01:39:31,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (131) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/131/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/131/galaxy_131.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/131/working/abundance.h5" -a -f "/galaxy/server/database/objects/e/8/0/dataset_e8091e88-9c10-4549-b418-1faa7da7d8a3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/131/working/abundance.h5" "/galaxy/server/database/objects/e/8/0/dataset_e8091e88-9c10-4549-b418-1faa7da7d8a3.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/131/working/abundance.tsv" -a -f "/galaxy/server/database/objects/6/c/0/dataset_6c0068b9-6db2-41b2-8d7c-58ae897c0e4e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/131/working/abundance.tsv" "/galaxy/server/database/objects/6/c/0/dataset_6c0068b9-6db2-41b2-8d7c-58ae897c0e4e.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:31,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:39:31,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:39:31,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:39:31,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:31,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:32,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:39,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bx9j7 with k8s id: gxy-bx9j7 succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:39:39,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 131: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:39:46,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 131 finished
galaxy.model.metadata DEBUG 2025-07-20 01:39:46,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 144
galaxy.model.metadata DEBUG 2025-07-20 01:39:46,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 145
galaxy.util WARNING 2025-07-20 01:39:47,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/8/0/dataset_e8091e88-9c10-4549-b418-1faa7da7d8a3.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/8/0/dataset_e8091e88-9c10-4549-b418-1faa7da7d8a3.dat'
galaxy.util WARNING 2025-07-20 01:39:47,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/c/0/dataset_6c0068b9-6db2-41b2-8d7c-58ae897c0e4e.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/c/0/dataset_6c0068b9-6db2-41b2-8d7c-58ae897c0e4e.dat'
galaxy.jobs INFO 2025-07-20 01:39:47,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 131 in /galaxy/server/database/jobs_directory/000/131
galaxy.jobs DEBUG 2025-07-20 01:39:47,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 131 executed (122.847 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:47,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:39:48,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 132
tpv.core.entities DEBUG 2025-07-20 01:39:48,332 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:39:48,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:39:48,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:39:48,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:39:48,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:39:48,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Working directory for job is: /galaxy/server/database/jobs_directory/000/132
galaxy.jobs.runners DEBUG 2025-07-20 01:39:48,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [132] queued (30.378 ms)
galaxy.jobs.handler INFO 2025-07-20 01:39:48,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:48,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 132
galaxy.jobs DEBUG 2025-07-20 01:39:48,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [132] prepared (68.363 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:39:48,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/132/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/132/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/132/configs/tmpi5lwvu8n']
galaxy.jobs.runners DEBUG 2025-07-20 01:39:48,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (132) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/132/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/132/galaxy_132.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:48,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:48,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:49,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-20 01:39:49,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 134, 133
tpv.core.entities DEBUG 2025-07-20 01:39:49,400 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:39:49,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:39:49,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:39:49,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:39:49,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:39:49,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Working directory for job is: /galaxy/server/database/jobs_directory/000/133
galaxy.jobs.runners DEBUG 2025-07-20 01:39:49,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [133] queued (35.000 ms)
galaxy.jobs.handler INFO 2025-07-20 01:39:49,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:49,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 133
tpv.core.entities DEBUG 2025-07-20 01:39:49,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:39:49,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:39:49,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:39:49,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:39:49,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:39:49,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Working directory for job is: /galaxy/server/database/jobs_directory/000/134
galaxy.jobs.runners DEBUG 2025-07-20 01:39:49,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [134] queued (53.966 ms)
galaxy.jobs.handler INFO 2025-07-20 01:39:49,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:49,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 134
galaxy.jobs DEBUG 2025-07-20 01:39:49,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [133] prepared (93.646 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:39:49,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/133/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/133/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/133/configs/tmpc1jct1qm']
galaxy.jobs.runners DEBUG 2025-07-20 01:39:49,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (133) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/133/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/133/galaxy_133.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:49,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:39:49,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [134] prepared (85.323 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:49,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:39:49,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/134/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/134/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/134/configs/tmpz1g5jq02']
galaxy.jobs.runners DEBUG 2025-07-20 01:39:49,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (134) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/134/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/134/galaxy_134.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:49,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:49,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:50,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:50,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:58,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-92w69 with k8s id: gxy-92w69 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:39:58,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bw26h with k8s id: gxy-bw26h succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:39:59,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 132: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:39:59,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 134: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:00,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qg4z2 with k8s id: gxy-qg4z2 succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:40:00,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 133: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:40:10,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 134 finished
galaxy.jobs.runners DEBUG 2025-07-20 01:40:10,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 132 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:40:10,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/134/working/data_fetch_upload_38bjdivn', 'object_id': 148}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:40:10,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/132/working/data_fetch_upload_jkmv9s5m', 'object_id': 146}]}]}]
galaxy.jobs INFO 2025-07-20 01:40:11,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 134 in /galaxy/server/database/jobs_directory/000/134
galaxy.jobs INFO 2025-07-20 01:40:11,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 132 in /galaxy/server/database/jobs_directory/000/132
galaxy.jobs DEBUG 2025-07-20 01:40:11,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 134 executed (163.712 ms)
galaxy.jobs DEBUG 2025-07-20 01:40:11,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 132 executed (162.351 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:11,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:11,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-20 01:40:11,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 133 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:40:11,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/133/working/data_fetch_upload_46rnkyk8', 'object_id': 147}]}]}]
galaxy.jobs INFO 2025-07-20 01:40:11,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 133 in /galaxy/server/database/jobs_directory/000/133
galaxy.jobs DEBUG 2025-07-20 01:40:11,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 133 executed (103.847 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:11,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:40:12,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 135
tpv.core.entities DEBUG 2025-07-20 01:40:12,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:40:12,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:40:12,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:40:12,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:40:12,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:40:12,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Working directory for job is: /galaxy/server/database/jobs_directory/000/135
galaxy.jobs.runners DEBUG 2025-07-20 01:40:12,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [135] queued (38.983 ms)
galaxy.jobs.handler INFO 2025-07-20 01:40:12,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:12,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 135
galaxy.jobs DEBUG 2025-07-20 01:40:12,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [135] prepared (50.570 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:12,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:40:12,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:12,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:40:12,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/135/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/5/a/0/dataset_5a0a52d7-79a7-4a10-8ff0-84022be5abb8.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o . --fr-stranded '/galaxy/server/database/objects/8/5/d/dataset_85d0aced-f8b8-4f7b-8384-83c2b3736691.dat' '/galaxy/server/database/objects/4/9/6/dataset_4968b5d5-8c03-4444-afdb-2693c79f64a2.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-20 01:40:12,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (135) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/135/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/135/galaxy_135.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/135/working/abundance.h5" -a -f "/galaxy/server/database/objects/2/0/f/dataset_20faeb49-f365-4780-9f4d-99f3aa3aa245.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/135/working/abundance.h5" "/galaxy/server/database/objects/2/0/f/dataset_20faeb49-f365-4780-9f4d-99f3aa3aa245.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/135/working/abundance.tsv" -a -f "/galaxy/server/database/objects/f/f/6/dataset_ff67a253-583f-4eb2-8de4-6de332eaa271.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/135/working/abundance.tsv" "/galaxy/server/database/objects/f/f/6/dataset_ff67a253-583f-4eb2-8de4-6de332eaa271.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:12,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:12,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:40:12,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:13,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:13,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:14,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j2mdk failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-j2mdk.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-20 01:40:17,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-20-00-51-1/jobs/gxy-j2mdk

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-j2mdk": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (135/gxy-j2mdk) tool_stdout: {
	"n_targets": 1,
	"n_bootstraps": 0,
	"n_processed": 100,
	"n_pseudoaligned": 37,
	"n_unique": 37,
	"p_pseudoaligned": 37.0,
	"p_unique": 37.0,
	"kallisto_version": "0.48.0",
	"index_version": 10,
	"start_time": "Sun Jul 20 01:40:14 2025",
	"call": "kallisto quant -i reference.kallisto --bootstrap-samples 0 --seed 42 --threads 8 -o . --fr-stranded /galaxy/server/database/objects/8/5/d/dataset_85d0aced-f8b8-4f7b-8384-83c2b3736691.dat /galaxy/server/database/objects/4/9/6/dataset_4968b5d5-8c03-4444-afdb-2693c79f64a2.dat"
}

galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (135/gxy-j2mdk) tool_stderr: 
[build] loading fasta file reference.fa
[build] k-mer length: 31
[build] counting k-mers ... done.
[build] building target de Bruijn graph ...  done 
[build] creating equivalence classes ...  done
[build] target de Bruijn graph has 1 contigs and contains 16269 k-mers 


[quant] fragment length distribution will be estimated from the data
[index] k-mer length: 31
[index] number of targets: 1
[index] number of k-mers: 16,269
[index] number of equivalence classes: 1
[quant] running in paired-end mode
[quant] will process pair 1: /galaxy/server/database/objects/8/5/d/dataset_85d0aced-f8b8-4f7b-8384-83c2b3736691.dat
                             /galaxy/server/database/objects/4/9/6/dataset_4968b5d5-8c03-4444-afdb-2693c79f64a2.dat
[quant] finding pseudoalignments for the reads ... done
[quant] processed 100 reads, 37 reads pseudoaligned
[quant] estimated average fragment length: 495.838
[   em] quantifying the abundances ... done
[   em] the Expectation-Maximization algorithm ran for 52 rounds


galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (135/gxy-j2mdk) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (135/gxy-j2mdk) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-j2mdk.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 135 (gxy-j2mdk)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-j2mdk to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:17,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (135/gxy-j2mdk) Terminated at user's request
galaxy.util WARNING 2025-07-20 01:40:17,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/0/f/dataset_20faeb49-f365-4780-9f4d-99f3aa3aa245.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/0/f/dataset_20faeb49-f365-4780-9f4d-99f3aa3aa245.dat'
galaxy.util WARNING 2025-07-20 01:40:17,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/f/6/dataset_ff67a253-583f-4eb2-8de4-6de332eaa271.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/f/6/dataset_ff67a253-583f-4eb2-8de4-6de332eaa271.dat'
galaxy.jobs.handler DEBUG 2025-07-20 01:40:18,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 136, 138, 137
tpv.core.entities DEBUG 2025-07-20 01:40:18,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:40:18,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:40:18,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:40:18,978 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:40:18,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:40:19,012 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Working directory for job is: /galaxy/server/database/jobs_directory/000/136
galaxy.jobs.runners DEBUG 2025-07-20 01:40:19,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [136] queued (43.035 ms)
galaxy.jobs.handler INFO 2025-07-20 01:40:19,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 136
tpv.core.entities DEBUG 2025-07-20 01:40:19,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:40:19,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:40:19,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:40:19,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:40:19,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:40:19,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Working directory for job is: /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-07-20 01:40:19,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [137] queued (74.604 ms)
galaxy.jobs.handler INFO 2025-07-20 01:40:19,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 137
tpv.core.entities DEBUG 2025-07-20 01:40:19,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:40:19,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:40:19,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:40:19,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:40:19,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [136] prepared (156.679 ms)
galaxy.jobs DEBUG 2025-07-20 01:40:19,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:40:19,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Working directory for job is: /galaxy/server/database/jobs_directory/000/138
galaxy.jobs.command_factory INFO 2025-07-20 01:40:19,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/136/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/136/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/136/configs/tmpfbiodo3o']
galaxy.jobs.runners DEBUG 2025-07-20 01:40:19,234 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [138] queued (56.337 ms)
galaxy.jobs.handler INFO 2025-07-20 01:40:19,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 138
galaxy.jobs.runners DEBUG 2025-07-20 01:40:19,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (136) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/136/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/136/galaxy_136.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:40:19,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [137] prepared (109.947 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:40:19,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/137/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/137/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/137/configs/tmp7qjghew7']
galaxy.jobs.runners DEBUG 2025-07-20 01:40:19,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (137) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/137/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/137/galaxy_137.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-20 01:40:19,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [138] prepared (89.387 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:40:19,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/138/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/138/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/138/configs/tmpdmxc6qpd']
galaxy.jobs.runners DEBUG 2025-07-20 01:40:19,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (138) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/138/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/138/galaxy_138.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:19,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:20,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:20,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:20,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:29,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2gpnm with k8s id: gxy-2gpnm succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:29,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6tjn5 with k8s id: gxy-6tjn5 succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:40:29,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 136: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:40:29,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 137: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:30,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2tpbt with k8s id: gxy-2tpbt succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:40:30,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 138: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:40:40,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 137 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:40:40,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/137/working/data_fetch_upload_lv18jc5r', 'object_id': 152}]}]}]
galaxy.jobs INFO 2025-07-20 01:40:40,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 137 in /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-07-20 01:40:40,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 136 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:40:40,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/136/working/data_fetch_upload_1c3ggh3s', 'object_id': 151}]}]}]
galaxy.jobs DEBUG 2025-07-20 01:40:40,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 137 executed (175.623 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:40,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-20 01:40:40,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 136 in /galaxy/server/database/jobs_directory/000/136
galaxy.jobs DEBUG 2025-07-20 01:40:40,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 136 executed (134.333 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:40,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-20 01:40:41,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 138 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:40:41,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/138/working/data_fetch_upload_f_b9yb4e', 'object_id': 153}]}]}]
galaxy.jobs INFO 2025-07-20 01:40:41,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 138 in /galaxy/server/database/jobs_directory/000/138
galaxy.jobs DEBUG 2025-07-20 01:40:41,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 138 executed (119.652 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:41,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:40:42,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 139
tpv.core.entities DEBUG 2025-07-20 01:40:42,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:40:42,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:40:42,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:40:42,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:40:42,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:40:42,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Working directory for job is: /galaxy/server/database/jobs_directory/000/139
galaxy.jobs.runners DEBUG 2025-07-20 01:40:42,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [139] queued (38.871 ms)
galaxy.jobs.handler INFO 2025-07-20 01:40:42,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:42,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 139
galaxy.jobs DEBUG 2025-07-20 01:40:42,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [139] prepared (79.751 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:42,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:40:42,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:42,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:40:42,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/139/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/0/4/7/dataset_047e7ce4-ae7d-49b1-96b9-a3b7556ecf10.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/4/3/c/dataset_43cda597-57ed-43d6-81a5-f3e73bda757f.dat' '/galaxy/server/database/objects/f/2/0/dataset_f205d44c-1f28-4ec7-a59c-8ca28e576f55.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-20 01:40:43,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (139) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/139/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/139/galaxy_139.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/139/working/abundance.h5" -a -f "/galaxy/server/database/objects/4/d/2/dataset_4d2603f6-12d1-4601-9513-565272619472.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/139/working/abundance.h5" "/galaxy/server/database/objects/4/d/2/dataset_4d2603f6-12d1-4601-9513-565272619472.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/139/working/abundance.tsv" -a -f "/galaxy/server/database/objects/2/6/0/dataset_2606e5cf-c747-47a3-ac20-554c0dc87b92.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/139/working/abundance.tsv" "/galaxy/server/database/objects/2/6/0/dataset_2606e5cf-c747-47a3-ac20-554c0dc87b92.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:43,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:43,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:40:43,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:40:43,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:43,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:43,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:47,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sg27n with k8s id: gxy-sg27n succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:40:47,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 139: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:40:55,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 139 finished
galaxy.model.metadata DEBUG 2025-07-20 01:40:55,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 154
galaxy.model.metadata DEBUG 2025-07-20 01:40:55,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 155
galaxy.util WARNING 2025-07-20 01:40:55,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/d/2/dataset_4d2603f6-12d1-4601-9513-565272619472.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/d/2/dataset_4d2603f6-12d1-4601-9513-565272619472.dat'
galaxy.util WARNING 2025-07-20 01:40:55,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/6/0/dataset_2606e5cf-c747-47a3-ac20-554c0dc87b92.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/6/0/dataset_2606e5cf-c747-47a3-ac20-554c0dc87b92.dat'
galaxy.jobs INFO 2025-07-20 01:40:55,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 139 in /galaxy/server/database/jobs_directory/000/139
galaxy.jobs DEBUG 2025-07-20 01:40:55,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 139 executed (134.969 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:55,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:40:57,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 141, 140
tpv.core.entities DEBUG 2025-07-20 01:40:57,163 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:40:57,163 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:40:57,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:40:57,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:40:57,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:40:57,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Working directory for job is: /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-07-20 01:40:57,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [140] queued (33.140 ms)
galaxy.jobs.handler INFO 2025-07-20 01:40:57,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:57,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 140
tpv.core.entities DEBUG 2025-07-20 01:40:57,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:40:57,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:40:57,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:40:57,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:40:57,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:40:57,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Working directory for job is: /galaxy/server/database/jobs_directory/000/141
galaxy.jobs.runners DEBUG 2025-07-20 01:40:57,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [141] queued (45.382 ms)
galaxy.jobs.handler INFO 2025-07-20 01:40:57,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:57,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 141
galaxy.jobs DEBUG 2025-07-20 01:40:57,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [140] prepared (76.806 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:40:57,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/140/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/140/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/140/configs/tmpo3sn14y6']
galaxy.jobs.runners DEBUG 2025-07-20 01:40:57,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (140) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/140/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/140/galaxy_140.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:57,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:40:57,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [141] prepared (81.785 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:57,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:40:57,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/141/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/141/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/141/configs/tmpq5lwa68a']
galaxy.jobs.runners DEBUG 2025-07-20 01:40:57,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (141) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/141/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/141/galaxy_141.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:57,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:57,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:58,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:40:58,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:07,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jhhf7 with k8s id: gxy-jhhf7 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:07,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rp5xb with k8s id: gxy-rp5xb succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:41:08,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 140: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:41:08,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 141: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:41:16,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 140 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:41:16,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/140/working/data_fetch_upload_dl6t0tvd', 'object_id': 156}]}]}]
galaxy.jobs INFO 2025-07-20 01:41:16,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 140 in /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-07-20 01:41:16,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 141 finished
galaxy.jobs DEBUG 2025-07-20 01:41:16,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 140 executed (158.687 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:16,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:41:16,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/141/working/data_fetch_upload_fvj2wxgi', 'object_id': 157}]}]}]
galaxy.jobs INFO 2025-07-20 01:41:16,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 141 in /galaxy/server/database/jobs_directory/000/141
galaxy.jobs DEBUG 2025-07-20 01:41:16,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 141 executed (144.869 ms)
galaxy.jobs.handler DEBUG 2025-07-20 01:41:16,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 142
tpv.core.entities DEBUG 2025-07-20 01:41:16,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:41:16,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:41:16,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:41:16,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:41:16,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:41:16,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Working directory for job is: /galaxy/server/database/jobs_directory/000/142
galaxy.jobs.runners DEBUG 2025-07-20 01:41:16,682 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [142] queued (34.574 ms)
galaxy.jobs.handler INFO 2025-07-20 01:41:16,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:16,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 142
galaxy.jobs DEBUG 2025-07-20 01:41:16,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [142] prepared (53.450 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:16,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:41:16,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:16,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:16,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:41:16,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/142/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/5/4/2/dataset_5423edaa-1172-48c6-bb43-b00a8134ba9c.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o . --single --fragment-length 200 --sd 20 '/galaxy/server/database/objects/1/0/b/dataset_10b019f3-70b2-48ab-960e-9364c99a3291.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-20 01:41:16,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (142) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/142/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/142/galaxy_142.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/142/working/abundance.h5" -a -f "/galaxy/server/database/objects/8/1/b/dataset_81bb48fe-124f-4cf5-be27-92e711a37027.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/142/working/abundance.h5" "/galaxy/server/database/objects/8/1/b/dataset_81bb48fe-124f-4cf5-be27-92e711a37027.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/142/working/abundance.tsv" -a -f "/galaxy/server/database/objects/4/8/e/dataset_48e15fbb-dc9d-413e-89e3-dae5f6f3afa2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/142/working/abundance.tsv" "/galaxy/server/database/objects/4/8/e/dataset_48e15fbb-dc9d-413e-89e3-dae5f6f3afa2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:16,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:16,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:41:16,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:16,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:16,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:18,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:21,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvr45 with k8s id: gxy-dvr45 succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:41:21,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 142: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:41:28,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 142 finished
galaxy.model.metadata DEBUG 2025-07-20 01:41:28,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 158
galaxy.model.metadata DEBUG 2025-07-20 01:41:28,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 159
galaxy.util WARNING 2025-07-20 01:41:28,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/1/b/dataset_81bb48fe-124f-4cf5-be27-92e711a37027.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/1/b/dataset_81bb48fe-124f-4cf5-be27-92e711a37027.dat'
galaxy.util WARNING 2025-07-20 01:41:28,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/8/e/dataset_48e15fbb-dc9d-413e-89e3-dae5f6f3afa2.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/8/e/dataset_48e15fbb-dc9d-413e-89e3-dae5f6f3afa2.dat'
galaxy.jobs INFO 2025-07-20 01:41:28,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 142 in /galaxy/server/database/jobs_directory/000/142
galaxy.jobs DEBUG 2025-07-20 01:41:28,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 142 executed (122.788 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:28,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:41:30,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 145, 144, 143
tpv.core.entities DEBUG 2025-07-20 01:41:30,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:41:30,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:41:30,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:41:30,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:41:30,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:41:31,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Working directory for job is: /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-07-20 01:41:31,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [143] queued (37.234 ms)
galaxy.jobs.handler INFO 2025-07-20 01:41:31,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 143
tpv.core.entities DEBUG 2025-07-20 01:41:31,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:41:31,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:41:31,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:41:31,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:41:31,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:41:31,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Working directory for job is: /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-07-20 01:41:31,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [144] queued (44.374 ms)
galaxy.jobs.handler INFO 2025-07-20 01:41:31,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 144
tpv.core.entities DEBUG 2025-07-20 01:41:31,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:41:31,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:41:31,094 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:41:31,100 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:41:31,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [143] prepared (90.077 ms)
galaxy.jobs DEBUG 2025-07-20 01:41:31,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:41:31,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Working directory for job is: /galaxy/server/database/jobs_directory/000/145
galaxy.jobs.command_factory INFO 2025-07-20 01:41:31,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/143/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/143/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/143/configs/tmp98b_z20x']
galaxy.jobs.runners DEBUG 2025-07-20 01:41:31,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [145] queued (56.937 ms)
galaxy.jobs.handler INFO 2025-07-20 01:41:31,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Job dispatched
galaxy.jobs.runners DEBUG 2025-07-20 01:41:31,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (143) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/143/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/143/galaxy_143.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 145
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:41:31,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [144] prepared (102.365 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:41:31,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/144/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/144/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/144/configs/tmphrmlpzya']
galaxy.jobs.runners DEBUG 2025-07-20 01:41:31,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (144) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/144/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/144/galaxy_144.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:41:31,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [145] prepared (81.376 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:41:31,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/145/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/145/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/145/configs/tmplowgy9on']
galaxy.jobs.runners DEBUG 2025-07-20 01:41:31,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (145) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/145/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/145/galaxy_145.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:31,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:32,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:32,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:33,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:41,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cg8sv with k8s id: gxy-cg8sv succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:41:41,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 143: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:42,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b7nml with k8s id: gxy-b7nml succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:42,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7j26c with k8s id: gxy-7j26c succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:41:42,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 144: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:41:42,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 145: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:41:53,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 143 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:41:53,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/143/working/data_fetch_upload_8dkqrgwo', 'object_id': 160}]}]}]
galaxy.jobs INFO 2025-07-20 01:41:53,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 143 in /galaxy/server/database/jobs_directory/000/143
galaxy.jobs DEBUG 2025-07-20 01:41:53,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 143 executed (162.615 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:53,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-20 01:41:53,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 144 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:41:53,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM_F.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/144/working/data_fetch_upload_eagmb7ea', 'object_id': 161}]}]}]
galaxy.jobs INFO 2025-07-20 01:41:54,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 144 in /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-07-20 01:41:54,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 145 finished
galaxy.jobs DEBUG 2025-07-20 01:41:54,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 144 executed (129.441 ms)
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:41:54,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM_R.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/145/working/data_fetch_upload_v_gdhxdn', 'object_id': 162}]}]}]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:54,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-20 01:41:54,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 145 in /galaxy/server/database/jobs_directory/000/145
galaxy.jobs DEBUG 2025-07-20 01:41:54,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 145 executed (117.274 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:54,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:41:54,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 146
tpv.core.entities DEBUG 2025-07-20 01:41:54,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:41:54,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:41:54,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:41:54,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:41:54,968 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:41:54,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Working directory for job is: /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners DEBUG 2025-07-20 01:41:54,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [146] queued (40.968 ms)
galaxy.jobs.handler INFO 2025-07-20 01:41:54,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:55,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 146
galaxy.jobs DEBUG 2025-07-20 01:41:55,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [146] prepared (56.061 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:55,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:41:55,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:55,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:41:55,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/146/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/8/7/d/dataset_87d5cae0-c638-4b9e-bb2e-437f6f04061a.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42   --pseudobam -o .  '/galaxy/server/database/objects/f/e/0/dataset_fe0c7b76-ef3a-453a-95a5-a668d5cd044d.dat' '/galaxy/server/database/objects/c/7/e/dataset_c7e7fdb1-7d4f-4909-a763-7b83ece1c429.dat'  && samtools sort --no-PG -O bam -@ ${GALAXY_SLOTS:-1} -T "${TMPDIR:-.}" -o '/galaxy/server/database/objects/6/f/a/dataset_6fa3651e-0183-4db8-a3c6-23e7b7e4f288.dat' pseudoalignments.bam && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-20 01:41:55,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (146) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/146/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/146/galaxy_146.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/146/working/abundance.h5" -a -f "/galaxy/server/database/objects/8/c/6/dataset_8c616a36-0cbf-463c-b2b4-0f4de06058fa.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/146/working/abundance.h5" "/galaxy/server/database/objects/8/c/6/dataset_8c616a36-0cbf-463c-b2b4-0f4de06058fa.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/146/working/abundance.tsv" -a -f "/galaxy/server/database/objects/a/7/e/dataset_a7eeacd3-bfed-4f97-9aa3-eefdfa4f4829.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/146/working/abundance.tsv" "/galaxy/server/database/objects/a/7/e/dataset_a7eeacd3-bfed-4f97-9aa3-eefdfa4f4829.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/146/working/abundance.txt" -a -f "/galaxy/server/database/objects/6/f/a/dataset_6fa3651e-0183-4db8-a3c6-23e7b7e4f288.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/146/working/abundance.txt" "/galaxy/server/database/objects/6/f/a/dataset_6fa3651e-0183-4db8-a3c6-23e7b7e4f288.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:55,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:55,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:41:55,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:41:55,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:55,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:55,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:41:59,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gnqbp with k8s id: gxy-gnqbp succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:41:59,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 146: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:42:07,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 146 finished
galaxy.model.metadata DEBUG 2025-07-20 01:42:07,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 163
galaxy.model.metadata DEBUG 2025-07-20 01:42:07,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 164
galaxy.model.metadata DEBUG 2025-07-20 01:42:07,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 165
galaxy.util WARNING 2025-07-20 01:42:07,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/c/6/dataset_8c616a36-0cbf-463c-b2b4-0f4de06058fa.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/c/6/dataset_8c616a36-0cbf-463c-b2b4-0f4de06058fa.dat'
galaxy.util WARNING 2025-07-20 01:42:07,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/7/e/dataset_a7eeacd3-bfed-4f97-9aa3-eefdfa4f4829.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/7/e/dataset_a7eeacd3-bfed-4f97-9aa3-eefdfa4f4829.dat'
galaxy.jobs INFO 2025-07-20 01:42:07,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 146 in /galaxy/server/database/jobs_directory/000/146
galaxy.jobs DEBUG 2025-07-20 01:42:07,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 146 executed (159.081 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:07,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:42:09,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 147
tpv.core.entities DEBUG 2025-07-20 01:42:09,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:42:09,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:42:09,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:42:09,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:42:09,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:42:09,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Working directory for job is: /galaxy/server/database/jobs_directory/000/147
galaxy.jobs.runners DEBUG 2025-07-20 01:42:09,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [147] queued (35.000 ms)
galaxy.jobs.handler INFO 2025-07-20 01:42:09,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:09,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 147
galaxy.jobs DEBUG 2025-07-20 01:42:09,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [147] prepared (67.045 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:42:09,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/147/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/147/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/147/configs/tmp1848lfyp']
galaxy.jobs.runners DEBUG 2025-07-20 01:42:09,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (147) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/147/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/147/galaxy_147.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:09,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:09,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:09,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-20 01:42:10,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 148
tpv.core.entities DEBUG 2025-07-20 01:42:10,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:42:10,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:42:10,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:42:10,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:42:10,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:42:10,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Working directory for job is: /galaxy/server/database/jobs_directory/000/148
galaxy.jobs.runners DEBUG 2025-07-20 01:42:10,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [148] queued (29.334 ms)
galaxy.jobs.handler INFO 2025-07-20 01:42:10,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:10,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 148
galaxy.jobs DEBUG 2025-07-20 01:42:10,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [148] prepared (55.183 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:42:10,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/148/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/148/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/148/configs/tmpm9pxmgbq']
galaxy.jobs.runners DEBUG 2025-07-20 01:42:10,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/148/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/148/galaxy_148.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:10,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:10,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:10,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:19,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vf44c with k8s id: gxy-vf44c succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:42:19,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 147: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-db8mf failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-db8mf.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-20 01:42:20,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-20-00-51-1/jobs/gxy-db8mf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-db8mf": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148/gxy-db8mf) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148/gxy-db8mf) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148/gxy-db8mf) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148/gxy-db8mf) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-db8mf.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 148 (gxy-db8mf)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-db8mf to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:20,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148/gxy-db8mf) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-07-20 01:42:26,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 147 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:42:26,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sacCer2_chrM_F.fq.gz', 'dbkey': 'sacCer2', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/147/working/data_fetch_upload_8in9b6v4', 'object_id': 166}]}]}]
galaxy.jobs INFO 2025-07-20 01:42:26,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 147 in /galaxy/server/database/jobs_directory/000/147
galaxy.jobs DEBUG 2025-07-20 01:42:26,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 147 executed (122.308 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:26,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:42:27,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 149
tpv.core.entities DEBUG 2025-07-20 01:42:27,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:42:27,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:42:27,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:42:27,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:42:27,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:42:27,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Working directory for job is: /galaxy/server/database/jobs_directory/000/149
galaxy.jobs.runners DEBUG 2025-07-20 01:42:27,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [149] queued (30.737 ms)
galaxy.jobs.handler INFO 2025-07-20 01:42:27,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:27,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 149
galaxy.jobs DEBUG 2025-07-20 01:42:27,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [149] prepared (57.517 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:42:27,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/149/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/149/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/149/configs/tmp1ov9d4zb']
galaxy.jobs.runners DEBUG 2025-07-20 01:42:27,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (149) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/149/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/149/galaxy_149.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:27,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:27,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:28,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-20 01:42:28,766 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 151, 150
tpv.core.entities DEBUG 2025-07-20 01:42:28,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:42:28,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:42:28,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:42:28,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:42:28,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:42:28,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Working directory for job is: /galaxy/server/database/jobs_directory/000/150
galaxy.jobs.runners DEBUG 2025-07-20 01:42:28,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [150] queued (32.842 ms)
galaxy.jobs.handler INFO 2025-07-20 01:42:28,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:28,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 150
tpv.core.entities DEBUG 2025-07-20 01:42:28,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:42:28,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:42:28,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:42:28,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:42:28,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:42:28,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Working directory for job is: /galaxy/server/database/jobs_directory/000/151
galaxy.jobs.runners DEBUG 2025-07-20 01:42:28,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [151] queued (42.963 ms)
galaxy.jobs.handler INFO 2025-07-20 01:42:28,903 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:28,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 151
galaxy.jobs DEBUG 2025-07-20 01:42:28,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [150] prepared (72.438 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:42:28,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/150/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/150/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/150/configs/tmplt0u2zqw']
galaxy.jobs.runners DEBUG 2025-07-20 01:42:28,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (150) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/150/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/150/galaxy_150.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:29,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:42:29,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [151] prepared (98.926 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:29,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:42:29,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/151/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/151/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/151/configs/tmp_vcsq17g']
galaxy.jobs.runners DEBUG 2025-07-20 01:42:29,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (151) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/151/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/151/galaxy_151.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:29,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:29,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:29,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:29,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:38,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-djpgc with k8s id: gxy-djpgc succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:42:38,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 149: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:39,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rj2sq with k8s id: gxy-rj2sq succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:39,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9wqz5 with k8s id: gxy-9wqz5 succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:42:39,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 150: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:42:39,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 151: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:42:49,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 149 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:42:50,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'hg38_transcripts.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/149/working/data_fetch_upload_vtve_h8i', 'object_id': 168}]}]}]
galaxy.jobs INFO 2025-07-20 01:42:50,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 149 in /galaxy/server/database/jobs_directory/000/149
galaxy.jobs DEBUG 2025-07-20 01:42:50,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 149 executed (179.041 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:50,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-20 01:42:50,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 151 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:42:50,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'hg38_R.fq.gz', 'dbkey': 'hg38', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/151/working/data_fetch_upload_mq20betp', 'object_id': 170}]}]}]
galaxy.jobs.runners DEBUG 2025-07-20 01:42:50,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 150 finished
galaxy.jobs INFO 2025-07-20 01:42:50,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 151 in /galaxy/server/database/jobs_directory/000/151
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:42:50,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'hg38_F.fq.gz', 'dbkey': 'hg38', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/150/working/data_fetch_upload_ya5o9ujo', 'object_id': 169}]}]}]
galaxy.jobs INFO 2025-07-20 01:42:51,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 150 in /galaxy/server/database/jobs_directory/000/150
galaxy.jobs DEBUG 2025-07-20 01:42:51,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 151 executed (163.281 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:51,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:42:51,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 150 executed (142.509 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:51,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:42:51,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 152
tpv.core.entities DEBUG 2025-07-20 01:42:51,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:42:51,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:42:51,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:42:51,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:42:51,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:42:51,580 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Working directory for job is: /galaxy/server/database/jobs_directory/000/152
galaxy.jobs.runners DEBUG 2025-07-20 01:42:51,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [152] queued (48.613 ms)
galaxy.jobs.handler INFO 2025-07-20 01:42:51,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:51,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 152
galaxy.jobs DEBUG 2025-07-20 01:42:51,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [152] prepared (63.849 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:42:51,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:42:51,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:42:51,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:42:51,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/152/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/2/5/a/dataset_25ab51bb-ddaa-4ab6-bd5e-ef6327c6e70a.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --fusion --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/8/5/c/dataset_85caaede-e5b4-4b87-8359-185fef52b9fa.dat' '/galaxy/server/database/objects/5/a/0/dataset_5a003280-a415-44c6-8d1c-766a4f34e7c9.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-20 01:42:51,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (152) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/152/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/152/galaxy_152.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/152/working/abundance.h5" -a -f "/galaxy/server/database/objects/1/3/5/dataset_135d43b9-8a3a-4a40-b090-848c715cd7eb.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/152/working/abundance.h5" "/galaxy/server/database/objects/1/3/5/dataset_135d43b9-8a3a-4a40-b090-848c715cd7eb.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/152/working/abundance.tsv" -a -f "/galaxy/server/database/objects/3/e/7/dataset_3e710505-7e96-48d8-8424-c51d5ab5859f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/152/working/abundance.tsv" "/galaxy/server/database/objects/3/e/7/dataset_3e710505-7e96-48d8-8424-c51d5ab5859f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/152/working/fusion.txt" -a -f "/galaxy/server/database/objects/c/e/d/dataset_ced3b7ae-50ff-4628-af4f-1d693a30526c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/152/working/fusion.txt" "/galaxy/server/database/objects/c/e/d/dataset_ced3b7ae-50ff-4628-af4f-1d693a30526c.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:51,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:42:51,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:42:51,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:42:51,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:51,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:52,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:42:56,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9mw9p with k8s id: gxy-9mw9p succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:42:56,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 152: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:43:04,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 152 finished
galaxy.model.metadata DEBUG 2025-07-20 01:43:04,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 171
galaxy.model.metadata DEBUG 2025-07-20 01:43:04,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 172
galaxy.model.metadata DEBUG 2025-07-20 01:43:04,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 173
galaxy.util WARNING 2025-07-20 01:43:04,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/3/5/dataset_135d43b9-8a3a-4a40-b090-848c715cd7eb.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/3/5/dataset_135d43b9-8a3a-4a40-b090-848c715cd7eb.dat'
galaxy.util WARNING 2025-07-20 01:43:04,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/e/7/dataset_3e710505-7e96-48d8-8424-c51d5ab5859f.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/e/7/dataset_3e710505-7e96-48d8-8424-c51d5ab5859f.dat'
galaxy.util WARNING 2025-07-20 01:43:04,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/e/d/dataset_ced3b7ae-50ff-4628-af4f-1d693a30526c.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/e/d/dataset_ced3b7ae-50ff-4628-af4f-1d693a30526c.dat'
galaxy.jobs INFO 2025-07-20 01:43:04,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 152 in /galaxy/server/database/jobs_directory/000/152
galaxy.jobs DEBUG 2025-07-20 01:43:04,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 152 executed (163.920 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:04,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:43:06,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 154, 155, 153
tpv.core.entities DEBUG 2025-07-20 01:43:06,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:06,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:06,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:06,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:06,903 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:06,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Working directory for job is: /galaxy/server/database/jobs_directory/000/153
galaxy.jobs.runners DEBUG 2025-07-20 01:43:06,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [153] queued (40.876 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:06,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:06,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 153
tpv.core.entities DEBUG 2025-07-20 01:43:06,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:06,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:06,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:06,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:06,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:06,998 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Working directory for job is: /galaxy/server/database/jobs_directory/000/154
galaxy.jobs.runners DEBUG 2025-07-20 01:43:07,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [154] queued (52.104 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:07,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 154
tpv.core.entities DEBUG 2025-07-20 01:43:07,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:07,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:07,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:07,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:07,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:07,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [153] prepared (103.334 ms)
galaxy.jobs DEBUG 2025-07-20 01:43:07,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Working directory for job is: /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-07-20 01:43:07,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [155] queued (58.594 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:07,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Job dispatched
galaxy.jobs.command_factory INFO 2025-07-20 01:43:07,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/153/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/153/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/153/configs/tmpryk169rq']
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 155
galaxy.jobs.runners DEBUG 2025-07-20 01:43:07,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/153/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/153/galaxy_153.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-20 01:43:07,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [154] prepared (96.652 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:43:07,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/154/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/154/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/154/configs/tmp3uq4fefw']
galaxy.jobs.runners DEBUG 2025-07-20 01:43:07,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (154) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/154/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/154/galaxy_154.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:43:07,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [155] prepared (96.480 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:43:07,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/155/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/155/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/155/configs/tmpfcqnwjua']
galaxy.jobs.runners DEBUG 2025-07-20 01:43:07,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (155) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/155/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/155/galaxy_155.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:07,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-20 01:43:08,095 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 157, 156
tpv.core.entities DEBUG 2025-07-20 01:43:08,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:08,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:08,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:08,134 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:08,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:08,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Working directory for job is: /galaxy/server/database/jobs_directory/000/156
galaxy.jobs.runners DEBUG 2025-07-20 01:43:08,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [156] queued (42.232 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:08,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 156
tpv.core.entities DEBUG 2025-07-20 01:43:08,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:08,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:08,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:08,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:08,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:08,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Working directory for job is: /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners DEBUG 2025-07-20 01:43:08,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [157] queued (46.973 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:08,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 157
galaxy.jobs DEBUG 2025-07-20 01:43:08,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [156] prepared (83.878 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:43:08,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/156/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/156/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/156/configs/tmp3zb45d4l']
galaxy.jobs.runners DEBUG 2025-07-20 01:43:08,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (156) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/156/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/156/galaxy_156.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:43:08,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [157] prepared (82.471 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:43:08,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/157/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/157/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/157/configs/tmp9b2j4op6']
galaxy.jobs.runners DEBUG 2025-07-20 01:43:08,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (157) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/157/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/157/galaxy_157.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:08,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:18,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wpd7q with k8s id: gxy-wpd7q succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:18,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gpptd with k8s id: gxy-gpptd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:18,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5gt4w with k8s id: gxy-5gt4w succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:18,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7nn5r with k8s id: gxy-7nn5r succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:43:18,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 153: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:43:18,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 154: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:43:18,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 155: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:43:18,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 157: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:19,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rvdpt with k8s id: gxy-rvdpt succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:43:34,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 153 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:43:34,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/153/working/data_fetch_upload_jfl5s2um', 'object_id': 174}]}]}]
galaxy.jobs.runners DEBUG 2025-07-20 01:43:34,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 154 finished
galaxy.jobs INFO 2025-07-20 01:43:34,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 153 in /galaxy/server/database/jobs_directory/000/153
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:43:34,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reads_forward.fastq.gz', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/154/working/data_fetch_upload_m4rv8ey3', 'object_id': 175}]}]}]
galaxy.jobs.runners DEBUG 2025-07-20 01:43:34,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 157 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:43:35,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'chromosome_size.tabular', 'dbkey': '?', 'ext': 'txt', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded txt file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/157/working/data_fetch_upload_cihfcll1', 'object_id': 178}]}]}]
galaxy.jobs INFO 2025-07-20 01:43:35,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 154 in /galaxy/server/database/jobs_directory/000/154
galaxy.jobs.runners DEBUG 2025-07-20 01:43:35,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 155 finished
galaxy.jobs DEBUG 2025-07-20 01:43:35,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 153 executed (258.528 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:35,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-20 01:43:35,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 157 in /galaxy/server/database/jobs_directory/000/157
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:43:35,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reads_reverse.fastq.gz', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/155/working/data_fetch_upload_12035rta', 'object_id': 176}]}]}]
galaxy.jobs DEBUG 2025-07-20 01:43:35,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 154 executed (269.590 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:35,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-20 01:43:35,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 155 in /galaxy/server/database/jobs_directory/000/155
galaxy.jobs DEBUG 2025-07-20 01:43:35,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 157 executed (222.919 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:35,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:43:35,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 155 executed (210.654 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:35,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-20 01:43:35,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 156: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:43:42,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 156 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:43:42,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'annotation.gtf.gz', 'dbkey': '?', 'ext': 'gtf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gtf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/156/working/data_fetch_upload_do9vyh_u', 'object_id': 177}]}]}]
galaxy.jobs INFO 2025-07-20 01:43:43,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 156 in /galaxy/server/database/jobs_directory/000/156
galaxy.jobs DEBUG 2025-07-20 01:43:43,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 156 executed (535.213 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:43,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:43:44,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 158
tpv.core.entities DEBUG 2025-07-20 01:43:44,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:44,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:44,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:44,508 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:44,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:44,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Working directory for job is: /galaxy/server/database/jobs_directory/000/158
galaxy.jobs.runners DEBUG 2025-07-20 01:43:44,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [158] queued (41.893 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:44,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:44,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 158
galaxy.jobs DEBUG 2025-07-20 01:43:44,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [158] prepared (58.185 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:43:44,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:43:44,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:43:44,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:43:44,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/158/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/a/8/e/dataset_a8e6c385-2d97-41ec-a255-eb35f98628f3.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/2/7/d/dataset_27d22063-0a7b-4214-ac49-09d00a3ce8f4.dat' '/galaxy/server/database/objects/d/f/3/dataset_df3d0f0b-57a4-4418-93c6-878d185cc3e5.dat'  --genomebam --gtf /galaxy/server/database/objects/5/d/0/dataset_5d0d5a65-441c-46cd-89f0-13b78a7fd090.dat --chromosomes /galaxy/server/database/objects/a/c/e/dataset_ace4cd52-476e-49ba-8b69-cd05d2a287fb.dat && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-20 01:43:44,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (158) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/158/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/158/galaxy_158.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/158/working/abundance.h5" -a -f "/galaxy/server/database/objects/a/a/e/dataset_aaead756-8b85-4acb-9043-319bf59d5d0c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/158/working/abundance.h5" "/galaxy/server/database/objects/a/a/e/dataset_aaead756-8b85-4acb-9043-319bf59d5d0c.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/158/working/abundance.tsv" -a -f "/galaxy/server/database/objects/1/0/5/dataset_105949ad-3616-4b08-b505-fa4c367d3c36.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/158/working/abundance.tsv" "/galaxy/server/database/objects/1/0/5/dataset_105949ad-3616-4b08-b505-fa4c367d3c36.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:44,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:43:44,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:43:44,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-20 01:43:44,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:44,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:45,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:48,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jh6zm with k8s id: gxy-jh6zm succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:43:49,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 158: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:43:56,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 158 finished
galaxy.model.metadata DEBUG 2025-07-20 01:43:56,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 179
galaxy.model.metadata DEBUG 2025-07-20 01:43:56,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 180
galaxy.util WARNING 2025-07-20 01:43:56,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/a/e/dataset_aaead756-8b85-4acb-9043-319bf59d5d0c.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/a/e/dataset_aaead756-8b85-4acb-9043-319bf59d5d0c.dat'
galaxy.util WARNING 2025-07-20 01:43:56,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/0/5/dataset_105949ad-3616-4b08-b505-fa4c367d3c36.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/0/5/dataset_105949ad-3616-4b08-b505-fa4c367d3c36.dat'
galaxy.jobs INFO 2025-07-20 01:43:56,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 158 in /galaxy/server/database/jobs_directory/000/158
galaxy.jobs DEBUG 2025-07-20 01:43:56,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 158 executed (150.577 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:56,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:43:59,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 160, 159
tpv.core.entities DEBUG 2025-07-20 01:43:59,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:59,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:59,862 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:59,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:59,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:59,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Working directory for job is: /galaxy/server/database/jobs_directory/000/159
galaxy.jobs.runners DEBUG 2025-07-20 01:43:59,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [159] queued (38.592 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:59,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:59,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 159
tpv.core.entities DEBUG 2025-07-20 01:43:59,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:43:59,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:43:59,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:43:59,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:43:59,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:43:59,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Working directory for job is: /galaxy/server/database/jobs_directory/000/160
galaxy.jobs.runners DEBUG 2025-07-20 01:43:59,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [160] queued (44.732 ms)
galaxy.jobs.handler INFO 2025-07-20 01:43:59,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:43:59,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 160
galaxy.jobs DEBUG 2025-07-20 01:44:00,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [159] prepared (80.957 ms)
galaxy.jobs.command_factory INFO 2025-07-20 01:44:00,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/159/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/159/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/159/configs/tmp53iboo7f']
galaxy.jobs.runners DEBUG 2025-07-20 01:44:00,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (159) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/159/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/159/galaxy_159.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-20 01:44:00,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [160] prepared (75.064 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:00,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:00,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-20 01:44:00,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/160/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/160/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/160/configs/tmpzm31zuey']
galaxy.jobs.runners DEBUG 2025-07-20 01:44:00,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (160) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/160/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/160/galaxy_160.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:00,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:00,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:00,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:01,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:10,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rk5r4 with k8s id: gxy-rk5r4 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:10,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9s8cs with k8s id: gxy-9s8cs succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:44:10,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 159: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:44:10,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 160: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:44:18,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 160 finished
galaxy.jobs.runners DEBUG 2025-07-20 01:44:18,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 159 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:44:18,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mouse_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/160/working/data_fetch_upload_vpvmvv2l', 'object_id': 182}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-20 01:44:18,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'human_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/159/working/data_fetch_upload_8amb487a', 'object_id': 181}]}]}]
galaxy.jobs INFO 2025-07-20 01:44:18,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 160 in /galaxy/server/database/jobs_directory/000/160
galaxy.jobs INFO 2025-07-20 01:44:18,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 159 in /galaxy/server/database/jobs_directory/000/159
galaxy.jobs DEBUG 2025-07-20 01:44:18,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 160 executed (190.943 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:18,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-20 01:44:18,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 159 executed (185.941 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:18,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-20 01:44:19,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 161
tpv.core.entities DEBUG 2025-07-20 01:44:19,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-20 01:44:19,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-20 01:44:19,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-20 01:44:19,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-20 01:44:19,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-20 01:44:19,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Working directory for job is: /galaxy/server/database/jobs_directory/000/161
galaxy.jobs.runners DEBUG 2025-07-20 01:44:19,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [161] queued (58.562 ms)
galaxy.jobs.handler INFO 2025-07-20 01:44:19,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:19,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 161
galaxy.jobs DEBUG 2025-07-20 01:44:19,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [161] prepared (65.574 ms)
galaxy.tool_util.deps.containers INFO 2025-07-20 01:44:19,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:44:19,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_dnadiff/mummer_dnadiff/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-07-20 01:44:19,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-20 01:44:19,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/161/tool_script.sh] for tool command [ln -s /galaxy/server/database/objects/4/5/5/dataset_45535ea9-f3c8-4df9-a9c1-a5e441983b37.dat reference.fa && ln -s /galaxy/server/database/objects/0/7/d/dataset_07d668db-4660-488c-8631-916daa5af1d7.dat query.fa && dnadiff 'reference.fa' 'query.fa']
galaxy.jobs.runners DEBUG 2025-07-20 01:44:19,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (161) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/161/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/161/galaxy_161.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.report" -a -f "/galaxy/server/database/objects/a/1/5/dataset_a15c861f-e58c-44de-8676-ed2c499912f0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.report" "/galaxy/server/database/objects/a/1/5/dataset_a15c861f-e58c-44de-8676-ed2c499912f0.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.delta" -a -f "/galaxy/server/database/objects/8/6/b/dataset_86bfc80c-15d5-4653-b526-c78f216f88fe.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.delta" "/galaxy/server/database/objects/8/6/b/dataset_86bfc80c-15d5-4653-b526-c78f216f88fe.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.1delta" -a -f "/galaxy/server/database/objects/8/b/9/dataset_8b9484d8-0253-4804-a72c-7ff7591aee0c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.1delta" "/galaxy/server/database/objects/8/b/9/dataset_8b9484d8-0253-4804-a72c-7ff7591aee0c.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.mdelta" -a -f "/galaxy/server/database/objects/6/6/7/dataset_6677a80d-b9ec-403f-9f98-bd687851b914.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.mdelta" "/galaxy/server/database/objects/6/6/7/dataset_6677a80d-b9ec-403f-9f98-bd687851b914.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.1coords" -a -f "/galaxy/server/database/objects/e/7/e/dataset_e7e443d9-ac4e-443e-832f-4fba14679809.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.1coords" "/galaxy/server/database/objects/e/7/e/dataset_e7e443d9-ac4e-443e-832f-4fba14679809.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.mcoords" -a -f "/galaxy/server/database/objects/7/2/e/dataset_72e3fecd-e73c-4b05-bc1b-bfbf2ba016ee.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.mcoords" "/galaxy/server/database/objects/7/2/e/dataset_72e3fecd-e73c-4b05-bc1b-bfbf2ba016ee.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.snps" -a -f "/galaxy/server/database/objects/a/8/f/dataset_a8fb11bd-f585-4467-840b-ac5ff5638bd3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.snps" "/galaxy/server/database/objects/a/8/f/dataset_a8fb11bd-f585-4467-840b-ac5ff5638bd3.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.rdiff" -a -f "/galaxy/server/database/objects/e/6/e/dataset_e6e7f30d-7767-44a2-a503-76950758cfb4.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.rdiff" "/galaxy/server/database/objects/e/6/e/dataset_e6e7f30d-7767-44a2-a503-76950758cfb4.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/161/working/out.qdiff" -a -f "/galaxy/server/database/objects/b/9/5/dataset_b95f7286-3bba-4cc0-a63f-a76c2935429f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/161/working/out.qdiff" "/galaxy/server/database/objects/b/9/5/dataset_b95f7286-3bba-4cc0-a63f-a76c2935429f.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:19,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-20 01:44:19,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-20 01:44:19,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_dnadiff/mummer_dnadiff/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-07-20 01:44:19,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:20,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:20,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:30,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6hhcb with k8s id: gxy-6hhcb succeeded
galaxy.jobs.runners DEBUG 2025-07-20 01:44:31,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 161: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-20 01:44:38,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 161 finished
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 183
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 184
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 185
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 186
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 187
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 188
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 189
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 190
galaxy.model.metadata DEBUG 2025-07-20 01:44:38,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 191
galaxy.util WARNING 2025-07-20 01:44:38,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/1/5/dataset_a15c861f-e58c-44de-8676-ed2c499912f0.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/1/5/dataset_a15c861f-e58c-44de-8676-ed2c499912f0.dat'
galaxy.util WARNING 2025-07-20 01:44:38,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/6/b/dataset_86bfc80c-15d5-4653-b526-c78f216f88fe.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/6/b/dataset_86bfc80c-15d5-4653-b526-c78f216f88fe.dat'
galaxy.util WARNING 2025-07-20 01:44:38,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/b/9/dataset_8b9484d8-0253-4804-a72c-7ff7591aee0c.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/b/9/dataset_8b9484d8-0253-4804-a72c-7ff7591aee0c.dat'
galaxy.util WARNING 2025-07-20 01:44:38,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/6/7/dataset_6677a80d-b9ec-403f-9f98-bd687851b914.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/6/7/dataset_6677a80d-b9ec-403f-9f98-bd687851b914.dat'
galaxy.util WARNING 2025-07-20 01:44:38,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/7/e/dataset_e7e443d9-ac4e-443e-832f-4fba14679809.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/7/e/dataset_e7e443d9-ac4e-443e-832f-4fba14679809.dat'
galaxy.util WARNING 2025-07-20 01:44:38,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/2/e/dataset_72e3fecd-e73c-4b05-bc1b-bfbf2ba016ee.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/2/e/dataset_72e3fecd-e73c-4b05-bc1b-bfbf2ba016ee.dat'
galaxy.util WARNING 2025-07-20 01:44:38,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/8/f/dataset_a8fb11bd-f585-4467-840b-ac5ff5638bd3.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/8/f/dataset_a8fb11bd-f585-4467-840b-ac5ff5638bd3.dat'
galaxy.util WARNING 2025-07-20 01:44:38,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/6/e/dataset_e6e7f30d-7767-44a2-a503-76950758cfb4.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/6/e/dataset_e6e7f30d-7767-44a2-a503-76950758cfb4.dat'
galaxy.util WARNING 2025-07-20 01:44:38,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/9/5/dataset_b95f7286-3bba-4cc0-a63f-a76c2935429f.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/9/5/dataset_b95f7286-3bba-4cc0-a63f-a76c2935429f.dat'
galaxy.jobs INFO 2025-07-20 01:44:38,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 161 in /galaxy/server/database/jobs_directory/000/161
galaxy.jobs DEBUG 2025-07-20 01:44:38,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 161 executed (249.735 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-20 01:44:38,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
