galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:04,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jmd7c with k8s id: gxy-jmd7c succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:01:04,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 4: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:01:12,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 4 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:01:12,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (4) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/4/working/outdir/CP011064.gbk] with element identifier [CP011064] for output [output] (4.498 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:01:12,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (4) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/4/working/outdir/CP021680.gbk] with element identifier [CP021680] for output [output] (0.826 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:01:12,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (4) Add dynamic collection datasets to history for output [output] (38.926 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:01:12,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 6
galaxy.util WARNING 2025-04-07 13:01:12,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/9/8/dataset_f9878836-dcb3-40da-94bc-faed7b4817d2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/9/8/dataset_f9878836-dcb3-40da-94bc-faed7b4817d2.dat'
galaxy.jobs INFO 2025-04-07 13:01:12,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 4 in /galaxy/server/database/jobs_directory/000/4
galaxy.jobs DEBUG 2025-04-07 13:01:12,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 4 executed (214.446 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:12,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 4 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:01:14,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 5
tpv.core.entities DEBUG 2025-04-07 13:01:14,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:01:14,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:01:14,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:01:15,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:01:15,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Working directory for job is: /galaxy/server/database/jobs_directory/000/5
galaxy.jobs.runners DEBUG 2025-04-07 13:01:15,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [5] queued (45.416 ms)
galaxy.jobs.handler INFO 2025-04-07 13:01:15,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (5) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:15,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 5
galaxy.jobs DEBUG 2025-04-07 13:01:15,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [5] prepared (74.959 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:01:15,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/5/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/5/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/5/configs/tmpjvrp9i7e']
galaxy.jobs.runners DEBUG 2025-04-07 13:01:15,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (5) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/5/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/5/galaxy_5.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:15,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:15,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:15,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:25,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r5pdn with k8s id: gxy-r5pdn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:01:25,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 5: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:01:33,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 5 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:01:33,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'accessions_1.tsv', 'dbkey': '?', 'ext': 'txt', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded txt file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/5/working/data_fetch_upload_7zkaz2z_', 'object_id': 9}]}]}]
galaxy.jobs INFO 2025-04-07 13:01:33,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 5 in /galaxy/server/database/jobs_directory/000/5
galaxy.jobs DEBUG 2025-04-07 13:01:34,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 5 executed (162.685 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:34,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 5 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:01:34,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 6
tpv.core.entities DEBUG 2025-04-07 13:01:34,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:01:34,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:01:34,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:01:34,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:01:34,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Working directory for job is: /galaxy/server/database/jobs_directory/000/6
galaxy.jobs.runners DEBUG 2025-04-07 13:01:34,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [6] queued (48.322 ms)
galaxy.jobs.handler INFO 2025-04-07 13:01:34,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (6) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:34,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 6
galaxy.jobs DEBUG 2025-04-07 13:01:34,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [6] prepared (60.284 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:01:34,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:01:34,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:01:34,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:01:34,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/6/tool_script.sh] for tool command [{ grep -v "^[ \t]*$" /galaxy/server/database/objects/6/7/1/dataset_671d7f28-b985-4d38-b5fe-e871b090f4f1.dat > accessions || { echo "No accession numbers in input. Aborting." 1>&2; exit 1; } } && mkdir outdir && cd outdir && ignore_errors=0 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'gff3' ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:01:34,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (6) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/6/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/6/galaxy_6.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/6/working/error.log" -a -f "/galaxy/server/database/objects/d/e/4/dataset_de42911b-8cbb-4b7a-ab0e-a9a5fda2bc0a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/6/working/error.log" "/galaxy/server/database/objects/d/e/4/dataset_de42911b-8cbb-4b7a-ab0e-a9a5fda2bc0a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:34,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:01:34,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:01:34,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:01:34,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:34,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:35,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:45,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sh9c4 with k8s id: gxy-sh9c4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:01:46,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 6: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:01:54,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 6 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:01:54,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (6) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/6/working/outdir/CP011064.gff] with element identifier [CP011064] for output [output] (3.503 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:01:54,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (6) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/6/working/outdir/CP021680.gff] with element identifier [CP021680] for output [output] (0.761 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:01:54,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (6) Add dynamic collection datasets to history for output [output] (34.998 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:01:54,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 10
galaxy.util WARNING 2025-04-07 13:01:54,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/e/4/dataset_de42911b-8cbb-4b7a-ab0e-a9a5fda2bc0a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/e/4/dataset_de42911b-8cbb-4b7a-ab0e-a9a5fda2bc0a.dat'
galaxy.jobs INFO 2025-04-07 13:01:54,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 6 in /galaxy/server/database/jobs_directory/000/6
galaxy.jobs DEBUG 2025-04-07 13:01:54,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 6 executed (200.465 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:54,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:01:55,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 7
tpv.core.entities DEBUG 2025-04-07 13:01:55,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:01:55,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:01:55,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:01:55,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:01:55,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Working directory for job is: /galaxy/server/database/jobs_directory/000/7
galaxy.jobs.runners DEBUG 2025-04-07 13:01:55,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [7] queued (41.511 ms)
galaxy.jobs.handler INFO 2025-04-07 13:01:55,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:55,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 7
galaxy.jobs DEBUG 2025-04-07 13:01:56,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [7] prepared (68.193 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:01:56,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/7/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/7/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/7/configs/tmpitzekcss']
galaxy.jobs.runners DEBUG 2025-04-07 13:01:56,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (7) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/7/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/7/galaxy_7.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:56,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:56,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:01:56,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:07,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mqbc9 with k8s id: gxy-mqbc9 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:02:07,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 7: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:02:15,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 7 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:02:15,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'accessions_1.tsv', 'dbkey': '?', 'ext': 'txt', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded txt file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/7/working/data_fetch_upload_1wn8asnj', 'object_id': 13}]}]}]
galaxy.jobs INFO 2025-04-07 13:02:15,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 7 in /galaxy/server/database/jobs_directory/000/7
galaxy.jobs DEBUG 2025-04-07 13:02:15,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 7 executed (166.477 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:15,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:02:16,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 8
tpv.core.entities DEBUG 2025-04-07 13:02:16,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:02:16,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:02:16,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:02:16,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:02:16,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Working directory for job is: /galaxy/server/database/jobs_directory/000/8
galaxy.jobs.runners DEBUG 2025-04-07 13:02:16,464 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [8] queued (46.353 ms)
galaxy.jobs.handler INFO 2025-04-07 13:02:16,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:16,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 8
galaxy.jobs DEBUG 2025-04-07 13:02:16,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [8] prepared (57.735 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:16,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:02:16,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:16,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:02:16,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/8/tool_script.sh] for tool command [{ grep -v "^[ \t]*$" /galaxy/server/database/objects/b/9/d/dataset_b9df17b2-05fb-4fba-a663-de9d23bebf4c.dat > accessions || { echo "No accession numbers in input. Aborting." 1>&2; exit 1; } } && mkdir outdir && cd outdir && ignore_errors=0 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'featuretable' ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:02:16,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (8) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/8/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/8/galaxy_8.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/8/working/error.log" -a -f "/galaxy/server/database/objects/9/e/9/dataset_9e953eb3-0485-470f-8580-9f922a03d967.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/8/working/error.log" "/galaxy/server/database/objects/9/e/9/dataset_9e953eb3-0485-470f-8580-9f922a03d967.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:16,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:16,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:02:16,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:16,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:16,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:17,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:26,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-99gn9 with k8s id: gxy-99gn9 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:02:26,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 8: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:02:34,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 8 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:02:34,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (8) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/8/working/outdir/CP011064.ft] with element identifier [CP011064] for output [output] (3.754 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:02:34,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (8) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/8/working/outdir/CP021680.ft] with element identifier [CP021680] for output [output] (0.660 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:02:34,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (8) Add dynamic collection datasets to history for output [output] (30.230 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:02:34,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 14
galaxy.util WARNING 2025-04-07 13:02:34,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/e/9/dataset_9e953eb3-0485-470f-8580-9f922a03d967.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/e/9/dataset_9e953eb3-0485-470f-8580-9f922a03d967.dat'
galaxy.jobs INFO 2025-04-07 13:02:34,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 8 in /galaxy/server/database/jobs_directory/000/8
galaxy.jobs DEBUG 2025-04-07 13:02:34,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 8 executed (187.101 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:34,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:02:35,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 9
tpv.core.entities DEBUG 2025-04-07 13:02:35,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:02:35,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:02:35,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:02:35,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:02:35,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Working directory for job is: /galaxy/server/database/jobs_directory/000/9
galaxy.jobs.runners DEBUG 2025-04-07 13:02:35,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [9] queued (38.632 ms)
galaxy.jobs.handler INFO 2025-04-07 13:02:35,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:35,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 9
galaxy.jobs DEBUG 2025-04-07 13:02:35,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [9] prepared (50.001 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:35,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:02:35,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:35,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:02:36,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/9/tool_script.sh] for tool command [echo 'CP011064,CP021680' | sed -r 's/(\,|__cn__)/\n/g' | grep -v "^[ \t]*$" > accessions && mkdir outdir && cd outdir && ignore_errors=0 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'fasta' --extended-validation all ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:02:36,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (9) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/9/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/9/galaxy_9.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/9/working/error.log" -a -f "/galaxy/server/database/objects/a/8/b/dataset_a8bafdda-e052-436e-99d5-4b74da61376a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/9/working/error.log" "/galaxy/server/database/objects/a/8/b/dataset_a8bafdda-e052-436e-99d5-4b74da61376a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:36,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:36,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:02:36,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:36,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:36,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:36,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:45,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rj5tc with k8s id: gxy-rj5tc succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:02:45,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 9: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:02:53,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 9 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:02:53,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (9) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/9/working/outdir/CP011064.fa] with element identifier [CP011064] for output [output] (6.877 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:02:53,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (9) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/9/working/outdir/CP021680.fa] with element identifier [CP021680] for output [output] (0.617 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:02:53,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (9) Add dynamic collection datasets to history for output [output] (28.171 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:02:54,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 17
galaxy.util WARNING 2025-04-07 13:02:54,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/8/b/dataset_a8bafdda-e052-436e-99d5-4b74da61376a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/8/b/dataset_a8bafdda-e052-436e-99d5-4b74da61376a.dat'
galaxy.jobs INFO 2025-04-07 13:02:54,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 9 in /galaxy/server/database/jobs_directory/000/9
galaxy.jobs DEBUG 2025-04-07 13:02:54,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 9 executed (155.849 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:54,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:02:55,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 10
tpv.core.entities DEBUG 2025-04-07 13:02:55,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:02:55,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:02:55,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:02:55,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:02:55,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Working directory for job is: /galaxy/server/database/jobs_directory/000/10
galaxy.jobs.runners DEBUG 2025-04-07 13:02:55,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [10] queued (44.563 ms)
galaxy.jobs.handler INFO 2025-04-07 13:02:55,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:55,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 10
galaxy.jobs DEBUG 2025-04-07 13:02:55,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [10] prepared (47.861 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:55,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:02:55,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:55,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:02:55,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/10/tool_script.sh] for tool command [echo 'CP011064,CP0XXXXX,CP021680' | sed -r 's/(\,|__cn__)/\n/g' | grep -v "^[ \t]*$" > accessions && mkdir outdir && cd outdir && ignore_errors=1 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'fasta' --extended-validation all ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:02:55,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (10) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/10/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/10/galaxy_10.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/10/working/failed.txt" -a -f "/galaxy/server/database/objects/2/5/2/dataset_25214430-5421-4a67-b438-b2b08c9f2275.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/10/working/failed.txt" "/galaxy/server/database/objects/2/5/2/dataset_25214430-5421-4a67-b438-b2b08c9f2275.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/10/working/error.log" -a -f "/galaxy/server/database/objects/2/7/d/dataset_27d0cc04-11f6-4854-8bb7-36f493b26ccc.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/10/working/error.log" "/galaxy/server/database/objects/2/7/d/dataset_27d0cc04-11f6-4854-8bb7-36f493b26ccc.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:55,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:55,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:02:55,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:02:55,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:55,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:02:55,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:08,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-85l9h with k8s id: gxy-85l9h succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:03:08,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 10: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:03:15,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 10 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:03:16,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (10) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/10/working/outdir/CP011064.fa] with element identifier [CP011064] for output [output] (4.107 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:03:16,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (10) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/10/working/outdir/CP021680.fa] with element identifier [CP021680] for output [output] (0.602 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:03:16,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (10) Add dynamic collection datasets to history for output [output] (35.512 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:03:16,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 21
galaxy.model.metadata DEBUG 2025-04-07 13:03:16,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 20
galaxy.util WARNING 2025-04-07 13:03:16,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/5/2/dataset_25214430-5421-4a67-b438-b2b08c9f2275.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/5/2/dataset_25214430-5421-4a67-b438-b2b08c9f2275.dat'
galaxy.util WARNING 2025-04-07 13:03:16,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/7/d/dataset_27d0cc04-11f6-4854-8bb7-36f493b26ccc.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/7/d/dataset_27d0cc04-11f6-4854-8bb7-36f493b26ccc.dat'
galaxy.jobs INFO 2025-04-07 13:03:16,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 10 in /galaxy/server/database/jobs_directory/000/10
galaxy.jobs DEBUG 2025-04-07 13:03:16,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 10 executed (205.830 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:17,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:03:17,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 11
tpv.core.entities DEBUG 2025-04-07 13:03:17,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:03:17,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:03:17,784 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:03:17,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:03:17,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Working directory for job is: /galaxy/server/database/jobs_directory/000/11
galaxy.jobs.runners DEBUG 2025-04-07 13:03:17,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [11] queued (53.147 ms)
galaxy.jobs.handler INFO 2025-04-07 13:03:17,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:17,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 11
galaxy.jobs DEBUG 2025-04-07 13:03:17,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [11] prepared (56.354 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:17,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:03:17,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:17,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:03:17,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/11/tool_script.sh] for tool command [echo 'CP0XXXXX' | sed -r 's/(\,|__cn__)/\n/g' | grep -v "^[ \t]*$" > accessions && mkdir outdir && cd outdir && ignore_errors=1 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'fasta' --extended-validation all ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:03:17,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (11) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/11/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/11/galaxy_11.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/11/working/failed.txt" -a -f "/galaxy/server/database/objects/4/1/0/dataset_4107d9b0-8ef7-4fde-acc1-c7651f1f1f89.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/11/working/failed.txt" "/galaxy/server/database/objects/4/1/0/dataset_4107d9b0-8ef7-4fde-acc1-c7651f1f1f89.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/11/working/error.log" -a -f "/galaxy/server/database/objects/e/9/f/dataset_e9fb9f35-3431-4acb-954a-61bdd671c2e7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/11/working/error.log" "/galaxy/server/database/objects/e/9/f/dataset_e9fb9f35-3431-4acb-954a-61bdd671c2e7.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:18,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:18,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:03:18,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:18,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:18,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:19,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:26,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kzdwn with k8s id: gxy-kzdwn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:03:26,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 11: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:03:34,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 11 finished
galaxy.model.metadata DEBUG 2025-04-07 13:03:34,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 25
galaxy.model.metadata DEBUG 2025-04-07 13:03:34,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 24
galaxy.util WARNING 2025-04-07 13:03:34,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/1/0/dataset_4107d9b0-8ef7-4fde-acc1-c7651f1f1f89.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/1/0/dataset_4107d9b0-8ef7-4fde-acc1-c7651f1f1f89.dat'
galaxy.util WARNING 2025-04-07 13:03:34,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/9/f/dataset_e9fb9f35-3431-4acb-954a-61bdd671c2e7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/9/f/dataset_e9fb9f35-3431-4acb-954a-61bdd671c2e7.dat'
galaxy.jobs INFO 2025-04-07 13:03:34,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 11 in /galaxy/server/database/jobs_directory/000/11
galaxy.jobs DEBUG 2025-04-07 13:03:34,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 11 executed (176.167 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:34,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:03:36,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 12
tpv.core.entities DEBUG 2025-04-07 13:03:36,218 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:03:36,218 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:03:36,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:03:36,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:03:36,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Working directory for job is: /galaxy/server/database/jobs_directory/000/12
galaxy.jobs.runners DEBUG 2025-04-07 13:03:36,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [12] queued (41.602 ms)
galaxy.jobs.handler INFO 2025-04-07 13:03:36,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:36,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 12
galaxy.jobs DEBUG 2025-04-07 13:03:36,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [12] prepared (62.305 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:36,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:03:36,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:36,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:03:36,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/12/tool_script.sh] for tool command [echo 'CP011064,CP0XXXXX,CP021680' | sed -r 's/(\,|__cn__)/\n/g' | grep -v "^[ \t]*$" > accessions && mkdir outdir && cd outdir && ignore_errors=0 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'fasta' --extended-validation all ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:03:36,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (12) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/12/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/12/galaxy_12.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/12/working/error.log" -a -f "/galaxy/server/database/objects/a/7/b/dataset_a7b3f9fd-dfe7-437e-9ce1-ffce10995f1d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/12/working/error.log" "/galaxy/server/database/objects/a/7/b/dataset_a7b3f9fd-dfe7-437e-9ce1-ffce10995f1d.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:36,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:36,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:03:36,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:36,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:36,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:37,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:44,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r8fln failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:44,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:44,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job: gxy-r8fln failed due to an unknown exit code from the tool.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:44,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:03:44,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-r8fln

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-r8fln": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners DEBUG 2025-04-07 13:03:44,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 12: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:03:53,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 12 finished
galaxy.tool_util.output_checker INFO 2025-04-07 13:03:53,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job error detected, failing job. Reasons are [{'type': 'exit_code', 'desc': 'Fatal error: Exit code 1 ()', 'exit_code': 1, 'code_desc': '', 'error_level': 3}]
galaxy.model.store.discover DEBUG 2025-04-07 13:03:53,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (12) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/12/working/outdir/CP011064.fa] with element identifier [CP011064] for output [output] (6.964 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:03:53,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (12) Add dynamic collection datasets to history for output [output] (18.514 ms)
galaxy.jobs DEBUG 2025-04-07 13:03:53,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (12) setting dataset 26 state to ERROR
galaxy.util WARNING 2025-04-07 13:03:53,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/7/b/dataset_a7b3f9fd-dfe7-437e-9ce1-ffce10995f1d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/7/b/dataset_a7b3f9fd-dfe7-437e-9ce1-ffce10995f1d.dat'
galaxy.jobs INFO 2025-04-07 13:03:53,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 12 in /galaxy/server/database/jobs_directory/000/12
galaxy.jobs DEBUG 2025-04-07 13:03:53,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 12 executed (159.187 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:53,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:03:54,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 13
tpv.core.entities DEBUG 2025-04-07 13:03:54,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:03:54,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:03:54,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:03:54,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:03:54,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Working directory for job is: /galaxy/server/database/jobs_directory/000/13
galaxy.jobs.runners DEBUG 2025-04-07 13:03:54,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [13] queued (31.849 ms)
galaxy.jobs.handler INFO 2025-04-07 13:03:54,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:54,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 13
galaxy.jobs DEBUG 2025-04-07 13:03:54,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [13] prepared (43.982 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:54,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:03:54,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:54,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:03:54,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/13/tool_script.sh] for tool command [echo 'CP011064__cn__CP021680' | sed -r 's/(\,|__cn__)/\n/g' | grep -v "^[ \t]*$" > accessions && mkdir outdir && cd outdir && ignore_errors=0 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'fasta' --extended-validation all ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:03:54,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (13) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/13/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/13/galaxy_13.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/13/working/error.log" -a -f "/galaxy/server/database/objects/e/0/b/dataset_e0bd58ad-9d43-4ad5-aada-09af3488a56f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/13/working/error.log" "/galaxy/server/database/objects/e/0/b/dataset_e0bd58ad-9d43-4ad5-aada-09af3488a56f.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:54,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:54,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:03:54,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:03:54,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:54,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:03:55,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:04,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zb6gg with k8s id: gxy-zb6gg succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:04:05,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 13: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:04:12,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 13 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:04:12,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (13) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/13/working/outdir/CP011064.fa] with element identifier [CP011064] for output [output] (5.178 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:04:12,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (13) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/13/working/outdir/CP021680.fa] with element identifier [CP021680] for output [output] (0.724 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:04:12,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (13) Add dynamic collection datasets to history for output [output] (38.097 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:04:12,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 28
galaxy.util WARNING 2025-04-07 13:04:12,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/0/b/dataset_e0bd58ad-9d43-4ad5-aada-09af3488a56f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/0/b/dataset_e0bd58ad-9d43-4ad5-aada-09af3488a56f.dat'
galaxy.jobs INFO 2025-04-07 13:04:12,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 13 in /galaxy/server/database/jobs_directory/000/13
galaxy.jobs DEBUG 2025-04-07 13:04:12,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 13 executed (185.550 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:12,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:04:13,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 14
tpv.core.entities DEBUG 2025-04-07 13:04:14,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:04:14,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:04:14,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:04:14,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:04:14,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Working directory for job is: /galaxy/server/database/jobs_directory/000/14
galaxy.jobs.runners DEBUG 2025-04-07 13:04:14,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [14] queued (42.176 ms)
galaxy.jobs.handler INFO 2025-04-07 13:04:14,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:14,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 14
galaxy.jobs DEBUG 2025-04-07 13:04:14,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [14] prepared (47.663 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:14,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:04:14,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:14,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:04:14,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/14/tool_script.sh] for tool command [echo 'NP_003192' | sed -r 's/(\,|__cn__)/\n/g' | grep -v "^[ \t]*$" > accessions && mkdir outdir && cd outdir && ignore_errors=0 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'protein' --format 'fasta' --extended-validation all ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:04:14,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (14) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/14/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/14/galaxy_14.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/14/working/error.log" -a -f "/galaxy/server/database/objects/c/4/e/dataset_c4e3cf80-9a0f-4b83-967b-418fc2e0eb7a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/14/working/error.log" "/galaxy/server/database/objects/c/4/e/dataset_c4e3cf80-9a0f-4b83-967b-418fc2e0eb7a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:14,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:14,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:04:14,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:14,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:14,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:14,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:22,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bs74l with k8s id: gxy-bs74l succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:04:22,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 14: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:04:29,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 14 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:04:29,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (14) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/14/working/outdir/NP_003192.fa] with element identifier [NP_003192] for output [output] (6.711 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:04:29,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (14) Add dynamic collection datasets to history for output [output] (17.566 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:04:29,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 31
galaxy.util WARNING 2025-04-07 13:04:29,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/4/e/dataset_c4e3cf80-9a0f-4b83-967b-418fc2e0eb7a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/4/e/dataset_c4e3cf80-9a0f-4b83-967b-418fc2e0eb7a.dat'
galaxy.jobs INFO 2025-04-07 13:04:29,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 14 in /galaxy/server/database/jobs_directory/000/14
galaxy.jobs DEBUG 2025-04-07 13:04:29,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 14 executed (148.187 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:29,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:04:31,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 15
tpv.core.entities DEBUG 2025-04-07 13:04:31,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:04:31,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:04:31,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:04:31,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:04:31,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Working directory for job is: /galaxy/server/database/jobs_directory/000/15
galaxy.jobs.runners DEBUG 2025-04-07 13:04:31,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [15] queued (39.207 ms)
galaxy.jobs.handler INFO 2025-04-07 13:04:31,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:31,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 15
galaxy.jobs DEBUG 2025-04-07 13:04:31,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [15] prepared (51.850 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:31,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:04:31,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:31,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:04:31,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/15/tool_script.sh] for tool command [echo 'NC_006666' | sed -r 's/(\,|__cn__)/\n/g' | grep -v "^[ \t]*$" > accessions && mkdir outdir && cd outdir && ignore_errors=0 && while read accession; do echo "Downloading accession number: " $accession " ..." >> ../error.log && ncbi-acc-download --molecule 'nucleotide' --format 'genbank' --extended-validation all --range 1697:2764 ${accession}; failure=$?; if [ $failure -ne 0 ]; then echo " failed." >> ../error.log; if [ $ignore_errors -ne 0 ]; then echo $accession >> ../failed.txt; else exit 1; fi; else echo " done." >> ../error.log; fi; sleep 2; done < ../accessions 2> >(tee -a ../error.log >&2)]
galaxy.jobs.runners DEBUG 2025-04-07 13:04:31,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (15) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/15/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/15/galaxy_15.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/15/working/error.log" -a -f "/galaxy/server/database/objects/c/c/9/dataset_cc950023-5841-41fe-a143-df5ec6f6be74.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/15/working/error.log" "/galaxy/server/database/objects/c/c/9/dataset_cc950023-5841-41fe-a143-df5ec6f6be74.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:31,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:31,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:04:31,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/ncbi_acc_download/ncbi_acc_download/0.2.8+galaxy0: ncbi-acc-download:0.2.8
galaxy.tool_util.deps.containers INFO 2025-04-07 13:04:31,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/ncbi-acc-download:0.2.8--pyh5e36f6f_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:31,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:32,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:38,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-277qm with k8s id: gxy-277qm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:04:38,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 15: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:04:45,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 15 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:04:45,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (15) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/15/working/outdir/NC_006666.gbk] with element identifier [NC_006666] for output [output] (5.272 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:04:45,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (15) Add dynamic collection datasets to history for output [output] (16.474 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:04:46,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 33
galaxy.util WARNING 2025-04-07 13:04:46,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/c/9/dataset_cc950023-5841-41fe-a143-df5ec6f6be74.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/c/9/dataset_cc950023-5841-41fe-a143-df5ec6f6be74.dat'
galaxy.jobs INFO 2025-04-07 13:04:46,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 15 in /galaxy/server/database/jobs_directory/000/15
galaxy.jobs DEBUG 2025-04-07 13:04:46,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 15 executed (148.532 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:46,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:04:49,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 16
tpv.core.entities DEBUG 2025-04-07 13:04:49,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:04:49,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:04:49,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:04:49,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:04:49,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Working directory for job is: /galaxy/server/database/jobs_directory/000/16
galaxy.jobs.runners DEBUG 2025-04-07 13:04:49,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [16] queued (43.797 ms)
galaxy.jobs.handler INFO 2025-04-07 13:04:49,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:49,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 16
galaxy.jobs DEBUG 2025-04-07 13:04:50,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [16] prepared (85.442 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:04:50,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/16/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/16/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/16/configs/tmpoaumscli']
galaxy.jobs.runners DEBUG 2025-04-07 13:04:50,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (16) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/16/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/16/galaxy_16.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:50,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:50,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:04:50,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:00,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q5455 with k8s id: gxy-q5455 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:05:00,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 16: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:05:08,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 16 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:05:08,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'fastq_invalid-line3', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/16/working/data_fetch_upload_ulpnsjl_', 'object_id': 35}]}]}]
galaxy.jobs INFO 2025-04-07 13:05:08,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 16 in /galaxy/server/database/jobs_directory/000/16
galaxy.jobs DEBUG 2025-04-07 13:05:08,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 16 executed (157.072 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:08,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:05:09,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 17
tpv.core.entities DEBUG 2025-04-07 13:05:09,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:05:09,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:05:09,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:05:09,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:05:09,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Working directory for job is: /galaxy/server/database/jobs_directory/000/17
galaxy.jobs.runners DEBUG 2025-04-07 13:05:09,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [17] queued (44.621 ms)
galaxy.jobs.handler INFO 2025-04-07 13:05:09,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:09,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 17
galaxy.jobs DEBUG 2025-04-07 13:05:09,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [17] prepared (50.764 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:09,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:05:09,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:09,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:05:09,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/17/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/c/1/9/dataset_c19090b8-a9dc-43c5-b831-12bcc85bfc50.dat' sanger '/galaxy/server/database/objects/f/9/c/dataset_f9c1bb31-0765-4148-94e0-f82f40738159.dat' sanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:05:09,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (17) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/17/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/17/galaxy_17.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:09,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:09,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:05:09,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:09,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:09,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:10,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:20,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tccbx with k8s id: gxy-tccbx succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:05:20,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 17: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:05:28,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 17 finished
galaxy.model.metadata DEBUG 2025-04-07 13:05:28,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 36
galaxy.jobs INFO 2025-04-07 13:05:28,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 17 in /galaxy/server/database/jobs_directory/000/17
galaxy.jobs DEBUG 2025-04-07 13:05:28,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 17 executed (163.149 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:28,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:05:30,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 18
tpv.core.entities DEBUG 2025-04-07 13:05:30,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:05:30,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:05:30,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:05:30,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:05:30,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Working directory for job is: /galaxy/server/database/jobs_directory/000/18
galaxy.jobs.runners DEBUG 2025-04-07 13:05:30,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [18] queued (37.409 ms)
galaxy.jobs.handler INFO 2025-04-07 13:05:30,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:30,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 18
galaxy.jobs DEBUG 2025-04-07 13:05:30,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [18] prepared (70.393 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:05:30,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/18/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/18/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/18/configs/tmpmrb35ia8']
galaxy.jobs.runners DEBUG 2025-04-07 13:05:31,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (18) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/18/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/18/galaxy_18.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:31,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:31,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:31,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:41,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9v7j7 with k8s id: gxy-9v7j7 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:05:42,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 18: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:05:49,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 18 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:05:49,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'fastq_invalid-line3', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/18/working/data_fetch_upload_cbtfrsab', 'object_id': 37}]}]}]
galaxy.jobs INFO 2025-04-07 13:05:50,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 18 in /galaxy/server/database/jobs_directory/000/18
galaxy.jobs DEBUG 2025-04-07 13:05:50,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 18 executed (174.142 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:50,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:05:51,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 19
tpv.core.entities DEBUG 2025-04-07 13:05:51,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:05:51,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:05:51,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:05:51,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:05:51,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Working directory for job is: /galaxy/server/database/jobs_directory/000/19
galaxy.jobs.runners DEBUG 2025-04-07 13:05:51,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [19] queued (45.617 ms)
galaxy.jobs.handler INFO 2025-04-07 13:05:51,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:51,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 19
galaxy.jobs DEBUG 2025-04-07 13:05:51,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [19] prepared (67.228 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:51,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:05:51,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:51,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:05:51,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/19/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/a/a/7/dataset_aa7ceede-89f3-42aa-9cab-c16ac0ed6d33.dat' sanger '/galaxy/server/database/objects/7/9/6/dataset_79649d81-3f16-4d01-b091-c70e01bd8891.dat' sanger ascii summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:05:51,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (19) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/19/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/19/galaxy_19.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:51,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:51,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:05:51,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:05:51,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:51,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:51,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:05:56,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zp7bl with k8s id: gxy-zp7bl succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:05:56,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 19: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:06:03,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 19 finished
galaxy.model.metadata DEBUG 2025-04-07 13:06:04,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 38
galaxy.jobs INFO 2025-04-07 13:06:04,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 19 in /galaxy/server/database/jobs_directory/000/19
galaxy.jobs DEBUG 2025-04-07 13:06:04,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 19 executed (143.051 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:04,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:06:05,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 20
tpv.core.entities DEBUG 2025-04-07 13:06:05,683 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:06:05,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:06:05,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:06:05,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:06:05,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Working directory for job is: /galaxy/server/database/jobs_directory/000/20
galaxy.jobs.runners DEBUG 2025-04-07 13:06:05,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [20] queued (50.892 ms)
galaxy.jobs.handler INFO 2025-04-07 13:06:05,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:05,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 20
galaxy.jobs DEBUG 2025-04-07 13:06:05,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [20] prepared (63.694 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:06:05,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/20/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/20/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/20/configs/tmpe7__e72z']
galaxy.jobs.runners DEBUG 2025-04-07 13:06:05,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (20) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/20/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/20/galaxy_20.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:05,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:05,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:06,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:16,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-56fpf with k8s id: gxy-56fpf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:06:16,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 20: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:06:24,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 20 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:06:24,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'fastq_invalid-line3', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/20/working/data_fetch_upload_w1t55ael', 'object_id': 39}]}]}]
galaxy.jobs INFO 2025-04-07 13:06:24,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 20 in /galaxy/server/database/jobs_directory/000/20
galaxy.jobs DEBUG 2025-04-07 13:06:24,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 20 executed (156.148 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:24,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:06:25,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 21
tpv.core.entities DEBUG 2025-04-07 13:06:25,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:06:25,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:06:25,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:06:25,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:06:25,169 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Working directory for job is: /galaxy/server/database/jobs_directory/000/21
galaxy.jobs.runners DEBUG 2025-04-07 13:06:25,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [21] queued (44.834 ms)
galaxy.jobs.handler INFO 2025-04-07 13:06:25,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:25,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 21
galaxy.jobs DEBUG 2025-04-07 13:06:25,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [21] prepared (57.654 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:25,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:06:25,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:25,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:06:25,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/21/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/3/0/c/dataset_30c460ec-181d-4718-b247-3c6593f550fd.dat' sanger '/galaxy/server/database/objects/f/2/4/dataset_f2459c83-c178-4963-839c-1fbc52471837.dat' sanger ascii summarize_input --no-fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:06:25,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (21) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/21/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/21/galaxy_21.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:25,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:25,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:06:25,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:25,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:25,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:26,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:29,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tnxzz failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:29,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:29,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job: gxy-tnxzz failed due to an unknown exit code from the tool.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:29,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:06:29,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-tnxzz

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-tnxzz": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners DEBUG 2025-04-07 13:06:29,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 21: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:06:37,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 21 finished
galaxy.tool_util.output_checker INFO 2025-04-07 13:06:37,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job error detected, failing job. Reasons are [{'type': 'exit_code', 'desc': 'Fatal error: Exit code 1 ()', 'exit_code': 1, 'code_desc': '', 'error_level': 3}]
galaxy.jobs DEBUG 2025-04-07 13:06:37,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (21) setting dataset 40 state to ERROR
galaxy.jobs INFO 2025-04-07 13:06:37,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 21 in /galaxy/server/database/jobs_directory/000/21
galaxy.jobs DEBUG 2025-04-07 13:06:37,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 21 executed (134.152 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:37,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:06:38,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 22
tpv.core.entities DEBUG 2025-04-07 13:06:38,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:06:38,463 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:06:38,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:06:38,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:06:38,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Working directory for job is: /galaxy/server/database/jobs_directory/000/22
galaxy.jobs.runners DEBUG 2025-04-07 13:06:38,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [22] queued (37.245 ms)
galaxy.jobs.handler INFO 2025-04-07 13:06:38,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:38,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 22
galaxy.jobs DEBUG 2025-04-07 13:06:38,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [22] prepared (75.183 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:06:38,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/22/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/22/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/22/configs/tmptftvb8fj']
galaxy.jobs.runners DEBUG 2025-04-07 13:06:38,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (22) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/22/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/22/galaxy_22.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:38,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:38,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:39,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:48,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7sj5w with k8s id: gxy-7sj5w succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:06:48,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 22: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:06:56,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 22 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:06:56,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/22/working/data_fetch_upload_yslh898c', 'object_id': 41}]}]}]
galaxy.jobs INFO 2025-04-07 13:06:56,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 22 in /galaxy/server/database/jobs_directory/000/22
galaxy.jobs DEBUG 2025-04-07 13:06:56,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 22 executed (158.331 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:56,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:06:56,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 23
tpv.core.entities DEBUG 2025-04-07 13:06:56,968 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:06:56,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:06:56,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:06:56,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:06:57,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Working directory for job is: /galaxy/server/database/jobs_directory/000/23
galaxy.jobs.runners DEBUG 2025-04-07 13:06:57,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [23] queued (55.003 ms)
galaxy.jobs.handler INFO 2025-04-07 13:06:57,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:57,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 23
galaxy.jobs DEBUG 2025-04-07 13:06:57,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [23] prepared (57.157 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:57,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:06:57,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:57,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:06:57,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/23/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/6/d/6/dataset_6d6f85d9-e4f0-4f43-a518-7f4f039ecf58.dat' sanger '/galaxy/server/database/objects/a/a/7/dataset_aa7f8def-4420-4889-9009-5c8d48360420.dat' sanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:06:57,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (23) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/23/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/23/galaxy_23.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:57,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:57,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:06:57,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:06:57,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:57,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:06:57,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:01,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zkwsw with k8s id: gxy-zkwsw succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:07:02,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 23: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:07:10,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 23 finished
galaxy.model.metadata DEBUG 2025-04-07 13:07:10,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 42
galaxy.jobs INFO 2025-04-07 13:07:10,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 23 in /galaxy/server/database/jobs_directory/000/23
galaxy.jobs DEBUG 2025-04-07 13:07:10,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 23 executed (141.422 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:10,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:07:11,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 24
tpv.core.entities DEBUG 2025-04-07 13:07:11,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:07:11,351 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:07:11,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:07:11,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:07:11,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Working directory for job is: /galaxy/server/database/jobs_directory/000/24
galaxy.jobs.runners DEBUG 2025-04-07 13:07:11,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [24] queued (50.135 ms)
galaxy.jobs.handler INFO 2025-04-07 13:07:11,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:11,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 24
galaxy.jobs DEBUG 2025-04-07 13:07:11,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [24] prepared (79.100 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:07:11,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/24/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/24/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/24/configs/tmpgy93xkhl']
galaxy.jobs.runners DEBUG 2025-04-07 13:07:11,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (24) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/24/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/24/galaxy_24.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:11,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:11,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:11,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:22,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m9tdr with k8s id: gxy-m9tdr succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:07:22,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 24: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:07:29,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 24 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:07:29,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_cssanger.fastqcssanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/24/working/data_fetch_upload_xgf52hoy', 'object_id': 43}]}]}]
galaxy.jobs INFO 2025-04-07 13:07:30,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 24 in /galaxy/server/database/jobs_directory/000/24
galaxy.jobs DEBUG 2025-04-07 13:07:30,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 24 executed (153.821 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:30,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:07:30,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 25
tpv.core.entities DEBUG 2025-04-07 13:07:30,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:07:30,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:07:30,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:07:30,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:07:30,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Working directory for job is: /galaxy/server/database/jobs_directory/000/25
galaxy.jobs.runners DEBUG 2025-04-07 13:07:30,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [25] queued (41.951 ms)
galaxy.jobs.handler INFO 2025-04-07 13:07:30,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:30,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 25
galaxy.jobs DEBUG 2025-04-07 13:07:30,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [25] prepared (52.165 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:07:30,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:07:30,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:07:30,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:07:30,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/25/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/0/1/1/dataset_01139be2-dac6-409e-9274-86e1795e6101.dat' cssanger '/galaxy/server/database/objects/5/e/f/dataset_5efa7dce-5a31-4418-a04f-9a459242d024.dat' cssanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:07:30,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (25) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/25/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/25/galaxy_25.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:31,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:07:31,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:07:31,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:07:31,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:31,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:31,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:36,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hgdr6 with k8s id: gxy-hgdr6 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:07:36,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 25: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:07:44,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 25 finished
galaxy.model.metadata DEBUG 2025-04-07 13:07:44,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 44
galaxy.jobs INFO 2025-04-07 13:07:44,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 25 in /galaxy/server/database/jobs_directory/000/25
galaxy.jobs DEBUG 2025-04-07 13:07:44,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 25 executed (129.284 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:44,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:07:46,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 26
tpv.core.entities DEBUG 2025-04-07 13:07:46,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:07:46,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:07:46,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:07:46,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:07:46,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Working directory for job is: /galaxy/server/database/jobs_directory/000/26
galaxy.jobs.runners DEBUG 2025-04-07 13:07:46,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [26] queued (39.690 ms)
galaxy.jobs.handler INFO 2025-04-07 13:07:46,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:46,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 26
galaxy.jobs DEBUG 2025-04-07 13:07:46,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [26] prepared (70.393 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:07:46,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/26/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/26/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/26/configs/tmpjfir5rbc']
galaxy.jobs.runners DEBUG 2025-04-07 13:07:46,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/26/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/26/galaxy_26.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:46,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:46,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:47,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:07:55,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hxz29 with k8s id: gxy-hxz29 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:07:55,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 26: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:08:03,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 26 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:08:03,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'illumina_full_range_original_illumina.fastqillumina', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/26/working/data_fetch_upload_n2vsvctv', 'object_id': 45}]}]}]
galaxy.jobs INFO 2025-04-07 13:08:03,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 26 in /galaxy/server/database/jobs_directory/000/26
galaxy.jobs DEBUG 2025-04-07 13:08:03,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 26 executed (161.571 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:03,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:08:04,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 27
tpv.core.entities DEBUG 2025-04-07 13:08:04,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:08:04,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:08:04,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:08:04,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:08:04,683 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Working directory for job is: /galaxy/server/database/jobs_directory/000/27
galaxy.jobs.runners DEBUG 2025-04-07 13:08:04,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [27] queued (45.585 ms)
galaxy.jobs.handler INFO 2025-04-07 13:08:04,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:04,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 27
galaxy.jobs DEBUG 2025-04-07 13:08:04,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [27] prepared (51.356 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:04,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:08:04,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:04,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:08:04,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/27/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/8/6/4/dataset_86432fd9-6ec7-4439-94e9-0fc3da268d62.dat' illumina '/galaxy/server/database/objects/a/e/0/dataset_ae02a991-fb84-4584-a321-8289530946cd.dat' sanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:08:04,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (27) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/27/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/27/galaxy_27.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:04,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:04,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:08:04,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:04,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:04,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:05,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:10,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5tzwq with k8s id: gxy-5tzwq succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:08:10,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 27: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:08:18,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 27 finished
galaxy.model.metadata DEBUG 2025-04-07 13:08:18,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 46
galaxy.jobs INFO 2025-04-07 13:08:18,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 27 in /galaxy/server/database/jobs_directory/000/27
galaxy.jobs DEBUG 2025-04-07 13:08:18,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 27 executed (135.256 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:18,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:08:19,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 28
tpv.core.entities DEBUG 2025-04-07 13:08:20,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:08:20,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:08:20,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:08:20,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:08:20,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Working directory for job is: /galaxy/server/database/jobs_directory/000/28
galaxy.jobs.runners DEBUG 2025-04-07 13:08:20,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [28] queued (43.277 ms)
galaxy.jobs.handler INFO 2025-04-07 13:08:20,067 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:20,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 28
galaxy.jobs DEBUG 2025-04-07 13:08:20,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [28] prepared (82.318 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:08:20,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/28/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/28/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/28/configs/tmp7c7dz5l7']
galaxy.jobs.runners DEBUG 2025-04-07 13:08:20,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (28) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/28/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/28/galaxy_28.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:20,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:20,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:21,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:30,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7gxnf with k8s id: gxy-7gxnf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:08:30,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 28: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:08:38,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 28 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:08:38,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'solexa_full_range_original_solexa.fastqsolexa', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/28/working/data_fetch_upload_c6bdywjt', 'object_id': 47}]}]}]
galaxy.jobs INFO 2025-04-07 13:08:38,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 28 in /galaxy/server/database/jobs_directory/000/28
galaxy.jobs DEBUG 2025-04-07 13:08:38,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 28 executed (163.354 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:38,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:08:39,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 29
tpv.core.entities DEBUG 2025-04-07 13:08:39,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:08:39,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:08:39,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:08:39,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:08:39,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Working directory for job is: /galaxy/server/database/jobs_directory/000/29
galaxy.jobs.runners DEBUG 2025-04-07 13:08:39,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [29] queued (44.270 ms)
galaxy.jobs.handler INFO 2025-04-07 13:08:39,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:39,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 29
galaxy.jobs DEBUG 2025-04-07 13:08:39,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [29] prepared (55.313 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:39,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:08:39,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:39,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:08:39,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/29/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/d/d/c/dataset_ddcb8246-6007-4f1e-932c-b84265b59159.dat' solexa '/galaxy/server/database/objects/e/5/9/dataset_e598c764-ca42-4a6e-883a-771a78eac54e.dat' sanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:08:39,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (29) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/29/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/29/galaxy_29.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:39,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:39,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:08:39,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:08:39,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:39,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:40,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:44,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s5vmh with k8s id: gxy-s5vmh succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:08:44,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 29: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:08:52,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 29 finished
galaxy.model.metadata DEBUG 2025-04-07 13:08:52,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 48
galaxy.jobs INFO 2025-04-07 13:08:52,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 29 in /galaxy/server/database/jobs_directory/000/29
galaxy.jobs DEBUG 2025-04-07 13:08:52,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 29 executed (116.557 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:53,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:08:53,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 30
tpv.core.entities DEBUG 2025-04-07 13:08:53,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:08:53,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:08:53,873 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:08:53,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:08:53,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Working directory for job is: /galaxy/server/database/jobs_directory/000/30
galaxy.jobs.runners DEBUG 2025-04-07 13:08:53,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [30] queued (41.312 ms)
galaxy.jobs.handler INFO 2025-04-07 13:08:53,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:53,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 30
galaxy.jobs DEBUG 2025-04-07 13:08:53,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [30] prepared (63.719 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:08:54,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/30/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/30/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/30/configs/tmpsa8nzo32']
galaxy.jobs.runners DEBUG 2025-04-07 13:08:54,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/30/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/30/galaxy_30.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:54,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:54,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:08:54,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:04,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b4kxg with k8s id: gxy-b4kxg succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:09:04,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 30: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:09:13,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 30 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:09:13,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_illumina.fastqillumina', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/30/working/data_fetch_upload_wbpbhwfj', 'object_id': 49}]}]}]
galaxy.jobs INFO 2025-04-07 13:09:13,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 30 in /galaxy/server/database/jobs_directory/000/30
galaxy.jobs DEBUG 2025-04-07 13:09:13,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 30 executed (229.696 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:13,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:09:14,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 31
tpv.core.entities DEBUG 2025-04-07 13:09:14,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:09:14,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:09:14,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:09:14,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:09:14,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Working directory for job is: /galaxy/server/database/jobs_directory/000/31
galaxy.jobs.runners DEBUG 2025-04-07 13:09:14,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [31] queued (46.195 ms)
galaxy.jobs.handler INFO 2025-04-07 13:09:14,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:14,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 31
galaxy.jobs DEBUG 2025-04-07 13:09:14,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [31] prepared (58.248 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:14,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:09:14,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:14,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:09:14,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/31/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/e/b/5/dataset_eb5a03fa-c65e-4a8f-8da2-45dd9d1da4d9.dat' sanger '/galaxy/server/database/objects/4/e/1/dataset_4e171a80-48ec-44ff-888f-552aea5b70f0.dat' sanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:09:14,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (31) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/31/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/31/galaxy_31.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:14,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:14,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:09:14,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:14,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:14,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:15,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:19,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8pkzb with k8s id: gxy-8pkzb succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:09:19,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 31: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:09:28,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 31 finished
galaxy.model.metadata DEBUG 2025-04-07 13:09:28,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 50
galaxy.jobs INFO 2025-04-07 13:09:28,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 31 in /galaxy/server/database/jobs_directory/000/31
galaxy.jobs DEBUG 2025-04-07 13:09:28,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 31 executed (140.780 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:28,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:09:29,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 32
tpv.core.entities DEBUG 2025-04-07 13:09:29,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:09:29,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:09:29,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:09:29,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:09:29,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Working directory for job is: /galaxy/server/database/jobs_directory/000/32
galaxy.jobs.runners DEBUG 2025-04-07 13:09:29,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [32] queued (40.019 ms)
galaxy.jobs.handler INFO 2025-04-07 13:09:29,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:29,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 32
galaxy.jobs DEBUG 2025-04-07 13:09:29,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [32] prepared (87.835 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:09:29,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/32/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/32/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/32/configs/tmpcjn_ncrr']
galaxy.jobs.runners DEBUG 2025-04-07 13:09:29,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (32) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/32/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/32/galaxy_32.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:29,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:30,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:30,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:40,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p7dz4 with k8s id: gxy-p7dz4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:09:41,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 32: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:09:48,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 32 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:09:49,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'illumina_full_range_original_illumina.fastqillumina', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/32/working/data_fetch_upload_ybvlqd3m', 'object_id': 51}]}]}]
galaxy.jobs INFO 2025-04-07 13:09:49,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 32 in /galaxy/server/database/jobs_directory/000/32
galaxy.jobs DEBUG 2025-04-07 13:09:49,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 32 executed (156.459 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:49,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:09:50,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 33
tpv.core.entities DEBUG 2025-04-07 13:09:50,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:09:50,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:09:50,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:09:50,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:09:50,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Working directory for job is: /galaxy/server/database/jobs_directory/000/33
galaxy.jobs.runners DEBUG 2025-04-07 13:09:50,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [33] queued (60.067 ms)
galaxy.jobs.handler INFO 2025-04-07 13:09:50,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:50,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 33
galaxy.jobs DEBUG 2025-04-07 13:09:50,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [33] prepared (55.696 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:50,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:09:50,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:50,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:09:50,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/33/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/e/6/8/dataset_e68229fc-f6a6-44d4-99cc-a8d081cd4b88.dat' illumina '/galaxy/server/database/objects/c/2/9/dataset_c29d216f-a85c-48dd-a7cf-f0bf550fc2ec.dat' illumina None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:09:50,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (33) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/33/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/33/galaxy_33.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:50,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:50,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:09:50,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:09:50,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:50,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:51,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:09:55,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t58rm with k8s id: gxy-t58rm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:09:55,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 33: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:10:03,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 33 finished
galaxy.model.metadata DEBUG 2025-04-07 13:10:03,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 52
galaxy.jobs INFO 2025-04-07 13:10:03,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 33 in /galaxy/server/database/jobs_directory/000/33
galaxy.jobs DEBUG 2025-04-07 13:10:03,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 33 executed (137.446 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:03,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:10:04,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 34
tpv.core.entities DEBUG 2025-04-07 13:10:04,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:10:04,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:10:04,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:10:04,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:10:04,699 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Working directory for job is: /galaxy/server/database/jobs_directory/000/34
galaxy.jobs.runners DEBUG 2025-04-07 13:10:04,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [34] queued (45.143 ms)
galaxy.jobs.handler INFO 2025-04-07 13:10:04,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:04,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 34
galaxy.jobs DEBUG 2025-04-07 13:10:04,804 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [34] prepared (77.437 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:10:04,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/34/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/34/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/34/configs/tmp4y6rwlts']
galaxy.jobs.runners DEBUG 2025-04-07 13:10:04,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (34) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/34/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/34/galaxy_34.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:04,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:04,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:05,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:15,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-488cx with k8s id: gxy-488cx succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:10:15,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 34: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:10:23,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 34 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:10:23,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'illumina_full_range_original_illumina.fastqillumina', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/34/working/data_fetch_upload_3sj7lxbe', 'object_id': 53}]}]}]
galaxy.jobs INFO 2025-04-07 13:10:23,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 34 in /galaxy/server/database/jobs_directory/000/34
galaxy.jobs DEBUG 2025-04-07 13:10:23,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 34 executed (152.926 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:23,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:10:24,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 35
tpv.core.entities DEBUG 2025-04-07 13:10:24,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:10:24,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:10:24,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:10:24,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:10:24,156 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Working directory for job is: /galaxy/server/database/jobs_directory/000/35
galaxy.jobs.runners DEBUG 2025-04-07 13:10:24,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [35] queued (46.036 ms)
galaxy.jobs.handler INFO 2025-04-07 13:10:24,170 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:24,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 35
galaxy.jobs DEBUG 2025-04-07 13:10:24,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [35] prepared (58.354 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:24,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:10:24,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:24,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:10:24,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/35/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/b/b/3/dataset_bb3f88a3-3d3c-4a72-8465-7edc232b6713.dat' illumina '/galaxy/server/database/objects/3/d/5/dataset_3d51c41d-ab74-4222-9221-5efc2194208b.dat' sanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:10:24,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (35) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/35/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/35/galaxy_35.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:24,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:24,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:10:24,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:24,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:24,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:25,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7ckds failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-7ckds.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:10:28,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-7ckds

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-7ckds": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (35/gxy-7ckds) tool_stdout: Groomed 2 illumina reads into sanger reads.
Based upon quality and sequence, the input data is valid for: illumina, illumina.gz, illumina.bz2, solexa, solexa.gz, solexa.bz2, sanger, sanger.gz, sanger.bz2
Input ASCII range: '@'(64) - '~'(126)
Input decimal range: 0 - 62

galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (35/gxy-7ckds) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (35/gxy-7ckds) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (35/gxy-7ckds) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-7ckds.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 35 (gxy-7ckds)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-7ckds to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:28,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (35/gxy-7ckds) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-07 13:10:30,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 36
tpv.core.entities DEBUG 2025-04-07 13:10:30,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:10:30,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:10:30,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:10:30,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:10:30,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Working directory for job is: /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-04-07 13:10:30,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [36] queued (36.590 ms)
galaxy.jobs.handler INFO 2025-04-07 13:10:30,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:30,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 36
galaxy.jobs DEBUG 2025-04-07 13:10:30,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [36] prepared (78.324 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:10:30,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/36/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/36/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/36/configs/tmpqtl24u8d']
galaxy.jobs.runners DEBUG 2025-04-07 13:10:30,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (36) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/36/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/36/galaxy_36.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:30,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:30,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:31,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:40,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-264tn with k8s id: gxy-264tn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:10:41,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 36: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:10:49,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 36 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:10:49,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'illumina_full_range_original_illumina.fastqillumina', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/36/working/data_fetch_upload_ah3njvrn', 'object_id': 55}]}]}]
galaxy.jobs INFO 2025-04-07 13:10:49,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs DEBUG 2025-04-07 13:10:49,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 36 executed (162.596 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:49,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:10:49,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 37
tpv.core.entities DEBUG 2025-04-07 13:10:49,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:10:49,755 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:10:49,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:10:49,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:10:49,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.runners DEBUG 2025-04-07 13:10:49,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (37.870 ms)
galaxy.jobs.handler INFO 2025-04-07 13:10:49,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:49,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 37
galaxy.jobs DEBUG 2025-04-07 13:10:49,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [37] prepared (49.670 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:49,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:10:49,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:49,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:10:49,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/f/b/5/dataset_fb547edb-ddfb-46be-9c34-6a94f2e721c6.dat' illumina '/galaxy/server/database/objects/b/8/a/dataset_b8a3c405-1dbd-4d3e-941f-7e85e61d1769.dat' solexa None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:10:49,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:49,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:49,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:10:49,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:10:49,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:50,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:50,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:10:55,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ljmgr with k8s id: gxy-ljmgr succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:10:55,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 37: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:11:03,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 37 finished
galaxy.model.metadata DEBUG 2025-04-07 13:11:03,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 56
galaxy.jobs INFO 2025-04-07 13:11:03,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 37 in /galaxy/server/database/jobs_directory/000/37
galaxy.jobs DEBUG 2025-04-07 13:11:03,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 37 executed (134.495 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:03,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:11:05,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38
tpv.core.entities DEBUG 2025-04-07 13:11:05,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:11:05,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:11:05,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:11:05,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:11:05,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-04-07 13:11:05,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (43.781 ms)
galaxy.jobs.handler INFO 2025-04-07 13:11:05,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:05,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 38
galaxy.jobs DEBUG 2025-04-07 13:11:05,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [38] prepared (83.322 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:11:05,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/38/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/38/configs/tmpbu7wdo5s']
galaxy.jobs.runners DEBUG 2025-04-07 13:11:05,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:05,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:05,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:06,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:16,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nntvm with k8s id: gxy-nntvm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:11:16,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 38: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:11:25,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 38 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:11:25,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'illumina_full_range_original_illumina.fastqillumina', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/38/working/data_fetch_upload_r2zfhbxk', 'object_id': 57}]}]}]
galaxy.jobs INFO 2025-04-07 13:11:25,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 38 in /galaxy/server/database/jobs_directory/000/38
galaxy.jobs DEBUG 2025-04-07 13:11:25,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 38 executed (151.506 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:25,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:11:25,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 39
tpv.core.entities DEBUG 2025-04-07 13:11:25,777 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:11:25,777 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:11:25,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:11:25,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:11:25,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.runners DEBUG 2025-04-07 13:11:25,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (40.153 ms)
galaxy.jobs.handler INFO 2025-04-07 13:11:25,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:25,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 39
galaxy.jobs DEBUG 2025-04-07 13:11:25,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [39] prepared (44.709 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:25,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:11:25,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:25,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:11:25,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/d/1/b/dataset_d1be7628-5b02-418c-9cca-35c2fb0794ac.dat' illumina '/galaxy/server/database/objects/4/1/c/dataset_41c5e02e-8d80-47f9-8317-0a2e445d1e26.dat' cssanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:11:25,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:25,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:25,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:11:25,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:25,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:26,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:26,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:30,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tw59q with k8s id: gxy-tw59q succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:11:30,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 39: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:11:38,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 39 finished
galaxy.model.metadata DEBUG 2025-04-07 13:11:38,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 58
galaxy.jobs INFO 2025-04-07 13:11:38,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 39 in /galaxy/server/database/jobs_directory/000/39
galaxy.jobs DEBUG 2025-04-07 13:11:38,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 39 executed (149.926 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:38,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:11:40,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 40
tpv.core.entities DEBUG 2025-04-07 13:11:40,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:11:40,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:11:40,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:11:40,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:11:40,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-04-07 13:11:40,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (49.035 ms)
galaxy.jobs.handler INFO 2025-04-07 13:11:40,177 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:40,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 40
galaxy.jobs DEBUG 2025-04-07 13:11:40,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [40] prepared (72.764 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:11:40,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/40/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/40/configs/tmphn5xbv4k']
galaxy.jobs.runners DEBUG 2025-04-07 13:11:40,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:40,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:40,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:40,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:49,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vvzvn with k8s id: gxy-vvzvn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:11:49,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 40: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:11:57,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 40 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:11:57,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/40/working/data_fetch_upload_qae1xl0x', 'object_id': 59}]}]}]
galaxy.jobs INFO 2025-04-07 13:11:57,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 40 in /galaxy/server/database/jobs_directory/000/40
galaxy.jobs DEBUG 2025-04-07 13:11:57,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 40 executed (151.159 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:57,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:11:58,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-04-07 13:11:58,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:11:58,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:11:58,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:11:58,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:11:58,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-04-07 13:11:58,599 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (43.693 ms)
galaxy.jobs.handler INFO 2025-04-07 13:11:58,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:58,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 41
galaxy.jobs DEBUG 2025-04-07 13:11:58,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [41] prepared (53.746 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:58,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:11:58,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:58,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:11:58,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/d/9/f/dataset_d9f3e4ef-401e-4f8b-838c-c54c27309d36.dat' sanger '/galaxy/server/database/objects/0/b/7/dataset_0b7241d5-1d3f-4df3-ac75-fadcbc92187e.dat' sanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:11:58,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:58,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:58,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:11:58,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:11:58,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:58,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:11:59,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:02,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mbcn4 with k8s id: gxy-mbcn4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:12:02,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:12:10,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 41 finished
galaxy.model.metadata DEBUG 2025-04-07 13:12:10,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 60
galaxy.jobs INFO 2025-04-07 13:12:10,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-04-07 13:12:10,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 41 executed (168.972 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:10,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:12:11,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 42
tpv.core.entities DEBUG 2025-04-07 13:12:11,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:12:11,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:12:11,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:12:11,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:12:11,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-04-07 13:12:11,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (40.229 ms)
galaxy.jobs.handler INFO 2025-04-07 13:12:11,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:11,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 42
galaxy.jobs DEBUG 2025-04-07 13:12:12,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [42] prepared (74.676 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:12:12,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/42/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/42/configs/tmpqzy08adv']
galaxy.jobs.runners DEBUG 2025-04-07 13:12:12,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:12,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:12,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:12,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:22,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-g6z4d with k8s id: gxy-g6z4d succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:12:22,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:12:29,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 42 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:12:29,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/42/working/data_fetch_upload_nk92vq_g', 'object_id': 61}]}]}]
galaxy.jobs INFO 2025-04-07 13:12:29,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs DEBUG 2025-04-07 13:12:29,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 42 executed (146.646 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:29,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:12:30,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43
tpv.core.entities DEBUG 2025-04-07 13:12:30,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:12:30,301 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:12:30,304 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:12:30,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:12:30,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-04-07 13:12:30,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (41.952 ms)
galaxy.jobs.handler INFO 2025-04-07 13:12:30,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:30,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 43
galaxy.jobs DEBUG 2025-04-07 13:12:30,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [43] prepared (71.639 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:12:30,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:12:30,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:12:30,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:12:30,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/a/1/9/dataset_a1972062-520a-49ab-995e-16dab844680d.dat' sanger '/galaxy/server/database/objects/e/7/3/dataset_e73eb035-454c-4479-b78b-097eaff4e168.dat' illumina None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:12:30,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:30,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:12:30,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:12:30,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:12:30,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:30,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:31,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:35,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b6bdn with k8s id: gxy-b6bdn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:12:35,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:12:43,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 43 finished
galaxy.model.metadata DEBUG 2025-04-07 13:12:43,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 62
galaxy.jobs INFO 2025-04-07 13:12:43,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-04-07 13:12:43,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 43 executed (114.956 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:43,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:12:44,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 44
tpv.core.entities DEBUG 2025-04-07 13:12:44,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:12:44,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:12:44,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:12:44,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:12:44,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.runners DEBUG 2025-04-07 13:12:44,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (34.987 ms)
galaxy.jobs.handler INFO 2025-04-07 13:12:44,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:44,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 44
galaxy.jobs DEBUG 2025-04-07 13:12:44,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [44] prepared (66.145 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:12:44,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/44/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/44/configs/tmpozyzxemh']
galaxy.jobs.runners DEBUG 2025-04-07 13:12:44,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:44,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:44,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:45,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:12:54,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zn22z with k8s id: gxy-zn22z succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:12:54,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:13:02,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 44 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:13:02,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/44/working/data_fetch_upload_btuheplm', 'object_id': 63}]}]}]
galaxy.jobs INFO 2025-04-07 13:13:02,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-04-07 13:13:02,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 44 executed (145.440 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:02,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:13:03,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-04-07 13:13:03,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:13:03,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:13:03,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:13:03,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:13:03,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-04-07 13:13:03,100 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (45.588 ms)
galaxy.jobs.handler INFO 2025-04-07 13:13:03,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:03,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-04-07 13:13:03,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [45] prepared (53.772 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:03,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:13:03,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:03,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:13:03,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/d/2/a/dataset_d2a9d21a-28c6-436b-bd75-87575e84ea06.dat' sanger '/galaxy/server/database/objects/7/6/c/dataset_76cda0a6-0f61-4bd7-944e-d951923a86c6.dat' solexa None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:13:03,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:03,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:03,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:13:03,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:03,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:03,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:03,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:07,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b89tk with k8s id: gxy-b89tk succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:13:08,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:13:16,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 45 finished
galaxy.model.metadata DEBUG 2025-04-07 13:13:16,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 64
galaxy.jobs INFO 2025-04-07 13:13:16,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-04-07 13:13:16,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 45 executed (128.206 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:16,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:13:17,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-04-07 13:13:17,399 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:13:17,399 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:13:17,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:13:17,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:13:17,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-04-07 13:13:17,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (42.363 ms)
galaxy.jobs.handler INFO 2025-04-07 13:13:17,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:17,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-04-07 13:13:17,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [46] prepared (73.534 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:13:17,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/46/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/46/configs/tmpnvpiuv7v']
galaxy.jobs.runners DEBUG 2025-04-07 13:13:17,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:17,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:17,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:18,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:29,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-twxhj with k8s id: gxy-twxhj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:13:29,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:13:37,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 46 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:13:37,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/46/working/data_fetch_upload_tj8veeiu', 'object_id': 65}]}]}]
galaxy.jobs INFO 2025-04-07 13:13:37,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-04-07 13:13:37,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 46 executed (162.263 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:37,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:13:37,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-04-07 13:13:37,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:13:37,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:13:37,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:13:37,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:13:37,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-04-07 13:13:37,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (41.039 ms)
galaxy.jobs.handler INFO 2025-04-07 13:13:37,925 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:37,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 47
galaxy.jobs DEBUG 2025-04-07 13:13:37,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [47] prepared (48.307 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:37,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:13:37,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:38,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:13:38,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/f/e/2/dataset_fe2a8d3d-747b-41d3-9713-e94b22192b74.dat' sanger '/galaxy/server/database/objects/8/0/3/dataset_8031b955-39cf-4c2d-8acb-009556faddb2.dat' cssanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:13:38,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:38,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:38,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:13:38,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:13:38,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:38,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:39,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:42,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kghkd with k8s id: gxy-kghkd succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:13:42,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:13:50,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 47 finished
galaxy.model.metadata DEBUG 2025-04-07 13:13:50,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 66
galaxy.jobs INFO 2025-04-07 13:13:50,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs DEBUG 2025-04-07 13:13:50,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 47 executed (149.703 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:50,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:13:51,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 48
tpv.core.entities DEBUG 2025-04-07 13:13:51,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:13:51,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:13:51,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:13:51,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:13:51,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-04-07 13:13:51,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (45.243 ms)
galaxy.jobs.handler INFO 2025-04-07 13:13:51,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:51,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 48
galaxy.jobs DEBUG 2025-04-07 13:13:51,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [48] prepared (78.290 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:13:51,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/48/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/48/configs/tmpqs84jup5']
galaxy.jobs.runners DEBUG 2025-04-07 13:13:51,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:51,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:51,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:13:52,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:01,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8b962 with k8s id: gxy-8b962 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:14:01,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:14:09,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 48 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:14:09,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'solexa_full_range_original_solexa.fastqsolexa', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/48/working/data_fetch_upload_m05jjflq', 'object_id': 67}]}]}]
galaxy.jobs INFO 2025-04-07 13:14:09,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs DEBUG 2025-04-07 13:14:09,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 48 executed (161.179 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:09,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:14:10,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 49
tpv.core.entities DEBUG 2025-04-07 13:14:10,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:14:10,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:14:10,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:14:10,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:14:10,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-04-07 13:14:10,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (40.525 ms)
galaxy.jobs.handler INFO 2025-04-07 13:14:10,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:10,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-04-07 13:14:10,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [49] prepared (42.883 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:10,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:14:10,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:10,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:14:10,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/9/f/1/dataset_9f117330-c632-418d-94df-db837e3f816a.dat' solexa '/galaxy/server/database/objects/b/1/4/dataset_b1493bcb-e8b8-4c08-bef9-a784639b7b62.dat' solexa None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:14:10,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:10,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:10,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:14:10,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:10,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:11,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:11,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:15,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-72ljl with k8s id: gxy-72ljl succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:14:15,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:14:23,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 49 finished
galaxy.model.metadata DEBUG 2025-04-07 13:14:23,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 68
galaxy.jobs INFO 2025-04-07 13:14:23,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs DEBUG 2025-04-07 13:14:23,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 49 executed (148.929 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:23,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:14:25,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-04-07 13:14:25,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:14:25,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:14:25,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:14:25,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:14:25,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-04-07 13:14:25,163 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (42.730 ms)
galaxy.jobs.handler INFO 2025-04-07 13:14:25,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:25,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-04-07 13:14:25,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [50] prepared (77.688 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:14:25,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/50/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/50/configs/tmp02j_vs31']
galaxy.jobs.runners DEBUG 2025-04-07 13:14:25,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:25,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:25,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:25,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:35,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6tn89 with k8s id: gxy-6tn89 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:14:35,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:14:44,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 50 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:14:44,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'solexa_full_range_original_solexa.fastqsolexa', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/50/working/data_fetch_upload_anunxjha', 'object_id': 69}]}]}]
galaxy.jobs INFO 2025-04-07 13:14:44,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-04-07 13:14:44,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 50 executed (190.727 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:44,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:14:45,560 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 51
tpv.core.entities DEBUG 2025-04-07 13:14:45,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:14:45,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:14:45,592 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:14:45,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:14:45,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-04-07 13:14:45,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (38.150 ms)
galaxy.jobs.handler INFO 2025-04-07 13:14:45,633 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:45,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 51
galaxy.jobs DEBUG 2025-04-07 13:14:45,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [51] prepared (62.348 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:45,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:14:45,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:45,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:14:45,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/f/1/5/dataset_f1505480-045d-4623-a1df-3f5088ce94a5.dat' solexa '/galaxy/server/database/objects/5/9/f/dataset_59fe1b3b-0b15-4fee-b15b-90a838e05c49.dat' illumina None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:14:45,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:45,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:45,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:14:45,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:14:45,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:45,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:46,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:50,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vd4tm with k8s id: gxy-vd4tm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:14:51,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:14:59,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 51 finished
galaxy.model.metadata DEBUG 2025-04-07 13:14:59,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 70
galaxy.jobs INFO 2025-04-07 13:14:59,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs DEBUG 2025-04-07 13:14:59,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 51 executed (160.974 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:14:59,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:15:00,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52
tpv.core.entities DEBUG 2025-04-07 13:15:00,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:15:00,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:15:00,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:15:00,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:15:00,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-04-07 13:15:00,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (45.176 ms)
galaxy.jobs.handler INFO 2025-04-07 13:15:00,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:00,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-04-07 13:15:01,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [52] prepared (84.084 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:15:01,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/52/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/52/configs/tmp24f9zfzy']
galaxy.jobs.runners DEBUG 2025-04-07 13:15:01,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:01,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:01,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:01,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:12,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jp7pw with k8s id: gxy-jp7pw succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:15:12,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:15:20,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 52 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:15:20,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'solexa_full_range_original_solexa.fastqsolexa', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/52/working/data_fetch_upload_u66t9low', 'object_id': 71}]}]}]
galaxy.jobs INFO 2025-04-07 13:15:20,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-04-07 13:15:20,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 52 executed (193.941 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:20,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:15:21,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 53
tpv.core.entities DEBUG 2025-04-07 13:15:21,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:15:21,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:15:21,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:15:21,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:15:21,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-04-07 13:15:21,524 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (44.784 ms)
galaxy.jobs.handler INFO 2025-04-07 13:15:21,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:21,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 53
galaxy.jobs DEBUG 2025-04-07 13:15:21,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [53] prepared (58.254 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:21,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:15:21,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:21,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:15:21,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/c/6/7/dataset_c67f907b-e1cf-46ce-9aa6-0d74c07f0e4c.dat' solexa '/galaxy/server/database/objects/9/d/2/dataset_9d25b1fb-454d-4ca8-87b8-ffa0a877ead0.dat' sanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:15:21,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:21,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:21,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:15:21,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:21,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:21,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:22,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:27,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-798xp with k8s id: gxy-798xp succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:15:27,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:15:35,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 53 finished
galaxy.model.metadata DEBUG 2025-04-07 13:15:35,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 72
galaxy.jobs INFO 2025-04-07 13:15:35,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs DEBUG 2025-04-07 13:15:35,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 53 executed (152.953 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:35,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:15:36,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54
tpv.core.entities DEBUG 2025-04-07 13:15:36,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:15:36,866 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:15:36,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:15:36,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:15:36,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-04-07 13:15:36,925 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (53.404 ms)
galaxy.jobs.handler INFO 2025-04-07 13:15:36,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:36,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 54
galaxy.jobs DEBUG 2025-04-07 13:15:37,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [54] prepared (88.258 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:15:37,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/54/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/54/configs/tmpqlc6lmil']
galaxy.jobs.runners DEBUG 2025-04-07 13:15:37,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:37,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:37,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:37,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:47,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s6785 with k8s id: gxy-s6785 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:15:48,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 54: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:15:56,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 54 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:15:56,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'solexa_full_range_original_solexa.fastqsolexa', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/54/working/data_fetch_upload_js8fbf2x', 'object_id': 73}]}]}]
galaxy.jobs INFO 2025-04-07 13:15:56,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 54 in /galaxy/server/database/jobs_directory/000/54
galaxy.jobs DEBUG 2025-04-07 13:15:56,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 54 executed (212.224 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:56,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:15:57,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 55
tpv.core.entities DEBUG 2025-04-07 13:15:57,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:15:57,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:15:57,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:15:57,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:15:57,419 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-04-07 13:15:57,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (45.570 ms)
galaxy.jobs.handler INFO 2025-04-07 13:15:57,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:57,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 55
galaxy.jobs DEBUG 2025-04-07 13:15:57,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [55] prepared (61.277 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:57,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:15:57,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:58,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:15:58,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/f/e/c/dataset_fec2e191-5506-4cf5-9834-a1937a461773.dat' solexa '/galaxy/server/database/objects/7/6/7/dataset_767dc7a3-49cd-42ac-bc44-fac99871b40b.dat' cssanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:15:58,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:58,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:58,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:15:58,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:15:58,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:58,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:15:58,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:03,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pfgkt with k8s id: gxy-pfgkt succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:16:03,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:16:11,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 55 finished
galaxy.model.metadata DEBUG 2025-04-07 13:16:11,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 74
galaxy.jobs INFO 2025-04-07 13:16:11,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-04-07 13:16:11,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 55 executed (169.704 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:11,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:16:12,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56
tpv.core.entities DEBUG 2025-04-07 13:16:12,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:16:12,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:16:12,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:16:12,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:16:12,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-04-07 13:16:12,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (48.114 ms)
galaxy.jobs.handler INFO 2025-04-07 13:16:12,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:12,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 56
galaxy.jobs DEBUG 2025-04-07 13:16:12,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [56] prepared (61.051 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:16:12,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/56/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/56/configs/tmp0osejnrd']
galaxy.jobs.runners DEBUG 2025-04-07 13:16:12,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:12,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:12,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:13,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:23,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q2h46 with k8s id: gxy-q2h46 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:16:23,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 56: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:16:31,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 56 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:16:31,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_cssanger.fastqcssanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/56/working/data_fetch_upload_oyqqpiqn', 'object_id': 75}]}]}]
galaxy.jobs INFO 2025-04-07 13:16:31,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 56 in /galaxy/server/database/jobs_directory/000/56
galaxy.jobs DEBUG 2025-04-07 13:16:31,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 56 executed (153.060 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:31,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:16:32,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 57
tpv.core.entities DEBUG 2025-04-07 13:16:32,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:16:32,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:16:32,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:16:32,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:16:32,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-04-07 13:16:32,269 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (46.930 ms)
galaxy.jobs.handler INFO 2025-04-07 13:16:32,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:32,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 57
galaxy.jobs DEBUG 2025-04-07 13:16:32,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [57] prepared (61.270 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:16:32,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:16:32,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:16:32,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:16:32,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/2/1/e/dataset_21ea3f29-736d-4252-8504-5863b5b3a91e.dat' cssanger '/galaxy/server/database/objects/4/a/1/dataset_4a1f5dc3-20fa-45a1-8f6c-66bf0802f766.dat' cssanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:16:32,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:32,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:16:32,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:16:32,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:16:32,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:32,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:33,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:37,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p5tgn with k8s id: gxy-p5tgn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:16:37,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 57: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:16:45,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 57 finished
galaxy.model.metadata DEBUG 2025-04-07 13:16:45,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 76
galaxy.jobs INFO 2025-04-07 13:16:45,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 57 in /galaxy/server/database/jobs_directory/000/57
galaxy.jobs DEBUG 2025-04-07 13:16:45,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 57 executed (129.058 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:45,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:16:47,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58
tpv.core.entities DEBUG 2025-04-07 13:16:47,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:16:47,610 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:16:47,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:16:47,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:16:47,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-04-07 13:16:47,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (41.115 ms)
galaxy.jobs.handler INFO 2025-04-07 13:16:47,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:47,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-04-07 13:16:47,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [58] prepared (65.241 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:16:47,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/58/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/58/configs/tmp0fq95ayh']
galaxy.jobs.runners DEBUG 2025-04-07 13:16:47,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:47,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:47,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:48,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:16:57,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b8wgr with k8s id: gxy-b8wgr succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:16:58,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:17:05,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 58 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:17:05,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_cssanger.fastqcssanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/58/working/data_fetch_upload_4c7x_u5z', 'object_id': 77}]}]}]
galaxy.jobs INFO 2025-04-07 13:17:05,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.jobs DEBUG 2025-04-07 13:17:06,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 58 executed (152.350 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:06,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:17:07,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-04-07 13:17:07,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:17:07,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:17:07,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:17:07,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:17:07,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-04-07 13:17:07,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (44.326 ms)
galaxy.jobs.handler INFO 2025-04-07 13:17:07,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:07,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-04-07 13:17:07,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [59] prepared (56.650 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:07,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:17:07,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:07,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:17:07,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/6/3/d/dataset_63d21a21-de61-4838-b08f-09062710f48a.dat' cssanger '/galaxy/server/database/objects/5/2/5/dataset_5253310d-b5c6-442c-af85-b64faf5c9a2d.dat' sanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:17:07,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:07,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:07,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:17:07,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:07,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:07,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:09,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:12,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ssm5p with k8s id: gxy-ssm5p succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:17:12,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:17:20,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 59 finished
galaxy.model.metadata DEBUG 2025-04-07 13:17:20,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.jobs INFO 2025-04-07 13:17:20,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-04-07 13:17:20,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 59 executed (160.048 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:20,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:17:21,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-04-07 13:17:21,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:17:21,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:17:21,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:17:21,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:17:21,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-04-07 13:17:21,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (59.921 ms)
galaxy.jobs.handler INFO 2025-04-07 13:17:21,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:21,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-04-07 13:17:21,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [60] prepared (66.612 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:17:21,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/60/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/60/configs/tmpxbbdgbcl']
galaxy.jobs.runners DEBUG 2025-04-07 13:17:21,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:21,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:21,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:22,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:32,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dnrcf with k8s id: gxy-dnrcf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:17:32,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:17:40,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 60 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:17:40,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_cssanger.fastqcssanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/60/working/data_fetch_upload_wc_w7x2d', 'object_id': 79}]}]}]
galaxy.jobs INFO 2025-04-07 13:17:40,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-04-07 13:17:40,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 60 executed (169.033 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:40,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:17:41,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-04-07 13:17:41,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:17:41,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:17:41,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:17:41,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:17:41,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-04-07 13:17:41,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (48.657 ms)
galaxy.jobs.handler INFO 2025-04-07 13:17:41,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:41,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-04-07 13:17:42,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [61] prepared (59.256 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:42,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:17:42,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:42,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:17:42,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/f/f/0/dataset_ff0631cc-3394-468a-b678-07db201b34e4.dat' cssanger '/galaxy/server/database/objects/9/f/b/dataset_9fb9803d-1ce6-470b-ba78-3be96bbfe0a9.dat' illumina None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:17:42,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:42,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:42,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:17:42,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:17:42,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:42,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:42,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:47,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-86h9b with k8s id: gxy-86h9b succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:17:47,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 61: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:17:56,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 61 finished
galaxy.model.metadata DEBUG 2025-04-07 13:17:56,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 80
galaxy.jobs INFO 2025-04-07 13:17:56,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 61 in /galaxy/server/database/jobs_directory/000/61
galaxy.jobs DEBUG 2025-04-07 13:17:56,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 61 executed (147.780 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:56,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:17:58,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-04-07 13:17:58,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:17:58,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:17:58,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:17:58,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:17:58,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-04-07 13:17:58,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (42.949 ms)
galaxy.jobs.handler INFO 2025-04-07 13:17:58,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:58,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-04-07 13:17:58,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [62] prepared (74.868 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:17:58,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/62/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/62/configs/tmpa4mi_zjt']
galaxy.jobs.runners DEBUG 2025-04-07 13:17:58,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:58,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:58,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:17:59,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:09,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tjb47 with k8s id: gxy-tjb47 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:18:09,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:18:18,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 62 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:18:18,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_cssanger.fastqcssanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/62/working/data_fetch_upload_ws3f6im2', 'object_id': 81}]}]}]
galaxy.jobs INFO 2025-04-07 13:18:18,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.jobs DEBUG 2025-04-07 13:18:18,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 62 executed (168.512 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:18,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:18:18,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-04-07 13:18:18,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:18:18,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:18:18,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:18:18,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:18:18,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-04-07 13:18:18,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (40.452 ms)
galaxy.jobs.handler INFO 2025-04-07 13:18:18,897 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:18,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-04-07 13:18:18,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [63] prepared (61.301 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:18,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:18:18,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:19,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:18:19,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/1/e/7/dataset_1e7a98dc-cddf-4774-8c12-f98d8eda4ea2.dat' cssanger '/galaxy/server/database/objects/d/7/e/dataset_d7e3dca9-b71f-4340-87b9-639383c3c95b.dat' solexa None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:18:19,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:19,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:19,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:18:19,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:19,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:19,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:19,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:23,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xgfmx with k8s id: gxy-xgfmx succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:18:24,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 63: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:18:33,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 63 finished
galaxy.model.metadata DEBUG 2025-04-07 13:18:33,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 82
galaxy.jobs INFO 2025-04-07 13:18:33,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 63 in /galaxy/server/database/jobs_directory/000/63
galaxy.jobs DEBUG 2025-04-07 13:18:33,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 63 executed (147.050 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:33,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:18:34,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-04-07 13:18:34,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:18:34,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:18:34,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:18:34,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:18:34,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-04-07 13:18:34,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (43.316 ms)
galaxy.jobs.handler INFO 2025-04-07 13:18:34,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:34,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-04-07 13:18:34,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [64] prepared (83.183 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:18:34,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/64/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/64/configs/tmp8wcxpda2']
galaxy.jobs.runners DEBUG 2025-04-07 13:18:34,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:34,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:34,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:34,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:45,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ph56g with k8s id: gxy-ph56g succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:18:45,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:18:54,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 64 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:18:54,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_cssanger_adapter_base_with_quality_score.fastqcssanger_fake_score', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/64/working/data_fetch_upload_qg0y38ae', 'object_id': 83}]}]}]
galaxy.jobs INFO 2025-04-07 13:18:54,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.jobs DEBUG 2025-04-07 13:18:54,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 64 executed (170.056 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:54,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:18:54,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65
tpv.core.entities DEBUG 2025-04-07 13:18:54,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:18:54,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:18:54,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:18:54,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:18:54,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-04-07 13:18:54,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (46.816 ms)
galaxy.jobs.handler INFO 2025-04-07 13:18:54,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:54,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-04-07 13:18:54,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [65] prepared (67.757 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:54,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:18:54,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:54,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:18:54,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/8/a/6/dataset_8a6dd453-f542-4e3d-a2c7-0f447b41dd9b.dat' cssanger '/galaxy/server/database/objects/e/a/8/dataset_ea8a037c-7e48-4053-84c5-90a3f8d09250.dat' cssanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:18:54,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:55,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:55,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:18:55,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:18:55,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:55,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:18:55,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:00,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gmr67 with k8s id: gxy-gmr67 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:19:00,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:19:08,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 65 finished
galaxy.model.metadata DEBUG 2025-04-07 13:19:09,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 84
galaxy.jobs INFO 2025-04-07 13:19:09,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-04-07 13:19:09,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 65 executed (153.175 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:09,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:19:11,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-04-07 13:19:11,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:19:11,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:19:11,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:19:11,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:19:11,218 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-04-07 13:19:11,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (34.822 ms)
galaxy.jobs.handler INFO 2025-04-07 13:19:11,228 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:11,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-04-07 13:19:11,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [66] prepared (88.521 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:19:11,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/66/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/66/configs/tmplsks2slg']
galaxy.jobs.runners DEBUG 2025-04-07 13:19:11,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:11,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:11,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:12,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:21,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rd4jd with k8s id: gxy-rd4jd succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:19:21,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:19:29,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 66 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:19:29,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'wrapping_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/66/working/data_fetch_upload_v18l7m4t', 'object_id': 85}]}]}]
galaxy.jobs INFO 2025-04-07 13:19:30,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.jobs DEBUG 2025-04-07 13:19:30,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 66 executed (180.074 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:30,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:19:30,683 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67
tpv.core.entities DEBUG 2025-04-07 13:19:30,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:19:30,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:19:30,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:19:30,740 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:19:30,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-04-07 13:19:30,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (44.832 ms)
galaxy.jobs.handler INFO 2025-04-07 13:19:30,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:30,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 67
galaxy.jobs DEBUG 2025-04-07 13:19:30,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [67] prepared (54.586 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:19:30,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:19:30,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:19:30,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:19:30,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/8/7/4/dataset_87471de0-fe40-4ab1-bf83-125950291a2e.dat' sanger '/galaxy/server/database/objects/5/8/7/dataset_587fb53a-a35c-4491-9d1d-390f919e9853.dat' sanger None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:19:30,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:30,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:19:30,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:19:30,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:19:30,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:31,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:31,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:35,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6n24h with k8s id: gxy-6n24h succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:19:35,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:19:44,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 67 finished
galaxy.model.metadata DEBUG 2025-04-07 13:19:44,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 86
galaxy.jobs INFO 2025-04-07 13:19:44,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs DEBUG 2025-04-07 13:19:44,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 67 executed (139.461 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:44,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:19:46,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 68
tpv.core.entities DEBUG 2025-04-07 13:19:46,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:19:46,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:19:46,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:19:46,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:19:46,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-04-07 13:19:46,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (45.352 ms)
galaxy.jobs.handler INFO 2025-04-07 13:19:46,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:46,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-04-07 13:19:46,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [68] prepared (86.689 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:19:46,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/68/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/68/configs/tmpuz4jcq63']
galaxy.jobs.runners DEBUG 2025-04-07 13:19:46,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:46,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:46,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:46,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:19:56,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qnf8f with k8s id: gxy-qnf8f succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:19:57,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 68: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:20:06,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 68 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:20:06,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'wrapping_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/68/working/data_fetch_upload_rwer1tm5', 'object_id': 87}]}]}]
galaxy.jobs INFO 2025-04-07 13:20:06,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 68 in /galaxy/server/database/jobs_directory/000/68
galaxy.jobs DEBUG 2025-04-07 13:20:06,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 68 executed (176.054 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:06,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:20:07,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69
tpv.core.entities DEBUG 2025-04-07 13:20:07,633 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:20:07,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:20:07,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:20:07,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:20:07,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-04-07 13:20:07,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (65.520 ms)
galaxy.jobs.handler INFO 2025-04-07 13:20:07,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:07,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 69
galaxy.jobs DEBUG 2025-04-07 13:20:07,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [69] prepared (55.239 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:07,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:20:07,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:07,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:20:07,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/0/a/3/dataset_0a31557a-07c1-4cae-aa02-1ae72d393a9f.dat' sanger '/galaxy/server/database/objects/9/3/9/dataset_9392cc05-e4a8-434c-a2b1-fb5f6cffc322.dat' illumina None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:20:07,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:07,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:07,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:20:07,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:07,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:07,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:09,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:13,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t6ncf with k8s id: gxy-t6ncf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:20:13,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:20:22,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 69 finished
galaxy.model.metadata DEBUG 2025-04-07 13:20:22,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 88
galaxy.jobs INFO 2025-04-07 13:20:22,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs DEBUG 2025-04-07 13:20:22,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 69 executed (152.199 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:22,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:20:24,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 70
tpv.core.entities DEBUG 2025-04-07 13:20:24,067 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:20:24,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:20:24,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:20:24,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:20:24,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-04-07 13:20:24,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (55.035 ms)
galaxy.jobs.handler INFO 2025-04-07 13:20:24,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:24,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 70
galaxy.jobs DEBUG 2025-04-07 13:20:24,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [70] prepared (91.674 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:20:24,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/70/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/70/configs/tmpdovixxjx']
galaxy.jobs.runners DEBUG 2025-04-07 13:20:24,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:24,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:24,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:25,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:35,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v8slr with k8s id: gxy-v8slr succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:20:35,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:20:43,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 70 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:20:43,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'wrapping_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/70/working/data_fetch_upload_nyiark16', 'object_id': 89}]}]}]
galaxy.jobs INFO 2025-04-07 13:20:44,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs DEBUG 2025-04-07 13:20:44,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 70 executed (157.398 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:44,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:20:44,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71
tpv.core.entities DEBUG 2025-04-07 13:20:44,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:20:44,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:20:44,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:20:44,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:20:44,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-04-07 13:20:44,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (39.310 ms)
galaxy.jobs.handler INFO 2025-04-07 13:20:44,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:44,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-04-07 13:20:44,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [71] prepared (42.258 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:44,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:20:44,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:44,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:20:44,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/e/f/8/dataset_ef8175e0-66da-4395-87e4-357ba332e3b4.dat' sanger '/galaxy/server/database/objects/8/4/4/dataset_844d4448-454d-46b6-be67-98a2eb92b741.dat' solexa None summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:20:44,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:44,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:44,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:20:44,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:20:44,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:44,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:45,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:49,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vjdnj with k8s id: gxy-vjdnj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:20:49,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 71: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:20:57,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 71 finished
galaxy.model.metadata DEBUG 2025-04-07 13:20:57,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 90
galaxy.jobs INFO 2025-04-07 13:20:57,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 71 in /galaxy/server/database/jobs_directory/000/71
galaxy.jobs DEBUG 2025-04-07 13:20:57,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 71 executed (114.050 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:20:57,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:21:05,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72
tpv.core.entities DEBUG 2025-04-07 13:21:05,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:21:05,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:21:05,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:21:05,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:21:05,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-04-07 13:21:05,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (44.931 ms)
galaxy.jobs.handler INFO 2025-04-07 13:21:05,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:05,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 72
galaxy.jobs DEBUG 2025-04-07 13:21:05,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [72] prepared (70.629 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:21:05,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/72/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/72/configs/tmpf94tcblj']
galaxy.jobs.runners DEBUG 2025-04-07 13:21:05,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:05,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:05,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:05,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:15,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n9pdx with k8s id: gxy-n9pdx succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:21:16,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:21:24,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 72 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:21:24,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_decimal_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/72/working/data_fetch_upload_hexmgofw', 'object_id': 91}]}]}]
galaxy.jobs INFO 2025-04-07 13:21:24,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs DEBUG 2025-04-07 13:21:24,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 72 executed (159.103 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:24,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:21:25,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 73
tpv.core.entities DEBUG 2025-04-07 13:21:25,561 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:21:25,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:21:25,567 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:21:25,582 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:21:25,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-04-07 13:21:25,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (48.627 ms)
galaxy.jobs.handler INFO 2025-04-07 13:21:25,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:25,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-04-07 13:21:25,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [73] prepared (57.566 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:25,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:21:25,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:25,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:21:25,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/f/8/6/dataset_f86309de-1ee8-474f-99ab-4ff4be0b622a.dat' sanger '/galaxy/server/database/objects/0/c/a/dataset_0caaf130-c35d-4205-b505-638ffce702c5.dat' sanger ascii summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:21:25,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:25,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:25,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:21:25,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:25,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:26,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:26,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:30,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dbmcg with k8s id: gxy-dbmcg succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:21:30,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:21:38,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 73 finished
galaxy.model.metadata DEBUG 2025-04-07 13:21:38,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 92
galaxy.jobs INFO 2025-04-07 13:21:38,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs DEBUG 2025-04-07 13:21:38,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 73 executed (160.069 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:38,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:21:39,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-04-07 13:21:39,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:21:39,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:21:39,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:21:39,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:21:39,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-04-07 13:21:39,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (44.439 ms)
galaxy.jobs.handler INFO 2025-04-07 13:21:39,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:39,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-04-07 13:21:40,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [74] prepared (87.881 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:21:40,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/74/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/74/configs/tmptlzje7fa']
galaxy.jobs.runners DEBUG 2025-04-07 13:21:40,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:40,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:40,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:41,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:50,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t8ht2 with k8s id: gxy-t8ht2 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:21:50,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 74: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:21:58,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 74 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:21:58,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/74/working/data_fetch_upload_sp2pmxqj', 'object_id': 93}]}]}]
galaxy.jobs INFO 2025-04-07 13:21:58,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 74 in /galaxy/server/database/jobs_directory/000/74
galaxy.jobs DEBUG 2025-04-07 13:21:58,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 74 executed (163.565 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:58,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:21:59,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 75
tpv.core.entities DEBUG 2025-04-07 13:21:59,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:21:59,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:21:59,500 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:21:59,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:21:59,532 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-04-07 13:21:59,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (45.795 ms)
galaxy.jobs.handler INFO 2025-04-07 13:21:59,548 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:59,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 75
galaxy.jobs DEBUG 2025-04-07 13:21:59,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [75] prepared (56.275 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:59,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:21:59,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:59,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:21:59,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/0/5/8/dataset_0589bf6c-628e-47f6-951d-dc89de0d901b.dat' sanger '/galaxy/server/database/objects/b/6/d/dataset_b6d40a98-362b-4c42-8abe-9e942dcd05d8.dat' sanger decimal summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:21:59,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:59,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:59,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:21:59,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:21:59,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:21:59,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:00,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:04,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gpdwn with k8s id: gxy-gpdwn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:22:04,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:22:12,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 75 finished
galaxy.model.metadata DEBUG 2025-04-07 13:22:12,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 94
galaxy.jobs INFO 2025-04-07 13:22:12,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-04-07 13:22:12,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 75 executed (125.517 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:12,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:22:13,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76
tpv.core.entities DEBUG 2025-04-07 13:22:13,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:22:13,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:22:13,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:22:13,875 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:22:13,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-04-07 13:22:13,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (44.059 ms)
galaxy.jobs.handler INFO 2025-04-07 13:22:13,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:13,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-04-07 13:22:14,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [76] prepared (120.168 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:22:14,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/76/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/76/configs/tmplbm9se_a']
galaxy.jobs.runners DEBUG 2025-04-07 13:22:14,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:14,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:14,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:14,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:24,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pjbfw with k8s id: gxy-pjbfw succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:22:24,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:22:32,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 76 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:22:32,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_tab_decimal_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/76/working/data_fetch_upload_yeoprrur', 'object_id': 95}]}]}]
galaxy.jobs INFO 2025-04-07 13:22:32,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.jobs DEBUG 2025-04-07 13:22:32,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 76 executed (153.387 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:32,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:22:33,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-04-07 13:22:33,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:22:33,301 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:22:33,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:22:33,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:22:33,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-04-07 13:22:33,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (45.825 ms)
galaxy.jobs.handler INFO 2025-04-07 13:22:33,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:33,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-04-07 13:22:33,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [77] prepared (59.558 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:22:33,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:22:33,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:22:33,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:22:33,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/b/6/2/dataset_b623a447-3558-4672-87f5-e71da1393436.dat' sanger '/galaxy/server/database/objects/2/4/1/dataset_241ff307-2e81-40c1-8a79-fcfc57823d25.dat' sanger ascii summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:22:33,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:33,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:22:33,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:22:33,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:22:33,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:33,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:33,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:37,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zdzsv with k8s id: gxy-zdzsv succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:22:37,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:22:45,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 77 finished
galaxy.model.metadata DEBUG 2025-04-07 13:22:45,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 96
galaxy.jobs INFO 2025-04-07 13:22:45,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-04-07 13:22:45,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 77 executed (129.543 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:45,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:22:47,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 78
tpv.core.entities DEBUG 2025-04-07 13:22:47,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:22:47,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:22:47,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:22:47,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:22:47,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-04-07 13:22:47,740 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (45.998 ms)
galaxy.jobs.handler INFO 2025-04-07 13:22:47,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:47,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 78
galaxy.jobs DEBUG 2025-04-07 13:22:47,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [78] prepared (76.922 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:22:47,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/78/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/78/configs/tmphh5q0x8h']
galaxy.jobs.runners DEBUG 2025-04-07 13:22:47,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:47,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:47,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:48,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:22:58,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mz9sg with k8s id: gxy-mz9sg succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:22:58,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:23:05,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 78 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:23:05,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'solexa_full_range_as_decimal_solexa.fastqsolexa', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/78/working/data_fetch_upload_m_pi4v_b', 'object_id': 97}]}]}]
galaxy.jobs INFO 2025-04-07 13:23:06,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.jobs DEBUG 2025-04-07 13:23:06,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 78 executed (150.550 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:06,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:23:07,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79
tpv.core.entities DEBUG 2025-04-07 13:23:07,209 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:23:07,210 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:23:07,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:23:07,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:23:07,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-04-07 13:23:07,259 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (43.008 ms)
galaxy.jobs.handler INFO 2025-04-07 13:23:07,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:07,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-04-07 13:23:07,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [79] prepared (53.794 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:07,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:23:07,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:07,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:23:07,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/9/2/2/dataset_922612c3-e7f7-4e5f-8bd3-a38dbc634428.dat' solexa '/galaxy/server/database/objects/5/4/c/dataset_54c2a790-3bc7-4f3c-8928-0a0496cd3c4e.dat' solexa ascii summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:23:07,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:07,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:07,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:23:07,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:07,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:07,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:09,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:13,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tblqj with k8s id: gxy-tblqj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:23:13,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:23:21,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 79 finished
galaxy.model.metadata DEBUG 2025-04-07 13:23:21,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 98
galaxy.jobs INFO 2025-04-07 13:23:21,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-04-07 13:23:21,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 79 executed (143.692 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:21,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:23:22,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 80
tpv.core.entities DEBUG 2025-04-07 13:23:22,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:23:22,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:23:22,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:23:22,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:23:22,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-04-07 13:23:22,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (43.040 ms)
galaxy.jobs.handler INFO 2025-04-07 13:23:22,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:22,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 80
galaxy.jobs DEBUG 2025-04-07 13:23:22,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [80] prepared (82.100 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:23:22,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/80/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/80/configs/tmpwfbydljg']
galaxy.jobs.runners DEBUG 2025-04-07 13:23:22,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:22,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:22,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:23,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:33,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5bjls with k8s id: gxy-5bjls succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:23:33,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:23:41,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 80 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:23:41,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'solexa_full_range_original_solexa.fastqsolexa', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/80/working/data_fetch_upload_9yvz6ewp', 'object_id': 99}]}]}]
galaxy.jobs INFO 2025-04-07 13:23:41,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-04-07 13:23:41,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 80 executed (132.047 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:41,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:23:42,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81
tpv.core.entities DEBUG 2025-04-07 13:23:42,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:23:42,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:23:42,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:23:42,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:23:42,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-04-07 13:23:42,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (30.656 ms)
galaxy.jobs.handler INFO 2025-04-07 13:23:42,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:42,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-04-07 13:23:42,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [81] prepared (54.221 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:42,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:23:42,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:42,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:23:42,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/f/d/f/dataset_fdfafa91-09b9-49e5-b9b0-e0ef2d0292cd.dat' solexa '/galaxy/server/database/objects/c/c/c/dataset_cccdccbc-177a-4c2e-9f54-2b710cd13e50.dat' solexa decimal summarize_input --fix-id]
galaxy.jobs.runners DEBUG 2025-04-07 13:23:42,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:42,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:42,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:23:42,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:23:42,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:42,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:42,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:46,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2qgbp with k8s id: gxy-2qgbp succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:23:46,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:23:54,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 81 finished
galaxy.model.metadata DEBUG 2025-04-07 13:23:54,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 100
galaxy.jobs INFO 2025-04-07 13:23:54,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-04-07 13:23:54,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 81 executed (113.065 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:55,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:23:56,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-04-07 13:23:56,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:23:56,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:23:56,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:23:56,403 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:23:56,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-04-07 13:23:56,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (39.803 ms)
galaxy.jobs.handler INFO 2025-04-07 13:23:56,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:56,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-04-07 13:23:56,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [82] prepared (74.247 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:23:56,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/82/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/82/configs/tmpevdbka1s']
galaxy.jobs.runners DEBUG 2025-04-07 13:23:56,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:56,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:56,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:23:56,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:06,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hxcrb with k8s id: gxy-hxcrb succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:24:07,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 82: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:24:15,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 82 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:24:15,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger.gz', 'dbkey': '?', 'ext': 'fastq.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/82/working/gxupload_0', 'object_id': 101}]}]}]
galaxy.jobs INFO 2025-04-07 13:24:15,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 82 in /galaxy/server/database/jobs_directory/000/82
galaxy.jobs DEBUG 2025-04-07 13:24:15,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 82 executed (141.923 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:15,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:24:15,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83
tpv.core.entities DEBUG 2025-04-07 13:24:15,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:24:15,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:24:15,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:24:15,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:24:15,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-04-07 13:24:15,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (64.824 ms)
galaxy.jobs.handler INFO 2025-04-07 13:24:15,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:15,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 83
galaxy.jobs DEBUG 2025-04-07 13:24:15,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [83] prepared (51.719 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:15,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:24:15,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:16,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:24:16,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/d/9/2/dataset_d92d1ca9-9c62-4247-b3c8-e012a9e00db1.dat' sanger.gz '/galaxy/server/database/objects/4/0/4/dataset_404a4f3c-cb7f-4d5a-8cb7-95b8638f2b11.dat' sanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:24:16,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:16,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:16,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:24:16,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:16,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:16,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:16,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:21,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v79hz with k8s id: gxy-v79hz succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:24:21,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:24:29,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 83 finished
galaxy.model.metadata DEBUG 2025-04-07 13:24:29,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 102
galaxy.jobs INFO 2025-04-07 13:24:29,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.jobs DEBUG 2025-04-07 13:24:29,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 83 executed (128.038 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:29,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:24:31,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 84
tpv.core.entities DEBUG 2025-04-07 13:24:31,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:24:31,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:24:31,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:24:31,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:24:31,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-04-07 13:24:31,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (38.141 ms)
galaxy.jobs.handler INFO 2025-04-07 13:24:31,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:31,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-04-07 13:24:31,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [84] prepared (78.500 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:24:31,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/84/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/84/configs/tmpzpgdr5ny']
galaxy.jobs.runners DEBUG 2025-04-07 13:24:31,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:31,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:31,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:32,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:42,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lqm7r with k8s id: gxy-lqm7r succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:24:42,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:24:50,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 84 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:24:50,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_as_cssanger.fastqcssanger.bz2', 'dbkey': '?', 'ext': 'fastq.bz2', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq.bz2 file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/84/working/gxupload_0', 'object_id': 103}]}]}]
galaxy.jobs INFO 2025-04-07 13:24:50,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-04-07 13:24:50,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 84 executed (211.115 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:50,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:24:51,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-04-07 13:24:51,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:24:51,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:24:51,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:24:51,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:24:51,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-04-07 13:24:51,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (34.958 ms)
galaxy.jobs.handler INFO 2025-04-07 13:24:51,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:51,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-04-07 13:24:51,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [85] prepared (53.352 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:51,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:24:51,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:51,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:24:51,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [gx-fastq-groomer '/galaxy/server/database/objects/0/7/7/dataset_07762e40-5f7f-4f6e-91de-7a74203b70e6.dat' cssanger.bz2 '/galaxy/server/database/objects/5/d/5/dataset_5d5169ff-cb07-4b84-8c40-c2322e057d09.dat' cssanger ascii summarize_input]
galaxy.jobs.runners DEBUG 2025-04-07 13:24:51,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:51,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:51,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:24:51,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/fastq_groomer/fastq_groomer/1.1.5+galaxy2: galaxy_sequence_utils:1.1.5
galaxy.tool_util.deps.containers INFO 2025-04-07 13:24:52,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/galaxy_sequence_utils:1.1.5--pyhdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:52,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:52,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:24:56,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8t4mt with k8s id: gxy-8t4mt succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:24:56,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:25:05,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 85 finished
galaxy.model.metadata DEBUG 2025-04-07 13:25:05,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 104
galaxy.jobs INFO 2025-04-07 13:25:05,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-04-07 13:25:05,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 85 executed (166.635 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:05,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:25:08,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87, 86
tpv.core.entities DEBUG 2025-04-07 13:25:08,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:25:08,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:25:08,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:25:08,218 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:25:08,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-04-07 13:25:08,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (42.720 ms)
galaxy.jobs.handler INFO 2025-04-07 13:25:08,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 86
tpv.core.entities DEBUG 2025-04-07 13:25:08,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:25:08,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:25:08,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:25:08,288 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:25:08,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-04-07 13:25:08,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (49.973 ms)
galaxy.jobs.handler INFO 2025-04-07 13:25:08,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-04-07 13:25:08,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [86] prepared (87.360 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:25:08,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/86/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/86/configs/tmpeuyg_cpq']
galaxy.jobs.runners DEBUG 2025-04-07 13:25:08,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:25:08,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [87] prepared (88.725 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:25:08,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/87/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/87/configs/tmpwbjd0ajh']
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:25:08,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:08,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:19,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5gv7v with k8s id: gxy-5gv7v succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:25:19,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:20,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-l8fpd with k8s id: gxy-l8fpd succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:25:20,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:25:28,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 86 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:25:28,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reheader_input_header.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/86/working/gxupload_0', 'object_id': 105}]}]}]
galaxy.jobs INFO 2025-04-07 13:25:28,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.jobs DEBUG 2025-04-07 13:25:28,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 86 executed (172.882 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:28,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:25:29,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 87 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:25:29,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reheader_input_file.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/87/working/gxupload_0', 'object_id': 106}]}]}]
galaxy.jobs INFO 2025-04-07 13:25:29,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs DEBUG 2025-04-07 13:25:29,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 87 executed (201.443 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:29,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:25:30,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-04-07 13:25:30,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:25:30,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:25:30,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:25:30,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:25:31,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-04-07 13:25:31,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (47.020 ms)
galaxy.jobs.handler INFO 2025-04-07 13:25:31,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:31,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-04-07 13:25:31,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [88] prepared (61.159 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:25:31,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:25:31,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_reheader/samtools_reheader/2.0.3: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-07 13:25:31,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:25:31,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/88/outputs/COMMAND_VERSION 2>&1;
samtools reheader '/galaxy/server/database/objects/5/e/f/dataset_5ef1ab50-de3d-4ed5-b464-d45efb3fa868.dat' '/galaxy/server/database/objects/f/e/7/dataset_fe7461ca-d588-44e6-a7b1-66fc904368ff.dat'  > '/galaxy/server/database/objects/6/7/3/dataset_6733773d-264d-425a-9ca6-f0ba9d66aada.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:25:31,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:31,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:25:31,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:25:31,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_reheader/samtools_reheader/2.0.3: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-07 13:25:31,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:31,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:32,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:41,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9wrvm with k8s id: gxy-9wrvm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:25:41,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:25:49,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 88 finished
galaxy.model.metadata DEBUG 2025-04-07 13:25:49,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 107
galaxy.jobs INFO 2025-04-07 13:25:49,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-04-07 13:25:49,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 88 executed (172.169 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:49,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:25:51,599 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90, 89
tpv.core.entities DEBUG 2025-04-07 13:25:51,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:25:51,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:25:51,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:25:51,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:25:51,666 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-04-07 13:25:51,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (37.149 ms)
galaxy.jobs.handler INFO 2025-04-07 13:25:51,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:51,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 89
tpv.core.entities DEBUG 2025-04-07 13:25:51,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:25:51,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:25:51,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:25:51,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:25:51,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-04-07 13:25:51,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (50.100 ms)
galaxy.jobs.handler INFO 2025-04-07 13:25:51,747 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:51,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-04-07 13:25:51,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [89] prepared (89.489 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:25:51,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/89/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/89/configs/tmp61tu6l1r']
galaxy.jobs.runners DEBUG 2025-04-07 13:25:51,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:51,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:25:51,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [90] prepared (83.092 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:51,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:25:51,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/90/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/90/configs/tmpoyt73eml']
galaxy.jobs.runners DEBUG 2025-04-07 13:25:51,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:51,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:51,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:52,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:25:53,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:03,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qn4dl with k8s id: gxy-qn4dl succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:03,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sc65n with k8s id: gxy-sc65n succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:26:03,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:26:03,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:26:12,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 89 finished
galaxy.jobs.runners DEBUG 2025-04-07 13:26:12,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 90 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:26:12,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reheader_input_header.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/89/working/gxupload_0', 'object_id': 108}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:26:12,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reheader_input_file.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/90/working/gxupload_0', 'object_id': 109}]}]}]
galaxy.jobs INFO 2025-04-07 13:26:12,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs INFO 2025-04-07 13:26:12,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-04-07 13:26:12,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 89 executed (222.129 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:12,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:26:12,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 90 executed (205.430 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:12,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:26:13,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-04-07 13:26:13,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:26:13,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:26:13,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:26:13,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:26:13,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-04-07 13:26:13,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (48.383 ms)
galaxy.jobs.handler INFO 2025-04-07 13:26:13,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:13,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-04-07 13:26:13,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [91] prepared (49.387 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:13,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:26:13,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_reheader/samtools_reheader/2.0.3: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:13,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:26:13,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/91/outputs/COMMAND_VERSION 2>&1;
samtools reheader '/galaxy/server/database/objects/c/4/a/dataset_c4a3d72f-8662-4b99-badf-283102ebb1d7.dat' '/galaxy/server/database/objects/b/6/9/dataset_b69290da-b955-4d74-89a3-3722f0c9c039.dat' --no-PG > '/galaxy/server/database/objects/7/3/b/dataset_73b4da4e-0a3f-4306-a4fd-53d45c63863a.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:26:13,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:13,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:13,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:26:13,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_reheader/samtools_reheader/2.0.3: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:13,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:13,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:14,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:18,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xhfr7 with k8s id: gxy-xhfr7 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:26:18,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:26:26,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 91 finished
galaxy.model.metadata DEBUG 2025-04-07 13:26:27,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 110
galaxy.jobs INFO 2025-04-07 13:26:27,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-04-07 13:26:27,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 91 executed (162.525 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:27,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:26:30,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92, 93
tpv.core.entities DEBUG 2025-04-07 13:26:30,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:26:30,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:26:30,699 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:26:30,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:26:30,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-04-07 13:26:30,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (43.327 ms)
galaxy.jobs.handler INFO 2025-04-07 13:26:30,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:30,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 92
tpv.core.entities DEBUG 2025-04-07 13:26:30,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:26:30,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:26:30,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:26:30,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:26:30,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-04-07 13:26:30,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (55.332 ms)
galaxy.jobs.handler INFO 2025-04-07 13:26:30,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:30,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 93
galaxy.jobs DEBUG 2025-04-07 13:26:30,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [92] prepared (91.734 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:26:30,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/92/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/92/configs/tmpy5ieeeqi']
galaxy.jobs.runners DEBUG 2025-04-07 13:26:30,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:30,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:26:30,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [93] prepared (84.001 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:30,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:26:30,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/93/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/93/configs/tmpah3m35ol']
galaxy.jobs.runners DEBUG 2025-04-07 13:26:30,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:30,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:30,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:31,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:31,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:41,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-84vcq with k8s id: gxy-84vcq succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:26:42,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:42,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v4xvc with k8s id: gxy-v4xvc succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:26:43,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:26:50,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 92 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:26:50,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/92/working/data_fetch_upload_if4vw9u9', 'object_id': 111}]}]}]
galaxy.jobs INFO 2025-04-07 13:26:50,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs DEBUG 2025-04-07 13:26:50,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 92 executed (195.972 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:50,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:26:51,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 93 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:26:51,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/93/working/gxupload_0', 'object_id': 112}]}]}]
galaxy.jobs INFO 2025-04-07 13:26:51,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs DEBUG 2025-04-07 13:26:51,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 93 executed (199.866 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:51,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:26:52,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94
tpv.core.entities DEBUG 2025-04-07 13:26:52,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:26:52,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:26:52,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:26:52,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:26:52,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-04-07 13:26:52,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (46.525 ms)
galaxy.jobs.handler INFO 2025-04-07 13:26:52,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:52,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-04-07 13:26:52,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [94] prepared (84.280 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:52,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:26:52,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:52,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:26:52,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/94/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/d/8/6/dataset_d862deed-31ef-4574-9f07-4cea53ca96f3.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/d/c/3/metadata_dc38ef6f-e332-45a0-8e01-665668ab43e4.dat' '0.bai' &&   samtools bedcov    -g 0   -G 1796 '/galaxy/server/database/objects/5/4/5/dataset_54567bf2-140e-4e69-8a8e-ca4d5d130783.dat' '0' > '/galaxy/server/database/objects/a/2/b/dataset_a2b3a929-0fe4-4e10-b731-45cb11b879a0.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:26:52,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:52,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:52,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:26:52,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:26:52,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:52,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:26:54,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:01,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-txdkf with k8s id: gxy-txdkf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:27:01,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:27:09,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 94 finished
galaxy.model.metadata DEBUG 2025-04-07 13:27:09,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 113
galaxy.jobs INFO 2025-04-07 13:27:09,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.jobs DEBUG 2025-04-07 13:27:09,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 94 executed (146.238 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:09,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:27:10,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95
tpv.core.entities DEBUG 2025-04-07 13:27:10,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:27:10,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:27:10,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:27:10,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:27:10,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-04-07 13:27:10,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (49.340 ms)
galaxy.jobs.handler INFO 2025-04-07 13:27:10,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:10,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 95
galaxy.jobs DEBUG 2025-04-07 13:27:10,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [95] prepared (83.931 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:27:10,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/95/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/95/configs/tmpgm6uvzts']
galaxy.jobs.runners DEBUG 2025-04-07 13:27:10,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:10,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:11,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:11,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-04-07 13:27:11,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97, 96
tpv.core.entities DEBUG 2025-04-07 13:27:11,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:27:11,866 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:27:11,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:27:11,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:27:11,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-04-07 13:27:11,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (43.612 ms)
galaxy.jobs.handler INFO 2025-04-07 13:27:11,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:11,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 96
tpv.core.entities DEBUG 2025-04-07 13:27:11,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:27:11,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:27:11,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:27:11,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:27:11,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-04-07 13:27:11,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (54.486 ms)
galaxy.jobs.handler INFO 2025-04-07 13:27:11,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:11,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-04-07 13:27:12,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [96] prepared (87.682 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:27:12,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/96/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/96/configs/tmpg5tplovs']
galaxy.jobs.runners DEBUG 2025-04-07 13:27:12,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:12,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:27:12,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [97] prepared (86.337 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:12,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:27:12,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/97/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/97/configs/tmp3i9t5kap']
galaxy.jobs.runners DEBUG 2025-04-07 13:27:12,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:12,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:12,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:12,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:12,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:21,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hl542 with k8s id: gxy-hl542 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:27:21,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:23,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hqt6z with k8s id: gxy-hqt6z succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:23,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qhht2 with k8s id: gxy-qhht2 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:27:24,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:27:24,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:27:33,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 95 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:27:33,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/95/working/data_fetch_upload_n42m31mr', 'object_id': 114}]}]}]
galaxy.jobs INFO 2025-04-07 13:27:34,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-04-07 13:27:34,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 95 executed (220.505 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:34,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:27:35,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 97 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:27:35,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/97/working/gxupload_0', 'object_id': 116}]}]}]
galaxy.jobs.runners DEBUG 2025-04-07 13:27:35,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 96 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:27:35,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/96/working/gxupload_0', 'object_id': 115}]}]}]
galaxy.jobs INFO 2025-04-07 13:27:35,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs INFO 2025-04-07 13:27:35,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs DEBUG 2025-04-07 13:27:35,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 97 executed (213.482 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:35,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:27:36,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 96 executed (198.988 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:36,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:27:36,599 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98
tpv.core.entities DEBUG 2025-04-07 13:27:36,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:27:36,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:27:36,647 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:27:36,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:27:36,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-04-07 13:27:36,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (52.691 ms)
galaxy.jobs.handler INFO 2025-04-07 13:27:36,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:36,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 98
galaxy.jobs DEBUG 2025-04-07 13:27:36,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [98] prepared (72.130 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:27:36,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:27:36,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:27:36,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:27:36,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/98/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/6/a/8/dataset_6a888b14-b1e3-4d1a-a021-5437d632b33e.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/b/0/5/metadata_b050d52b-50f6-4e4e-af53-890c9a1b3d14.dat' '0.bai' && ln -s '/galaxy/server/database/objects/7/3/3/dataset_733efb6a-fa78-4cd9-8ca9-f4197781368f.dat' '1' && ln -s '/galaxy/server/database/objects/_metadata_files/1/2/0/metadata_120615d0-5481-4b03-ad97-ff15cb6bb1b5.dat' '1.bai' &&   samtools bedcov    -g 0   -G 1796 '/galaxy/server/database/objects/1/f/1/dataset_1f1568f9-4e3e-410d-85be-eea33d35d785.dat' '0' '1' > '/galaxy/server/database/objects/4/8/7/dataset_48746399-24c9-464e-8832-87f12339774a.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:27:36,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:36,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:27:36,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:27:36,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:27:36,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:36,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:37,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:41,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-47vx4 with k8s id: gxy-47vx4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:27:41,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:27:49,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 98 finished
galaxy.model.metadata DEBUG 2025-04-07 13:27:49,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 117
galaxy.jobs INFO 2025-04-07 13:27:49,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs DEBUG 2025-04-07 13:27:49,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 98 executed (138.427 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:49,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:27:50,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100, 99
tpv.core.entities DEBUG 2025-04-07 13:27:51,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:27:51,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:27:51,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:27:51,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:27:51,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-04-07 13:27:51,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (44.242 ms)
galaxy.jobs.handler INFO 2025-04-07 13:27:51,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:51,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 99
tpv.core.entities DEBUG 2025-04-07 13:27:51,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:27:51,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:27:51,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:27:51,094 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:27:51,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-04-07 13:27:51,134 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (55.832 ms)
galaxy.jobs.handler INFO 2025-04-07 13:27:51,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:51,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-04-07 13:27:51,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [99] prepared (98.807 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:27:51,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/99/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/99/configs/tmpjhivrn45']
galaxy.jobs.runners DEBUG 2025-04-07 13:27:51,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:27:51,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [100] prepared (89.413 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:51,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:51,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:27:51,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/100/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/100/configs/tmpk24f3771']
galaxy.jobs.runners DEBUG 2025-04-07 13:27:51,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:51,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:51,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:52,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:27:52,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:01,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4rfz9 with k8s id: gxy-4rfz9 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:28:01,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:02,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-stdcf with k8s id: gxy-stdcf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:28:02,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:28:09,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 99 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:28:09,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/99/working/data_fetch_upload_wlf7hlwn', 'object_id': 118}]}]}]
galaxy.jobs INFO 2025-04-07 13:28:09,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs DEBUG 2025-04-07 13:28:10,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 99 executed (165.124 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:10,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:28:11,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 100 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:28:11,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/100/working/gxupload_0', 'object_id': 119}]}]}]
galaxy.jobs INFO 2025-04-07 13:28:11,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-04-07 13:28:11,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 100 executed (211.715 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:11,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:28:11,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 101
tpv.core.entities DEBUG 2025-04-07 13:28:11,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:28:11,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:28:11,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:28:11,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:28:11,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-04-07 13:28:11,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (52.901 ms)
galaxy.jobs.handler INFO 2025-04-07 13:28:11,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:11,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 101
galaxy.jobs DEBUG 2025-04-07 13:28:11,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [101] prepared (70.279 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:28:11,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:28:11,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:28:11,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:28:11,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/101/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/5/4/9/dataset_549385ca-0b97-4110-bc6d-224a08957c19.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/4/2/2/metadata_422813ff-2cc2-43ca-bf99-681f625f9013.dat' '0.bai' &&   samtools bedcov -Q 40    -g 0   -G 1796 '/galaxy/server/database/objects/d/a/6/dataset_da62307b-63c9-49d5-9382-689a28de6917.dat' '0' > '/galaxy/server/database/objects/8/4/f/dataset_84f9342a-0fcf-4138-9644-e45c7bf3c0e4.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:28:11,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:11,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:28:11,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:28:11,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:28:11,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:12,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:12,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:16,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vz4c7 with k8s id: gxy-vz4c7 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:28:16,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:28:24,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 101 finished
galaxy.model.metadata DEBUG 2025-04-07 13:28:24,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 120
galaxy.jobs INFO 2025-04-07 13:28:24,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-04-07 13:28:24,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 101 executed (142.937 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:24,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:28:27,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 103, 104, 102
tpv.core.entities DEBUG 2025-04-07 13:28:27,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:28:27,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:28:27,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:28:27,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:28:27,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-04-07 13:28:27,170 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (43.740 ms)
galaxy.jobs.handler INFO 2025-04-07 13:28:27,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 102
tpv.core.entities DEBUG 2025-04-07 13:28:27,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:28:27,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:28:27,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:28:27,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:28:27,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-04-07 13:28:27,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (56.142 ms)
galaxy.jobs.handler INFO 2025-04-07 13:28:27,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 103
tpv.core.entities DEBUG 2025-04-07 13:28:27,269 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:28:27,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:28:27,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:28:27,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [102] prepared (105.432 ms)
galaxy.jobs DEBUG 2025-04-07 13:28:27,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:28:27,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Working directory for job is: /galaxy/server/database/jobs_directory/000/104
galaxy.jobs.command_factory INFO 2025-04-07 13:28:27,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/102/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/102/configs/tmpgzy9imve']
galaxy.jobs.runners DEBUG 2025-04-07 13:28:27,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [104] queued (61.888 ms)
galaxy.jobs.handler INFO 2025-04-07 13:28:27,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 104
galaxy.jobs.runners DEBUG 2025-04-07 13:28:27,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:28:27,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [103] prepared (114.886 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:28:27,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/103/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/103/configs/tmpob5k3p4y']
galaxy.jobs.runners DEBUG 2025-04-07 13:28:27,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:28:27,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [104] prepared (105.100 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:28:27,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/104/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/104/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/104/configs/tmpoa7qiqiy']
galaxy.jobs.runners DEBUG 2025-04-07 13:28:27,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (104) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/104/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/104/galaxy_104.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:27,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-frkhz failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-frkhz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:28:38,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-frkhz

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-frkhz": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102/gxy-frkhz) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102/gxy-frkhz) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102/gxy-frkhz) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102/gxy-frkhz) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-frkhz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:38,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 102 (gxy-frkhz)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q4h7b failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-frkhz to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-q4h7b.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (102/gxy-frkhz) Terminated at user's request
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:28:39,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-q4h7b

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-q4h7b": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qsv9k with k8s id: gxy-qsv9k succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (104/gxy-q4h7b) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (104/gxy-q4h7b) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (104/gxy-q4h7b) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (104/gxy-q4h7b) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-q4h7b.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 104 (gxy-q4h7b)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-q4h7b to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:39,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (104/gxy-q4h7b) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-04-07 13:28:39,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-04-07 13:28:40,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 106, 105
tpv.core.entities DEBUG 2025-04-07 13:28:40,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:28:40,666 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:28:40,671 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:28:40,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:28:40,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Working directory for job is: /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-04-07 13:28:40,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [105] queued (43.576 ms)
galaxy.jobs.handler INFO 2025-04-07 13:28:40,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:40,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 105
tpv.core.entities DEBUG 2025-04-07 13:28:40,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:28:40,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:28:40,755 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:28:40,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:28:40,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Working directory for job is: /galaxy/server/database/jobs_directory/000/106
galaxy.jobs.runners DEBUG 2025-04-07 13:28:40,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [106] queued (53.538 ms)
galaxy.jobs.handler INFO 2025-04-07 13:28:40,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:40,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 106
galaxy.jobs DEBUG 2025-04-07 13:28:40,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [105] prepared (100.453 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:28:40,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/105/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/105/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/105/configs/tmpi9rwxktu']
galaxy.jobs.runners DEBUG 2025-04-07 13:28:40,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (105) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/105/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/105/galaxy_105.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:28:40,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [106] prepared (84.730 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:40,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:40,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:28:40,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/106/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/106/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/106/configs/tmpeftmt246']
galaxy.jobs.runners DEBUG 2025-04-07 13:28:40,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (106) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/106/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/106/galaxy_106.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:40,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:40,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:41,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:41,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-04-07 13:28:47,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 103 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:28:47,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/103/working/gxupload_0', 'object_id': 122}]}]}]
galaxy.jobs INFO 2025-04-07 13:28:47,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs DEBUG 2025-04-07 13:28:48,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 103 executed (197.062 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:48,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:52,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5x7nv with k8s id: gxy-5x7nv succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:28:52,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8zcm4 with k8s id: gxy-8zcm4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:28:52,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 105: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:28:52,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 106: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:29:00,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 105 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:29:00,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/105/working/data_fetch_upload_p3onkit2', 'object_id': 124}]}]}]
galaxy.jobs INFO 2025-04-07 13:29:00,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 105 in /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-04-07 13:29:00,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 106 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:29:00,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/106/working/gxupload_0', 'object_id': 125}]}]}]
galaxy.jobs DEBUG 2025-04-07 13:29:00,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 105 executed (187.926 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:00,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-04-07 13:29:00,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 106 in /galaxy/server/database/jobs_directory/000/106
galaxy.jobs DEBUG 2025-04-07 13:29:00,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 106 executed (257.625 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:00,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:29:01,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 107
tpv.core.entities DEBUG 2025-04-07 13:29:01,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:29:01,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:29:01,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:29:01,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:29:01,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Working directory for job is: /galaxy/server/database/jobs_directory/000/107
galaxy.jobs.runners DEBUG 2025-04-07 13:29:01,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [107] queued (41.965 ms)
galaxy.jobs.handler INFO 2025-04-07 13:29:01,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:01,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 107
galaxy.jobs DEBUG 2025-04-07 13:29:01,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [107] prepared (55.609 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:01,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:29:01,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:01,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:29:01,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/107/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/107/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/2/0/b/dataset_20b90060-c27c-4160-bd81-1530370525a5.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/6/6/0/metadata_6602b916-1cf3-4bc9-af7b-2dbecf0092d2.dat' '0.bai' &&   samtools bedcov    -g 82   -G 1796 '/galaxy/server/database/objects/b/4/4/dataset_b44c0b2e-d40f-4890-9cdd-9c4818f2d3e2.dat' '0' > '/galaxy/server/database/objects/b/2/5/dataset_b250d785-cf50-44f8-866e-8a1ac78c8b48.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:29:01,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (107) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/107/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/107/galaxy_107.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:01,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:01,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:29:01,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:01,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:01,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:02,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:06,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dn5mn with k8s id: gxy-dn5mn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:29:06,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 107: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:29:14,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 107 finished
galaxy.model.metadata DEBUG 2025-04-07 13:29:14,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 126
galaxy.jobs INFO 2025-04-07 13:29:14,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 107 in /galaxy/server/database/jobs_directory/000/107
galaxy.jobs DEBUG 2025-04-07 13:29:14,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 107 executed (144.584 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:14,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:29:15,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 109, 108
tpv.core.entities DEBUG 2025-04-07 13:29:15,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:29:15,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:29:15,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:29:15,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:29:15,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Working directory for job is: /galaxy/server/database/jobs_directory/000/108
galaxy.jobs.runners DEBUG 2025-04-07 13:29:15,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [108] queued (39.315 ms)
galaxy.jobs.handler INFO 2025-04-07 13:29:15,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:15,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 108
tpv.core.entities DEBUG 2025-04-07 13:29:15,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:29:15,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:29:15,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:29:15,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:29:15,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Working directory for job is: /galaxy/server/database/jobs_directory/000/109
galaxy.jobs.runners DEBUG 2025-04-07 13:29:15,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [109] queued (65.011 ms)
galaxy.jobs.handler INFO 2025-04-07 13:29:15,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:15,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 109
galaxy.jobs DEBUG 2025-04-07 13:29:15,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [108] prepared (97.260 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:29:15,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/108/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/108/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/108/configs/tmpvere6978']
galaxy.jobs.runners DEBUG 2025-04-07 13:29:16,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (108) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/108/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/108/galaxy_108.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:16,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:29:16,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [109] prepared (91.807 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:16,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:29:16,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/109/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/109/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/109/configs/tmpxi5dry9w']
galaxy.jobs.runners DEBUG 2025-04-07 13:29:16,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (109) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/109/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/109/galaxy_109.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:16,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:16,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:16,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:17,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:26,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vvx8z with k8s id: gxy-vvx8z succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:29:26,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 108: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:27,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-66c52 with k8s id: gxy-66c52 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:29:27,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 109: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:29:35,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 108 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:29:35,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/108/working/data_fetch_upload_waljbga_', 'object_id': 127}]}]}]
galaxy.jobs INFO 2025-04-07 13:29:35,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 108 in /galaxy/server/database/jobs_directory/000/108
galaxy.jobs DEBUG 2025-04-07 13:29:35,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 108 executed (142.560 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:35,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:29:35,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 109 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:29:35,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/109/working/gxupload_0', 'object_id': 128}]}]}]
galaxy.jobs INFO 2025-04-07 13:29:36,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 109 in /galaxy/server/database/jobs_directory/000/109
galaxy.jobs DEBUG 2025-04-07 13:29:36,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 109 executed (156.007 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:36,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:29:36,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 110
tpv.core.entities DEBUG 2025-04-07 13:29:36,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:29:36,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:29:36,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:29:36,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:29:36,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Working directory for job is: /galaxy/server/database/jobs_directory/000/110
galaxy.jobs.runners DEBUG 2025-04-07 13:29:36,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [110] queued (40.127 ms)
galaxy.jobs.handler INFO 2025-04-07 13:29:36,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:36,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 110
galaxy.jobs DEBUG 2025-04-07 13:29:36,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [110] prepared (69.273 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:36,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:29:36,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:36,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:29:36,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/110/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/110/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/6/e/8/dataset_6e81b170-9cec-42a1-a179-35371f0714a3.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/9/a/5/metadata_9a56e155-96c2-469d-9101-bc0982bd11df.dat' '0.bai' &&   samtools bedcov    -g 0   -G 82 '/galaxy/server/database/objects/5/3/d/dataset_53dc13dc-f35f-476e-8524-112673d73793.dat' '0' > '/galaxy/server/database/objects/6/e/4/dataset_6e47e55d-9f2d-44e4-8ebb-a2a324277bbd.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:29:36,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (110) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/110/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/110/galaxy_110.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:36,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:36,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:29:36,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:29:36,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:36,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:37,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:40,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r4v4r with k8s id: gxy-r4v4r succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:29:41,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 110: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:29:49,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 110 finished
galaxy.model.metadata DEBUG 2025-04-07 13:29:49,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 129
galaxy.jobs INFO 2025-04-07 13:29:49,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 110 in /galaxy/server/database/jobs_directory/000/110
galaxy.jobs DEBUG 2025-04-07 13:29:49,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 110 executed (132.050 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:49,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:29:50,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 112, 111
tpv.core.entities DEBUG 2025-04-07 13:29:50,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:29:50,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:29:50,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:29:50,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:29:50,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Working directory for job is: /galaxy/server/database/jobs_directory/000/111
galaxy.jobs.runners DEBUG 2025-04-07 13:29:50,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [111] queued (38.274 ms)
galaxy.jobs.handler INFO 2025-04-07 13:29:50,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:50,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 111
tpv.core.entities DEBUG 2025-04-07 13:29:50,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:29:50,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:29:50,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:29:50,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:29:50,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Working directory for job is: /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-04-07 13:29:50,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [112] queued (53.327 ms)
galaxy.jobs.handler INFO 2025-04-07 13:29:50,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:50,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 112
galaxy.jobs DEBUG 2025-04-07 13:29:50,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [111] prepared (85.849 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:29:50,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/111/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/111/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/111/configs/tmpdku8t54k']
galaxy.jobs.runners DEBUG 2025-04-07 13:29:50,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (111) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/111/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/111/galaxy_111.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:50,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:29:50,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [112] prepared (89.687 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:50,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:29:50,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/112/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/112/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/112/configs/tmpwyq279c7']
galaxy.jobs.runners DEBUG 2025-04-07 13:29:50,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (112) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/112/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/112/galaxy_112.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:51,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:51,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:52,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:29:52,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:01,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4spsj with k8s id: gxy-4spsj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:01,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xzpgx with k8s id: gxy-xzpgx succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:30:01,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 111: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:30:01,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 112: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:30:10,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 111 finished
galaxy.jobs.runners DEBUG 2025-04-07 13:30:10,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 112 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:30:10,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/111/working/data_fetch_upload_ivairorr', 'object_id': 130}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:30:10,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/112/working/gxupload_0', 'object_id': 131}]}]}]
galaxy.jobs INFO 2025-04-07 13:30:10,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.jobs INFO 2025-04-07 13:30:10,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.jobs DEBUG 2025-04-07 13:30:10,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 111 executed (293.686 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:10,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:30:10,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 112 executed (245.888 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:10,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:30:11,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 113
tpv.core.entities DEBUG 2025-04-07 13:30:11,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:30:11,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:30:11,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:30:11,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:30:11,387 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Working directory for job is: /galaxy/server/database/jobs_directory/000/113
galaxy.jobs.runners DEBUG 2025-04-07 13:30:11,399 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [113] queued (56.226 ms)
galaxy.jobs.handler INFO 2025-04-07 13:30:11,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:11,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 113
galaxy.jobs DEBUG 2025-04-07 13:30:11,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [113] prepared (72.573 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:11,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:30:11,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:11,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:30:11,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/113/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/113/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/e/c/4/dataset_ec41c3b6-c39f-4430-beaa-a98e06ff5a17.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/1/1/a/metadata_11a7844a-cb8a-4af5-989c-52d48893572f.dat' '0.bai' &&   samtools bedcov    -g 0   -G 1796 -d 10 '/galaxy/server/database/objects/f/3/7/dataset_f37903a1-1959-4a3c-8c3c-00cea343b22f.dat' '0' > '/galaxy/server/database/objects/a/d/b/dataset_adb41e20-256c-4b6e-b1a4-ffc2a1c6c0a5.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:30:11,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (113) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/113/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/113/galaxy_113.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:11,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:11,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:30:11,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_bedcov/samtools_bedcov/2.0.4: samtools:1.15.1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:11,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.15.1--h6899075_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:11,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:12,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:16,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b92g9 with k8s id: gxy-b92g9 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:30:16,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 113: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:30:25,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 113 finished
galaxy.model.metadata DEBUG 2025-04-07 13:30:25,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 132
galaxy.jobs INFO 2025-04-07 13:30:25,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 113 in /galaxy/server/database/jobs_directory/000/113
galaxy.jobs DEBUG 2025-04-07 13:30:25,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 113 executed (132.193 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:25,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:30:28,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 114
tpv.core.entities DEBUG 2025-04-07 13:30:28,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:30:28,873 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:30:28,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:30:28,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:30:28,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Working directory for job is: /galaxy/server/database/jobs_directory/000/114
galaxy.jobs.runners DEBUG 2025-04-07 13:30:28,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [114] queued (43.145 ms)
galaxy.jobs.handler INFO 2025-04-07 13:30:28,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:28,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 114
galaxy.jobs DEBUG 2025-04-07 13:30:29,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [114] prepared (68.029 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:30:29,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/114/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/114/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/114/configs/tmpqe_ap0x5']
galaxy.jobs.runners DEBUG 2025-04-07 13:30:29,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (114) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/114/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/114/galaxy_114.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:29,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:29,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:29,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:39,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkmfg with k8s id: gxy-hkmfg succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:30:40,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 114: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:30:48,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 114 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:30:48,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/114/working/data_fetch_upload_uoz0hqui', 'object_id': 133}]}]}]
galaxy.jobs INFO 2025-04-07 13:30:48,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 114 in /galaxy/server/database/jobs_directory/000/114
galaxy.jobs DEBUG 2025-04-07 13:30:48,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 114 executed (175.007 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:48,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:30:49,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 115
tpv.core.entities DEBUG 2025-04-07 13:30:49,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:30:49,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:30:49,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:30:49,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:30:49,427 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Working directory for job is: /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-04-07 13:30:49,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [115] queued (59.394 ms)
galaxy.jobs.handler INFO 2025-04-07 13:30:49,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:49,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 115
galaxy.jobs DEBUG 2025-04-07 13:30:49,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [115] prepared (105.448 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:49,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:30:49,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:49,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:30:49,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/115/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/115/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/5/8/e/dataset_58e09e12-697d-4747-b389-768b281ffc9f.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GT --gensample "/galaxy/server/database/objects/1/9/d/dataset_19dcd1a7-f63f-461f-b868-93ace8971820.dat,/galaxy/server/database/objects/b/9/a/dataset_b9ab63f4-7b4d-436e-bd9b-61499898aeae.dat"                      "/galaxy/server/database/objects/5/8/e/dataset_58e09e12-697d-4747-b389-768b281ffc9f.dat" .]
galaxy.jobs.runners DEBUG 2025-04-07 13:30:49,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (115) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/115/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/115/galaxy_115.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:49,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:49,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:30:49,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:30:49,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:49,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:30:51,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:00,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nkdzc with k8s id: gxy-nkdzc succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:31:00,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 115: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:31:08,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 115 finished
galaxy.model.metadata DEBUG 2025-04-07 13:31:08,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.model.metadata DEBUG 2025-04-07 13:31:08,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 135
galaxy.jobs INFO 2025-04-07 13:31:08,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs DEBUG 2025-04-07 13:31:08,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 115 executed (116.819 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:08,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:31:09,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 116
tpv.core.entities DEBUG 2025-04-07 13:31:09,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:31:09,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:31:09,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:31:09,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:31:09,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Working directory for job is: /galaxy/server/database/jobs_directory/000/116
galaxy.jobs.runners DEBUG 2025-04-07 13:31:09,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [116] queued (43.476 ms)
galaxy.jobs.handler INFO 2025-04-07 13:31:09,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:09,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 116
galaxy.jobs DEBUG 2025-04-07 13:31:10,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [116] prepared (78.737 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:31:10,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/116/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/116/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/116/configs/tmp5ansp8ef']
galaxy.jobs.runners DEBUG 2025-04-07 13:31:10,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (116) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/116/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/116/galaxy_116.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:10,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:10,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:10,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:20,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tgp8l with k8s id: gxy-tgp8l succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:31:20,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 116: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:31:28,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 116 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:31:28,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/116/working/data_fetch_upload_3z_qe_vw', 'object_id': 136}]}]}]
galaxy.jobs INFO 2025-04-07 13:31:28,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 116 in /galaxy/server/database/jobs_directory/000/116
galaxy.jobs DEBUG 2025-04-07 13:31:28,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 116 executed (190.507 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:28,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:31:29,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 117
tpv.core.entities DEBUG 2025-04-07 13:31:29,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:31:29,332 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:31:29,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:31:29,351 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:31:29,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Working directory for job is: /galaxy/server/database/jobs_directory/000/117
galaxy.jobs.runners DEBUG 2025-04-07 13:31:29,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [117] queued (49.496 ms)
galaxy.jobs.handler INFO 2025-04-07 13:31:29,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:29,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 117
galaxy.jobs DEBUG 2025-04-07 13:31:29,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [117] prepared (73.784 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:31:29,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:31:29,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:31:29,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:31:29,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/117/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/117/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/0/3/d/dataset_03d1cfa5-3fbf-4b0f-a196-1508c9a7f458.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GP --3N6 --gensample "/galaxy/server/database/objects/6/c/4/dataset_6c4ce07b-c556-4079-9b99-041335506e56.dat,/galaxy/server/database/objects/a/5/5/dataset_a559a33a-15ed-4a95-ae3b-011ceb92169e.dat"                      "/galaxy/server/database/objects/0/3/d/dataset_03d1cfa5-3fbf-4b0f-a196-1508c9a7f458.dat" .]
galaxy.jobs.runners DEBUG 2025-04-07 13:31:29,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (117) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/117/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/117/galaxy_117.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:29,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:31:29,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:31:29,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:31:29,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:29,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:30,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:34,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9gnqj with k8s id: gxy-9gnqj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:31:34,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 117: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:31:43,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 117 finished
galaxy.model.metadata DEBUG 2025-04-07 13:31:43,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 137
galaxy.model.metadata DEBUG 2025-04-07 13:31:43,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 138
galaxy.jobs INFO 2025-04-07 13:31:43,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 117 in /galaxy/server/database/jobs_directory/000/117
galaxy.jobs DEBUG 2025-04-07 13:31:43,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 117 executed (137.991 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:43,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:31:44,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 118
tpv.core.entities DEBUG 2025-04-07 13:31:44,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:31:44,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:31:44,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:31:44,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:31:44,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Working directory for job is: /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-04-07 13:31:44,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [118] queued (44.123 ms)
galaxy.jobs.handler INFO 2025-04-07 13:31:44,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:44,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 118
galaxy.jobs DEBUG 2025-04-07 13:31:44,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [118] prepared (78.275 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:31:45,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/118/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/118/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/118/configs/tmpy438imje']
galaxy.jobs.runners DEBUG 2025-04-07 13:31:45,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (118) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/118/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/118/galaxy_118.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:45,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:45,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:45,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:31:55,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ddbbj with k8s id: gxy-ddbbj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:31:56,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 118: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:32:03,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 118 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:32:03,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'check.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/118/working/data_fetch_upload_hn79zzdn', 'object_id': 139}]}]}]
galaxy.jobs INFO 2025-04-07 13:32:03,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 118 in /galaxy/server/database/jobs_directory/000/118
galaxy.jobs DEBUG 2025-04-07 13:32:03,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 118 executed (171.536 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:03,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:32:04,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 119
tpv.core.entities DEBUG 2025-04-07 13:32:04,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:32:04,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:32:04,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:32:04,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:32:04,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Working directory for job is: /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners DEBUG 2025-04-07 13:32:04,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [119] queued (44.786 ms)
galaxy.jobs.handler INFO 2025-04-07 13:32:04,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:04,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 119
galaxy.jobs DEBUG 2025-04-07 13:32:04,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [119] prepared (69.372 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:04,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:32:04,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:04,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:32:04,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/119/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/119/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/5/5/4/dataset_55452681-7178-473d-b289-68eed559d3a9.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GT --3N6 --vcf-ids --gensample "/galaxy/server/database/objects/3/3/2/dataset_3326f147-ddd9-4cf0-a9e4-b8bb2124054a.dat,/galaxy/server/database/objects/5/7/a/dataset_57a286c4-7f1a-410a-9cca-593220519543.dat"                      "/galaxy/server/database/objects/5/5/4/dataset_55452681-7178-473d-b289-68eed559d3a9.dat" .]
galaxy.jobs.runners DEBUG 2025-04-07 13:32:04,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (119) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/119/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/119/galaxy_119.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:04,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:04,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:32:04,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:04,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:04,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:04,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:10,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9gqmh with k8s id: gxy-9gqmh succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:32:10,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:32:18,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 119 finished
galaxy.model.metadata DEBUG 2025-04-07 13:32:18,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 140
galaxy.model.metadata DEBUG 2025-04-07 13:32:18,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.jobs INFO 2025-04-07 13:32:18,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs DEBUG 2025-04-07 13:32:18,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 119 executed (122.251 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:18,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:32:19,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 120
tpv.core.entities DEBUG 2025-04-07 13:32:19,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:32:19,666 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:32:19,671 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:32:19,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:32:19,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Working directory for job is: /galaxy/server/database/jobs_directory/000/120
galaxy.jobs.runners DEBUG 2025-04-07 13:32:19,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [120] queued (44.205 ms)
galaxy.jobs.handler INFO 2025-04-07 13:32:19,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:19,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 120
galaxy.jobs DEBUG 2025-04-07 13:32:19,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [120] prepared (73.912 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:32:19,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/120/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/120/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/120/configs/tmpz9whlvh1']
galaxy.jobs.runners DEBUG 2025-04-07 13:32:19,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/120/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/120/galaxy_120.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:19,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:19,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:20,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:30,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gprvm with k8s id: gxy-gprvm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:32:30,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 120: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:32:38,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 120 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:32:38,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/120/working/data_fetch_upload_6jqcjcse', 'object_id': 142}]}]}]
galaxy.jobs INFO 2025-04-07 13:32:38,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 120 in /galaxy/server/database/jobs_directory/000/120
galaxy.jobs DEBUG 2025-04-07 13:32:38,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 120 executed (169.917 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:38,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:32:39,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 121
tpv.core.entities DEBUG 2025-04-07 13:32:39,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:32:39,113 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:32:39,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:32:39,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:32:39,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Working directory for job is: /galaxy/server/database/jobs_directory/000/121
galaxy.jobs.runners DEBUG 2025-04-07 13:32:39,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [121] queued (40.531 ms)
galaxy.jobs.handler INFO 2025-04-07 13:32:39,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:39,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 121
galaxy.jobs DEBUG 2025-04-07 13:32:39,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [121] prepared (69.496 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:39,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:32:39,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:39,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:32:39,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/121/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/121/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/c/a/6/dataset_ca692be7-2453-4398-89b9-8b8a70b41796.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert   --hapsample "/galaxy/server/database/objects/b/b/4/dataset_bb4a0ad1-0707-4a5d-ae53-7d8ee110ac46.dat,/galaxy/server/database/objects/9/3/d/dataset_93d81d30-15d0-42e1-aa75-493fa680d818.dat"                      "/galaxy/server/database/objects/c/a/6/dataset_ca692be7-2453-4398-89b9-8b8a70b41796.dat" .]
galaxy.jobs.runners DEBUG 2025-04-07 13:32:39,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (121) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/121/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/121/galaxy_121.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:39,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:39,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:32:39,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:32:39,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:39,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:40,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:44,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8s9sn with k8s id: gxy-8s9sn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:32:44,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 121: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:32:52,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 121 finished
galaxy.model.metadata DEBUG 2025-04-07 13:32:52,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 143
galaxy.model.metadata DEBUG 2025-04-07 13:32:52,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 144
galaxy.jobs INFO 2025-04-07 13:32:52,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 121 in /galaxy/server/database/jobs_directory/000/121
galaxy.jobs DEBUG 2025-04-07 13:32:52,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 121 executed (119.204 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:52,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:32:53,582 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 122
tpv.core.entities DEBUG 2025-04-07 13:32:53,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:32:53,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:32:53,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:32:53,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:32:53,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Working directory for job is: /galaxy/server/database/jobs_directory/000/122
galaxy.jobs.runners DEBUG 2025-04-07 13:32:53,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [122] queued (34.399 ms)
galaxy.jobs.handler INFO 2025-04-07 13:32:53,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:53,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 122
galaxy.jobs DEBUG 2025-04-07 13:32:53,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [122] prepared (72.528 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:32:53,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/122/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/122/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/122/configs/tmp0z5ski9f']
galaxy.jobs.runners DEBUG 2025-04-07 13:32:53,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (122) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/122/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/122/galaxy_122.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:53,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:53,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:32:54,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:03,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dmwcc with k8s id: gxy-dmwcc succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:33:03,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 122: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:33:12,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 122 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:33:12,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/122/working/data_fetch_upload_vgdkkj1r', 'object_id': 145}]}]}]
galaxy.jobs INFO 2025-04-07 13:33:12,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 122 in /galaxy/server/database/jobs_directory/000/122
galaxy.jobs DEBUG 2025-04-07 13:33:12,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 122 executed (152.308 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:12,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:33:13,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 123
tpv.core.entities DEBUG 2025-04-07 13:33:13,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:33:13,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:33:13,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:33:13,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:33:13,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Working directory for job is: /galaxy/server/database/jobs_directory/000/123
galaxy.jobs.runners DEBUG 2025-04-07 13:33:13,108 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [123] queued (53.502 ms)
galaxy.jobs.handler INFO 2025-04-07 13:33:13,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:13,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 123
galaxy.jobs DEBUG 2025-04-07 13:33:13,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [123] prepared (71.629 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:13,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:33:13,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:13,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:33:13,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/123/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/123/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/e/1/d/dataset_e1d0b8b5-6e90-49f3-b4bb-1b360726236e.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert   --haplegendsample "/galaxy/server/database/objects/f/8/a/dataset_f8a26adb-a79c-41bb-a1c2-72a8e5f6ec7f.dat,/galaxy/server/database/objects/f/1/e/dataset_f1ee3878-7469-4fec-a7e7-128d0c958ffd.dat,/galaxy/server/database/objects/3/a/b/dataset_3abac534-34a7-4822-8760-235b5691df46.dat"                      "/galaxy/server/database/objects/e/1/d/dataset_e1d0b8b5-6e90-49f3-b4bb-1b360726236e.dat" .]
galaxy.jobs.runners DEBUG 2025-04-07 13:33:13,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (123) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/123/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/123/galaxy_123.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:13,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:13,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:33:13,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:13,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:13,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:13,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:17,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b9mxs with k8s id: gxy-b9mxs succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:33:18,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 123: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:33:26,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 123 finished
galaxy.model.metadata DEBUG 2025-04-07 13:33:26,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 146
galaxy.model.metadata DEBUG 2025-04-07 13:33:26,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 147
galaxy.model.metadata DEBUG 2025-04-07 13:33:26,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 148
galaxy.jobs INFO 2025-04-07 13:33:26,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 123 in /galaxy/server/database/jobs_directory/000/123
galaxy.jobs DEBUG 2025-04-07 13:33:26,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 123 executed (169.274 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:26,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:33:28,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 124
tpv.core.entities DEBUG 2025-04-07 13:33:28,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:33:28,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:33:28,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:33:28,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:33:28,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Working directory for job is: /galaxy/server/database/jobs_directory/000/124
galaxy.jobs.runners DEBUG 2025-04-07 13:33:28,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [124] queued (38.178 ms)
galaxy.jobs.handler INFO 2025-04-07 13:33:28,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:28,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 124
galaxy.jobs DEBUG 2025-04-07 13:33:28,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [124] prepared (88.231 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:33:28,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/124/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/124/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/124/configs/tmpv5elwl8g']
galaxy.jobs.runners DEBUG 2025-04-07 13:33:28,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (124) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/124/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/124/galaxy_124.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:28,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:28,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:28,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-frx52 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-frx52.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:33:38,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-frx52

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-frx52": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (124/gxy-frx52) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (124/gxy-frx52) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (124/gxy-frx52) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (124/gxy-frx52) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-frx52.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 124 (gxy-frx52)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-frx52 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:38,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (124/gxy-frx52) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-07 13:33:39,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 125
tpv.core.entities DEBUG 2025-04-07 13:33:39,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:33:39,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:33:39,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:33:39,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:33:39,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Working directory for job is: /galaxy/server/database/jobs_directory/000/125
galaxy.jobs.runners DEBUG 2025-04-07 13:33:39,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [125] queued (41.827 ms)
galaxy.jobs.handler INFO 2025-04-07 13:33:39,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:39,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 125
galaxy.jobs DEBUG 2025-04-07 13:33:39,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [125] prepared (73.457 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:33:39,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/125/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/125/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/125/configs/tmpr63qb1nj']
galaxy.jobs.runners DEBUG 2025-04-07 13:33:39,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (125) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/125/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/125/galaxy_125.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:39,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:39,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:40,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:50,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b6jgf with k8s id: gxy-b6jgf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:33:50,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 125: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:33:58,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 125 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:33:58,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/125/working/data_fetch_upload_9cs5nclk', 'object_id': 150}]}]}]
galaxy.jobs INFO 2025-04-07 13:33:58,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 125 in /galaxy/server/database/jobs_directory/000/125
galaxy.jobs DEBUG 2025-04-07 13:33:58,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 125 executed (158.379 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:58,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:33:59,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 126
tpv.core.entities DEBUG 2025-04-07 13:33:59,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:33:59,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:33:59,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:33:59,195 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:33:59,218 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Working directory for job is: /galaxy/server/database/jobs_directory/000/126
galaxy.jobs.runners DEBUG 2025-04-07 13:33:59,227 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [126] queued (45.574 ms)
galaxy.jobs.handler INFO 2025-04-07 13:33:59,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:59,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 126
galaxy.jobs DEBUG 2025-04-07 13:33:59,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [126] prepared (54.631 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:59,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:33:59,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:59,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:33:59,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/126/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/126/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/d/a/d/dataset_dadf622e-fca4-4a98-a60f-06f723f45cab.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GT --gensample "/galaxy/server/database/objects/2/c/8/dataset_2c8aa696-66a0-4e67-916e-a33347530472.dat,/galaxy/server/database/objects/e/2/c/dataset_e2cd3d31-0f91-40d5-842f-86b41c6188ea.dat"           --regions-overlap 1            "/galaxy/server/database/objects/d/a/d/dataset_dadf622e-fca4-4a98-a60f-06f723f45cab.dat" .]
galaxy.jobs.runners DEBUG 2025-04-07 13:33:59,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (126) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/126/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/126/galaxy_126.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:59,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:59,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:33:59,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:33:59,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:59,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:33:59,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8r62w failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8r62w.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:34:04,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-8r62w

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-8r62w": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (126/gxy-8r62w) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (126/gxy-8r62w) tool_stderr: Gen file: /galaxy/server/database/objects/2/c/8/dataset_2c8aa696-66a0-4e67-916e-a33347530472.dat
Sample file: /galaxy/server/database/objects/e/2/c/dataset_e2cd3d31-0f91-40d5-842f-86b41c6188ea.dat
Warning: non-biallelic records are skipped. Consider splitting multi-allelic records into biallelic records using 'bcftools norm -m-'.
32 records written, 1 skipped: 0/1/0/0 no-ALT/non-biallelic/filtered/duplicated

galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (126/gxy-8r62w) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (126/gxy-8r62w) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8r62w.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 126 (gxy-8r62w)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-8r62w to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:04,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (126/gxy-8r62w) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-07 13:34:07,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 127
tpv.core.entities DEBUG 2025-04-07 13:34:07,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:34:07,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:34:07,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:34:07,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:34:07,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Working directory for job is: /galaxy/server/database/jobs_directory/000/127
galaxy.jobs.runners DEBUG 2025-04-07 13:34:07,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [127] queued (44.232 ms)
galaxy.jobs.handler INFO 2025-04-07 13:34:07,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:07,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 127
galaxy.jobs DEBUG 2025-04-07 13:34:07,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [127] prepared (79.677 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:34:07,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/127/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/127/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/127/configs/tmp07x0rx_u']
galaxy.jobs.runners DEBUG 2025-04-07 13:34:07,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (127) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/127/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/127/galaxy_127.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:07,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:07,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:08,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:18,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6fbnj with k8s id: gxy-6fbnj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:34:18,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 127: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:34:26,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 127 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:34:26,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'plugin1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/127/working/data_fetch_upload_1dmwsf5x', 'object_id': 153}]}]}]
galaxy.jobs INFO 2025-04-07 13:34:26,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 127 in /galaxy/server/database/jobs_directory/000/127
galaxy.jobs DEBUG 2025-04-07 13:34:26,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 127 executed (164.186 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:27,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:34:28,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 128
tpv.core.entities DEBUG 2025-04-07 13:34:28,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:34:28,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:34:28,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:34:28,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:34:28,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Working directory for job is: /galaxy/server/database/jobs_directory/000/128
galaxy.jobs.runners DEBUG 2025-04-07 13:34:28,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [128] queued (45.626 ms)
galaxy.jobs.handler INFO 2025-04-07 13:34:28,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:28,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 128
galaxy.jobs DEBUG 2025-04-07 13:34:28,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [128] prepared (79.068 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:28,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:34:28,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_dosage/bcftools_plugin_dosage/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:28,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:34:28,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/128/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/128/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/7/5/dataset_375722e4-8aa5-41ae-9854-6350f7823323.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools plugin dosage                 input.vcf.gz   > '/galaxy/server/database/objects/f/f/8/dataset_ff83e320-27c5-461c-bf7c-7becffb1523b.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:34:28,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (128) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/128/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/128/galaxy_128.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:28,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:28,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:34:28,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_dosage/bcftools_plugin_dosage/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:28,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:28,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:28,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:32,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-44bkk failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:32,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:32,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-44bkk.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:32,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:34:33,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-44bkk

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-44bkk": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (128/gxy-44bkk) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (128/gxy-44bkk) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (128/gxy-44bkk) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (128/gxy-44bkk) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-44bkk.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 128 (gxy-44bkk)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-44bkk to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:33,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (128/gxy-44bkk) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-07 13:34:37,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 130, 129
tpv.core.entities DEBUG 2025-04-07 13:34:37,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:34:37,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:34:37,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:34:37,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:34:37,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Working directory for job is: /galaxy/server/database/jobs_directory/000/129
galaxy.jobs.runners DEBUG 2025-04-07 13:34:37,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [129] queued (40.723 ms)
galaxy.jobs.handler INFO 2025-04-07 13:34:37,403 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:37,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 129
tpv.core.entities DEBUG 2025-04-07 13:34:37,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:34:37,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:34:37,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:34:37,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:34:37,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Working directory for job is: /galaxy/server/database/jobs_directory/000/130
galaxy.jobs.runners DEBUG 2025-04-07 13:34:37,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [130] queued (51.442 ms)
galaxy.jobs.handler INFO 2025-04-07 13:34:37,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:37,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 130
galaxy.jobs DEBUG 2025-04-07 13:34:37,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [129] prepared (85.912 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:34:37,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/129/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/129/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/129/configs/tmpf3kknaby']
galaxy.jobs.runners DEBUG 2025-04-07 13:34:37,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (129) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/129/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/129/galaxy_129.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:37,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:34:37,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [130] prepared (81.647 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:37,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:34:37,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/130/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/130/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/130/configs/tmp4s5mzq61']
galaxy.jobs.runners DEBUG 2025-04-07 13:34:37,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (130) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/130/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/130/galaxy_130.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:37,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:37,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:38,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:38,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:48,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-x9zxx with k8s id: gxy-x9zxx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:48,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-g5txn with k8s id: gxy-g5txn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:34:48,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 130: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:34:48,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 129: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:34:57,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 129 finished
galaxy.jobs.runners DEBUG 2025-04-07 13:34:57,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 130 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:34:57,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'slice_in.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/129/working/data_fetch_upload_eqxd6s1r', 'object_id': 155}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:34:57,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'regions.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/130/working/data_fetch_upload_rj66f1gp', 'object_id': 156}]}]}]
galaxy.jobs INFO 2025-04-07 13:34:57,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 129 in /galaxy/server/database/jobs_directory/000/129
galaxy.jobs INFO 2025-04-07 13:34:57,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 130 in /galaxy/server/database/jobs_directory/000/130
galaxy.jobs DEBUG 2025-04-07 13:34:57,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 129 executed (195.271 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:57,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:34:57,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 130 executed (179.158 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:57,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:34:57,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 131
tpv.core.entities DEBUG 2025-04-07 13:34:57,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:34:57,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:34:57,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:34:58,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:34:58,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Working directory for job is: /galaxy/server/database/jobs_directory/000/131
galaxy.jobs.runners DEBUG 2025-04-07 13:34:58,031 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [131] queued (37.595 ms)
galaxy.jobs.handler INFO 2025-04-07 13:34:58,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:58,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 131
galaxy.jobs DEBUG 2025-04-07 13:34:58,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [131] prepared (60.583 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:58,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:34:58,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcftools_slice/vcftools_slice/0.1: vcftools:0.1.11
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:58,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcftools:0.1.11--2,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:34:58,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/131/tool_script.sh] for tool command [vcf-sort /galaxy/server/database/objects/2/2/e/dataset_22eb7900-4f4b-408e-b936-ea2854825f30.dat > sorted.vcf  ; echo '#dummy header' | cat - /galaxy/server/database/objects/8/8/1/dataset_881a3f4b-fdf4-4929-8bce-79d9be35023a.dat > regions.bed  ; vcftools --vcf sorted.vcf --out output --bed regions.bed --recode]
galaxy.jobs.runners DEBUG 2025-04-07 13:34:58,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (131) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/131/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/131/galaxy_131.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/131/working/output.recode.vcf" -a -f "/galaxy/server/database/objects/2/c/0/dataset_2c0bed09-24d6-4911-8337-286d082f94a2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/131/working/output.recode.vcf" "/galaxy/server/database/objects/2/c/0/dataset_2c0bed09-24d6-4911-8337-286d082f94a2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:58,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:58,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:34:58,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcftools_slice/vcftools_slice/0.1: vcftools:0.1.11
galaxy.tool_util.deps.containers INFO 2025-04-07 13:34:58,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcftools:0.1.11--2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:58,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:34:58,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:09,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9rsvw with k8s id: gxy-9rsvw succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:35:09,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 131: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:35:17,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 131 finished
galaxy.tool_util.output_checker INFO 2025-04-07 13:35:17,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job failed because of contents in the standard error stream: [/galaxy/server/database/jobs_directory/000/131/tool_script.sh: line 22: vcf-sort: command not found
]
galaxy.jobs DEBUG 2025-04-07 13:35:17,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (131) setting dataset 157 state to ERROR
galaxy.jobs INFO 2025-04-07 13:35:17,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 131 in /galaxy/server/database/jobs_directory/000/131
galaxy.jobs DEBUG 2025-04-07 13:35:17,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 131 executed (175.310 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:17,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:35:20,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 132
tpv.core.entities DEBUG 2025-04-07 13:35:20,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:35:20,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:35:20,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:35:20,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:35:20,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Working directory for job is: /galaxy/server/database/jobs_directory/000/132
galaxy.jobs.runners DEBUG 2025-04-07 13:35:20,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [132] queued (55.051 ms)
galaxy.jobs.handler INFO 2025-04-07 13:35:20,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:20,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 132
galaxy.jobs DEBUG 2025-04-07 13:35:20,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [132] prepared (80.397 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:35:20,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/132/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/132/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/132/configs/tmp0wp4403c']
galaxy.jobs.runners DEBUG 2025-04-07 13:35:20,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (132) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/132/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/132/galaxy_132.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:20,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:20,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:21,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:31,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tnbp4 with k8s id: gxy-tnbp4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:35:31,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 132: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:35:39,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 132 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:35:39,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/132/working/gxupload_0', 'object_id': 158}]}]}]
galaxy.jobs INFO 2025-04-07 13:35:39,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 132 in /galaxy/server/database/jobs_directory/000/132
galaxy.jobs DEBUG 2025-04-07 13:35:39,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 132 executed (359.016 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:39,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:35:39,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 133
tpv.core.entities DEBUG 2025-04-07 13:35:39,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:35:39,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:35:39,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:35:40,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:35:40,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Working directory for job is: /galaxy/server/database/jobs_directory/000/133
galaxy.jobs.runners DEBUG 2025-04-07 13:35:40,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [133] queued (50.313 ms)
galaxy.jobs.handler INFO 2025-04-07 13:35:40,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:40,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 133
galaxy.jobs DEBUG 2025-04-07 13:35:40,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [133] prepared (56.340 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:35:40,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:35:40,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:35:40,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:35:40,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/133/tool_script.sh] for tool command [poretools winner --type fwd '/galaxy/server/database/objects/5/c/a/dataset_5ca961d3-e991-47a7-896e-7eb01e958d9b.dat' > '/galaxy/server/database/objects/3/4/b/dataset_34b3a96b-c919-4bea-8556-2f5cde7c7db1.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:35:40,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (133) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/133/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/133/galaxy_133.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:40,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:35:40,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:35:40,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:35:40,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:40,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:35:41,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:20,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rftrm with k8s id: gxy-rftrm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:36:20,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 133: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:36:29,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 133 finished
galaxy.model.metadata DEBUG 2025-04-07 13:36:29,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 159
galaxy.jobs INFO 2025-04-07 13:36:29,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 133 in /galaxy/server/database/jobs_directory/000/133
galaxy.jobs DEBUG 2025-04-07 13:36:29,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 133 executed (184.571 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:29,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:36:31,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 134
tpv.core.entities DEBUG 2025-04-07 13:36:31,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:36:31,169 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:36:31,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:36:31,195 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:36:31,227 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Working directory for job is: /galaxy/server/database/jobs_directory/000/134
galaxy.jobs.runners DEBUG 2025-04-07 13:36:31,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [134] queued (62.395 ms)
galaxy.jobs.handler INFO 2025-04-07 13:36:31,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:31,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 134
galaxy.jobs DEBUG 2025-04-07 13:36:31,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [134] prepared (103.093 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:36:31,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/134/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/134/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/134/configs/tmplf9jgboo']
galaxy.jobs.runners DEBUG 2025-04-07 13:36:31,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (134) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/134/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/134/galaxy_134.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:31,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:31,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:32,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:42,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gr4tg with k8s id: gxy-gr4tg succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:36:42,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 134: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:36:51,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 134 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:36:51,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/134/working/gxupload_0', 'object_id': 160}]}]}]
galaxy.jobs INFO 2025-04-07 13:36:52,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 134 in /galaxy/server/database/jobs_directory/000/134
galaxy.jobs DEBUG 2025-04-07 13:36:52,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 134 executed (690.731 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:52,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:36:52,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 135
tpv.core.entities DEBUG 2025-04-07 13:36:52,875 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:36:52,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:36:52,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:36:52,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:36:52,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Working directory for job is: /galaxy/server/database/jobs_directory/000/135
galaxy.jobs.runners DEBUG 2025-04-07 13:36:52,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [135] queued (56.044 ms)
galaxy.jobs.handler INFO 2025-04-07 13:36:52,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:52,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 135
galaxy.jobs DEBUG 2025-04-07 13:36:53,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [135] prepared (59.748 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:36:53,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:36:53,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:36:53,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:36:53,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/135/tool_script.sh] for tool command [poretools winner --type rev '/galaxy/server/database/objects/0/7/a/dataset_07ac549c-84b1-42ca-8c88-0ee373a8b6b7.dat' > '/galaxy/server/database/objects/3/8/7/dataset_387324aa-a77c-4236-8683-b4b7d8d8dad0.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:36:53,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (135) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/135/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/135/galaxy_135.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:53,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:36:53,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:36:53,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:36:53,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:53,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:54,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:36:59,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9dwt6 with k8s id: gxy-9dwt6 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:36:59,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 135: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:37:10,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 135 finished
galaxy.model.metadata DEBUG 2025-04-07 13:37:10,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 161
galaxy.jobs INFO 2025-04-07 13:37:10,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 135 in /galaxy/server/database/jobs_directory/000/135
galaxy.jobs DEBUG 2025-04-07 13:37:10,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 135 executed (201.485 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:10,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:37:12,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 136
tpv.core.entities DEBUG 2025-04-07 13:37:12,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:37:12,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:37:12,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:37:12,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:37:12,564 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Working directory for job is: /galaxy/server/database/jobs_directory/000/136
galaxy.jobs.runners DEBUG 2025-04-07 13:37:12,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [136] queued (47.019 ms)
galaxy.jobs.handler INFO 2025-04-07 13:37:12,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:12,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 136
galaxy.jobs DEBUG 2025-04-07 13:37:12,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [136] prepared (76.870 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:37:12,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/136/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/136/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/136/configs/tmpsnqkebpx']
galaxy.jobs.runners DEBUG 2025-04-07 13:37:12,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (136) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/136/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/136/galaxy_136.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:12,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:12,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:13,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:23,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dw8kx with k8s id: gxy-dw8kx succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:37:23,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 136: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:37:34,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 136 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:37:34,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/136/working/gxupload_0', 'object_id': 162}]}]}]
galaxy.jobs INFO 2025-04-07 13:37:35,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 136 in /galaxy/server/database/jobs_directory/000/136
galaxy.jobs DEBUG 2025-04-07 13:37:35,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 136 executed (615.183 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:35,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:37:36,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 137
tpv.core.entities DEBUG 2025-04-07 13:37:36,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:37:36,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:37:36,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:37:36,207 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:37:36,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Working directory for job is: /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-04-07 13:37:36,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [137] queued (67.231 ms)
galaxy.jobs.handler INFO 2025-04-07 13:37:36,260 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:36,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 137
galaxy.jobs DEBUG 2025-04-07 13:37:36,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [137] prepared (69.510 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:37:36,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:37:36,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:37:36,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:37:36,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/137/tool_script.sh] for tool command [poretools winner --type 2D '/galaxy/server/database/objects/4/a/8/dataset_4a8dfa7c-9650-4e2f-a92b-e4184abf3dd7.dat' > '/galaxy/server/database/objects/a/f/7/dataset_af73c02e-29a1-4d49-9d2b-e34fefcd073d.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:37:36,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (137) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/137/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/137/galaxy_137.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:36,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:37:36,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:37:36,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:37:36,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:36,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:37,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:41,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-btwkv with k8s id: gxy-btwkv succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:37:42,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 137: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:37:51,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 137 finished
galaxy.model.metadata DEBUG 2025-04-07 13:37:51,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 163
galaxy.jobs INFO 2025-04-07 13:37:51,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 137 in /galaxy/server/database/jobs_directory/000/137
galaxy.jobs DEBUG 2025-04-07 13:37:51,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 137 executed (149.157 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:51,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:37:53,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 138
tpv.core.entities DEBUG 2025-04-07 13:37:53,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:37:53,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:37:53,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:37:53,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:37:53,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Working directory for job is: /galaxy/server/database/jobs_directory/000/138
galaxy.jobs.runners DEBUG 2025-04-07 13:37:53,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [138] queued (66.490 ms)
galaxy.jobs.handler INFO 2025-04-07 13:37:53,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:53,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 138
galaxy.jobs DEBUG 2025-04-07 13:37:53,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [138] prepared (94.857 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:37:53,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/138/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/138/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/138/configs/tmpo0f3ngug']
galaxy.jobs.runners DEBUG 2025-04-07 13:37:54,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (138) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/138/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/138/galaxy_138.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:54,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:54,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:37:54,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:05,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5q6hd with k8s id: gxy-5q6hd succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:38:05,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 138: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:38:14,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 138 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:38:14,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/138/working/gxupload_0', 'object_id': 164}]}]}]
galaxy.jobs INFO 2025-04-07 13:38:15,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 138 in /galaxy/server/database/jobs_directory/000/138
galaxy.jobs DEBUG 2025-04-07 13:38:15,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 138 executed (564.255 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:15,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:38:16,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 139
tpv.core.entities DEBUG 2025-04-07 13:38:16,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:38:16,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:38:16,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:38:16,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:38:16,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Working directory for job is: /galaxy/server/database/jobs_directory/000/139
galaxy.jobs.runners DEBUG 2025-04-07 13:38:16,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [139] queued (61.332 ms)
galaxy.jobs.handler INFO 2025-04-07 13:38:16,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:16,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 139
galaxy.jobs DEBUG 2025-04-07 13:38:16,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [139] prepared (73.621 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:38:16,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:38:16,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:38:16,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:38:16,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/139/tool_script.sh] for tool command [poretools winner --type best '/galaxy/server/database/objects/7/2/4/dataset_7247ee2a-6da1-4826-834c-6106c4dba2e3.dat' > '/galaxy/server/database/objects/1/e/4/dataset_1e4998cb-f825-4163-b226-fbe5074fee3d.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:38:16,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (139) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/139/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/139/galaxy_139.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:16,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:38:16,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:38:16,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_winner/poretools_winner/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-04-07 13:38:16,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:16,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:17,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:22,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s9g5q with k8s id: gxy-s9g5q succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:38:22,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 139: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:38:32,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 139 finished
galaxy.model.metadata DEBUG 2025-04-07 13:38:32,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 165
galaxy.jobs INFO 2025-04-07 13:38:32,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 139 in /galaxy/server/database/jobs_directory/000/139
galaxy.jobs DEBUG 2025-04-07 13:38:32,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 139 executed (206.022 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:32,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:38:35,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 142, 141, 140
tpv.core.entities DEBUG 2025-04-07 13:38:35,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:38:35,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:38:35,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:38:36,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:38:36,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Working directory for job is: /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-04-07 13:38:36,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [140] queued (49.517 ms)
galaxy.jobs.handler INFO 2025-04-07 13:38:36,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 140
tpv.core.entities DEBUG 2025-04-07 13:38:36,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:38:36,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:38:36,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:38:36,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:38:36,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Working directory for job is: /galaxy/server/database/jobs_directory/000/141
galaxy.jobs.runners DEBUG 2025-04-07 13:38:36,134 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [141] queued (71.567 ms)
galaxy.jobs.handler INFO 2025-04-07 13:38:36,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 141
tpv.core.entities DEBUG 2025-04-07 13:38:36,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:38:36,159 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:38:36,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:38:36,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [140] prepared (138.702 ms)
galaxy.jobs DEBUG 2025-04-07 13:38:36,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:38:36,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Working directory for job is: /galaxy/server/database/jobs_directory/000/142
galaxy.jobs.command_factory INFO 2025-04-07 13:38:36,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/140/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/140/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/140/configs/tmpv3oad0c9']
galaxy.jobs.runners DEBUG 2025-04-07 13:38:36,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [142] queued (98.100 ms)
galaxy.jobs.handler INFO 2025-04-07 13:38:36,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Job dispatched
galaxy.jobs.runners DEBUG 2025-04-07 13:38:36,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (140) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/140/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/140/galaxy_140.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 142
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:38:36,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [141] prepared (155.077 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:38:36,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/141/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/141/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/141/configs/tmp5zxlo3yx']
galaxy.jobs.runners DEBUG 2025-04-07 13:38:36,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (141) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/141/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/141/galaxy_141.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:38:36,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [142] prepared (136.052 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:38:36,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/142/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/142/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/142/configs/tmppz47j4fo']
galaxy.jobs.runners DEBUG 2025-04-07 13:38:36,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (142) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/142/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/142/galaxy_142.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:36,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:37,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:37,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:37,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:48,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bmvwn with k8s id: gxy-bmvwn succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:38:48,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 140: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:49,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tsqvm with k8s id: gxy-tsqvm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:38:49,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 142: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:38:50,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-htbxz with k8s id: gxy-htbxz succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:38:50,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 141: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:39:02,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 140 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:39:03,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'computeGCBias_result1.tabular', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/140/working/data_fetch_upload_dlq8gc62', 'object_id': 166}]}]}]
galaxy.jobs INFO 2025-04-07 13:39:03,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 140 in /galaxy/server/database/jobs_directory/000/140
galaxy.jobs DEBUG 2025-04-07 13:39:03,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 140 executed (295.231 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:03,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:39:04,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 142 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:39:04,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sequence.2bit', 'dbkey': '?', 'ext': 'twobit', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded twobit file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/142/working/gxupload_0', 'object_id': 168}]}]}]
galaxy.jobs INFO 2025-04-07 13:39:04,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 142 in /galaxy/server/database/jobs_directory/000/142
galaxy.jobs DEBUG 2025-04-07 13:39:04,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 142 executed (216.713 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:04,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:39:04,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 141 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:39:04,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'paired_chr2L.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/141/working/gxupload_0', 'object_id': 167}]}]}]
galaxy.jobs INFO 2025-04-07 13:39:05,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 141 in /galaxy/server/database/jobs_directory/000/141
galaxy.jobs DEBUG 2025-04-07 13:39:05,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 141 executed (249.593 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:05,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:39:06,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 143
tpv.core.entities DEBUG 2025-04-07 13:39:06,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_correct_gc_bias/deeptools_correct_gc_bias/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:39:06,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:39:06,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:39:06,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:39:06,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Working directory for job is: /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-04-07 13:39:06,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [143] queued (68.830 ms)
galaxy.jobs.handler INFO 2025-04-07 13:39:06,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:06,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 143
galaxy.jobs DEBUG 2025-04-07 13:39:06,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [143] prepared (122.797 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:39:06,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:39:06,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_correct_gc_bias/deeptools_correct_gc_bias/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-07 13:39:06,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:39:06,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/143/tool_script.sh] for tool command [correctGCBias --version > /galaxy/server/database/jobs_directory/000/143/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/9/7/b/dataset_97b00c9b-60ad-40af-b3bc-c0f361ef5deb.dat' local_bamInput.bam && ln -s '/galaxy/server/database/objects/_metadata_files/a/1/5/metadata_a1540ac6-2197-4fb6-a6ad-04806cfabb67.dat' local_bamInput.bam.bai &&  correctGCBias --numberOfProcessors "${GALAXY_SLOTS:-4}" --bamfile local_bamInput.bam --GCbiasFrequenciesFile '/galaxy/server/database/objects/8/2/5/dataset_82511573-7d38-4e58-a2e8-535f10220762.dat'   --genome /galaxy/server/database/objects/a/3/b/dataset_a3bfa155-1825-4e04-bf2e-5c0c693c0abe.dat   --effectiveGenomeSize 10050  --correctedFile corrected.bam]
galaxy.jobs.runners DEBUG 2025-04-07 13:39:06,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (143) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/143/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/143/galaxy_143.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/143/working/corrected.bam" -a -f "/galaxy/server/database/objects/3/4/9/dataset_3498c925-b79d-4511-b968-23381637240c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/143/working/corrected.bam" "/galaxy/server/database/objects/3/4/9/dataset_3498c925-b79d-4511-b968-23381637240c.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:06,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:39:06,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:39:06,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_correct_gc_bias/deeptools_correct_gc_bias/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-04-07 13:39:06,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:06,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:07,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:32,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4lw4b with k8s id: gxy-4lw4b succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:39:32,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 143: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:39:43,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 143 finished
galaxy.model.metadata DEBUG 2025-04-07 13:39:43,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 169
galaxy.util WARNING 2025-04-07 13:39:43,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/4/9/dataset_3498c925-b79d-4511-b968-23381637240c.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/4/9/dataset_3498c925-b79d-4511-b968-23381637240c.dat'
galaxy.jobs INFO 2025-04-07 13:39:43,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 143 in /galaxy/server/database/jobs_directory/000/143
galaxy.jobs DEBUG 2025-04-07 13:39:43,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 143 executed (189.421 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:43,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:39:47,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 145, 144, 146
tpv.core.entities DEBUG 2025-04-07 13:39:47,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:39:47,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:39:47,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:39:47,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:39:47,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Working directory for job is: /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-04-07 13:39:47,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [144] queued (64.451 ms)
galaxy.jobs.handler INFO 2025-04-07 13:39:47,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 144
tpv.core.entities DEBUG 2025-04-07 13:39:47,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:39:47,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:39:47,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:39:47,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:39:47,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Working directory for job is: /galaxy/server/database/jobs_directory/000/145
galaxy.jobs.runners DEBUG 2025-04-07 13:39:47,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [145] queued (88.015 ms)
galaxy.jobs.handler INFO 2025-04-07 13:39:47,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 145
tpv.core.entities DEBUG 2025-04-07 13:39:47,568 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:39:47,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:39:47,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:39:47,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [144] prepared (157.451 ms)
galaxy.jobs DEBUG 2025-04-07 13:39:47,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-04-07 13:39:47,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/144/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/144/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/144/configs/tmphakag90o']
galaxy.jobs.runners DEBUG 2025-04-07 13:39:47,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (144) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/144/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/144/galaxy_144.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:39:47,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Working directory for job is: /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners DEBUG 2025-04-07 13:39:47,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [146] queued (109.829 ms)
galaxy.jobs.handler INFO 2025-04-07 13:39:47,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 146
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:39:47,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [145] prepared (185.297 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:39:47,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/145/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/145/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/145/configs/tmpdfyqwqtq']
galaxy.jobs.runners DEBUG 2025-04-07 13:39:47,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (145) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/145/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/145/galaxy_145.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:39:47,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [146] prepared (159.656 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:39:47,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/146/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/146/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/146/configs/tmpqxpgmijt']
galaxy.jobs.runners DEBUG 2025-04-07 13:39:47,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (146) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/146/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/146/galaxy_146.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:47,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:48,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:48,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:48,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:39:48,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q4dwz failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-q4dwz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:40:00,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-q4dwz

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-q4dwz": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tl6bb with k8s id: gxy-tl6bb succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (145/gxy-q4dwz) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (145/gxy-q4dwz) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (145/gxy-q4dwz) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (145/gxy-q4dwz) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-q4dwz.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 145 (gxy-q4dwz)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-q4dwz to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:00,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (145/gxy-q4dwz) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-04-07 13:40:00,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 146: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:01,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kwrgl with k8s id: gxy-kwrgl succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:40:01,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 144: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:40:11,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 146 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:40:11,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 't_R2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/146/working/gxupload_0', 'object_id': 172}]}]}]
galaxy.jobs INFO 2025-04-07 13:40:11,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 146 in /galaxy/server/database/jobs_directory/000/146
galaxy.jobs DEBUG 2025-04-07 13:40:11,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 146 executed (281.232 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:11,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:40:12,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 144 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:40:12,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ref.fa.gz', 'dbkey': '?', 'ext': 'fasta.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/144/working/gxupload_0', 'object_id': 170}]}]}]
galaxy.jobs INFO 2025-04-07 13:40:12,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 144 in /galaxy/server/database/jobs_directory/000/144
galaxy.jobs DEBUG 2025-04-07 13:40:12,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 144 executed (254.095 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:12,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:40:14,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 148, 149, 147
tpv.core.entities DEBUG 2025-04-07 13:40:14,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:40:14,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:40:14,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:40:14,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:40:14,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Working directory for job is: /galaxy/server/database/jobs_directory/000/147
galaxy.jobs.runners DEBUG 2025-04-07 13:40:14,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [147] queued (64.535 ms)
galaxy.jobs.handler INFO 2025-04-07 13:40:14,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 147
tpv.core.entities DEBUG 2025-04-07 13:40:14,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:40:14,539 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:40:14,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:40:14,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:40:14,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Working directory for job is: /galaxy/server/database/jobs_directory/000/148
galaxy.jobs.runners DEBUG 2025-04-07 13:40:14,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [148] queued (84.879 ms)
galaxy.jobs.handler INFO 2025-04-07 13:40:14,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 148
tpv.core.entities DEBUG 2025-04-07 13:40:14,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:40:14,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:40:14,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:40:14,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:40:14,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [147] prepared (157.653 ms)
galaxy.jobs DEBUG 2025-04-07 13:40:14,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Working directory for job is: /galaxy/server/database/jobs_directory/000/149
galaxy.jobs.command_factory INFO 2025-04-07 13:40:14,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/147/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/147/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/147/configs/tmpy329syid']
galaxy.jobs.runners DEBUG 2025-04-07 13:40:14,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [149] queued (90.421 ms)
galaxy.jobs.handler INFO 2025-04-07 13:40:14,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 149
galaxy.jobs.runners DEBUG 2025-04-07 13:40:14,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (147) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/147/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/147/galaxy_147.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:40:14,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [148] prepared (166.462 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:40:14,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/148/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/148/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/148/configs/tmpyrh_11ke']
galaxy.jobs.runners DEBUG 2025-04-07 13:40:14,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (148) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/148/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/148/galaxy_148.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:40:14,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [149] prepared (130.117 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:40:14,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/149/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/149/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/149/configs/tmpaa_3zs69']
galaxy.jobs.runners DEBUG 2025-04-07 13:40:14,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (149) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/149/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/149/galaxy_149.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:14,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:15,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:15,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:15,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:15,804 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cbxzv failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-cbxzv.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:40:28,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-cbxzv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-cbxzv": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hvxfj with k8s id: gxy-hvxfj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (148/gxy-cbxzv) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (148/gxy-cbxzv) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (148/gxy-cbxzv) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (148/gxy-cbxzv) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-cbxzv.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 148 (gxy-cbxzv)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-cbxzv to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:28,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (148/gxy-cbxzv) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-04-07 13:40:28,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 149: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:29,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tpljm with k8s id: gxy-tpljm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:40:29,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 147: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:40:41,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 149 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:40:41,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 't_R2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/149/working/gxupload_0', 'object_id': 175}]}]}]
galaxy.jobs INFO 2025-04-07 13:40:41,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 149 in /galaxy/server/database/jobs_directory/000/149
galaxy.jobs DEBUG 2025-04-07 13:40:41,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 149 executed (246.525 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:41,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:40:42,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 147 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:40:42,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ref.fa.gz', 'dbkey': '?', 'ext': 'fasta.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/147/working/gxupload_0', 'object_id': 173}]}]}]
galaxy.jobs INFO 2025-04-07 13:40:42,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 147 in /galaxy/server/database/jobs_directory/000/147
galaxy.jobs DEBUG 2025-04-07 13:40:42,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 147 executed (255.130 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:42,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:40:45,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 150
tpv.core.entities DEBUG 2025-04-07 13:40:45,685 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:40:45,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:40:45,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:40:45,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:40:45,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Working directory for job is: /galaxy/server/database/jobs_directory/000/150
galaxy.jobs.runners DEBUG 2025-04-07 13:40:45,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [150] queued (71.075 ms)
galaxy.jobs.handler INFO 2025-04-07 13:40:45,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:45,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 150
galaxy.jobs DEBUG 2025-04-07 13:40:45,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [150] prepared (106.672 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:40:45,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/150/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/150/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/150/configs/tmpjaslqcm5']
galaxy.jobs.runners DEBUG 2025-04-07 13:40:45,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (150) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/150/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/150/galaxy_150.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:45,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:46,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:40:46,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:00,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bhbjr with k8s id: gxy-bhbjr succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:41:00,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 150: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:41:11,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 150 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:41:11,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/150/working/data_fetch_upload_hw8uozgx', 'object_id': 176}]}]}]
galaxy.jobs INFO 2025-04-07 13:41:12,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 150 in /galaxy/server/database/jobs_directory/000/150
galaxy.jobs DEBUG 2025-04-07 13:41:12,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 150 executed (223.284 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:12,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:41:13,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 151
tpv.core.entities DEBUG 2025-04-07 13:41:13,548 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:41:13,549 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:41:13,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:41:13,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:41:13,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Working directory for job is: /galaxy/server/database/jobs_directory/000/151
galaxy.jobs.runners DEBUG 2025-04-07 13:41:13,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [151] queued (61.694 ms)
galaxy.jobs.handler INFO 2025-04-07 13:41:13,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:13,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 151
galaxy.jobs DEBUG 2025-04-07 13:41:13,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [151] prepared (127.581 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:41:13,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:41:13,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:41:13,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:41:14,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/151/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/151/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/4/e/c/dataset_4ecaf616-9d7c-4853-a148-9e46ec073d51.dat' -o '/galaxy/server/database/objects/4/e/c/dataset_4ecaf616-9d7c-4853-a148-9e46ec073d51_files'  -m '/galaxy/server/database/objects/1/3/e/dataset_13e385d5-2403-498e-8d32-844654b5816d.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'    -C 'Mut-WT,WT-Mut'    -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/4/e/c/dataset_4ecaf616-9d7c-4853-a148-9e46ec073d51_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:41:14,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/151/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/151/galaxy_151.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:14,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:41:14,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:41:14,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:41:14,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:14,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:15,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vphj5 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-vphj5.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:41:43,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-vphj5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-vphj5": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151/gxy-vphj5) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151/gxy-vphj5) tool_stderr: Warning message:
In Sys.setlocale("LC_MESSAGES", "en_US.UTF-8") :
  OS reports request to set locale to "en_US.UTF-8" cannot be honored
Warning message:
In plot.xy(xy, type, ...) : NAs introduced by coercion
Warning message:
In plot.xy(xy, type, ...) : NAs introduced by coercion

galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151/gxy-vphj5) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151/gxy-vphj5) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-vphj5.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 151 (gxy-vphj5)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-vphj5 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:43,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:44,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151/gxy-vphj5) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-07 13:41:45,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 152
tpv.core.entities DEBUG 2025-04-07 13:41:45,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:41:45,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:41:45,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:41:45,351 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:41:45,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Working directory for job is: /galaxy/server/database/jobs_directory/000/152
galaxy.jobs.runners DEBUG 2025-04-07 13:41:45,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [152] queued (38.121 ms)
galaxy.jobs.handler INFO 2025-04-07 13:41:45,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:45,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 152
galaxy.jobs DEBUG 2025-04-07 13:41:45,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [152] prepared (78.754 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:41:45,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/152/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/152/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/152/configs/tmp4j4o06lb']
galaxy.jobs.runners DEBUG 2025-04-07 13:41:45,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (152) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/152/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/152/galaxy_152.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:45,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:45,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:45,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:41:55,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lm25s with k8s id: gxy-lm25s succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:41:55,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 152: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:42:03,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 152 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:42:03,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix-complex.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/152/working/data_fetch_upload_zywx0fhr', 'object_id': 178}]}]}]
galaxy.jobs INFO 2025-04-07 13:42:03,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 152 in /galaxy/server/database/jobs_directory/000/152
galaxy.jobs DEBUG 2025-04-07 13:42:03,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 152 executed (144.492 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:03,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:42:03,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 153
tpv.core.entities DEBUG 2025-04-07 13:42:03,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:42:03,784 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:42:03,791 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:42:03,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:42:03,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Working directory for job is: /galaxy/server/database/jobs_directory/000/153
galaxy.jobs.runners DEBUG 2025-04-07 13:42:03,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [153] queued (52.695 ms)
galaxy.jobs.handler INFO 2025-04-07 13:42:03,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:03,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 153
galaxy.jobs DEBUG 2025-04-07 13:42:03,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [153] prepared (66.374 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:03,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:42:03,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:03,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:42:03,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/153/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/153/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/7/e/9/dataset_7e9a4fb8-d45f-40f5-baeb-82c80759f78b.dat' -o '/galaxy/server/database/objects/7/e/9/dataset_7e9a4fb8-d45f-40f5-baeb-82c80759f78b_files'  -m '/galaxy/server/database/objects/a/7/6/dataset_a7677746-f1b3-4dc8-b134-dc65f1a0a002.dat' -i 'Genotype::MutA,MutA,MutA,MutB,MutB,MutB,WTA,WTA,WTA,WTB,WTB,WTB'    -C '(MutA-MutB)-(WTA-WTB)'    -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/7/e/9/dataset_7e9a4fb8-d45f-40f5-baeb-82c80759f78b_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:42:04,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (153) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/153/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/153/galaxy_153.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:04,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:04,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:42:04,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:04,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:04,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:04,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:13,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6nnz9 with k8s id: gxy-6nnz9 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:42:13,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 153: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:42:21,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 153 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:42:21,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (153) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/153/working/output_dir/edgeR_(MutA-MutB)-(WTA-WTB).tsv] with element identifier [edgeR_(MutA-MutB)-(WTA-WTB)] for output [outTables] (4.726 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:42:21,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (153) Add dynamic collection datasets to history for output [outTables] (22.432 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:42:21,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 179
galaxy.jobs INFO 2025-04-07 13:42:21,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 153 in /galaxy/server/database/jobs_directory/000/153
galaxy.jobs DEBUG 2025-04-07 13:42:21,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 153 executed (176.366 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:21,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:42:23,207 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 155, 154
tpv.core.entities DEBUG 2025-04-07 13:42:23,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:42:23,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:42:23,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:42:23,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:42:23,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Working directory for job is: /galaxy/server/database/jobs_directory/000/154
galaxy.jobs.runners DEBUG 2025-04-07 13:42:23,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [154] queued (43.817 ms)
galaxy.jobs.handler INFO 2025-04-07 13:42:23,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:23,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 154
tpv.core.entities DEBUG 2025-04-07 13:42:23,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:42:23,304 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:42:23,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:42:23,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:42:23,351 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Working directory for job is: /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-04-07 13:42:23,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [155] queued (49.168 ms)
galaxy.jobs.handler INFO 2025-04-07 13:42:23,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:23,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 155
galaxy.jobs DEBUG 2025-04-07 13:42:23,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [154] prepared (86.354 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:42:23,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/154/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/154/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/154/configs/tmp_mk5d6xb']
galaxy.jobs.runners DEBUG 2025-04-07 13:42:23,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (154) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/154/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/154/galaxy_154.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:23,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:42:23,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [155] prepared (85.893 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:23,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:42:23,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/155/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/155/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/155/configs/tmps6j5n6m0']
galaxy.jobs.runners DEBUG 2025-04-07 13:42:23,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (155) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/155/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/155/galaxy_155.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:23,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:23,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:24,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:24,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:34,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4sbbz with k8s id: gxy-4sbbz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:34,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cnk2k with k8s id: gxy-cnk2k succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:42:34,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 154: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:42:34,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 155: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:42:43,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 155 finished
galaxy.jobs.runners DEBUG 2025-04-07 13:42:43,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 154 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:42:43,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'anno.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/155/working/data_fetch_upload_6vwv998_', 'object_id': 182}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:42:43,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/154/working/data_fetch_upload_cntf_w2_', 'object_id': 181}]}]}]
galaxy.jobs INFO 2025-04-07 13:42:43,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 155 in /galaxy/server/database/jobs_directory/000/155
galaxy.jobs INFO 2025-04-07 13:42:43,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 154 in /galaxy/server/database/jobs_directory/000/154
galaxy.jobs DEBUG 2025-04-07 13:42:43,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 155 executed (197.925 ms)
galaxy.jobs DEBUG 2025-04-07 13:42:43,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 154 executed (204.492 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:43,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:43,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:42:43,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 156
tpv.core.entities DEBUG 2025-04-07 13:42:44,012 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:42:44,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:42:44,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:42:44,031 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:42:44,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Working directory for job is: /galaxy/server/database/jobs_directory/000/156
galaxy.jobs.runners DEBUG 2025-04-07 13:42:44,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [156] queued (44.131 ms)
galaxy.jobs.handler INFO 2025-04-07 13:42:44,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:44,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 156
galaxy.jobs DEBUG 2025-04-07 13:42:44,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [156] prepared (69.196 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:44,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:42:44,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:44,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:42:44,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/156/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/156/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/7/0/d/dataset_70d608e2-82cb-44ff-a4fc-1bc1381c1e25.dat' -o '/galaxy/server/database/objects/7/0/d/dataset_70d608e2-82cb-44ff-a4fc-1bc1381c1e25_files'  -m '/galaxy/server/database/objects/0/1/8/dataset_01827652-4010-432f-8c41-8741cd7bb282.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'  -a '/galaxy/server/database/objects/a/e/d/dataset_aedd8732-884c-408c-bd9c-e4159b9b3b47.dat'   -C 'Mut-WT'    -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/7/0/d/dataset_70d608e2-82cb-44ff-a4fc-1bc1381c1e25_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:42:44,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (156) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/156/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/156/galaxy_156.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:44,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:44,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:42:44,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:42:44,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:44,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:44,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:42:54,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pb6z2 with k8s id: gxy-pb6z2 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:42:54,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 156: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:43:02,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 156 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:43:02,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (156) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/156/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (6.124 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:43:02,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (156) Add dynamic collection datasets to history for output [outTables] (22.148 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:43:02,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 183
galaxy.jobs INFO 2025-04-07 13:43:02,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 156 in /galaxy/server/database/jobs_directory/000/156
galaxy.jobs DEBUG 2025-04-07 13:43:02,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 156 executed (198.138 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:02,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:43:03,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 157
tpv.core.entities DEBUG 2025-04-07 13:43:03,463 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:43:03,464 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:43:03,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:43:03,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:43:03,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Working directory for job is: /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners DEBUG 2025-04-07 13:43:03,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [157] queued (46.412 ms)
galaxy.jobs.handler INFO 2025-04-07 13:43:03,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:03,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 157
galaxy.jobs DEBUG 2025-04-07 13:43:03,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [157] prepared (69.108 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:43:03,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/157/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/157/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/157/configs/tmp789sw7t7']
galaxy.jobs.runners DEBUG 2025-04-07 13:43:03,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (157) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/157/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/157/galaxy_157.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:03,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:03,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:04,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:14,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kxr5d with k8s id: gxy-kxr5d succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:43:14,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 157: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:43:22,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 157 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:43:22,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/157/working/data_fetch_upload_lhllu5l4', 'object_id': 185}]}]}]
galaxy.jobs INFO 2025-04-07 13:43:23,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 157 in /galaxy/server/database/jobs_directory/000/157
galaxy.jobs DEBUG 2025-04-07 13:43:23,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 157 executed (167.726 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:23,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:43:23,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 158
tpv.core.entities DEBUG 2025-04-07 13:43:23,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:43:23,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:43:23,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:43:23,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:43:23,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Working directory for job is: /galaxy/server/database/jobs_directory/000/158
galaxy.jobs.runners DEBUG 2025-04-07 13:43:24,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [158] queued (49.688 ms)
galaxy.jobs.handler INFO 2025-04-07 13:43:24,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:24,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 158
galaxy.jobs DEBUG 2025-04-07 13:43:24,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [158] prepared (67.529 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:43:24,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:43:24,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:43:24,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:43:24,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/158/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/158/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/5/2/f/dataset_52f5888a-edbb-4282-9911-d98864f53e76.dat' -o '/galaxy/server/database/objects/5/2/f/dataset_52f5888a-edbb-4282-9911-d98864f53e76_files'  -m '/galaxy/server/database/objects/f/a/a/dataset_faa34b7c-eb4a-4803-893b-19ed761ae771.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'    -C 'Mut-WT'   -r  -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/5/2/f/dataset_52f5888a-edbb-4282-9911-d98864f53e76_files'/*.tsv output_dir/  && cp '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R' '/galaxy/server/database/objects/6/f/8/dataset_6f81290e-f5ac-494d-94c5-8566ecb5735b.dat']
galaxy.jobs.runners DEBUG 2025-04-07 13:43:24,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (158) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/158/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/158/galaxy_158.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:24,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:43:24,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:43:24,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:43:24,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:24,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:25,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:33,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-87s8q with k8s id: gxy-87s8q succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:43:33,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 158: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:43:42,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 158 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:43:42,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (158) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/158/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (4.155 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:43:42,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (158) Add dynamic collection datasets to history for output [outTables] (20.708 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:43:42,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 186
galaxy.model.metadata DEBUG 2025-04-07 13:43:42,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 187
galaxy.util WARNING 2025-04-07 13:43:42,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/f/8/dataset_6f81290e-f5ac-494d-94c5-8566ecb5735b.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/f/8/dataset_6f81290e-f5ac-494d-94c5-8566ecb5735b.dat'
galaxy.jobs INFO 2025-04-07 13:43:42,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 158 in /galaxy/server/database/jobs_directory/000/158
galaxy.jobs DEBUG 2025-04-07 13:43:42,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 158 executed (217.606 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:42,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:43:44,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 159
tpv.core.entities DEBUG 2025-04-07 13:43:44,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:43:44,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:43:44,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:43:44,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:43:44,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Working directory for job is: /galaxy/server/database/jobs_directory/000/159
galaxy.jobs.runners DEBUG 2025-04-07 13:43:44,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [159] queued (30.716 ms)
galaxy.jobs.handler INFO 2025-04-07 13:43:44,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:44,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 159
galaxy.jobs DEBUG 2025-04-07 13:43:44,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [159] prepared (68.921 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:43:44,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/159/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/159/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/159/configs/tmpa62awrz3']
galaxy.jobs.runners DEBUG 2025-04-07 13:43:44,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (159) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/159/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/159/galaxy_159.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:44,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:44,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:45,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:43:54,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lprxj with k8s id: gxy-lprxj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:43:55,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 159: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:44:02,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 159 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:44:02,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/159/working/data_fetch_upload_tttzdx6m', 'object_id': 189}]}]}]
galaxy.jobs INFO 2025-04-07 13:44:03,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 159 in /galaxy/server/database/jobs_directory/000/159
galaxy.jobs DEBUG 2025-04-07 13:44:03,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 159 executed (129.092 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:03,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:44:03,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 160
tpv.core.entities DEBUG 2025-04-07 13:44:04,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:44:04,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:44:04,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:44:04,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:44:04,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Working directory for job is: /galaxy/server/database/jobs_directory/000/160
galaxy.jobs.runners DEBUG 2025-04-07 13:44:04,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [160] queued (46.476 ms)
galaxy.jobs.handler INFO 2025-04-07 13:44:04,065 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:04,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 160
galaxy.jobs DEBUG 2025-04-07 13:44:04,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [160] prepared (68.464 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:04,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:44:04,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:04,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:44:04,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/160/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/160/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/e/9/7/dataset_e9700e57-f0d9-4613-98ff-80ab5ac3d727.dat' -o '/galaxy/server/database/objects/e/9/7/dataset_e9700e57-f0d9-4613-98ff-80ab5ac3d727_files'  -m '/galaxy/server/database/objects/d/f/0/dataset_df0d0940-5b0e-44f9-8ace-1ba2a92e5bc8.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT|Batch::b1,b2,b3,b1,b2,b3'    -C 'Mut-WT'    -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/e/9/7/dataset_e9700e57-f0d9-4613-98ff-80ab5ac3d727_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:44:04,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (160) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/160/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/160/galaxy_160.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:04,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:04,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:44:04,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:04,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:04,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:04,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:14,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vtkzf with k8s id: gxy-vtkzf succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:44:14,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 160: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:44:22,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 160 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:44:22,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (160) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/160/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (4.377 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:44:22,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (160) Add dynamic collection datasets to history for output [outTables] (20.748 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:44:22,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 190
galaxy.jobs INFO 2025-04-07 13:44:22,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 160 in /galaxy/server/database/jobs_directory/000/160
galaxy.jobs DEBUG 2025-04-07 13:44:22,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 160 executed (158.713 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:22,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:44:24,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 162, 161
tpv.core.entities DEBUG 2025-04-07 13:44:24,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:44:24,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:44:24,549 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:44:24,565 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:44:24,581 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Working directory for job is: /galaxy/server/database/jobs_directory/000/161
galaxy.jobs.runners DEBUG 2025-04-07 13:44:24,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [161] queued (38.715 ms)
galaxy.jobs.handler INFO 2025-04-07 13:44:24,591 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:24,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 161
tpv.core.entities DEBUG 2025-04-07 13:44:24,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:44:24,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:44:24,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:44:24,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:44:24,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Working directory for job is: /galaxy/server/database/jobs_directory/000/162
galaxy.jobs.runners DEBUG 2025-04-07 13:44:24,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [162] queued (55.206 ms)
galaxy.jobs.handler INFO 2025-04-07 13:44:24,670 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:24,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 162
galaxy.jobs DEBUG 2025-04-07 13:44:24,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [161] prepared (90.821 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:44:24,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/161/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/161/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/161/configs/tmpy8tt12po']
galaxy.jobs.runners DEBUG 2025-04-07 13:44:24,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (161) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/161/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/161/galaxy_161.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:44:24,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [162] prepared (79.829 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:24,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:24,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:44:24,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/162/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/162/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/162/configs/tmp98v1cnmm']
galaxy.jobs.runners DEBUG 2025-04-07 13:44:24,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (162) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/162/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/162/galaxy_162.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:24,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:24,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:25,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:25,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:35,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nmn8j with k8s id: gxy-nmn8j succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:35,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nh2f4 with k8s id: gxy-nh2f4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:44:35,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 161: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:44:35,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 162: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:44:44,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 161 finished
galaxy.jobs.runners DEBUG 2025-04-07 13:44:44,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 162 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:44:44,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/161/working/data_fetch_upload__8s30m1f', 'object_id': 192}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:44:44,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'factorinfo.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/162/working/data_fetch_upload_3iydqhr_', 'object_id': 193}]}]}]
galaxy.jobs INFO 2025-04-07 13:44:44,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 161 in /galaxy/server/database/jobs_directory/000/161
galaxy.jobs INFO 2025-04-07 13:44:44,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 162 in /galaxy/server/database/jobs_directory/000/162
galaxy.jobs DEBUG 2025-04-07 13:44:44,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 161 executed (156.116 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:44,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:44:44,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 162 executed (165.858 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:44,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:44:45,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 163
tpv.core.entities DEBUG 2025-04-07 13:44:45,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:44:45,304 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:44:45,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:44:45,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:44:45,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Working directory for job is: /galaxy/server/database/jobs_directory/000/163
galaxy.jobs.runners DEBUG 2025-04-07 13:44:45,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [163] queued (35.189 ms)
galaxy.jobs.handler INFO 2025-04-07 13:44:45,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:45,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 163
galaxy.jobs DEBUG 2025-04-07 13:44:45,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [163] prepared (59.688 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:45,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:44:45,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:45,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:44:45,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/163/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/163/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/1/1/c/dataset_11c4d844-5ab9-423d-a506-766388938978.dat' -o '/galaxy/server/database/objects/1/1/c/dataset_11c4d844-5ab9-423d-a506-766388938978_files'  -m '/galaxy/server/database/objects/4/6/8/dataset_468adbdf-00c6-49af-982b-7732a641cd0d.dat' -f '/galaxy/server/database/objects/4/5/0/dataset_450ee4ff-f551-46b9-9e9c-608aa4defe16.dat'    -C 'Mut-WT'    -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/1/1/c/dataset_11c4d844-5ab9-423d-a506-766388938978_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:44:45,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (163) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/163/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/163/galaxy_163.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:45,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:45,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:44:45,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:44:45,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:45,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:45,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:44:54,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9bkbb with k8s id: gxy-9bkbb succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:44:55,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 163: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:45:03,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 163 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:45:03,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (163) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/163/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (4.092 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:45:03,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (163) Add dynamic collection datasets to history for output [outTables] (19.111 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:45:03,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 194
galaxy.jobs INFO 2025-04-07 13:45:03,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 163 in /galaxy/server/database/jobs_directory/000/163
galaxy.jobs DEBUG 2025-04-07 13:45:03,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 163 executed (155.487 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:03,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:45:04,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 164
tpv.core.entities DEBUG 2025-04-07 13:45:04,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:04,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:45:04,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:04,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:45:04,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Working directory for job is: /galaxy/server/database/jobs_directory/000/164
galaxy.jobs.runners DEBUG 2025-04-07 13:45:04,802 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [164] queued (42.803 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:04,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:04,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 164
galaxy.jobs DEBUG 2025-04-07 13:45:04,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [164] prepared (70.701 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:45:04,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/164/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/164/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/164/configs/tmpp7pyi0cc']
galaxy.jobs.runners DEBUG 2025-04-07 13:45:04,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (164) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/164/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/164/galaxy_164.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:04,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:04,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:05,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:16,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p85qx with k8s id: gxy-p85qx succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:45:16,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 164: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:45:25,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 164 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:45:25,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/164/working/data_fetch_upload_jkp8ri8u', 'object_id': 196}]}]}]
galaxy.jobs INFO 2025-04-07 13:45:25,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 164 in /galaxy/server/database/jobs_directory/000/164
galaxy.jobs DEBUG 2025-04-07 13:45:25,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 164 executed (176.116 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:25,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:45:26,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 165
tpv.core.entities DEBUG 2025-04-07 13:45:26,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:26,288 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:45:26,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:26,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:45:26,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Working directory for job is: /galaxy/server/database/jobs_directory/000/165
galaxy.jobs.runners DEBUG 2025-04-07 13:45:26,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [165] queued (55.598 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:26,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:26,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 165
galaxy.jobs DEBUG 2025-04-07 13:45:26,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [165] prepared (76.947 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:45:26,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:45:26,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:45:26,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:45:26,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/165/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/165/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/1/0/3/dataset_1031d601-9e25-4bd9-b6fe-fae23dafae31.dat' -o '/galaxy/server/database/objects/1/0/3/dataset_1031d601-9e25-4bd9-b6fe-fae23dafae31_files'  -m '/galaxy/server/database/objects/1/4/4/dataset_144ff592-3338-4b4d-8417-d1da425bff5d.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'    -C 'Mut-WT'   -x  -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/1/0/3/dataset_1031d601-9e25-4bd9-b6fe-fae23dafae31_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:45:26,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (165) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/165/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/165/galaxy_165.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:26,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:45:26,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:45:26,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:45:26,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:26,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:27,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:36,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6xhlj with k8s id: gxy-6xhlj succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:45:36,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 165: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:45:44,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 165 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:45:44,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (165) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/165/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (3.830 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:45:44,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (165) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/165/working/output_dir/edgeR_normcounts.tsv] with element identifier [edgeR_normcounts] for output [outTables] (0.678 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:45:44,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (165) Add dynamic collection datasets to history for output [outTables] (27.781 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:45:44,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 197
galaxy.jobs INFO 2025-04-07 13:45:45,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 165 in /galaxy/server/database/jobs_directory/000/165
galaxy.jobs DEBUG 2025-04-07 13:45:45,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 165 executed (182.528 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:45,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:45:46,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 166
tpv.core.entities DEBUG 2025-04-07 13:45:46,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:46,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:45:46,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:46,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:45:46,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Working directory for job is: /galaxy/server/database/jobs_directory/000/166
galaxy.jobs.runners DEBUG 2025-04-07 13:45:46,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [166] queued (39.424 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:46,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:46,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 166
galaxy.jobs DEBUG 2025-04-07 13:45:46,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [166] prepared (66.602 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:45:46,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/166/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/166/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/166/configs/tmpbwgfchns']
galaxy.jobs.runners DEBUG 2025-04-07 13:45:46,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (166) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/166/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/166/galaxy_166.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:46,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:47,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:47,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lgnhw failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lgnhw.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:45:56,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-lgnhw

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-lgnhw": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (166/gxy-lgnhw) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (166/gxy-lgnhw) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (166/gxy-lgnhw) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (166/gxy-lgnhw) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lgnhw.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 166 (gxy-lgnhw)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-lgnhw to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:56,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:57,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (166/gxy-lgnhw) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-07 13:45:59,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 169, 168, 171, 167, 170
tpv.core.entities DEBUG 2025-04-07 13:45:59,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:59,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:45:59,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:59,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:45:59,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Working directory for job is: /galaxy/server/database/jobs_directory/000/167
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [167] queued (46.069 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:59,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 167
tpv.core.entities DEBUG 2025-04-07 13:45:59,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:59,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:45:59,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:59,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:45:59,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Working directory for job is: /galaxy/server/database/jobs_directory/000/168
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [168] queued (54.629 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:59,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 168
tpv.core.entities DEBUG 2025-04-07 13:45:59,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:59,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:45:59,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:59,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [167] prepared (108.385 ms)
galaxy.jobs DEBUG 2025-04-07 13:45:59,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-04-07 13:45:59,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/167/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/167/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/167/configs/tmpa3jfqh1u']
galaxy.jobs DEBUG 2025-04-07 13:45:59,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Working directory for job is: /galaxy/server/database/jobs_directory/000/169
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (167) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/167/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/167/galaxy_167.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [169] queued (72.714 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:59,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 169
tpv.core.entities DEBUG 2025-04-07 13:45:59,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:59,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Mapped job to destination id: k8s
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:45:59,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:59,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [168] prepared (131.974 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:45:59,427 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:45:59,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Working directory for job is: /galaxy/server/database/jobs_directory/000/170
galaxy.jobs.command_factory INFO 2025-04-07 13:45:59,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/168/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/168/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/168/configs/tmpnyih1q72']
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [170] queued (85.338 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:59,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 170
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (168) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/168/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/168/galaxy_168.ec; sh -c "exit $return_code"
tpv.core.entities DEBUG 2025-04-07 13:45:59,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:45:59,500 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:45:59,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:45:59,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [169] prepared (146.816 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:45:59,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-04-07 13:45:59,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/169/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/169/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/169/configs/tmplskvy5f0']
galaxy.jobs DEBUG 2025-04-07 13:45:59,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Working directory for job is: /galaxy/server/database/jobs_directory/000/171
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (169) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/169/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/169/galaxy_169.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [171] queued (96.419 ms)
galaxy.jobs.handler INFO 2025-04-07 13:45:59,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 171
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:45:59,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [170] prepared (154.147 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:45:59,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/170/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/170/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/170/configs/tmpa5_dhel5']
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (170) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/170/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/170/galaxy_170.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:45:59,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [171] prepared (102.377 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:45:59,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/171/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/171/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/171/configs/tmpbq_jlwqi']
galaxy.jobs.runners DEBUG 2025-04-07 13:45:59,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (171) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/171/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/171/galaxy_171.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:45:59,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-04-07 13:46:00,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 172, 173
tpv.core.entities DEBUG 2025-04-07 13:46:00,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:46:00,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:46:00,647 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:46:00,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:46:00,685 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Working directory for job is: /galaxy/server/database/jobs_directory/000/172
galaxy.jobs.runners DEBUG 2025-04-07 13:46:00,695 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [172] queued (47.369 ms)
galaxy.jobs.handler INFO 2025-04-07 13:46:00,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 172
tpv.core.entities DEBUG 2025-04-07 13:46:00,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:46:00,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:46:00,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:46:00,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:46:00,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Working directory for job is: /galaxy/server/database/jobs_directory/000/173
galaxy.jobs.runners DEBUG 2025-04-07 13:46:00,784 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [173] queued (54.991 ms)
galaxy.jobs.handler INFO 2025-04-07 13:46:00,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 173
galaxy.jobs DEBUG 2025-04-07 13:46:00,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [172] prepared (94.214 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:46:00,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/172/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/172/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/172/configs/tmphng964am']
galaxy.jobs.runners DEBUG 2025-04-07 13:46:00,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (172) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/172/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/172/galaxy_172.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:46:00,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [173] prepared (91.354 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:46:00,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/173/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/173/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/173/configs/tmpn435b5jg']
galaxy.jobs.runners DEBUG 2025-04-07 13:46:00,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (173) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/173/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/173/galaxy_173.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:00,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:01,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:01,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:01,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:11,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ph2cz with k8s id: gxy-ph2cz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:11,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nhq4j with k8s id: gxy-nhq4j succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:46:11,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 167: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:46:11,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 168: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:12,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t45l8 with k8s id: gxy-t45l8 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:12,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5h4bc with k8s id: gxy-5h4bc succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:12,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-89fvs with k8s id: gxy-89fvs succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:12,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cpt88 with k8s id: gxy-cpt88 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:12,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n99tp with k8s id: gxy-n99tp succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:46:12,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 169: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:46:12,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 170: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:46:27,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 167 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:46:27,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'WT1.counts', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/167/working/data_fetch_upload_e0kn0iw_', 'object_id': 201}]}]}]
galaxy.jobs INFO 2025-04-07 13:46:28,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 167 in /galaxy/server/database/jobs_directory/000/167
galaxy.jobs DEBUG 2025-04-07 13:46:28,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 167 executed (305.572 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:28,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:46:28,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 168 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:46:28,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'WT2.counts', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/168/working/data_fetch_upload_60p9xeek', 'object_id': 202}]}]}]
galaxy.jobs.runners DEBUG 2025-04-07 13:46:28,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 171: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs INFO 2025-04-07 13:46:28,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 168 in /galaxy/server/database/jobs_directory/000/168
galaxy.jobs DEBUG 2025-04-07 13:46:28,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 168 executed (289.261 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:28,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:46:29,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 172: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:46:29,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 169 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:46:29,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'WT3.counts', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/169/working/data_fetch_upload_uv79sp23', 'object_id': 203}]}]}]
galaxy.jobs.runners DEBUG 2025-04-07 13:46:30,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 170 finished
galaxy.jobs INFO 2025-04-07 13:46:30,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 169 in /galaxy/server/database/jobs_directory/000/169
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:46:30,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Mut1.counts', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/170/working/data_fetch_upload_n913yi70', 'object_id': 204}]}]}]
galaxy.jobs INFO 2025-04-07 13:46:30,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 170 in /galaxy/server/database/jobs_directory/000/170
galaxy.jobs DEBUG 2025-04-07 13:46:30,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 169 executed (303.641 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:30,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-04-07 13:46:30,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 170 executed (313.857 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:30,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:46:30,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 173: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:46:41,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 171 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:46:41,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Mut2.counts', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/171/working/data_fetch_upload_x0srw5ui', 'object_id': 205}]}]}]
galaxy.jobs INFO 2025-04-07 13:46:41,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 171 in /galaxy/server/database/jobs_directory/000/171
galaxy.jobs DEBUG 2025-04-07 13:46:41,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 171 executed (202.665 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:41,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:46:42,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 172 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:46:42,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'Mut3.counts', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/172/working/data_fetch_upload_me8opvx2', 'object_id': 206}]}]}]
galaxy.jobs INFO 2025-04-07 13:46:42,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 172 in /galaxy/server/database/jobs_directory/000/172
galaxy.jobs DEBUG 2025-04-07 13:46:42,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 172 executed (166.127 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:42,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-04-07 13:46:43,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 173 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:46:43,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'anno.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/173/working/data_fetch_upload_tqsrkz7e', 'object_id': 207}]}]}]
galaxy.jobs INFO 2025-04-07 13:46:43,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 173 in /galaxy/server/database/jobs_directory/000/173
galaxy.jobs DEBUG 2025-04-07 13:46:43,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 173 executed (145.663 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:43,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:46:44,419 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 174
tpv.core.entities DEBUG 2025-04-07 13:46:44,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:46:44,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:46:44,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:46:44,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:46:44,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Working directory for job is: /galaxy/server/database/jobs_directory/000/174
galaxy.jobs.runners DEBUG 2025-04-07 13:46:44,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [174] queued (46.944 ms)
galaxy.jobs.handler INFO 2025-04-07 13:46:44,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:44,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 174
galaxy.jobs DEBUG 2025-04-07 13:46:44,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [174] prepared (102.126 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:46:44,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:46:44,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:46:44,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:46:44,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/174/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/174/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/3/d/4/dataset_3d4bbac0-0c3e-4bea-a5ee-13ba9738f8cd.dat' -o '/galaxy/server/database/objects/3/d/4/dataset_3d4bbac0-0c3e-4bea-a5ee-13ba9738f8cd_files'                          -j '[["Genotype", [{"Mut": ["/galaxy/server/database/objects/9/e/1/dataset_9e19f039-d46b-4594-9069-437b528a0d59.dat", "/galaxy/server/database/objects/f/0/b/dataset_f0b27ae1-d381-4744-b5a8-1af2cc80c023.dat", "/galaxy/server/database/objects/d/d/8/dataset_dd8fdf8a-40aa-4b15-a5e9-16c8bce3a003.dat"]}, {"WT": ["/galaxy/server/database/objects/e/3/b/dataset_e3bbd314-c7ac-4c6b-82bd-893395698823.dat", "/galaxy/server/database/objects/0/3/0/dataset_03026aff-38fc-4921-9d4c-23411a1ed35a.dat", "/galaxy/server/database/objects/c/9/f/dataset_c9f241e9-a689-4a01-b68b-ec95ae1caa47.dat"]}]], ["Batch", [{"b3": ["/galaxy/server/database/objects/c/9/f/dataset_c9f241e9-a689-4a01-b68b-ec95ae1caa47.dat", "/galaxy/server/database/objects/d/d/8/dataset_dd8fdf8a-40aa-4b15-a5e9-16c8bce3a003.dat"]}, {"b2": ["/galaxy/server/database/objects/0/3/0/dataset_03026aff-38fc-4921-9d4c-23411a1ed35a.dat", "/galaxy/server/database/objects/f/0/b/dataset_f0b27ae1-d381-4744-b5a8-1af2cc80c023.dat"]}, {"b1": ["/galaxy/server/database/objects/e/3/b/dataset_e3bbd314-c7ac-4c6b-82bd-893395698823.dat", "/galaxy/server/database/objects/9/e/1/dataset_9e19f039-d46b-4594-9069-437b528a0d59.dat"]}]]]'   -a '/galaxy/server/database/objects/7/7/1/dataset_77115606-579a-4021-a73b-f4e4e4426c0f.dat'   -C 'Mut-WT,WT-Mut'   -x  -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/3/d/4/dataset_3d4bbac0-0c3e-4bea-a5ee-13ba9738f8cd_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:46:44,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (174) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/174/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/174/galaxy_174.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:44,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:46:44,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:46:44,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:46:44,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:44,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:45,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:46:54,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q98fl with k8s id: gxy-q98fl succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:46:55,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 174: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:47:03,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 174 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:47:03,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (174) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/174/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (13.670 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:47:03,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (174) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/174/working/output_dir/edgeR_WT-Mut.tsv] with element identifier [edgeR_WT-Mut] for output [outTables] (1.096 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:47:03,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (174) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/174/working/output_dir/edgeR_normcounts.tsv] with element identifier [edgeR_normcounts] for output [outTables] (1.151 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:47:03,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (174) Add dynamic collection datasets to history for output [outTables] (43.257 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:47:03,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 208
galaxy.jobs INFO 2025-04-07 13:47:03,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 174 in /galaxy/server/database/jobs_directory/000/174
galaxy.jobs DEBUG 2025-04-07 13:47:03,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 174 executed (248.347 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:03,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:47:04,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 175
tpv.core.entities DEBUG 2025-04-07 13:47:04,925 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:47:04,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:47:04,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:47:04,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:47:04,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Working directory for job is: /galaxy/server/database/jobs_directory/000/175
galaxy.jobs.runners DEBUG 2025-04-07 13:47:04,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [175] queued (40.257 ms)
galaxy.jobs.handler INFO 2025-04-07 13:47:04,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:04,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 175
galaxy.jobs DEBUG 2025-04-07 13:47:05,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [175] prepared (79.692 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:47:05,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/175/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/175/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/175/configs/tmp80yf05pa']
galaxy.jobs.runners DEBUG 2025-04-07 13:47:05,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (175) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/175/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/175/galaxy_175.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:05,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:05,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:06,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:15,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gcjz4 with k8s id: gxy-gcjz4 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:47:15,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 175: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:47:23,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 175 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:47:24,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/175/working/data_fetch_upload_cnznel21', 'object_id': 212}]}]}]
galaxy.jobs INFO 2025-04-07 13:47:24,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 175 in /galaxy/server/database/jobs_directory/000/175
galaxy.jobs DEBUG 2025-04-07 13:47:24,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 175 executed (184.429 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:24,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:47:25,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 176
tpv.core.entities DEBUG 2025-04-07 13:47:25,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:47:25,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:47:25,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:47:25,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:47:25,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Working directory for job is: /galaxy/server/database/jobs_directory/000/176
galaxy.jobs.runners DEBUG 2025-04-07 13:47:25,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [176] queued (57.956 ms)
galaxy.jobs.handler INFO 2025-04-07 13:47:25,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:25,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 176
galaxy.jobs DEBUG 2025-04-07 13:47:25,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [176] prepared (63.509 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:47:25,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:47:25,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:47:25,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:47:25,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/176/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/176/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/6/1/e/dataset_61e5bd89-1375-4b93-b9e9-b8ed928af8b2.dat' -o '/galaxy/server/database/objects/6/1/e/dataset_61e5bd89-1375-4b93-b9e9-b8ed928af8b2_files'  -m '/galaxy/server/database/objects/c/c/7/dataset_cc745e72-db6f-4e06-8523-109c83de2111.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'    -C 'Mut-WT'  -c '1000.0' -s '3'   -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/6/1/e/dataset_61e5bd89-1375-4b93-b9e9-b8ed928af8b2_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:47:25,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (176) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/176/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/176/galaxy_176.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:25,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:47:25,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:47:25,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:47:25,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:25,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:26,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:34,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tkp9b with k8s id: gxy-tkp9b succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:47:34,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 176: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:47:42,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 176 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:47:42,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (176) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/176/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (4.906 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:47:42,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (176) Add dynamic collection datasets to history for output [outTables] (19.516 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:47:42,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 213
galaxy.jobs INFO 2025-04-07 13:47:42,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 176 in /galaxy/server/database/jobs_directory/000/176
galaxy.jobs DEBUG 2025-04-07 13:47:42,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 176 executed (160.754 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:42,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:47:44,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 177
tpv.core.entities DEBUG 2025-04-07 13:47:44,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:47:44,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:47:44,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:47:44,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:47:44,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Working directory for job is: /galaxy/server/database/jobs_directory/000/177
galaxy.jobs.runners DEBUG 2025-04-07 13:47:44,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [177] queued (38.839 ms)
galaxy.jobs.handler INFO 2025-04-07 13:47:44,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:44,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 177
galaxy.jobs DEBUG 2025-04-07 13:47:45,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [177] prepared (79.920 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:47:45,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/177/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/177/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/177/configs/tmpi3h_7h0f']
galaxy.jobs.runners DEBUG 2025-04-07 13:47:45,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (177) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/177/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/177/galaxy_177.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:45,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:45,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:45,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:47:55,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8hgn2 with k8s id: gxy-8hgn2 succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:47:55,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 177: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:48:04,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 177 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:48:04,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/177/working/data_fetch_upload_adff0w40', 'object_id': 215}]}]}]
galaxy.jobs INFO 2025-04-07 13:48:04,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 177 in /galaxy/server/database/jobs_directory/000/177
galaxy.jobs DEBUG 2025-04-07 13:48:04,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 177 executed (161.853 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:04,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:48:05,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 178
tpv.core.entities DEBUG 2025-04-07 13:48:05,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:48:05,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:48:05,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:48:05,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:48:05,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Working directory for job is: /galaxy/server/database/jobs_directory/000/178
galaxy.jobs.runners DEBUG 2025-04-07 13:48:05,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [178] queued (43.083 ms)
galaxy.jobs.handler INFO 2025-04-07 13:48:05,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:05,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 178
galaxy.jobs DEBUG 2025-04-07 13:48:05,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [178] prepared (53.804 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:05,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:48:05,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:05,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:48:05,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/178/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/178/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/2/c/c/dataset_2cc3312f-defa-4d1b-9778-5afb1a45e523.dat' -o '/galaxy/server/database/objects/2/c/c/dataset_2cc3312f-defa-4d1b-9778-5afb1a45e523_files'  -m '/galaxy/server/database/objects/2/f/7/dataset_2f7ec25c-b65f-4e2b-995f-7e8cd715af26.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'    -C 'Mut-WT'  -z '10' -s '3'   -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/2/c/c/dataset_2cc3312f-defa-4d1b-9778-5afb1a45e523_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:48:05,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (178) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/178/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/178/galaxy_178.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:05,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:05,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:48:05,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:05,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:05,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:06,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:15,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sr2xz with k8s id: gxy-sr2xz succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:48:15,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 178: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:48:23,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 178 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:48:23,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (178) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/178/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (4.008 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:48:23,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (178) Add dynamic collection datasets to history for output [outTables] (17.683 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:48:23,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 216
galaxy.jobs INFO 2025-04-07 13:48:23,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 178 in /galaxy/server/database/jobs_directory/000/178
galaxy.jobs DEBUG 2025-04-07 13:48:23,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 178 executed (153.518 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:23,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:48:24,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 179
tpv.core.entities DEBUG 2025-04-07 13:48:24,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:48:24,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:48:24,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:48:24,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:48:24,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Working directory for job is: /galaxy/server/database/jobs_directory/000/179
galaxy.jobs.runners DEBUG 2025-04-07 13:48:24,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [179] queued (40.161 ms)
galaxy.jobs.handler INFO 2025-04-07 13:48:24,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:24,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 179
galaxy.jobs DEBUG 2025-04-07 13:48:24,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [179] prepared (73.014 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:48:25,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/179/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/179/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/179/configs/tmpitzh9bwy']
galaxy.jobs.runners DEBUG 2025-04-07 13:48:25,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (179) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/179/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/179/galaxy_179.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:25,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:25,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:25,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:35,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jk2fl with k8s id: gxy-jk2fl succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:48:35,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 179: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:48:43,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 179 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:48:43,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/179/working/data_fetch_upload_ztspf4qh', 'object_id': 218}]}]}]
galaxy.jobs INFO 2025-04-07 13:48:43,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 179 in /galaxy/server/database/jobs_directory/000/179
galaxy.jobs DEBUG 2025-04-07 13:48:43,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 179 executed (218.209 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:43,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:48:44,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 180
tpv.core.entities DEBUG 2025-04-07 13:48:44,301 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:48:44,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:48:44,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:48:44,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:48:44,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Working directory for job is: /galaxy/server/database/jobs_directory/000/180
galaxy.jobs.runners DEBUG 2025-04-07 13:48:44,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [180] queued (38.382 ms)
galaxy.jobs.handler INFO 2025-04-07 13:48:44,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:44,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 180
galaxy.jobs DEBUG 2025-04-07 13:48:44,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [180] prepared (65.953 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:44,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:48:44,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:44,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:48:44,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/180/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/180/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/1/d/1/dataset_1d141e74-acf1-4600-856e-c0e26bd70905.dat' -o '/galaxy/server/database/objects/1/d/1/dataset_1d141e74-acf1-4600-856e-c0e26bd70905_files'  -m '/galaxy/server/database/objects/1/b/8/dataset_1b8435b9-e056-4803-b43f-a988c8fe392a.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'    -C 'Mut-WT'  -z '1000' -y   -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/1/d/1/dataset_1d141e74-acf1-4600-856e-c0e26bd70905_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:48:44,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (180) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/180/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/180/galaxy_180.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:44,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:44,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:48:44,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:48:44,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:44,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:45,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jzjsp failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jzjsp.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-04-07 13:48:53,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-04-07-12-40-1/jobs/gxy-jzjsp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-jzjsp": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (180/gxy-jzjsp) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (180/gxy-jzjsp) tool_stderr: Warning message:
In Sys.setlocale("LC_MESSAGES", "en_US.UTF-8") :
  OS reports request to set locale to "en_US.UTF-8" cannot be honored
Warning message:
In plot.xy(xy, type, ...) : NAs introduced by coercion
Warning message:
In plot.xy(xy, type, ...) : NAs introduced by coercion

galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (180/gxy-jzjsp) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (180/gxy-jzjsp) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jzjsp.
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 180 (gxy-jzjsp)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-jzjsp to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:53,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (180/gxy-jzjsp) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-04-07 13:48:55,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 181, 182
tpv.core.entities DEBUG 2025-04-07 13:48:55,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:48:55,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:48:55,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:48:55,611 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:48:55,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Working directory for job is: /galaxy/server/database/jobs_directory/000/181
galaxy.jobs.runners DEBUG 2025-04-07 13:48:55,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [181] queued (46.115 ms)
galaxy.jobs.handler INFO 2025-04-07 13:48:55,647 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:55,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 181
tpv.core.entities DEBUG 2025-04-07 13:48:55,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:48:55,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:48:55,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:48:55,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:48:55,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Working directory for job is: /galaxy/server/database/jobs_directory/000/182
galaxy.jobs.runners DEBUG 2025-04-07 13:48:55,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [182] queued (52.875 ms)
galaxy.jobs.handler INFO 2025-04-07 13:48:55,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:55,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 182
galaxy.jobs DEBUG 2025-04-07 13:48:55,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [181] prepared (90.802 ms)
galaxy.jobs.command_factory INFO 2025-04-07 13:48:55,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/181/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/181/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/181/configs/tmph_mdhx_m']
galaxy.jobs.runners DEBUG 2025-04-07 13:48:55,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (181) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/181/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/181/galaxy_181.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-04-07 13:48:55,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [182] prepared (81.745 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:55,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:55,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-04-07 13:48:55,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/182/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/182/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/182/configs/tmpdh61cnfs']
galaxy.jobs.runners DEBUG 2025-04-07 13:48:55,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (182) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/182/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/182/galaxy_182.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:55,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:55,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:56,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:48:56,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:07,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-48r6g with k8s id: gxy-48r6g succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:07,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-54nzm with k8s id: gxy-54nzm succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:49:07,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 181: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:49:07,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 182: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:49:16,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 182 finished
galaxy.jobs.runners DEBUG 2025-04-07 13:49:16,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 181 finished
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:49:16,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'contrasts_file.txt', 'dbkey': '?', 'ext': 'txt', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded txt file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/182/working/data_fetch_upload_zj7kz4vq', 'object_id': 221}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-04-07 13:49:16,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/181/working/data_fetch_upload_lxf08p0v', 'object_id': 220}]}]}]
galaxy.jobs INFO 2025-04-07 13:49:16,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 182 in /galaxy/server/database/jobs_directory/000/182
galaxy.jobs INFO 2025-04-07 13:49:16,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 181 in /galaxy/server/database/jobs_directory/000/181
galaxy.jobs DEBUG 2025-04-07 13:49:16,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 182 executed (215.441 ms)
galaxy.jobs DEBUG 2025-04-07 13:49:16,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 181 executed (201.816 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:16,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:16,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-04-07 13:49:17,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 183
tpv.core.entities DEBUG 2025-04-07 13:49:17,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/.*, abstract=False, cores=1, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-04-07 13:49:17,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-04-07 13:49:17,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-04-07 13:49:17,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-04-07 13:49:17,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Working directory for job is: /galaxy/server/database/jobs_directory/000/183
galaxy.jobs.runners DEBUG 2025-04-07 13:49:17,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [183] queued (45.238 ms)
galaxy.jobs.handler INFO 2025-04-07 13:49:17,259 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:17,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 183
galaxy.jobs DEBUG 2025-04-07 13:49:17,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [183] prepared (71.522 ms)
galaxy.tool_util.deps.containers INFO 2025-04-07 13:49:17,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:49:17,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:49:17,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-04-07 13:49:17,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/183/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/183/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/edger/ae2aad0a6d50/edger/edger.R'  -R '/galaxy/server/database/objects/7/b/9/dataset_7b9068d9-a328-4f04-8abb-6fcbf916cf04.dat' -o '/galaxy/server/database/objects/7/b/9/dataset_7b9068d9-a328-4f04-8abb-6fcbf916cf04_files'  -m '/galaxy/server/database/objects/f/8/8/dataset_f88bff5e-ec6b-401a-9ff2-8648269570de.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT|Batch::b1,b2,b3,b1,b2,b3'   -F '~ 0 + Genotype + Batch'  -C '/galaxy/server/database/objects/0/c/9/dataset_0c9cf3ab-102e-4ae9-8d12-058aac50a163.dat'    -l '0.0' -p '0.05' -d 'BH' -n 'TMM' -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/7/b/9/dataset_7b9068d9-a328-4f04-8abb-6fcbf916cf04_files'/*.tsv output_dir/]
galaxy.jobs.runners DEBUG 2025-04-07 13:49:17,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (183) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/183/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/183/galaxy_183.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:17,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-04-07 13:49:17,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-04-07 13:49:17,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5: mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59
galaxy.tool_util.deps.containers INFO 2025-04-07 13:49:17,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-9c87747efa8cda7e1443d2bbc042981e5013079d:37a458ff34650b8a579f26dcd3a8814ea455dd59-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:17,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:18,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:27,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p228s with k8s id: gxy-p228s succeeded
galaxy.jobs.runners DEBUG 2025-04-07 13:49:27,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 183: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-04-07 13:49:35,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 183 finished
galaxy.model.store.discover DEBUG 2025-04-07 13:49:35,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (183) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/183/working/output_dir/edgeR_(2*Mut_3*WT)-WT.tsv] with element identifier [edgeR_(2*Mut_3*WT)-WT] for output [outTables] (6.558 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:49:35,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (183) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/183/working/output_dir/edgeR_Mut-WT.tsv] with element identifier [edgeR_Mut-WT] for output [outTables] (0.830 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:49:35,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (183) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/183/working/output_dir/edgeR_WT-Mut.tsv] with element identifier [edgeR_WT-Mut] for output [outTables] (0.701 ms)
galaxy.model.store.discover DEBUG 2025-04-07 13:49:35,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (183) Add dynamic collection datasets to history for output [outTables] (40.374 ms)
galaxy.model.metadata DEBUG 2025-04-07 13:49:35,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 222
galaxy.jobs INFO 2025-04-07 13:49:35,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 183 in /galaxy/server/database/jobs_directory/000/183
galaxy.jobs DEBUG 2025-04-07 13:49:35,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 183 executed (206.988 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-04-07 13:49:35,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
