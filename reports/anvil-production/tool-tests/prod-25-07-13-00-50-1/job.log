galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:19,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7knkg with k8s id: gxy-7knkg succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:19,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zg4cm with k8s id: gxy-zg4cm succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:10:19,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 7: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:10:19,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 8: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:10:28,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 8 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:10:28,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.1.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/8/working/data_fetch_upload_yw9g_cha', 'object_id': 8}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:10:28,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 7 finished
galaxy.jobs INFO 2025-07-13 01:10:28,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 8 in /galaxy/server/database/jobs_directory/000/8
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:10:28,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/7/working/gxupload_0', 'object_id': 7}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:10:28,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 8 executed (202.243 ms)
galaxy.jobs INFO 2025-07-13 01:10:28,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 7 in /galaxy/server/database/jobs_directory/000/7
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:28,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:10:28,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 7 executed (177.289 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:28,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:10:28,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 9
tpv.core.entities DEBUG 2025-07-13 01:10:29,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:10:29,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:10:29,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:10:29,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:10:29,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:10:29,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Working directory for job is: /galaxy/server/database/jobs_directory/000/9
galaxy.jobs.runners DEBUG 2025-07-13 01:10:29,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [9] queued (43.636 ms)
galaxy.jobs.handler INFO 2025-07-13 01:10:29,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:29,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 9
galaxy.jobs DEBUG 2025-07-13 01:10:29,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [9] prepared (70.986 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:10:29,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:10:29,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.mulled.util DEBUG 2025-07-13 01:10:29,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Querying https://quay.io/api/v1/repository for repos within biocontainers
galaxy.tool_util.deps.containers INFO 2025-07-13 01:10:57,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:10:57,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/9/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/9/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/4/9/5/dataset_495e1ace-2b11-449b-b114-d189d4d73394.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/0/9/2/metadata_092322f0-ce88-41a2-a79f-d83fe96cf699.dat' '0.bai' &&   samtools depth  -b '/galaxy/server/database/objects/1/0/1/dataset_101d2a23-02b7-4331-b3cd-4f74b725df42.dat' 0 > '/galaxy/server/database/objects/3/9/b/dataset_39b8e8a6-5896-4412-93e8-e566ed3b6ac0.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:10:57,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (9) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/9/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/9/galaxy_9.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:57,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:10:57,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:10:57,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-07-13 01:10:57,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:57,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:10:57,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:11,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gr5b8 with k8s id: gxy-gr5b8 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:11:11,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 9: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:11:18,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 9 finished
galaxy.model.metadata DEBUG 2025-07-13 01:11:18,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 9
galaxy.jobs INFO 2025-07-13 01:11:18,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 9 in /galaxy/server/database/jobs_directory/000/9
galaxy.jobs DEBUG 2025-07-13 01:11:19,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 9 executed (125.008 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:19,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:11:25,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 10
tpv.core.entities DEBUG 2025-07-13 01:11:25,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:11:25,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:11:25,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:11:25,151 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:11:25,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:11:25,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Working directory for job is: /galaxy/server/database/jobs_directory/000/10
galaxy.jobs.runners DEBUG 2025-07-13 01:11:25,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [10] queued (40.249 ms)
galaxy.jobs.handler INFO 2025-07-13 01:11:25,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:25,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 10
galaxy.jobs DEBUG 2025-07-13 01:11:25,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [10] prepared (62.653 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:11:25,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/10/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/10/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/10/configs/tmpv5zvyo6e']
galaxy.jobs.runners DEBUG 2025-07-13 01:11:25,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (10) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/10/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/10/galaxy_10.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:25,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:25,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:26,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nwj87 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-nwj87.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:11:35,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-nwj87

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-nwj87": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (10/gxy-nwj87) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (10/gxy-nwj87) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (10/gxy-nwj87) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (10/gxy-nwj87) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-nwj87.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 10 (gxy-nwj87)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-nwj87 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:35,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (10/gxy-nwj87) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-07-13 01:11:37,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 11, 12
tpv.core.entities DEBUG 2025-07-13 01:11:37,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:11:37,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:11:37,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:11:37,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:11:37,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:11:37,464 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Working directory for job is: /galaxy/server/database/jobs_directory/000/11
galaxy.jobs.runners DEBUG 2025-07-13 01:11:37,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [11] queued (34.209 ms)
galaxy.jobs.handler INFO 2025-07-13 01:11:37,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:37,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 11
tpv.core.entities DEBUG 2025-07-13 01:11:37,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:11:37,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:11:37,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:11:37,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:11:37,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:11:37,524 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Working directory for job is: /galaxy/server/database/jobs_directory/000/12
galaxy.jobs.runners DEBUG 2025-07-13 01:11:37,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [12] queued (38.967 ms)
galaxy.jobs.handler INFO 2025-07-13 01:11:37,532 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:37,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 12
galaxy.jobs DEBUG 2025-07-13 01:11:37,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [11] prepared (74.649 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:11:37,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/11/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/11/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/11/configs/tmp5w1j27hj']
galaxy.jobs.runners DEBUG 2025-07-13 01:11:37,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (11) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/11/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/11/galaxy_11.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:37,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:11:37,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [12] prepared (71.630 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:37,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:11:37,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/12/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/12/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/12/configs/tmptjwfeuyr']
galaxy.jobs.runners DEBUG 2025-07-13 01:11:37,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (12) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/12/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/12/galaxy_12.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:37,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:37,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:38,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:38,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:48,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dck95 with k8s id: gxy-dck95 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:48,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7rwnx with k8s id: gxy-7rwnx succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:11:48,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 11: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:11:48,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 12: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:11:56,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 11 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:11:56,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/11/working/gxupload_0', 'object_id': 11}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:11:56,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 12 finished
galaxy.jobs INFO 2025-07-13 01:11:56,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 11 in /galaxy/server/database/jobs_directory/000/11
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:11:56,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/12/working/gxupload_0', 'object_id': 12}]}]}]
galaxy.jobs INFO 2025-07-13 01:11:56,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 12 in /galaxy/server/database/jobs_directory/000/12
galaxy.jobs DEBUG 2025-07-13 01:11:56,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 11 executed (204.153 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:56,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:11:56,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 12 executed (169.281 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:56,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:11:56,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 13
tpv.core.entities DEBUG 2025-07-13 01:11:56,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:11:56,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:11:56,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:11:56,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:11:56,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:11:56,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Working directory for job is: /galaxy/server/database/jobs_directory/000/13
galaxy.jobs.runners DEBUG 2025-07-13 01:11:56,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [13] queued (41.708 ms)
galaxy.jobs.handler INFO 2025-07-13 01:11:56,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:56,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 13
galaxy.jobs DEBUG 2025-07-13 01:11:57,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [13] prepared (73.550 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:11:57,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:11:57,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-07-13 01:11:57,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:11:57,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/13/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/13/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/7/6/5/dataset_765e01cf-c886-4e3b-9d1e-20ba8ad700e1.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/3/2/0/metadata_320eee5e-3fde-4c80-a353-b21d2c4e47c0.dat' '0.bai' && ln -s '/galaxy/server/database/objects/d/2/1/dataset_d2167a42-221b-4c40-bcfc-676770b638ef.dat' '1' && ln -s '/galaxy/server/database/objects/_metadata_files/e/2/c/metadata_e2c3a4f7-7b0f-47b3-b12d-7be6936f13f0.dat' '1.bai' &&   samtools depth -a 0 1 > '/galaxy/server/database/objects/b/d/4/dataset_bd4c362b-0b64-41f8-813c-6ec67876c528.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:11:57,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (13) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/13/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/13/galaxy_13.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:57,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:11:57,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:11:57,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-07-13 01:11:57,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:57,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:11:58,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:02,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-69j6z with k8s id: gxy-69j6z succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:12:02,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 13: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:12:10,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 13 finished
galaxy.model.metadata DEBUG 2025-07-13 01:12:10,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 13
galaxy.jobs INFO 2025-07-13 01:12:10,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 13 in /galaxy/server/database/jobs_directory/000/13
galaxy.jobs DEBUG 2025-07-13 01:12:10,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 13 executed (119.706 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:10,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:12:12,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 14, 15
tpv.core.entities DEBUG 2025-07-13 01:12:12,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:12:12,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:12:12,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:12:12,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:12:12,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:12:12,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Working directory for job is: /galaxy/server/database/jobs_directory/000/14
galaxy.jobs.runners DEBUG 2025-07-13 01:12:12,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [14] queued (28.468 ms)
galaxy.jobs.handler INFO 2025-07-13 01:12:12,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:12,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 14
tpv.core.entities DEBUG 2025-07-13 01:12:12,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:12:12,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:12:12,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:12:12,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:12:12,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:12:12,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Working directory for job is: /galaxy/server/database/jobs_directory/000/15
galaxy.jobs.runners DEBUG 2025-07-13 01:12:12,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [15] queued (53.892 ms)
galaxy.jobs.handler INFO 2025-07-13 01:12:12,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:12,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 15
galaxy.jobs DEBUG 2025-07-13 01:12:12,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [14] prepared (84.882 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:12:12,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/14/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/14/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/14/configs/tmpnl9oht48']
galaxy.jobs.runners DEBUG 2025-07-13 01:12:12,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (14) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/14/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/14/galaxy_14.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:12,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:12:12,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [15] prepared (78.686 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:12,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:12:12,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/15/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/15/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/15/configs/tmpdr7h6n8k']
galaxy.jobs.runners DEBUG 2025-07-13 01:12:12,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (15) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/15/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/15/galaxy_15.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:12,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:12,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:13,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:13,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:23,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hmc6m with k8s id: gxy-hmc6m succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:23,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v8z8r with k8s id: gxy-v8z8r succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:12:23,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 14: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:12:23,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 15: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:12:31,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 15 finished
galaxy.jobs.runners DEBUG 2025-07-13 01:12:31,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 14 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:12:31,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/15/working/gxupload_0', 'object_id': 15}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:12:31,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'eboVir3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/14/working/gxupload_0', 'object_id': 14}]}]}]
galaxy.jobs INFO 2025-07-13 01:12:31,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 15 in /galaxy/server/database/jobs_directory/000/15
galaxy.jobs INFO 2025-07-13 01:12:31,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 14 in /galaxy/server/database/jobs_directory/000/14
galaxy.jobs DEBUG 2025-07-13 01:12:31,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 15 executed (186.086 ms)
galaxy.jobs DEBUG 2025-07-13 01:12:31,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 14 executed (181.649 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:31,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:31,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:12:31,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 16
tpv.core.entities DEBUG 2025-07-13 01:12:31,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:12:31,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:12:31,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:12:31,862 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:12:31,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:12:31,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Working directory for job is: /galaxy/server/database/jobs_directory/000/16
galaxy.jobs.runners DEBUG 2025-07-13 01:12:31,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [16] queued (42.289 ms)
galaxy.jobs.handler INFO 2025-07-13 01:12:31,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:31,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 16
galaxy.jobs DEBUG 2025-07-13 01:12:31,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [16] prepared (53.509 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:12:31,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:12:31,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-07-13 01:12:32,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:12:32,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/16/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/16/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/7/1/d/dataset_71d40978-3581-469e-8698-1f92f84a3a67.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/2/e/e/metadata_2ee93bbe-0ea1-46d4-9ab3-e448334e43c8.dat' '0.bai' && ln -s '/galaxy/server/database/objects/0/9/0/dataset_09044cd1-e9eb-4583-9f92-f6e9a9472ada.dat' '1' && ln -s '/galaxy/server/database/objects/_metadata_files/1/c/4/metadata_1c42d5be-dd50-43dc-927e-0fd16b25f98d.dat' '1.bai' &&   samtools depth  -l 10 -m 4 -q 11 -Q 12 0 1 > '/galaxy/server/database/objects/7/7/6/dataset_776eeab5-196e-4466-9fce-5f29bd646a3b.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:12:32,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (16) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/16/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/16/galaxy_16.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:32,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:12:32,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:12:32,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-07-13 01:12:32,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:32,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:33,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:37,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nwfs4 with k8s id: gxy-nwfs4 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:12:37,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 16: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:12:45,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 16 finished
galaxy.model.metadata DEBUG 2025-07-13 01:12:45,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 16
galaxy.jobs INFO 2025-07-13 01:12:45,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 16 in /galaxy/server/database/jobs_directory/000/16
galaxy.jobs DEBUG 2025-07-13 01:12:45,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 16 executed (123.466 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:45,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:12:48,269 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 17
tpv.core.entities DEBUG 2025-07-13 01:12:48,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:12:48,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:12:48,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:12:48,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:12:48,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:12:48,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Working directory for job is: /galaxy/server/database/jobs_directory/000/17
galaxy.jobs.runners DEBUG 2025-07-13 01:12:48,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [17] queued (34.250 ms)
galaxy.jobs.handler INFO 2025-07-13 01:12:48,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:48,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 17
galaxy.jobs DEBUG 2025-07-13 01:12:48,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [17] prepared (61.223 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:12:48,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/17/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/17/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/17/configs/tmp3276jpm4']
galaxy.jobs.runners DEBUG 2025-07-13 01:12:48,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (17) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/17/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/17/galaxy_17.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:48,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:48,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:49,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:12:59,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-47b88 with k8s id: gxy-47b88 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:12:59,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 17: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:13:07,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 17 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:13:07,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'samtools-rmdup-input1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/17/working/gxupload_0', 'object_id': 17}]}]}]
galaxy.jobs INFO 2025-07-13 01:13:07,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 17 in /galaxy/server/database/jobs_directory/000/17
galaxy.jobs DEBUG 2025-07-13 01:13:07,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 17 executed (160.446 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:07,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:13:08,755 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 18
tpv.core.entities DEBUG 2025-07-13 01:13:08,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:13:08,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:13:08,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:13:08,791 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:13:08,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:13:08,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Working directory for job is: /galaxy/server/database/jobs_directory/000/18
galaxy.jobs.runners DEBUG 2025-07-13 01:13:08,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [18] queued (32.362 ms)
galaxy.jobs.handler INFO 2025-07-13 01:13:08,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:08,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 18
galaxy.jobs DEBUG 2025-07-13 01:13:08,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [18] prepared (59.225 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:08,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:13:08,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/2.0.1: samtools:1.3.1
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:08,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.3.1--h60f3df9_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:13:08,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/18/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/18/outputs/COMMAND_VERSION 2>&1;
samtools rmdup  '/galaxy/server/database/objects/5/c/a/dataset_5ca2cab8-9791-47d3-a697-633b23b24a42.dat' '/galaxy/server/database/objects/1/f/2/dataset_1f2d54ca-5b87-4c88-a464-42c2949ef623.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:13:08,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (18) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/18/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/18/galaxy_18.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:09,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:09,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:13:09,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/2.0.1: samtools:1.3.1
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:09,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.3.1--h60f3df9_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:09,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:09,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:17,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cnrxv with k8s id: gxy-cnrxv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:13:18,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 18: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:13:25,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 18 finished
galaxy.model.metadata DEBUG 2025-07-13 01:13:25,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 18
galaxy.jobs INFO 2025-07-13 01:13:25,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 18 in /galaxy/server/database/jobs_directory/000/18
galaxy.jobs DEBUG 2025-07-13 01:13:25,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 18 executed (138.503 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:25,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:13:29,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 19, 20
tpv.core.entities DEBUG 2025-07-13 01:13:29,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:13:29,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:13:29,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:13:29,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:13:29,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:13:29,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Working directory for job is: /galaxy/server/database/jobs_directory/000/19
galaxy.jobs.runners DEBUG 2025-07-13 01:13:29,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [19] queued (40.146 ms)
galaxy.jobs.handler INFO 2025-07-13 01:13:29,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 19
tpv.core.entities DEBUG 2025-07-13 01:13:29,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:13:29,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:13:29,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:13:29,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:13:29,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:13:29,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Working directory for job is: /galaxy/server/database/jobs_directory/000/20
galaxy.jobs.runners DEBUG 2025-07-13 01:13:29,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [20] queued (49.150 ms)
galaxy.jobs.handler INFO 2025-07-13 01:13:29,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 20
galaxy.jobs DEBUG 2025-07-13 01:13:29,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [19] prepared (83.861 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:13:29,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/19/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/19/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/19/configs/tmp7gd4dbfb']
galaxy.jobs.runners DEBUG 2025-07-13 01:13:29,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (19) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/19/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/19/galaxy_19.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:13:29,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [20] prepared (88.140 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:13:29,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/20/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/20/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/20/configs/tmplfldqwl8']
galaxy.jobs.runners DEBUG 2025-07-13 01:13:29,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (20) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/20/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/20/galaxy_20.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:29,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:39,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lmdn5 with k8s id: gxy-lmdn5 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:13:39,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 20: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:40,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kdznz with k8s id: gxy-kdznz succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:13:40,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 19: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:13:47,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 20 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:13:47,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ref.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/20/working/data_fetch_upload_zwko4wlz', 'object_id': 20}]}]}]
galaxy.jobs INFO 2025-07-13 01:13:47,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 20 in /galaxy/server/database/jobs_directory/000/20
galaxy.jobs DEBUG 2025-07-13 01:13:47,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 20 executed (142.199 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:47,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:13:48,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 19 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:13:48,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/19/working/gxupload_0', 'object_id': 19}]}]}]
galaxy.jobs INFO 2025-07-13 01:13:48,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 19 in /galaxy/server/database/jobs_directory/000/19
galaxy.jobs DEBUG 2025-07-13 01:13:48,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 19 executed (205.673 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:48,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:13:49,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 21
tpv.core.entities DEBUG 2025-07-13 01:13:49,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:13:49,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:13:49,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:13:49,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:13:49,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:13:49,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Working directory for job is: /galaxy/server/database/jobs_directory/000/21
galaxy.jobs.runners DEBUG 2025-07-13 01:13:49,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [21] queued (47.762 ms)
galaxy.jobs.handler INFO 2025-07-13 01:13:49,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:49,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 21
galaxy.security.object_wrapper WARNING 2025-07-13 01:13:49,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to create dynamic subclass SafeStringWrapper__galaxy.model.none_like.None__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis for <class 'galaxy.model.none_like.NoneDataset'>, None: type() doesn't support MRO entry resolution; use types.new_class()
cheetah_DynamicallyCompiledCheetahTemplate_1752369230_0102904_89797.py:89: SyntaxWarning: invalid escape sequence '\W'
cheetah_DynamicallyCompiledCheetahTemplate_1752369230_0102904_89797.py:131: SyntaxWarning: invalid escape sequence '\W'
galaxy.jobs DEBUG 2025-07-13 01:13:50,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [21] prepared (136.144 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:50,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:13:50,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:50,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:13:50,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/21/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/21/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/f/7/2/dataset_f72a8a01-073a-4e5d-a85a-9aee886a8f6b.dat' mpileup_1.bam && ln -s '/galaxy/server/database/objects/_metadata_files/4/2/7/metadata_427a7e01-4f36-4b22-8b61-7159161f250d.dat' mpileup_1.bam.bai &&  ln -s '/galaxy/server/database/objects/a/d/f/dataset_adf7f247-1856-4211-acd6-b58bee59b30b.dat' ref.fa && samtools faidx ref.fa &&           bcftools mpileup  --fasta-ref 'ref.fa'      -d "250"                  --threads ${GALAXY_SLOTS:-4}   --output-type 'v'   mpileup_1.bam > '/galaxy/server/database/objects/4/e/d/dataset_4ed1c136-8d23-4302-a637-f6c09d36b632.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:13:50,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (21) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/21/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/21/galaxy_21.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:50,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:50,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:13:50,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:13:50,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:50,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:13:51,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:01,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rxnbp with k8s id: gxy-rxnbp succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:14:02,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 21: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:14:09,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 21 finished
galaxy.model.metadata DEBUG 2025-07-13 01:14:09,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 21
galaxy.jobs INFO 2025-07-13 01:14:09,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 21 in /galaxy/server/database/jobs_directory/000/21
galaxy.jobs DEBUG 2025-07-13 01:14:09,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 21 executed (103.737 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:09,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:14:11,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 22, 23
tpv.core.entities DEBUG 2025-07-13 01:14:11,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:14:11,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:14:11,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:14:11,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:14:11,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:14:11,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Working directory for job is: /galaxy/server/database/jobs_directory/000/22
galaxy.jobs.runners DEBUG 2025-07-13 01:14:11,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [22] queued (42.416 ms)
galaxy.jobs.handler INFO 2025-07-13 01:14:11,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 22
tpv.core.entities DEBUG 2025-07-13 01:14:11,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:14:11,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:14:11,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:14:11,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:14:11,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:14:11,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Working directory for job is: /galaxy/server/database/jobs_directory/000/23
galaxy.jobs.runners DEBUG 2025-07-13 01:14:11,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [23] queued (48.279 ms)
galaxy.jobs.handler INFO 2025-07-13 01:14:11,463 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 23
galaxy.jobs DEBUG 2025-07-13 01:14:11,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [22] prepared (86.636 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:14:11,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/22/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/22/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/22/configs/tmpzeix235w']
galaxy.jobs.runners DEBUG 2025-07-13 01:14:11,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (22) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/22/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/22/galaxy_22.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:14:11,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [23] prepared (74.168 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:14:11,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/23/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/23/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/23/configs/tmpq0br4tq3']
galaxy.jobs.runners DEBUG 2025-07-13 01:14:11,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (23) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/23/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/23/galaxy_23.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:11,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:21,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mrwvm with k8s id: gxy-mrwvm succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:21,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gnl5x with k8s id: gxy-gnl5x succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:14:21,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 22: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:14:21,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 23: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:14:29,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 22 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:14:29,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.3.cram', 'dbkey': '?', 'ext': 'cram', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded cram file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/22/working/gxupload_0', 'object_id': 22}]}]}]
galaxy.jobs INFO 2025-07-13 01:14:29,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 22 in /galaxy/server/database/jobs_directory/000/22
galaxy.jobs.runners DEBUG 2025-07-13 01:14:29,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 23 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:14:29,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ref.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/23/working/data_fetch_upload_ofasjwf0', 'object_id': 23}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:14:29,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 22 executed (189.014 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:29,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:14:29,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 23 in /galaxy/server/database/jobs_directory/000/23
galaxy.jobs DEBUG 2025-07-13 01:14:30,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 23 executed (153.786 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:30,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:14:31,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 24
tpv.core.entities DEBUG 2025-07-13 01:14:31,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:14:31,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:14:31,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:14:31,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:14:31,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:14:31,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Working directory for job is: /galaxy/server/database/jobs_directory/000/24
galaxy.jobs.runners DEBUG 2025-07-13 01:14:31,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [24] queued (38.133 ms)
galaxy.jobs.handler INFO 2025-07-13 01:14:31,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:31,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 24
galaxy.jobs DEBUG 2025-07-13 01:14:31,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [24] prepared (80.840 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:14:31,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:14:31,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:14:31,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:14:31,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/24/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/24/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/8/2/b/dataset_82b265ef-ca54-4a00-ad3f-de157e8cfc8f.dat' mpileup_3.cram && ln -s '/galaxy/server/database/objects/_metadata_files/d/9/c/metadata_d9cc7bb9-dd98-4770-8271-80ce8f18c3b7.dat' mpileup_3.cram.crai &&  ln -s '/galaxy/server/database/objects/7/a/6/dataset_7a6698c1-62f2-4502-972f-19f9d29f4fe5.dat' ref.fa && samtools faidx ref.fa &&           bcftools mpileup  --fasta-ref 'ref.fa'      -d "250"                  --threads ${GALAXY_SLOTS:-4}   --output-type 'v'   mpileup_3.cram > '/galaxy/server/database/objects/e/2/f/dataset_e2f6a7e3-935c-4813-a620-445d188fd09e.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:14:31,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (24) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/24/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/24/galaxy_24.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:31,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:14:31,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:14:31,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:14:31,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:31,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:31,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:35,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sgkrt with k8s id: gxy-sgkrt succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:14:36,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 24: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:14:43,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 24 finished
galaxy.model.metadata DEBUG 2025-07-13 01:14:43,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 24
galaxy.jobs INFO 2025-07-13 01:14:43,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 24 in /galaxy/server/database/jobs_directory/000/24
galaxy.jobs DEBUG 2025-07-13 01:14:43,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 24 executed (83.679 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:43,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:14:45,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 25, 26, 27, 28
tpv.core.entities DEBUG 2025-07-13 01:14:45,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:14:45,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:14:45,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:14:45,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:14:45,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:14:45,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Working directory for job is: /galaxy/server/database/jobs_directory/000/25
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [25] queued (41.766 ms)
galaxy.jobs.handler INFO 2025-07-13 01:14:45,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 25
tpv.core.entities DEBUG 2025-07-13 01:14:45,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:14:45,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:14:45,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:14:45,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:14:45,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:14:45,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Working directory for job is: /galaxy/server/database/jobs_directory/000/26
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,522 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [26] queued (49.343 ms)
galaxy.jobs.handler INFO 2025-07-13 01:14:45,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 26
tpv.core.entities DEBUG 2025-07-13 01:14:45,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:14:45,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:14:45,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:14:45,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:14:45,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [25] prepared (97.610 ms)
galaxy.jobs DEBUG 2025-07-13 01:14:45,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:14:45,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Working directory for job is: /galaxy/server/database/jobs_directory/000/27
galaxy.jobs.command_factory INFO 2025-07-13 01:14:45,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/25/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/25/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/25/configs/tmpnibhsuh8']
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [27] queued (60.815 ms)
galaxy.jobs.handler INFO 2025-07-13 01:14:45,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 27
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (25) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/25/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/25/galaxy_25.ec; sh -c "exit $return_code"
tpv.core.entities DEBUG 2025-07-13 01:14:45,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:14:45,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:14:45,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:14:45,633 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Dispatching to k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:14:45,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [26] prepared (113.062 ms)
galaxy.jobs DEBUG 2025-07-13 01:14:45,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Persisting job destination (destination id: k8s)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:14:45,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/26/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/26/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/26/configs/tmphapsx6zd']
galaxy.jobs DEBUG 2025-07-13 01:14:45,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Working directory for job is: /galaxy/server/database/jobs_directory/000/28
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [28] queued (74.048 ms)
galaxy.jobs.handler INFO 2025-07-13 01:14:45,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Job dispatched
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/26/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/26/galaxy_26.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 28
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:14:45,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [27] prepared (124.047 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:14:45,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/27/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/27/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/27/configs/tmpjz159lqj']
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (27) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/27/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/27/galaxy_27.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:14:45,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [28] prepared (95.555 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:14:45,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/28/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/28/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/28/configs/tmpm137nr1z']
galaxy.jobs.runners DEBUG 2025-07-13 01:14:45,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (28) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/28/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/28/galaxy_28.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:45,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:46,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:46,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:46,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:47,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:57,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r27bx with k8s id: gxy-r27bx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:57,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c65jr with k8s id: gxy-c65jr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:57,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-65zg9 with k8s id: gxy-65zg9 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:14:57,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv5b8 with k8s id: gxy-fv5b8 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:14:57,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 25: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:14:57,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 26: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:14:57,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 27: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:14:57,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 28: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:15:13,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 25 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:15:13,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/25/working/gxupload_0', 'object_id': 25}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:15:13,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 27 finished
galaxy.jobs.runners DEBUG 2025-07-13 01:15:14,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 26 finished
galaxy.jobs.runners DEBUG 2025-07-13 01:15:14,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 28 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:15:14,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/27/working/gxupload_0', 'object_id': 27}]}]}]
galaxy.jobs INFO 2025-07-13 01:15:14,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 25 in /galaxy/server/database/jobs_directory/000/25
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:15:14,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ref.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/28/working/data_fetch_upload_c26untrt', 'object_id': 28}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:15:14,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/26/working/gxupload_0', 'object_id': 26}]}]}]
galaxy.jobs INFO 2025-07-13 01:15:14,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 27 in /galaxy/server/database/jobs_directory/000/27
galaxy.jobs INFO 2025-07-13 01:15:14,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 28 in /galaxy/server/database/jobs_directory/000/28
galaxy.jobs INFO 2025-07-13 01:15:14,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 26 in /galaxy/server/database/jobs_directory/000/26
galaxy.jobs DEBUG 2025-07-13 01:15:14,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 25 executed (427.695 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:14,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:15:14,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 27 executed (244.798 ms)
galaxy.jobs DEBUG 2025-07-13 01:15:14,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 28 executed (220.059 ms)
galaxy.jobs DEBUG 2025-07-13 01:15:14,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 26 executed (237.731 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:14,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:14,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:14,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:15:14,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 29
tpv.core.entities DEBUG 2025-07-13 01:15:14,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:14,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:14,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:14,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:14,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:15:14,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Working directory for job is: /galaxy/server/database/jobs_directory/000/29
galaxy.jobs.runners DEBUG 2025-07-13 01:15:14,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [29] queued (46.472 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:14,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:14,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 29
galaxy.jobs DEBUG 2025-07-13 01:15:15,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [29] prepared (101.770 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:15:15,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:15:15,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:15:15,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:15:15,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/29/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/29/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/c/e/e/dataset_ceeb8316-1357-4e04-8330-8b21b6cd46fe.dat' mpileup_1.bam && ln -s '/galaxy/server/database/objects/_metadata_files/a/1/a/metadata_a1a98b6b-6ed2-4042-9f27-daeff2a4f78e.dat' mpileup_1.bam.bai && ln -s '/galaxy/server/database/objects/7/a/d/dataset_7ad4fc93-16d3-4d9d-afa0-3620eb9c09ca.dat' mpileup_2.bam && ln -s '/galaxy/server/database/objects/_metadata_files/c/d/2/metadata_cd220fc5-3df5-49e1-a20a-3ae08721b1ec.dat' mpileup_2.bam.bai && ln -s '/galaxy/server/database/objects/e/2/4/dataset_e2499306-a1c0-4faf-8c82-4130c44a2c49.dat' mpileup_3.bam && ln -s '/galaxy/server/database/objects/_metadata_files/0/2/b/metadata_02bc5adf-1441-496a-bde8-8eb3b2592298.dat' mpileup_3.bam.bai &&  ln -s '/galaxy/server/database/objects/a/f/0/dataset_af0de945-7d29-4585-8651-4d7383667ed3.dat' ref.fa && samtools faidx ref.fa &&           bcftools mpileup  --fasta-ref 'ref.fa'      -d "250"    --annotate "DP,INFO/AD,DV"            --regions '17:100-110'         --threads ${GALAXY_SLOTS:-4}   --output-type 'v'   mpileup_1.bam mpileup_2.bam mpileup_3.bam > '/galaxy/server/database/objects/0/6/d/dataset_06d51303-fb8f-4331-9857-5fd01d7fd7fc.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:15,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (29) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/29/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/29/galaxy_29.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:15,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:15:15,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:15:15,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:15:15,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:15,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:15,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:19,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m8x8k with k8s id: gxy-m8x8k succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:15:19,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 29: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:15:27,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 29 finished
galaxy.model.metadata DEBUG 2025-07-13 01:15:27,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 29
galaxy.jobs INFO 2025-07-13 01:15:27,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 29 in /galaxy/server/database/jobs_directory/000/29
galaxy.jobs DEBUG 2025-07-13 01:15:27,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 29 executed (91.385 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:27,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:15:29,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 30, 32, 31, 33
tpv.core.entities DEBUG 2025-07-13 01:15:29,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:29,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:29,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:29,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:29,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:15:29,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Working directory for job is: /galaxy/server/database/jobs_directory/000/30
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [30] queued (39.893 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:29,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 30
tpv.core.entities DEBUG 2025-07-13 01:15:29,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:29,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:29,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:29,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:29,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:15:29,316 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Working directory for job is: /galaxy/server/database/jobs_directory/000/32
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [32] queued (49.908 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:29,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 32
tpv.core.entities DEBUG 2025-07-13 01:15:29,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:29,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:29,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:29,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:29,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [30] prepared (97.655 ms)
galaxy.jobs DEBUG 2025-07-13 01:15:29,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-07-13 01:15:29,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/30/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/30/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/30/configs/tmpjyne1uyw']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (30) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/30/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/30/galaxy_30.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:15:29,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Working directory for job is: /galaxy/server/database/jobs_directory/000/31
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [31] queued (57.382 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:29,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 31
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
tpv.core.entities DEBUG 2025-07-13 01:15:29,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:29,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:29,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:29,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Dispatching to k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:15:29,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [32] prepared (113.740 ms)
galaxy.jobs DEBUG 2025-07-13 01:15:29,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-07-13 01:15:29,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/32/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/32/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/32/configs/tmpbd3atc1f']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (32) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/32/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/32/galaxy_32.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:15:29,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Working directory for job is: /galaxy/server/database/jobs_directory/000/33
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [33] queued (81.280 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:29,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 33
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:15:29,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [31] prepared (121.085 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:15:29,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/31/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/31/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/31/configs/tmpshx8j4li']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (31) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/31/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/31/galaxy_31.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:15:29,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [33] prepared (98.850 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:15:29,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/33/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/33/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/33/configs/tmpfcn5uckb']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:29,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (33) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/33/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/33/galaxy_33.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:29,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:30,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:30,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:30,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xm4q4 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xm4q4.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:15:40,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-xm4q4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-xm4q4": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (32/gxy-xm4q4) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (32/gxy-xm4q4) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (32/gxy-xm4q4) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (32/gxy-xm4q4) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xm4q4.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 32 (gxy-xm4q4)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-xm4q4 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-txfjr with k8s id: gxy-txfjr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:40,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2zgrm with k8s id: gxy-2zgrm succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:41,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (32/gxy-xm4q4) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-07-13 01:15:41,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 31: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:15:41,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 33: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:41,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kg4r2 with k8s id: gxy-kg4r2 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:15:42,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 30: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:15:52,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 31 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:15:53,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/31/working/gxupload_0', 'object_id': 31}]}]}]
galaxy.jobs INFO 2025-07-13 01:15:53,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 31 in /galaxy/server/database/jobs_directory/000/31
galaxy.jobs.runners DEBUG 2025-07-13 01:15:53,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 33 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:15:53,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ref.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/33/working/data_fetch_upload_upmje00z', 'object_id': 33}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:15:53,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 31 executed (255.274 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:53,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:15:53,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 33 in /galaxy/server/database/jobs_directory/000/33
galaxy.jobs DEBUG 2025-07-13 01:15:53,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 33 executed (164.177 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:53,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:15:54,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 30 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:15:54,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/30/working/gxupload_0', 'object_id': 30}]}]}]
galaxy.jobs INFO 2025-07-13 01:15:54,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 30 in /galaxy/server/database/jobs_directory/000/30
galaxy.jobs DEBUG 2025-07-13 01:15:54,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 30 executed (212.425 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:54,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:15:55,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 34
tpv.core.entities DEBUG 2025-07-13 01:15:55,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:55,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:55,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:55,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:55,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:15:55,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Working directory for job is: /galaxy/server/database/jobs_directory/000/34
galaxy.jobs.runners DEBUG 2025-07-13 01:15:55,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [34] queued (40.909 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:55,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:55,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 34
galaxy.jobs DEBUG 2025-07-13 01:15:55,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [34] prepared (80.640 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:15:55,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/34/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/34/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/34/configs/tmpwqyhfica']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:55,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (34) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/34/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/34/galaxy_34.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:55,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:55,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:15:56,269 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 36, 35, 37
tpv.core.entities DEBUG 2025-07-13 01:15:56,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:56,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:56,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:56,304 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:56,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:15:56,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Working directory for job is: /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-07-13 01:15:56,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [36] queued (38.675 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:56,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 36
tpv.core.entities DEBUG 2025-07-13 01:15:56,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:56,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:56,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:56,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:56,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:15:56,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Working directory for job is: /galaxy/server/database/jobs_directory/000/35
galaxy.jobs.runners DEBUG 2025-07-13 01:15:56,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [35] queued (53.083 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:56,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 35
tpv.core.entities DEBUG 2025-07-13 01:15:56,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:15:56,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:15:56,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:15:56,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:15:56,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [36] prepared (100.783 ms)
galaxy.jobs DEBUG 2025-07-13 01:15:56,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:15:56,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.command_factory INFO 2025-07-13 01:15:56,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/36/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/36/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/36/configs/tmpcymsffz2']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:56,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (66.191 ms)
galaxy.jobs.handler INFO 2025-07-13 01:15:56,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 37
galaxy.jobs.runners DEBUG 2025-07-13 01:15:56,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (36) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/36/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/36/galaxy_36.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:15:56,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [35] prepared (112.675 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:15:56,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/35/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/35/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/35/configs/tmpn5of7sme']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:56,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (35) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/35/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/35/galaxy_35.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:15:56,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [37] prepared (93.498 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:15:56,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/37/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/37/configs/tmpphapz5au']
galaxy.jobs.runners DEBUG 2025-07-13 01:15:56,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:56,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:57,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:57,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:15:57,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:06,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pm6cg with k8s id: gxy-pm6cg succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:16:06,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 34: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:07,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tsjkv with k8s id: gxy-tsjkv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:16:07,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 37: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:08,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mn6th with k8s id: gxy-mn6th succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:08,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w7xgq with k8s id: gxy-w7xgq succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:16:08,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 36: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:16:08,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 35: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:16:22,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 34 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:16:22,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/34/working/gxupload_0', 'object_id': 34}]}]}]
galaxy.jobs INFO 2025-07-13 01:16:22,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 34 in /galaxy/server/database/jobs_directory/000/34
galaxy.jobs DEBUG 2025-07-13 01:16:22,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 34 executed (413.705 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:22,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:16:23,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 37 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:16:24,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.ref.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/37/working/data_fetch_upload_e5l5uso_', 'object_id': 37}]}]}]
galaxy.jobs INFO 2025-07-13 01:16:24,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 37 in /galaxy/server/database/jobs_directory/000/37
galaxy.jobs DEBUG 2025-07-13 01:16:24,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 37 executed (210.814 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:24,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:16:24,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 36 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:16:24,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.3.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/36/working/gxupload_0', 'object_id': 36}]}]}]
galaxy.jobs INFO 2025-07-13 01:16:24,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-07-13 01:16:24,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 35 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:16:24,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/35/working/gxupload_0', 'object_id': 35}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:16:24,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 36 executed (204.379 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:24,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:16:24,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 35 in /galaxy/server/database/jobs_directory/000/35
galaxy.jobs DEBUG 2025-07-13 01:16:25,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 35 executed (206.534 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:25,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:16:26,464 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38
tpv.core.entities DEBUG 2025-07-13 01:16:26,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:16:26,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/.*, abstract=False, cores=7, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:16:26,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:16:26,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:16:26,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:16:26,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-07-13 01:16:26,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (47.348 ms)
galaxy.jobs.handler INFO 2025-07-13 01:16:26,565 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:26,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 38
galaxy.jobs DEBUG 2025-07-13 01:16:26,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [38] prepared (108.185 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:16:26,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:16:26,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:16:26,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:16:26,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/38/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/9/b/9/dataset_9b910959-46a6-4a79-8769-239060fa14d6.dat' mpileup_1.bam && ln -s '/galaxy/server/database/objects/_metadata_files/f/5/3/metadata_f531f45c-b501-4883-889b-6c4274842c26.dat' mpileup_1.bam.bai && ln -s '/galaxy/server/database/objects/4/7/1/dataset_471387ed-f9b1-4490-9774-1f25a5c0c8eb.dat' mpileup_2.bam && ln -s '/galaxy/server/database/objects/_metadata_files/4/c/f/metadata_4cf4c675-cd9c-4102-8166-07ee5bb52528.dat' mpileup_2.bam.bai && ln -s '/galaxy/server/database/objects/1/7/b/dataset_17b395a5-a7f7-41c6-bb72-31c4b0bd06b4.dat' mpileup_3.bam && ln -s '/galaxy/server/database/objects/_metadata_files/e/c/e/metadata_eceb2066-2245-4adc-8bfe-178562eae8ec.dat' mpileup_3.bam.bai &&  ln -s '/galaxy/server/database/objects/8/8/6/dataset_886e7f38-012b-4f46-9508-a14c7f0fa590.dat' ref.fa && samtools faidx ref.fa &&           bcftools mpileup  --fasta-ref 'ref.fa'      --skip-all-set 20 -d "250"               --regions '17:1050-1060'         --threads ${GALAXY_SLOTS:-4}   --output-type 'v'   mpileup_1.bam mpileup_2.bam mpileup_3.bam > '/galaxy/server/database/objects/7/9/1/dataset_791fe230-ff1c-446c-afd0-cf0300e8e695.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:16:26,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:26,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:16:26,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:16:26,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_mpileup/bcftools_mpileup/1.15.1+galaxy4: mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:16:26,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-f7a49c68bd00a9e0147f58c2f0ef0a7bd67e944b:0bba35a3d4a3832a30878d78eef61805a2e1475a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:26,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:28,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mwb5c failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-mwb5c.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:16:33,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-mwb5c

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-mwb5c": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (38/gxy-mwb5c) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (38/gxy-mwb5c) tool_stderr: [mpileup] 3 samples in 3 input files
[mpileup] maximum number of reads per input file set to -d 250

galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (38/gxy-mwb5c) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (38/gxy-mwb5c) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-mwb5c.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 38 (gxy-mwb5c)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-mwb5c to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:33,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (38/gxy-mwb5c) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-07-13 01:16:34,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 40, 39
tpv.core.entities DEBUG 2025-07-13 01:16:34,747 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:16:34,747 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:16:34,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:16:34,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:16:34,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:16:34,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-07-13 01:16:34,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (38.117 ms)
galaxy.jobs.handler INFO 2025-07-13 01:16:34,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:34,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 40
tpv.core.entities DEBUG 2025-07-13 01:16:34,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:16:34,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:16:34,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:16:34,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:16:34,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:16:34,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.runners DEBUG 2025-07-13 01:16:34,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (54.500 ms)
galaxy.jobs.handler INFO 2025-07-13 01:16:34,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:34,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 39
galaxy.jobs DEBUG 2025-07-13 01:16:34,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [40] prepared (92.223 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:16:34,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/40/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/40/configs/tmpuqb7bq0y']
galaxy.jobs.runners DEBUG 2025-07-13 01:16:34,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:34,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:16:34,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [39] prepared (90.163 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:34,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:16:35,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/39/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/39/configs/tmp4iu5tsr7']
galaxy.jobs.runners DEBUG 2025-07-13 01:16:35,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:35,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:35,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:35,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:35,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:44,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-87t9k failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:44,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-87t9k.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:16:45,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-87t9k

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-87t9k": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (40/gxy-87t9k) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (40/gxy-87t9k) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (40/gxy-87t9k) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (40/gxy-87t9k) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-87t9k.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 40 (gxy-87t9k)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-87t9k to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:45,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (40/gxy-87t9k) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:46,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pwj4g with k8s id: gxy-pwj4g succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:16:46,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 39: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:16:54,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 39 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:16:54,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mpileup.1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/39/working/gxupload_0', 'object_id': 39}]}]}]
galaxy.jobs INFO 2025-07-13 01:16:54,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 39 in /galaxy/server/database/jobs_directory/000/39
galaxy.jobs DEBUG 2025-07-13 01:16:54,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 39 executed (217.441 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:54,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:16:56,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-07-13 01:16:56,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:16:56,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:16:56,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:16:56,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:16:56,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:16:56,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-07-13 01:16:56,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (39.763 ms)
galaxy.jobs.handler INFO 2025-07-13 01:16:56,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:56,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 41
galaxy.jobs DEBUG 2025-07-13 01:16:56,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [41] prepared (72.364 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:16:56,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/41/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/41/configs/tmpdu5t564s']
galaxy.jobs.runners DEBUG 2025-07-13 01:16:56,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:56,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:56,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:16:57,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:07,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rbqwr with k8s id: gxy-rbqwr succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:17:07,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:17:15,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 41 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:17:15,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/41/working/data_fetch_upload_9itb241x', 'object_id': 41}]}]}]
galaxy.jobs INFO 2025-07-13 01:17:15,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-07-13 01:17:15,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 41 executed (172.119 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:15,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:17:16,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 42
tpv.core.entities DEBUG 2025-07-13 01:17:16,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:17:16,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:17:16,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:17:16,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:17:16,802 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:17:16,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-07-13 01:17:16,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (29.865 ms)
galaxy.jobs.handler INFO 2025-07-13 01:17:16,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:16,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 42
galaxy.jobs DEBUG 2025-07-13 01:17:16,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [42] prepared (86.335 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:16,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:17:16,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:17,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:17:17,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/42/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/c/2/7/dataset_c27a8d7a-fa43-4480-8342-ea66ef1dc81d.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter  --SnpGap "2" --IndelGap "2" --mode 'x'                 --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/c/9/7/dataset_c975abc8-18be-4482-84a5-64a123c3b121.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:17:17,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:17,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:17,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:17:17,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:17,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:17,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:18,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:28,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qx9sj with k8s id: gxy-qx9sj succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:17:29,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:17:37,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 42 finished
galaxy.model.metadata DEBUG 2025-07-13 01:17:37,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 42
galaxy.jobs INFO 2025-07-13 01:17:37,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs DEBUG 2025-07-13 01:17:37,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 42 executed (148.380 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:37,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:17:38,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43
tpv.core.entities DEBUG 2025-07-13 01:17:38,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:17:38,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:17:38,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:17:38,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:17:38,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:17:38,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-07-13 01:17:38,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (42.158 ms)
galaxy.jobs.handler INFO 2025-07-13 01:17:38,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:38,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 43
galaxy.jobs DEBUG 2025-07-13 01:17:38,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [43] prepared (75.035 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:17:38,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/43/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/43/configs/tmp2z6rmxaj']
galaxy.jobs.runners DEBUG 2025-07-13 01:17:38,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:38,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:38,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:39,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:48,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2m9wr with k8s id: gxy-2m9wr succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:17:48,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:17:56,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 43 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:17:56,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.2.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/43/working/data_fetch_upload_wltsvpry', 'object_id': 43}]}]}]
galaxy.jobs INFO 2025-07-13 01:17:56,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-07-13 01:17:56,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 43 executed (145.617 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:56,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:17:56,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 44
tpv.core.entities DEBUG 2025-07-13 01:17:56,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:17:56,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:17:56,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:17:56,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:17:56,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:17:56,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.runners DEBUG 2025-07-13 01:17:56,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (39.926 ms)
galaxy.jobs.handler INFO 2025-07-13 01:17:56,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:56,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 44
galaxy.jobs DEBUG 2025-07-13 01:17:56,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [44] prepared (52.468 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:56,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:17:56,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:56,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:17:56,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/44/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/a/6/2/dataset_a626a731-3b35-46c8-ae48-34904ef17a57.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter  --set-GTs "."              --exclude 'QUAL==59.2 || (INDEL=0 & (FMT/GQ=25 | FMT/DP=10))'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/0/c/4/dataset_0c464b9f-be6f-4e0e-b44d-5f89784acf8a.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:17:56,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:56,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:56,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:17:56,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:17:56,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:56,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:17:57,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-58f2g failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-58f2g.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:18:01,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-58f2g

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-58f2g": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44/gxy-58f2g) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44/gxy-58f2g) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44/gxy-58f2g) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44/gxy-58f2g) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-58f2g.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 44 (gxy-58f2g)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-58f2g to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:01,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (44/gxy-58f2g) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-07-13 01:18:02,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-07-13 01:18:02,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:18:02,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:18:02,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:18:02,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:18:02,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:18:02,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-07-13 01:18:02,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (42.859 ms)
galaxy.jobs.handler INFO 2025-07-13 01:18:02,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:02,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-07-13 01:18:03,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [45] prepared (80.834 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:18:03,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/45/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/45/configs/tmpuldgpzod']
galaxy.jobs.runners DEBUG 2025-07-13 01:18:03,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:03,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:03,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:03,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:13,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-28fhn with k8s id: gxy-28fhn succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:18:14,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:18:21,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 45 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:18:21,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.3.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/45/working/data_fetch_upload_96oxfajp', 'object_id': 45}]}]}]
galaxy.jobs INFO 2025-07-13 01:18:21,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-07-13 01:18:21,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 45 executed (151.823 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:21,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:18:22,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-07-13 01:18:22,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:18:22,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:18:22,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:18:22,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:18:22,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:18:22,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-07-13 01:18:22,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (44.161 ms)
galaxy.jobs.handler INFO 2025-07-13 01:18:22,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:22,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-07-13 01:18:22,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [46] prepared (53.829 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:22,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:18:22,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:22,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:18:22,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/46/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/6/e/5/dataset_6e51e2ce-cd84-417e-9da8-1b00bdd36e8a.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter               --exclude 'INFO/DP=19'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/e/3/9/dataset_e394f37a-075a-4a9c-8830-99d9b14e14b6.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:18:22,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:22,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:22,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:18:22,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:22,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:22,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:22,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:28,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tntkx with k8s id: gxy-tntkx succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:18:28,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:18:36,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 46 finished
galaxy.model.metadata DEBUG 2025-07-13 01:18:36,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 46
galaxy.jobs INFO 2025-07-13 01:18:36,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-07-13 01:18:36,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 46 executed (129.251 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:36,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:18:37,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-07-13 01:18:37,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:18:37,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:18:37,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:18:37,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:18:37,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:18:37,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-07-13 01:18:37,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (29.912 ms)
galaxy.jobs.handler INFO 2025-07-13 01:18:37,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:37,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 47
galaxy.jobs DEBUG 2025-07-13 01:18:37,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [47] prepared (80.510 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:18:37,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/47/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/47/configs/tmp5h6dn95x']
galaxy.jobs.runners DEBUG 2025-07-13 01:18:37,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:37,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:37,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:38,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:48,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7t6vq with k8s id: gxy-7t6vq succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:18:48,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:18:56,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 47 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:18:56,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.3.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/47/working/data_fetch_upload_7fddgkd5', 'object_id': 47}]}]}]
galaxy.jobs INFO 2025-07-13 01:18:56,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs DEBUG 2025-07-13 01:18:56,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 47 executed (159.518 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:56,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:18:57,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 48
tpv.core.entities DEBUG 2025-07-13 01:18:57,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:18:57,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:18:57,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:18:57,200 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:18:57,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:18:57,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-07-13 01:18:57,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (41.571 ms)
galaxy.jobs.handler INFO 2025-07-13 01:18:57,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:57,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 48
galaxy.jobs DEBUG 2025-07-13 01:18:57,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [48] prepared (59.824 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:57,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:18:57,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:57,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:18:57,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/48/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/1/0/3/dataset_1031542f-b537-43e6-88e1-1ba36ef9143d.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter  --soft-filter 'XX'                   --exclude 'INFO/DP=19'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/c/4/2/dataset_c42a10fe-c2fd-4c9e-b129-511acec32e5e.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:18:57,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:57,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:57,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:18:57,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:18:57,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:57,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:18:58,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:02,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tztkq with k8s id: gxy-tztkq succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:19:02,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:19:10,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 48 finished
galaxy.model.metadata DEBUG 2025-07-13 01:19:10,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 48
galaxy.jobs INFO 2025-07-13 01:19:10,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs DEBUG 2025-07-13 01:19:10,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 48 executed (128.608 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:10,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:19:11,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 49
tpv.core.entities DEBUG 2025-07-13 01:19:11,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:19:11,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:19:11,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:19:11,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:19:11,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:19:11,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-07-13 01:19:11,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (38.888 ms)
galaxy.jobs.handler INFO 2025-07-13 01:19:11,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:11,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-07-13 01:19:11,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [49] prepared (71.480 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:19:11,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/49/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/49/configs/tmp3jauo7q3']
galaxy.jobs.runners DEBUG 2025-07-13 01:19:11,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:11,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:11,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:12,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:21,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fpj2c with k8s id: gxy-fpj2c succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:19:22,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:19:29,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 49 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:19:29,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.3.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/49/working/data_fetch_upload_hw_0feps', 'object_id': 49}]}]}]
galaxy.jobs INFO 2025-07-13 01:19:29,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs DEBUG 2025-07-13 01:19:29,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 49 executed (159.184 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:29,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:19:31,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-07-13 01:19:31,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:19:31,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:19:31,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:19:31,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:19:31,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:19:31,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-07-13 01:19:31,156 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (44.965 ms)
galaxy.jobs.handler INFO 2025-07-13 01:19:31,159 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:31,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-07-13 01:19:31,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [50] prepared (51.886 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:19:31,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:19:31,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:19:31,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:19:31,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/50/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/0/f/c/dataset_0fc18ca7-8d8b-4b0b-b1e8-d6c2c6fe57d1.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter  --soft-filter 'XX'                   --exclude 'INFO/DP=19'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/e/6/7/dataset_e67c1b56-2131-4f31-b03d-2834e63adae9.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:19:31,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:31,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:19:31,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:19:31,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:19:31,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:31,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:31,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:36,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p7lj8 with k8s id: gxy-p7lj8 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:19:36,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:19:43,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 50 finished
galaxy.model.metadata DEBUG 2025-07-13 01:19:44,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 50
galaxy.jobs INFO 2025-07-13 01:19:44,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-07-13 01:19:44,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 50 executed (116.612 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:44,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:19:45,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 51
tpv.core.entities DEBUG 2025-07-13 01:19:45,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:19:45,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:19:45,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:19:45,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:19:45,532 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:19:45,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-07-13 01:19:45,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (44.476 ms)
galaxy.jobs.handler INFO 2025-07-13 01:19:45,565 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:45,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 51
galaxy.jobs DEBUG 2025-07-13 01:19:45,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [51] prepared (77.865 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:19:45,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/51/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/51/configs/tmpxyo4d4c_']
galaxy.jobs.runners DEBUG 2025-07-13 01:19:45,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:45,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:45,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:46,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:19:56,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dwp8c with k8s id: gxy-dwp8c succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:19:56,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:20:04,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 51 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:20:04,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.3.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/51/working/data_fetch_upload_pw56fe44', 'object_id': 51}]}]}]
galaxy.jobs INFO 2025-07-13 01:20:04,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs DEBUG 2025-07-13 01:20:04,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 51 executed (170.159 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:04,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:20:05,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52
tpv.core.entities DEBUG 2025-07-13 01:20:06,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:20:06,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:20:06,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:20:06,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:20:06,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:20:06,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-07-13 01:20:06,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (37.425 ms)
galaxy.jobs.handler INFO 2025-07-13 01:20:06,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:06,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-07-13 01:20:06,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [52] prepared (59.710 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:06,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:20:06,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:06,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:20:06,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/52/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/0/b/d/dataset_0bdd6909-0ec5-45d0-9e04-cb2265a9479b.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter  --soft-filter 'XX'                   --exclude 'INFO/DP=19'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/5/4/6/dataset_546fd9b1-fda3-4c5d-a0f4-b70f94f9a277.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:20:06,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:06,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:06,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:20:06,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:06,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:06,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:06,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:11,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4shkn with k8s id: gxy-4shkn succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:20:11,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:20:19,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 52 finished
galaxy.model.metadata DEBUG 2025-07-13 01:20:19,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 52
galaxy.jobs INFO 2025-07-13 01:20:19,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-07-13 01:20:19,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 52 executed (137.316 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:19,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:20:21,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 53
tpv.core.entities DEBUG 2025-07-13 01:20:21,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:20:21,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:20:21,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:20:21,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:20:21,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:20:21,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-07-13 01:20:21,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (38.956 ms)
galaxy.jobs.handler INFO 2025-07-13 01:20:21,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:21,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 53
galaxy.jobs DEBUG 2025-07-13 01:20:21,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [53] prepared (80.003 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:20:21,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/53/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/53/configs/tmpp052ygex']
galaxy.jobs.runners DEBUG 2025-07-13 01:20:21,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:21,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:21,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:22,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:31,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hktbm with k8s id: gxy-hktbm succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:20:32,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:20:39,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 53 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:20:39,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.3.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/53/working/data_fetch_upload_fvuo1hpn', 'object_id': 53}]}]}]
galaxy.jobs INFO 2025-07-13 01:20:39,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs DEBUG 2025-07-13 01:20:39,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 53 executed (155.048 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:40,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:20:40,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54
tpv.core.entities DEBUG 2025-07-13 01:20:40,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:20:40,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:20:40,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:20:40,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:20:40,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:20:40,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-07-13 01:20:40,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (44.538 ms)
galaxy.jobs.handler INFO 2025-07-13 01:20:40,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:40,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 54
galaxy.jobs DEBUG 2025-07-13 01:20:40,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [54] prepared (59.594 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:40,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:20:40,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:41,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:20:41,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/54/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/5/2/f/dataset_52fb44d7-f214-4ed2-bf17-547e271e11c8.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter  --soft-filter 'XX'                   --exclude 'INFO/DP=19'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/7/b/2/dataset_7b21ea5c-0c29-4e7b-97b8-241f7426d568.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:20:41,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:41,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:41,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:20:41,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:20:41,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:41,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:41,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jfqgl failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jfqgl.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:20:46,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-jfqgl

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-jfqgl": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (54/gxy-jfqgl) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (54/gxy-jfqgl) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (54/gxy-jfqgl) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (54/gxy-jfqgl) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jfqgl.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 54 (gxy-jfqgl)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-jfqgl to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:46,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (54/gxy-jfqgl) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-07-13 01:20:47,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 55
tpv.core.entities DEBUG 2025-07-13 01:20:47,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:20:47,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:20:47,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:20:47,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:20:47,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:20:47,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-07-13 01:20:47,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (34.972 ms)
galaxy.jobs.handler INFO 2025-07-13 01:20:47,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:47,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 55
galaxy.jobs DEBUG 2025-07-13 01:20:47,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [55] prepared (69.165 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:20:47,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/55/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/55/configs/tmp2wuvip87']
galaxy.jobs.runners DEBUG 2025-07-13 01:20:47,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:47,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:47,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:48,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:20:58,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mzc98 with k8s id: gxy-mzc98 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:20:58,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:21:06,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 55 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:21:06,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.3.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/55/working/data_fetch_upload_54v6rw1b', 'object_id': 55}]}]}]
galaxy.jobs INFO 2025-07-13 01:21:06,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-07-13 01:21:06,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 55 executed (142.810 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:06,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:21:07,524 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56
tpv.core.entities DEBUG 2025-07-13 01:21:07,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:21:07,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:21:07,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:21:07,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:21:07,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:21:07,591 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-07-13 01:21:07,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (39.160 ms)
galaxy.jobs.handler INFO 2025-07-13 01:21:07,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:07,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 56
galaxy.jobs DEBUG 2025-07-13 01:21:07,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [56] prepared (57.878 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:07,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:21:07,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:07,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:21:07,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/56/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/9/6/9/dataset_969685e3-aa15-4554-a66b-00b6ed1eaedb.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter               --exclude 'FMT/GT="0/2"'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/e/4/0/dataset_e403a397-c07d-4a64-8499-c0bb5e73b1aa.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:21:07,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:07,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:07,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:21:07,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:07,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:07,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:08,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:12,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9glqg with k8s id: gxy-9glqg succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:21:12,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 56: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:21:20,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 56 finished
galaxy.model.metadata DEBUG 2025-07-13 01:21:20,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 56
galaxy.jobs INFO 2025-07-13 01:21:20,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 56 in /galaxy/server/database/jobs_directory/000/56
galaxy.jobs DEBUG 2025-07-13 01:21:20,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 56 executed (114.377 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:20,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:21:21,878 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 57
tpv.core.entities DEBUG 2025-07-13 01:21:21,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:21:21,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:21:21,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:21:21,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:21:21,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:21:21,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-07-13 01:21:21,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (47.869 ms)
galaxy.jobs.handler INFO 2025-07-13 01:21:21,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:21,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 57
galaxy.jobs DEBUG 2025-07-13 01:21:22,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [57] prepared (75.413 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:21:22,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/57/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/57/configs/tmpzbaob518']
galaxy.jobs.runners DEBUG 2025-07-13 01:21:22,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:22,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:22,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:22,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jspmx failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jspmx.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:21:32,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-jspmx

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-jspmx": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57/gxy-jspmx) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57/gxy-jspmx) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57/gxy-jspmx) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57/gxy-jspmx) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jspmx.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 57 (gxy-jspmx)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-jspmx to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:32,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:33,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57/gxy-jspmx) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-07-13 01:21:34,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58
tpv.core.entities DEBUG 2025-07-13 01:21:34,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:21:34,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:21:34,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:21:34,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:21:34,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:21:34,259 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-07-13 01:21:34,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (40.781 ms)
galaxy.jobs.handler INFO 2025-07-13 01:21:34,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:34,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-07-13 01:21:34,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [58] prepared (68.596 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:21:34,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/58/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/58/configs/tmpbbr075qv']
galaxy.jobs.runners DEBUG 2025-07-13 01:21:34,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:34,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:34,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:34,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:45,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qlzs9 with k8s id: gxy-qlzs9 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:21:45,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:21:53,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 58 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:21:53,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.2.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/58/working/data_fetch_upload_hyr_936r', 'object_id': 58}]}]}]
galaxy.jobs INFO 2025-07-13 01:21:53,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.jobs DEBUG 2025-07-13 01:21:53,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 58 executed (167.394 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:53,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:21:54,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-07-13 01:21:54,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:21:54,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:21:54,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:21:54,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:21:54,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:21:54,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-07-13 01:21:54,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (47.632 ms)
galaxy.jobs.handler INFO 2025-07-13 01:21:54,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:54,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-07-13 01:21:54,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [59] prepared (58.189 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:54,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:21:54,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:55,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:21:55,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/59/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/8/1/7/dataset_81726ab3-e31c-4c1c-bf76-c825e763b5c2.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter     --regions-overlap 1         --include 'FMT/GT="0/0" && AC[*]=2'      --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/6/4/3/dataset_643d3d10-0ac4-4c09-a035-48f9ad2287b3.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:21:55,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:55,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:55,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:21:55,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:21:55,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:55,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:21:56,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c7764 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-c7764.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:22:00,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-c7764

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-c7764": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59/gxy-c7764) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59/gxy-c7764) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59/gxy-c7764) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59/gxy-c7764) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-c7764.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 59 (gxy-c7764)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-c7764 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:00,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59/gxy-c7764) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-07-13 01:22:01,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-07-13 01:22:01,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:22:01,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:22:01,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:22:01,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:22:01,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:22:01,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-07-13 01:22:01,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (47.198 ms)
galaxy.jobs.handler INFO 2025-07-13 01:22:01,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:01,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-07-13 01:22:02,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [60] prepared (77.981 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:22:02,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/60/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/60/configs/tmpsvsqvi32']
galaxy.jobs.runners DEBUG 2025-07-13 01:22:02,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:02,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:02,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:03,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:13,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kdv7b with k8s id: gxy-kdv7b succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:22:13,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:22:21,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 60 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:22:21,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'filter.3.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/60/working/data_fetch_upload_h1zs6sxv', 'object_id': 60}]}]}]
galaxy.jobs INFO 2025-07-13 01:22:21,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-07-13 01:22:21,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 60 executed (179.163 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:21,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:22:22,419 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-07-13 01:22:22,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:22:22,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:22:22,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:22:22,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:22:22,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:22:22,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-07-13 01:22:22,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (41.267 ms)
galaxy.jobs.handler INFO 2025-07-13 01:22:22,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:22,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-07-13 01:22:22,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [61] prepared (55.881 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:22,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:22:22,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:22,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:22:22,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/61/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/2/8/6/dataset_286af588-e3fb-4fad-9b75-0435b1ea5742.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools filter  --mode '+x' --soft-filter 'XX'   --mask-overlap 1                 --exclude 'INFO/DP=19'    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/7/d/a/dataset_7daaf87f-4d0d-437c-a459-92c95a706010.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:22:22,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:22,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:22,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:22:22,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_filter/bcftools_filter/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:22,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:22,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:23,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:27,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kqlvs with k8s id: gxy-kqlvs succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:22:27,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 61: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:22:36,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 61 finished
galaxy.model.metadata DEBUG 2025-07-13 01:22:36,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 61
galaxy.jobs INFO 2025-07-13 01:22:36,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 61 in /galaxy/server/database/jobs_directory/000/61
galaxy.jobs DEBUG 2025-07-13 01:22:36,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 61 executed (158.586 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:36,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:22:38,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-07-13 01:22:38,873 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:22:38,873 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:22:38,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:22:38,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:22:38,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:22:38,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-07-13 01:22:38,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (46.141 ms)
galaxy.jobs.handler INFO 2025-07-13 01:22:38,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:38,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-07-13 01:22:39,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [62] prepared (81.243 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:22:39,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/62/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/62/configs/tmp385d54nn']
galaxy.jobs.runners DEBUG 2025-07-13 01:22:39,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:39,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:39,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:39,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:50,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gx9zg with k8s id: gxy-gx9zg succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:22:50,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:22:58,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 62 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:22:58,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/62/working/data_fetch_upload_ooayk66i', 'object_id': 62}]}]}]
galaxy.jobs INFO 2025-07-13 01:22:58,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.jobs DEBUG 2025-07-13 01:22:58,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 62 executed (175.026 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:58,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:22:59,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-07-13 01:22:59,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:22:59,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:22:59,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:22:59,403 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:22:59,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:22:59,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-07-13 01:22:59,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (39.481 ms)
galaxy.jobs.handler INFO 2025-07-13 01:22:59,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:59,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-07-13 01:22:59,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [63] prepared (56.841 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:59,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:22:59,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfcombine/vcfcombine/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:59,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:22:59,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [vcfcombine  '/galaxy/server/database/objects/0/1/d/dataset_01d1dd27-ef07-4592-b823-2894c8dae4f5.dat' '/galaxy/server/database/objects/0/1/d/dataset_01d1dd27-ef07-4592-b823-2894c8dae4f5.dat'  > '/galaxy/server/database/objects/0/3/0/dataset_0308df02-9454-4a4f-ab21-c67825768c03.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:22:59,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:59,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:59,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:22:59,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfcombine/vcfcombine/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-07-13 01:22:59,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:22:59,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:00,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:13,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6g8jn with k8s id: gxy-6g8jn succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:23:13,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 63: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:23:21,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 63 finished
galaxy.model.metadata DEBUG 2025-07-13 01:23:21,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 63
galaxy.jobs INFO 2025-07-13 01:23:21,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 63 in /galaxy/server/database/jobs_directory/000/63
galaxy.jobs DEBUG 2025-07-13 01:23:21,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 63 executed (137.514 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:21,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:23:25,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-07-13 01:23:25,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:23:25,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:23:25,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:23:25,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:23:26,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:23:26,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-07-13 01:23:26,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (40.829 ms)
galaxy.jobs.handler INFO 2025-07-13 01:23:26,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:26,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-07-13 01:23:26,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [64] prepared (71.197 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:23:26,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/64/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/64/configs/tmpu1ehbzaz']
galaxy.jobs.runners DEBUG 2025-07-13 01:23:26,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:26,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:26,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:26,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:36,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zcscg with k8s id: gxy-zcscg succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:23:37,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:23:44,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 64 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:23:44,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/64/working/gxupload_0', 'object_id': 64}]}]}]
galaxy.jobs INFO 2025-07-13 01:23:45,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.jobs DEBUG 2025-07-13 01:23:45,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 64 executed (333.939 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:45,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:23:45,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65
tpv.core.entities DEBUG 2025-07-13 01:23:45,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:23:45,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:23:45,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:23:45,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:23:45,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:23:45,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-07-13 01:23:45,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (32.654 ms)
galaxy.jobs.handler INFO 2025-07-13 01:23:45,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:45,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-07-13 01:23:45,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [65] prepared (58.035 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:23:45,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:23:45,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-07-13 01:23:45,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:23:45,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools squiggle '/galaxy/server/database/objects/8/c/8/dataset_8c80191b-9e72-4236-a455-b100a42b8d57.dat' --saveas png --num-facets 6]
galaxy.jobs.runners DEBUG 2025-07-13 01:23:45,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:45,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:23:45,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:23:45,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-07-13 01:23:45,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:45,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:23:46,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:24:31,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qhxnl with k8s id: gxy-qhxnl succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:24:31,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:24:40,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 65 finished
galaxy.model.metadata DEBUG 2025-07-13 01:24:40,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 65
galaxy.jobs INFO 2025-07-13 01:24:40,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-07-13 01:24:40,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 65 executed (318.118 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:24:40,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:24:42,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-07-13 01:24:42,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:24:42,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:24:42,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:24:42,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:24:42,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:24:42,699 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-07-13 01:24:42,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (42.198 ms)
galaxy.jobs.handler INFO 2025-07-13 01:24:42,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:24:42,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-07-13 01:24:42,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [66] prepared (84.050 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:24:42,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/66/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/66/configs/tmpzj2kkme6']
galaxy.jobs.runners DEBUG 2025-07-13 01:24:42,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:24:42,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:24:42,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:24:43,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:24:53,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j9k9t with k8s id: gxy-j9k9t succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:24:53,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:25:01,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 66 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:25:01,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/66/working/gxupload_0', 'object_id': 70}]}]}]
galaxy.jobs INFO 2025-07-13 01:25:01,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.jobs DEBUG 2025-07-13 01:25:02,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 66 executed (328.883 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:02,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:25:03,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67
tpv.core.entities DEBUG 2025-07-13 01:25:03,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:25:03,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:25:03,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:25:03,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:25:03,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:25:03,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-07-13 01:25:03,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (49.323 ms)
galaxy.jobs.handler INFO 2025-07-13 01:25:03,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:03,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 67
galaxy.jobs DEBUG 2025-07-13 01:25:03,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [67] prepared (49.859 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:03,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:25:03,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:03,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:25:03,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools squiggle '/galaxy/server/database/objects/a/5/0/dataset_a501d743-423f-418b-a54c-a7a2b937c15c.dat' --saveas pdf --num-facets 6]
galaxy.jobs.runners DEBUG 2025-07-13 01:25:03,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:03,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:03,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:25:03,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_squiggle/poretools_squiggle/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:03,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:03,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:03,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qcc2z failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-qcc2z.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:25:17,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-qcc2z

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-qcc2z": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67/gxy-qcc2z) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67/gxy-qcc2z) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67/gxy-qcc2z) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67/gxy-qcc2z) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-qcc2z.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 67 (gxy-qcc2z)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-qcc2z to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:17,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67/gxy-qcc2z) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-07-13 01:25:20,560 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 68
tpv.core.entities DEBUG 2025-07-13 01:25:20,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:25:20,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:25:20,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:25:20,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:25:20,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:25:20,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-07-13 01:25:20,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (35.273 ms)
galaxy.jobs.handler INFO 2025-07-13 01:25:20,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:20,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-07-13 01:25:20,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [68] prepared (69.001 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:25:20,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/68/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/68/configs/tmpk_y_v8y4']
galaxy.jobs.runners DEBUG 2025-07-13 01:25:20,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:20,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:20,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:21,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:31,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv778 with k8s id: gxy-fv778 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:25:32,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 68: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:25:39,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 68 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:25:39,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/68/working/gxupload_0', 'object_id': 72}]}]}]
galaxy.jobs INFO 2025-07-13 01:25:39,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 68 in /galaxy/server/database/jobs_directory/000/68
galaxy.jobs DEBUG 2025-07-13 01:25:40,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 68 executed (210.883 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:40,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:25:41,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69
tpv.core.entities DEBUG 2025-07-13 01:25:41,074 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:25:41,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:25:41,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:25:41,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:25:41,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:25:41,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-07-13 01:25:41,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (50.720 ms)
galaxy.jobs.handler INFO 2025-07-13 01:25:41,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:41,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 69
cheetah_DynamicallyCompiledCheetahTemplate_1752369941_214933_12245.py:93: SyntaxWarning: invalid escape sequence '\.'
cheetah_DynamicallyCompiledCheetahTemplate_1752369941_214933_12245.py:124: SyntaxWarning: invalid escape sequence '\.'
galaxy.jobs DEBUG 2025-07-13 01:25:41,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [69] prepared (90.837 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:41,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:25:41,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:41,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:25:41,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [bamPEFragmentSize --version > /galaxy/server/database/jobs_directory/000/69/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/5/7/5/dataset_57572cbc-c3c2-45f9-8bbc-1f356d049e0d.dat' './0.bam' && ln -s '/galaxy/server/database/objects/_metadata_files/9/e/d/metadata_9edcbe3d-8402-49a5-9fa6-bbb5a294a267.dat' './0.bam.bai' &&   bamPEFragmentSize --numberOfProcessors "${GALAXY_SLOTS:-4}" --bamfiles '0.bam' --samplesLabel 'bowtie2 test1.bam' --histogram '/galaxy/server/database/objects/a/9/4/dataset_a94209be-45a2-4e05-abd8-13ccff84eb09.dat' --plotFileFormat 'png' --plotTitle 'Test Plot' > '/galaxy/server/database/objects/b/b/c/dataset_bbcd789f-40b5-4546-95e8-c62ff43bf23f.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:25:41,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:41,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:41,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:25:41,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-07-13 01:25:41,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:41,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:25:41,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:11,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7np4w with k8s id: gxy-7np4w succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:26:11,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:26:19,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 69 finished
galaxy.model.metadata DEBUG 2025-07-13 01:26:19,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 73
galaxy.model.metadata DEBUG 2025-07-13 01:26:19,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 74
galaxy.model.metadata DEBUG 2025-07-13 01:26:19,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 75
galaxy.model.metadata DEBUG 2025-07-13 01:26:19,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 76
galaxy.jobs INFO 2025-07-13 01:26:19,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs DEBUG 2025-07-13 01:26:20,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 69 executed (306.989 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:20,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:26:21,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 70
tpv.core.entities DEBUG 2025-07-13 01:26:21,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:26:21,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:26:21,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:26:21,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:26:21,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:26:22,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-07-13 01:26:22,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (38.364 ms)
galaxy.jobs.handler INFO 2025-07-13 01:26:22,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:22,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 70
galaxy.jobs DEBUG 2025-07-13 01:26:22,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [70] prepared (82.962 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:26:22,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/70/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/70/configs/tmp7uethnf8']
galaxy.jobs.runners DEBUG 2025-07-13 01:26:22,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:22,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:22,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:22,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:34,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7vl7t with k8s id: gxy-7vl7t succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:26:34,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:26:42,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 70 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:26:42,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/70/working/gxupload_0', 'object_id': 77}]}]}]
galaxy.jobs INFO 2025-07-13 01:26:42,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs DEBUG 2025-07-13 01:26:42,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 70 executed (161.721 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:42,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:26:43,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71
tpv.core.entities DEBUG 2025-07-13 01:26:43,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:26:43,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:26:43,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:26:43,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:26:43,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:26:43,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-07-13 01:26:43,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (48.258 ms)
galaxy.jobs.handler INFO 2025-07-13 01:26:43,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:43,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-07-13 01:26:43,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [71] prepared (56.465 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:26:43,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:26:43,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-07-13 01:26:43,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:26:43,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [bamPEFragmentSize --version > /galaxy/server/database/jobs_directory/000/71/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/b/6/b/dataset_b6b63427-8e6c-408a-9de4-b4ff6066ba1d.dat' './0.bam' && ln -s '/galaxy/server/database/objects/_metadata_files/b/8/d/metadata_b8dc70a8-6199-48eb-9f89-c2de58ad0511.dat' './0.bam.bai' &&   bamPEFragmentSize --numberOfProcessors "${GALAXY_SLOTS:-4}" --bamfiles '0.bam' --samplesLabel 'bowtie2 test1.bam' --plotTitle '' --binSize '1000' --distanceBetweenBins '1000000'  --maxFragmentLength '0'     --table '/galaxy/server/database/objects/1/b/5/dataset_1b51e425-9b3b-4df7-bd2d-7eeeb431cb04.dat' --outRawFragmentLengths '/galaxy/server/database/objects/4/d/a/dataset_4da3e621-22a1-4517-bcfa-d489ec645fe9.dat' > '/galaxy/server/database/objects/3/4/1/dataset_341e1e52-e441-4b21-b9ec-1224cd815d70.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:26:43,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:43,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:26:43,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:26:43,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_pe_fragmentsize/deeptools_bam_pe_fragmentsize/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-07-13 01:26:43,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:43,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:44,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:26:56,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-74fnv with k8s id: gxy-74fnv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:26:56,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 71: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:27:04,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 71 finished
galaxy.model.metadata DEBUG 2025-07-13 01:27:04,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.model.metadata DEBUG 2025-07-13 01:27:04,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 79
galaxy.model.metadata DEBUG 2025-07-13 01:27:04,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 80
galaxy.util WARNING 2025-07-13 01:27:04,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/d/a/dataset_4da3e621-22a1-4517-bcfa-d489ec645fe9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/d/a/dataset_4da3e621-22a1-4517-bcfa-d489ec645fe9.dat'
galaxy.jobs INFO 2025-07-13 01:27:04,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 71 in /galaxy/server/database/jobs_directory/000/71
galaxy.jobs DEBUG 2025-07-13 01:27:04,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 71 executed (171.031 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:04,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:27:07,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72, 73
tpv.core.entities DEBUG 2025-07-13 01:27:08,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:27:08,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:27:08,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:27:08,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:27:08,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:27:08,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-07-13 01:27:08,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (45.825 ms)
galaxy.jobs.handler INFO 2025-07-13 01:27:08,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 72
tpv.core.entities DEBUG 2025-07-13 01:27:08,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:27:08,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:27:08,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:27:08,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:27:08,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:27:08,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-07-13 01:27:08,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (52.898 ms)
galaxy.jobs.handler INFO 2025-07-13 01:27:08,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-07-13 01:27:08,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [72] prepared (99.744 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:27:08,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/72/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/72/configs/tmpclzsh2mo']
galaxy.jobs.runners DEBUG 2025-07-13 01:27:08,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:27:08,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [73] prepared (85.240 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:27:08,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/73/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/73/configs/tmpwirxudyg']
galaxy.jobs.runners DEBUG 2025-07-13 01:27:08,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:08,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:19,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pnkcr with k8s id: gxy-pnkcr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:19,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z826c with k8s id: gxy-z826c succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:27:19,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:27:19,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:27:27,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 73 finished
galaxy.jobs.runners DEBUG 2025-07-13 01:27:27,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 72 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:27:27,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fasta1.fa', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/73/working/data_fetch_upload_pucakks_', 'object_id': 82}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:27:27,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-mt-genome.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/72/working/data_fetch_upload_wldrb_dl', 'object_id': 81}]}]}]
galaxy.jobs INFO 2025-07-13 01:27:27,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs INFO 2025-07-13 01:27:27,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs DEBUG 2025-07-13 01:27:27,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 73 executed (187.190 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:27,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:27:27,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 72 executed (178.322 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:27,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:27:28,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-07-13 01:27:28,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:27:28,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:27:28,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:27:28,682 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:27:28,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:27:28,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-07-13 01:27:28,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (44.288 ms)
galaxy.jobs.handler INFO 2025-07-13 01:27:28,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:28,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-07-13 01:27:28,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [74] prepared (89.505 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:27:28,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:27:28,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:27:29,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:27:29,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/74/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/4/3/6/dataset_43639537-7be6-4c79-a232-cca95225bd66.dat' reference.fa && minimap2 -x sr    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/7/1/8/dataset_7188ee9f-c37f-489b-bfcc-52923cfa120d.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/8/a/e/dataset_8ae94ada-f30a-4198-9b4c-46b39acbc64a.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:27:29,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:29,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:27:29,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:27:29,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:27:29,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:29,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:30,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:38,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4fgph with k8s id: gxy-4fgph succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:27:38,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 74: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:27:46,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 74 finished
galaxy.model.metadata DEBUG 2025-07-13 01:27:46,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 83
galaxy.jobs INFO 2025-07-13 01:27:46,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 74 in /galaxy/server/database/jobs_directory/000/74
galaxy.jobs DEBUG 2025-07-13 01:27:46,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 74 executed (148.970 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:46,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:27:48,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76, 75
tpv.core.entities DEBUG 2025-07-13 01:27:48,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:27:48,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:27:48,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:27:48,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:27:48,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:27:48,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-07-13 01:27:48,170 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (39.953 ms)
galaxy.jobs.handler INFO 2025-07-13 01:27:48,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:48,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 75
tpv.core.entities DEBUG 2025-07-13 01:27:48,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:27:48,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:27:48,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:27:48,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:27:48,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:27:48,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-07-13 01:27:48,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (59.548 ms)
galaxy.jobs.handler INFO 2025-07-13 01:27:48,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:48,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-07-13 01:27:48,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [75] prepared (98.950 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:27:48,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/75/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/75/configs/tmplx1kj_rh']
galaxy.jobs.runners DEBUG 2025-07-13 01:27:48,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:27:48,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [76] prepared (83.997 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:48,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:48,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:27:48,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/76/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/76/configs/tmp0_3vnxf3']
galaxy.jobs.runners DEBUG 2025-07-13 01:27:48,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:48,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:48,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:49,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:49,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:58,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gsrhd with k8s id: gxy-gsrhd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:27:59,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ccvjn with k8s id: gxy-ccvjn succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:27:59,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:27:59,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:28:07,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 75 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:28:07,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-mt-genome.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/75/working/data_fetch_upload_f6108olh', 'object_id': 84}]}]}]
galaxy.jobs INFO 2025-07-13 01:28:07,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-07-13 01:28:07,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 76 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:28:07,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fasta1.fa', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/76/working/data_fetch_upload_cheojxe1', 'object_id': 85}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:28:07,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 75 executed (172.175 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:07,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:28:07,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.jobs DEBUG 2025-07-13 01:28:07,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 76 executed (170.595 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:07,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:28:08,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-07-13 01:28:08,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:28:08,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:28:08,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:28:08,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:28:08,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:28:08,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-07-13 01:28:08,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (42.350 ms)
galaxy.jobs.handler INFO 2025-07-13 01:28:08,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:08,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-07-13 01:28:08,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [77] prepared (61.121 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:08,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:28:08,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:08,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:28:08,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/77/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/9/6/8/dataset_968142f3-17fe-42cf-a4ee-6f35d62fbfde.dat' reference.fa && minimap2 -x sr    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/f/8/8/dataset_f88ebc5f-a42e-40f9-8ee7-2e657d69534c.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -T "${TMPDIR:-.}" -@${GALAXY_SLOTS:-2} -O CRAM --reference reference.fa --output-fmt-option no_ref -o '/galaxy/server/database/objects/4/f/e/dataset_4fe54da7-d600-447d-be51-71e69edda8c3.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:28:08,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:08,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:08,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:28:08,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:09,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:09,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:10,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:13,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xt6m5 with k8s id: gxy-xt6m5 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:28:13,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:28:21,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 77 finished
galaxy.model.metadata DEBUG 2025-07-13 01:28:21,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 86
galaxy.jobs INFO 2025-07-13 01:28:21,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-07-13 01:28:21,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 77 executed (146.083 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:21,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:28:23,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 78, 80, 79
tpv.core.entities DEBUG 2025-07-13 01:28:23,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:28:23,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:28:23,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:28:23,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:28:23,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:28:23,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-07-13 01:28:23,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (31.416 ms)
galaxy.jobs.handler INFO 2025-07-13 01:28:23,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 78
tpv.core.entities DEBUG 2025-07-13 01:28:23,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:28:23,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:28:23,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:28:23,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:28:23,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:28:23,374 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-07-13 01:28:23,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (55.132 ms)
galaxy.jobs.handler INFO 2025-07-13 01:28:23,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 79
tpv.core.entities DEBUG 2025-07-13 01:28:23,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:28:23,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:28:23,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:28:23,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:28:23,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:28:23,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [78] prepared (106.802 ms)
galaxy.jobs DEBUG 2025-07-13 01:28:23,458 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-07-13 01:28:23,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (54.307 ms)
galaxy.jobs.handler INFO 2025-07-13 01:28:23,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.command_factory INFO 2025-07-13 01:28:23,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/78/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/78/configs/tmpdjplye0y']
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 80
galaxy.jobs.runners DEBUG 2025-07-13 01:28:23,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:28:23,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [79] prepared (112.241 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:28:23,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/79/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/79/configs/tmpsky0ne6t']
galaxy.jobs.runners DEBUG 2025-07-13 01:28:23,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:28:23,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [80] prepared (94.171 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:28:23,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/80/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/80/configs/tmp0v7uultc']
galaxy.jobs.runners DEBUG 2025-07-13 01:28:23,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:23,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:24,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:24,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:24,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:34,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-djfl6 with k8s id: gxy-djfl6 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:34,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pzrzp with k8s id: gxy-pzrzp succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:34,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-74hrw with k8s id: gxy-74hrw succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:28:34,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:28:34,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:28:34,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:28:46,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 78 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:28:46,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-mt-genome.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/78/working/data_fetch_upload_rq3f_bff', 'object_id': 87}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:28:46,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 79 finished
galaxy.jobs INFO 2025-07-13 01:28:46,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:28:46,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/79/working/data_fetch_upload_p2pu7uww', 'object_id': 88}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:28:46,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 80 finished
galaxy.jobs DEBUG 2025-07-13 01:28:47,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 78 executed (238.136 ms)
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:28:47,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/80/working/data_fetch_upload_rge2cud5', 'object_id': 89}]}]}]
galaxy.jobs INFO 2025-07-13 01:28:47,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:47,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:28:47,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-07-13 01:28:47,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 79 executed (214.703 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:47,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:28:47,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 80 executed (225.760 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:47,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:28:48,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81
tpv.core.entities DEBUG 2025-07-13 01:28:48,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:28:48,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:28:48,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:28:48,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:28:48,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:28:48,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-07-13 01:28:48,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (31.179 ms)
galaxy.jobs.handler INFO 2025-07-13 01:28:48,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:48,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-07-13 01:28:48,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [81] prepared (61.221 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:48,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:28:48,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:48,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:28:48,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/81/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/0/8/d/dataset_08d8df48-b170-4e24-a66e-93ff0fc08285.dat' reference.fa && minimap2 -x sr    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/c/7/3/dataset_c73269cf-0559-4a4f-bf99-7875c8b05b08.dat' '/galaxy/server/database/objects/8/1/3/dataset_813d5922-11b2-4cf4-b83f-c22c0a5f7013.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/c/6/3/dataset_c6333849-5764-41a0-b898-641736b51dbc.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:28:48,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:48,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:48,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:28:48,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:28:48,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:48,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:49,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:28:53,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h8v4j with k8s id: gxy-h8v4j succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:28:53,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:29:01,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 81 finished
galaxy.model.metadata DEBUG 2025-07-13 01:29:01,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 90
galaxy.jobs INFO 2025-07-13 01:29:01,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-07-13 01:29:01,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 81 executed (148.690 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:01,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:29:02,587 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-07-13 01:29:02,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:02,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:02,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:02,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:02,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:29:02,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-07-13 01:29:02,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (33.255 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:02,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:02,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-07-13 01:29:02,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [82] prepared (65.439 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:29:02,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/82/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/82/configs/tmp_284i8iy']
galaxy.jobs.runners DEBUG 2025-07-13 01:29:02,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:02,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:02,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:03,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:29:03,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83, 84
tpv.core.entities DEBUG 2025-07-13 01:29:03,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:03,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:03,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:03,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:03,705 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:29:03,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-07-13 01:29:03,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (39.189 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:03,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:03,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 83
tpv.core.entities DEBUG 2025-07-13 01:29:03,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:03,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:03,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:03,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:03,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:29:03,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-07-13 01:29:03,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (51.024 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:03,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:03,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-07-13 01:29:03,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [83] prepared (87.572 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:29:03,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/83/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/83/configs/tmpyqs4fc9h']
galaxy.jobs.runners DEBUG 2025-07-13 01:29:03,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:29:03,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [84] prepared (82.553 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:03,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:03,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:29:03,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/84/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/84/configs/tmpvdki0p89']
galaxy.jobs.runners DEBUG 2025-07-13 01:29:03,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:03,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:03,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:04,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:04,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9bj4c failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-9bj4c.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-07-13 01:29:13,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-07-13-00-50-1/jobs/gxy-9bj4c

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-9bj4c": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-9bj4c) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-9bj4c) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-9bj4c) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-9bj4c) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-9bj4c.
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 82 (gxy-9bj4c)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-9bj4c to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:13,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82/gxy-9bj4c) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:14,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n5szk with k8s id: gxy-n5szk succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:14,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ks9pv with k8s id: gxy-ks9pv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:29:14,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:29:14,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-07-13 01:29:15,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87, 86, 85
tpv.core.entities DEBUG 2025-07-13 01:29:15,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:15,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:15,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:15,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:15,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:29:15,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-07-13 01:29:15,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (46.530 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:15,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 85
tpv.core.entities DEBUG 2025-07-13 01:29:15,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:15,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:15,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:15,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:15,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:29:15,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-07-13 01:29:15,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (84.344 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:15,228 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 86
tpv.core.entities DEBUG 2025-07-13 01:29:15,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:15,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:15,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:15,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:15,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [85] prepared (175.072 ms)
galaxy.jobs DEBUG 2025-07-13 01:29:15,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-07-13 01:29:15,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/85/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/85/configs/tmpfpu5seih']
galaxy.jobs DEBUG 2025-07-13 01:29:15,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-07-13 01:29:15,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (102.020 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:15,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners DEBUG 2025-07-13 01:29:15,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:29:15,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [86] prepared (176.510 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:29:15,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/86/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/86/configs/tmps5zw6yc7']
galaxy.jobs.runners DEBUG 2025-07-13 01:29:15,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 87
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:29:15,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [87] prepared (112.773 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:29:15,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/87/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/87/configs/tmp9mw5l780']
galaxy.jobs.runners DEBUG 2025-07-13 01:29:15,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:15,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:16,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:16,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:16,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-07-13 01:29:23,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 83 finished
galaxy.jobs.runners DEBUG 2025-07-13 01:29:23,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 84 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:29:23,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/83/working/data_fetch_upload_fhlsxqhn', 'object_id': 92}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:29:23,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/84/working/data_fetch_upload_paiz_djg', 'object_id': 93}]}]}]
galaxy.jobs INFO 2025-07-13 01:29:23,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.jobs INFO 2025-07-13 01:29:23,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-07-13 01:29:23,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 83 executed (173.480 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:23,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:29:23,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 84 executed (185.668 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:23,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:26,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f2xtb with k8s id: gxy-f2xtb succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:26,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5d6dw with k8s id: gxy-5d6dw succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:26,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kk9g5 with k8s id: gxy-kk9g5 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:29:26,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:29:26,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:29:26,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:29:39,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 86 finished
galaxy.jobs.runners DEBUG 2025-07-13 01:29:39,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 87 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:29:39,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/86/working/data_fetch_upload_dyn4kj8i', 'object_id': 95}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:29:39,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/87/working/data_fetch_upload_4sxbd9qc', 'object_id': 96}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:29:39,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 85 finished
galaxy.jobs INFO 2025-07-13 01:29:39,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:29:39,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-mt-genome.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/85/working/data_fetch_upload_mt081c64', 'object_id': 94}]}]}]
galaxy.jobs INFO 2025-07-13 01:29:39,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs DEBUG 2025-07-13 01:29:39,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 86 executed (199.044 ms)
galaxy.jobs INFO 2025-07-13 01:29:39,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:39,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:29:39,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 87 executed (208.376 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:39,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:29:39,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 85 executed (173.632 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:39,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:29:40,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-07-13 01:29:40,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:40,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:40,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:40,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:40,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:29:40,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-07-13 01:29:40,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (67.168 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:40,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:40,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-07-13 01:29:40,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [88] prepared (94.997 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:29:40,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:29:40,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:29:40,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:29:40,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/88/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/3/5/7/dataset_357a9f43-7435-4004-920e-b5b8491348cc.dat' reference.fa && minimap2 -x sr    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/b/3/b/dataset_b3b1a7c3-e469-44cc-b323-d297df919957.dat' '/galaxy/server/database/objects/4/a/2/dataset_4a2158e3-0717-41aa-b56b-de2325813e28.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/3/0/4/dataset_304c2d58-34f6-4f37-b084-93c80aa29c35.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:29:40,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:40,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:29:40,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:29:40,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:29:40,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:40,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:41,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:44,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-965lv with k8s id: gxy-965lv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:29:44,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:29:52,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 88 finished
galaxy.model.metadata DEBUG 2025-07-13 01:29:52,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 97
galaxy.jobs INFO 2025-07-13 01:29:52,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-07-13 01:29:52,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 88 executed (129.513 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:52,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:29:54,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 89
tpv.core.entities DEBUG 2025-07-13 01:29:54,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:29:54,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:29:54,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:29:54,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:29:54,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:29:54,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-07-13 01:29:54,554 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (46.543 ms)
galaxy.jobs.handler INFO 2025-07-13 01:29:54,557 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:54,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 89
galaxy.jobs DEBUG 2025-07-13 01:29:54,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [89] prepared (71.697 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:29:54,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/89/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/89/configs/tmpxfhvk761']
galaxy.jobs.runners DEBUG 2025-07-13 01:29:54,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:54,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:54,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:29:55,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:05,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c9qw2 with k8s id: gxy-c9qw2 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:30:06,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:30:14,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 89 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:30:14,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mini_reads.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/89/working/data_fetch_upload_by46vuc_', 'object_id': 98}]}]}]
galaxy.jobs INFO 2025-07-13 01:30:14,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-07-13 01:30:14,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 89 executed (169.472 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:14,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:30:14,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90
tpv.core.entities DEBUG 2025-07-13 01:30:14,968 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:30:14,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:30:14,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:30:14,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:30:14,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:30:15,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-07-13 01:30:15,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (46.477 ms)
galaxy.jobs.handler INFO 2025-07-13 01:30:15,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:15,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-07-13 01:30:15,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [90] prepared (61.782 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:15,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:30:15,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:15,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:30:15,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/90/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/4/8/9/dataset_489c1103-e010-471a-ba59-eeb002df7026.dat' reference.fa && minimap2 -x ava-ont    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/4/8/9/dataset_489c1103-e010-471a-ba59-eeb002df7026.dat' > '/galaxy/server/database/objects/2/f/3/dataset_2f328775-695a-4caa-a1af-4d90dc07e047.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:30:15,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:15,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:15,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:30:15,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:15,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:15,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:16,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:19,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lnzbv with k8s id: gxy-lnzbv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:30:19,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:30:27,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 90 finished
galaxy.model.metadata DEBUG 2025-07-13 01:30:27,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 99
galaxy.jobs INFO 2025-07-13 01:30:27,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-07-13 01:30:27,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 90 executed (140.030 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:27,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:30:28,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-07-13 01:30:28,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:30:28,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:30:28,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:30:28,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:30:28,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:30:28,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-07-13 01:30:28,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (41.927 ms)
galaxy.jobs.handler INFO 2025-07-13 01:30:28,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:28,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-07-13 01:30:28,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [91] prepared (72.008 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:30:28,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/91/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/91/configs/tmpxz9r4g7_']
galaxy.jobs.runners DEBUG 2025-07-13 01:30:28,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:28,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:28,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:29,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:38,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fzzz6 with k8s id: gxy-fzzz6 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:30:38,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:30:46,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 91 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:30:46,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'minimap2-self-homology.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/91/working/data_fetch_upload_bjfw94w_', 'object_id': 100}]}]}]
galaxy.jobs INFO 2025-07-13 01:30:46,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-07-13 01:30:46,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 91 executed (136.168 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:46,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:30:47,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92
tpv.core.entities DEBUG 2025-07-13 01:30:47,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:30:47,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:30:47,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:30:47,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:30:47,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:30:47,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-07-13 01:30:47,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (50.967 ms)
galaxy.jobs.handler INFO 2025-07-13 01:30:47,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:47,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 92
galaxy.jobs DEBUG 2025-07-13 01:30:47,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [92] prepared (58.367 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:47,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:30:47,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:48,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:30:48,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/92/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/0/5/1/dataset_0510c574-7f4e-42a2-84eb-c5befae00db7.dat' reference.fa && minimap2 -DP -k19 -w19 -m200    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/0/5/1/dataset_0510c574-7f4e-42a2-84eb-c5befae00db7.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/7/5/3/dataset_753b0642-2456-4c79-b38e-a915b97d34fe.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:30:48,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:48,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:48,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:30:48,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:30:48,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:48,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:48,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:30:52,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fp45p with k8s id: gxy-fp45p succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:30:53,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:31:01,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 92 finished
galaxy.model.metadata DEBUG 2025-07-13 01:31:01,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 101
galaxy.jobs INFO 2025-07-13 01:31:01,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs DEBUG 2025-07-13 01:31:01,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 92 executed (160.104 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:01,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:31:03,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94, 93
tpv.core.entities DEBUG 2025-07-13 01:31:03,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:31:03,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:31:03,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:31:03,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:31:03,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:31:03,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-07-13 01:31:03,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (42.428 ms)
galaxy.jobs.handler INFO 2025-07-13 01:31:03,351 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:03,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 93
tpv.core.entities DEBUG 2025-07-13 01:31:03,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:31:03,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:31:03,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:31:03,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:31:03,399 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:31:03,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-07-13 01:31:03,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (57.896 ms)
galaxy.jobs.handler INFO 2025-07-13 01:31:03,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:03,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-07-13 01:31:03,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [93] prepared (101.684 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:31:03,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/93/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/93/configs/tmpryp6ghc_']
galaxy.jobs.runners DEBUG 2025-07-13 01:31:03,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:03,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:31:03,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [94] prepared (92.102 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:03,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:31:03,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/94/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/94/configs/tmpot00_7yp']
galaxy.jobs.runners DEBUG 2025-07-13 01:31:03,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:03,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:03,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:03,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:04,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:14,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vmp2z with k8s id: gxy-vmp2z succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:14,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zp2gf with k8s id: gxy-zp2gf succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:31:14,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:31:14,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:31:23,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 94 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:31:23,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fasta1.fa', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/94/working/data_fetch_upload_froc3efx', 'object_id': 103}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:31:23,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 93 finished
galaxy.jobs INFO 2025-07-13 01:31:23,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:31:23,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-mt-genome.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/93/working/data_fetch_upload_rs269eav', 'object_id': 102}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:31:23,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 94 executed (193.511 ms)
galaxy.jobs INFO 2025-07-13 01:31:23,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:23,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:31:23,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 93 executed (154.558 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:23,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:31:23,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95
tpv.core.entities DEBUG 2025-07-13 01:31:23,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:31:23,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:31:23,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:31:23,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:31:23,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:31:23,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-07-13 01:31:23,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (44.992 ms)
galaxy.jobs.handler INFO 2025-07-13 01:31:23,966 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:23,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 95
galaxy.jobs DEBUG 2025-07-13 01:31:24,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [95] prepared (55.634 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:31:24,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:31:24,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:31:24,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:31:24,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/95/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/0/1/f/dataset_01fa35d2-2d6b-4f48-89e7-d35d9e220600.dat' reference.fa && minimap2 -x sr    --q-occ-frac 0.01  --mask-len 100      -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/e/1/8/dataset_e1844b3e-0369-4259-9c9b-6f24ea7958ac.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/3/9/a/dataset_39a15c02-238a-4dbe-a50d-f681f9cd946b.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:31:24,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:24,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:31:24,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:31:24,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:31:24,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:24,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:24,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:29,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s6l27 with k8s id: gxy-s6l27 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:31:29,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:31:37,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 95 finished
galaxy.model.metadata DEBUG 2025-07-13 01:31:37,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 104
galaxy.jobs INFO 2025-07-13 01:31:37,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-07-13 01:31:38,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 95 executed (149.076 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:38,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:31:39,260 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 96
tpv.core.entities DEBUG 2025-07-13 01:31:39,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:31:39,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:31:39,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:31:39,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:31:39,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:31:39,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-07-13 01:31:39,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (39.365 ms)
galaxy.jobs.handler INFO 2025-07-13 01:31:39,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:39,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 96
galaxy.jobs DEBUG 2025-07-13 01:31:39,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [96] prepared (80.478 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:31:39,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/96/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/96/configs/tmp7dnlca4e']
galaxy.jobs.runners DEBUG 2025-07-13 01:31:39,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:39,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:39,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:39,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:31:40,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97
tpv.core.entities DEBUG 2025-07-13 01:31:40,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:31:40,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:31:40,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:31:40,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:31:40,400 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:31:40,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-07-13 01:31:40,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (40.621 ms)
galaxy.jobs.handler INFO 2025-07-13 01:31:40,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:40,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-07-13 01:31:40,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [97] prepared (72.026 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:31:40,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/97/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/97/configs/tmpvle_rber']
galaxy.jobs.runners DEBUG 2025-07-13 01:31:40,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:40,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:40,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:40,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:50,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fpcxw with k8s id: gxy-fpcxw succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:31:50,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:51,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sknjm with k8s id: gxy-sknjm succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:31:51,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:31:58,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 96 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:31:58,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pacbio_hifi_assembly.fa.gz', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/96/working/data_fetch_upload_cay6hnbj', 'object_id': 105}]}]}]
galaxy.jobs INFO 2025-07-13 01:31:58,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs DEBUG 2025-07-13 01:31:58,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 96 executed (191.907 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:58,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:31:59,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 97 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:31:59,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pacbio_hifi_reads.fasta.gz', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/97/working/data_fetch_upload_9fcm_l7n', 'object_id': 106}]}]}]
galaxy.jobs INFO 2025-07-13 01:31:59,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs DEBUG 2025-07-13 01:31:59,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 97 executed (162.596 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:59,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:31:59,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98
tpv.core.entities DEBUG 2025-07-13 01:31:59,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:31:59,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:31:59,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:31:59,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:31:59,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:31:59,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-07-13 01:31:59,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (44.572 ms)
galaxy.jobs.handler INFO 2025-07-13 01:31:59,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:31:59,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 98
galaxy.jobs DEBUG 2025-07-13 01:31:59,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [98] prepared (63.559 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:31:59,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:31:59,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:31:59,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:32:00,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/98/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/5/8/e/dataset_58e45aa3-37e5-4509-a741-23c94079b775.dat' reference.fa && minimap2 -x map-hifi    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/1/9/8/dataset_198a7fea-34a1-43b1-8763-860decba2635.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/2/2/8/dataset_22875362-1abc-4832-addf-72f5e757ea64.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:32:00,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:00,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:32:00,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:32:00,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:32:00,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:00,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:00,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:05,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ldcvd with k8s id: gxy-ldcvd succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:32:05,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:32:13,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 98 finished
galaxy.model.metadata DEBUG 2025-07-13 01:32:13,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 107
galaxy.jobs INFO 2025-07-13 01:32:13,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs DEBUG 2025-07-13 01:32:13,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 98 executed (144.867 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:13,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:32:16,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100, 99
tpv.core.entities DEBUG 2025-07-13 01:32:16,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:32:16,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:32:16,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:32:16,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:32:16,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:32:16,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-07-13 01:32:16,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (41.106 ms)
galaxy.jobs.handler INFO 2025-07-13 01:32:16,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 99
tpv.core.entities DEBUG 2025-07-13 01:32:16,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:32:16,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:32:16,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:32:16,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:32:16,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:32:16,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-07-13 01:32:16,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (54.734 ms)
galaxy.jobs.handler INFO 2025-07-13 01:32:16,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-07-13 01:32:16,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [99] prepared (89.504 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:32:16,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/99/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/99/configs/tmpppadb3zd']
galaxy.jobs.runners DEBUG 2025-07-13 01:32:16,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:32:16,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [100] prepared (81.964 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:32:16,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/100/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/100/configs/tmpuho5rzzp']
galaxy.jobs.runners DEBUG 2025-07-13 01:32:16,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:16,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:27,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7kbnd with k8s id: gxy-7kbnd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:27,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gxt7s with k8s id: gxy-gxt7s succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:32:27,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:32:27,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:32:35,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 99 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:32:35,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pacbio_hifi_assembly.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/99/working/data_fetch_upload_uvc6k8ki', 'object_id': 108}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:32:35,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 100 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:32:35,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pacbio_hifi_reads.fasta.gz', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/100/working/data_fetch_upload_k6hg4g3y', 'object_id': 109}]}]}]
galaxy.jobs INFO 2025-07-13 01:32:35,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs INFO 2025-07-13 01:32:35,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-07-13 01:32:35,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 99 executed (178.207 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:35,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:32:35,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 100 executed (157.375 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:35,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:32:36,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 101
tpv.core.entities DEBUG 2025-07-13 01:32:36,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:32:36,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:32:36,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:32:36,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:32:36,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:32:36,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-07-13 01:32:36,855 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (39.588 ms)
galaxy.jobs.handler INFO 2025-07-13 01:32:36,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:36,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 101
galaxy.jobs DEBUG 2025-07-13 01:32:36,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [101] prepared (45.002 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:32:36,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:32:36,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:32:37,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:32:37,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/101/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/2/1/3/dataset_21360436-41b7-45e6-bf2d-8305cc66be7b.dat' reference.fa && minimap2 -x map-hifi    --q-occ-frac 0.01       -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/1/8/a/dataset_18a8722c-e6bb-4d52-bd8e-334b988c3a9a.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/9/b/e/dataset_9be3f485-6708-49a8-b915-c4746c51edcc.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:32:37,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:37,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:32:37,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:32:37,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:32:37,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:37,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:37,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:42,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z7bzz with k8s id: gxy-z7bzz succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:32:42,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:32:50,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 101 finished
galaxy.model.metadata DEBUG 2025-07-13 01:32:50,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 110
galaxy.jobs INFO 2025-07-13 01:32:50,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-07-13 01:32:50,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 101 executed (126.197 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:50,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:32:52,151 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 102, 103
tpv.core.entities DEBUG 2025-07-13 01:32:52,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:32:52,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:32:52,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:32:52,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:32:52,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:32:52,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-07-13 01:32:52,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (40.860 ms)
galaxy.jobs.handler INFO 2025-07-13 01:32:52,233 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:52,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 102
tpv.core.entities DEBUG 2025-07-13 01:32:52,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:32:52,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:32:52,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:32:52,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:32:52,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:32:52,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-07-13 01:32:52,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (46.906 ms)
galaxy.jobs.handler INFO 2025-07-13 01:32:52,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:52,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 103
galaxy.jobs DEBUG 2025-07-13 01:32:52,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [102] prepared (84.908 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:32:52,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/102/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/102/configs/tmpv8szzym9']
galaxy.jobs.runners DEBUG 2025-07-13 01:32:52,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:52,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:32:52,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [103] prepared (85.568 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:32:52,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/103/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/103/configs/tmpci4tqhgf']
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:52,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:32:52,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:52,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:52,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:53,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:32:54,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:03,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fhkqz with k8s id: gxy-fhkqz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:03,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lx52x with k8s id: gxy-lx52x succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:33:04,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 102: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:33:04,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:33:12,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 103 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:33:12,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fasta1.fa', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/103/working/data_fetch_upload_jxaetr7c', 'object_id': 112}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:33:12,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 102 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:33:12,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-mt-genome.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/102/working/data_fetch_upload_8f55h6be', 'object_id': 111}]}]}]
galaxy.jobs INFO 2025-07-13 01:33:12,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs INFO 2025-07-13 01:33:12,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 102 in /galaxy/server/database/jobs_directory/000/102
galaxy.jobs DEBUG 2025-07-13 01:33:12,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 103 executed (158.031 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:12,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:33:12,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 102 executed (155.766 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:12,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:33:12,701 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 104
tpv.core.entities DEBUG 2025-07-13 01:33:12,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:33:12,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/.*, abstract=False, cores=8, mem=32, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:33:12,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:33:12,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:33:12,747 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:33:12,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Working directory for job is: /galaxy/server/database/jobs_directory/000/104
galaxy.jobs.runners DEBUG 2025-07-13 01:33:12,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [104] queued (39.370 ms)
galaxy.jobs.handler INFO 2025-07-13 01:33:12,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:12,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 104
galaxy.jobs DEBUG 2025-07-13 01:33:12,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [104] prepared (61.085 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:12,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:33:12,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:12,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:33:12,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/104/tool_script.sh] for tool command [minimap2 --version > /galaxy/server/database/jobs_directory/000/104/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/f/b/8/dataset_fb84fc6d-bdd9-49cb-8062-a91f606fa5f8.dat' reference.fa && minimap2 -x sr    --q-occ-frac 0.01  -U 10,30      -t ${GALAXY_SLOTS:-4} reference.fa '/galaxy/server/database/objects/7/6/b/dataset_76bc75f6-3126-474b-95d1-78bd0441d2f7.dat' -a | samtools view --no-PG -hT reference.fa | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O BAM -o '/galaxy/server/database/objects/c/c/1/dataset_cc14efed-d434-4a61-a418-74cdad55d817.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:33:12,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (104) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/104/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/104/galaxy_104.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:12,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:12,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:33:12,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1: mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:12,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:3a70f8bc7e17b723591f6132418640cfdbc88246-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:12,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:13,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:18,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jvfpk with k8s id: gxy-jvfpk succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:33:18,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 104: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:33:25,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 104 finished
galaxy.model.metadata DEBUG 2025-07-13 01:33:25,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 113
galaxy.jobs INFO 2025-07-13 01:33:25,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 104 in /galaxy/server/database/jobs_directory/000/104
galaxy.jobs DEBUG 2025-07-13 01:33:26,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 104 executed (131.979 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:26,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:33:29,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 106, 105
tpv.core.entities DEBUG 2025-07-13 01:33:29,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:33:29,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:33:29,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:33:29,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:33:29,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:33:29,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Working directory for job is: /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-07-13 01:33:29,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [105] queued (43.405 ms)
galaxy.jobs.handler INFO 2025-07-13 01:33:29,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:29,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 105
tpv.core.entities DEBUG 2025-07-13 01:33:29,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:33:29,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:33:29,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:33:29,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:33:29,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:33:29,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Working directory for job is: /galaxy/server/database/jobs_directory/000/106
galaxy.jobs.runners DEBUG 2025-07-13 01:33:29,298 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [106] queued (50.436 ms)
galaxy.jobs.handler INFO 2025-07-13 01:33:29,301 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:29,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 106
galaxy.jobs DEBUG 2025-07-13 01:33:29,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [105] prepared (87.408 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:33:29,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/105/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/105/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/105/configs/tmptqelgrta']
galaxy.jobs.runners DEBUG 2025-07-13 01:33:29,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (105) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/105/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/105/galaxy_105.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:33:29,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [106] prepared (72.068 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:29,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:29,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:33:29,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/106/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/106/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/106/configs/tmpnmp_pmbi']
galaxy.jobs.runners DEBUG 2025-07-13 01:33:29,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (106) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/106/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/106/galaxy_106.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:29,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:29,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:30,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:30,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:40,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zwwr2 with k8s id: gxy-zwwr2 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:40,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sfbsw with k8s id: gxy-sfbsw succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:33:41,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 105: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:33:41,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 106: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:33:49,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 106 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:33:49,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/106/working/data_fetch_upload_i_6t7w14', 'object_id': 115}]}]}]
galaxy.jobs INFO 2025-07-13 01:33:49,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 106 in /galaxy/server/database/jobs_directory/000/106
galaxy.jobs DEBUG 2025-07-13 01:33:49,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 106 executed (166.109 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:49,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:33:49,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 105 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:33:49,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/105/working/gxupload_0', 'object_id': 114}]}]}]
galaxy.jobs INFO 2025-07-13 01:33:49,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 105 in /galaxy/server/database/jobs_directory/000/105
galaxy.jobs DEBUG 2025-07-13 01:33:49,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 105 executed (167.965 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:49,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:33:50,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 107
tpv.core.entities DEBUG 2025-07-13 01:33:50,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:33:50,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:33:50,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:33:50,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:33:50,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:33:50,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Working directory for job is: /galaxy/server/database/jobs_directory/000/107
galaxy.jobs.runners DEBUG 2025-07-13 01:33:50,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [107] queued (41.749 ms)
galaxy.jobs.handler INFO 2025-07-13 01:33:50,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:50,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 107
galaxy.jobs DEBUG 2025-07-13 01:33:50,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [107] prepared (93.108 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:50,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:33:50,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:51,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:33:51,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/107/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/107/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/0/a/6/dataset_0a6ef747-7277-422c-9015-fafe2deed4bb.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/d/9/3/dataset_d93a7b9d-d140-4daf-b16b-af8a748337ae.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/9/d/1/metadata_9d1c86cb-fa33-40eb-8e4b-57c3c9f366c5.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/d/9/6/dataset_d965e368-4b64-473c-bcb3-91389c056266.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:33:51,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (107) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/107/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/107/galaxy_107.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:51,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:51,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:33:51,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:33:51,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:51,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:33:52,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:07,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kk7cl with k8s id: gxy-kk7cl succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:34:07,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 107: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:34:15,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 107 finished
galaxy.model.metadata DEBUG 2025-07-13 01:34:15,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 116
galaxy.jobs INFO 2025-07-13 01:34:15,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 107 in /galaxy/server/database/jobs_directory/000/107
galaxy.jobs DEBUG 2025-07-13 01:34:15,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 107 executed (112.370 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:15,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:34:17,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 109, 108
tpv.core.entities DEBUG 2025-07-13 01:34:17,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:34:17,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:34:17,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:34:17,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:34:17,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:34:17,403 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Working directory for job is: /galaxy/server/database/jobs_directory/000/108
galaxy.jobs.runners DEBUG 2025-07-13 01:34:17,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [108] queued (44.038 ms)
galaxy.jobs.handler INFO 2025-07-13 01:34:17,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:17,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 108
tpv.core.entities DEBUG 2025-07-13 01:34:17,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:34:17,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:34:17,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:34:17,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:34:17,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:34:17,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Working directory for job is: /galaxy/server/database/jobs_directory/000/109
galaxy.jobs.runners DEBUG 2025-07-13 01:34:17,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [109] queued (45.581 ms)
galaxy.jobs.handler INFO 2025-07-13 01:34:17,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:17,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 109
galaxy.jobs DEBUG 2025-07-13 01:34:17,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [108] prepared (80.013 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:34:17,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/108/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/108/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/108/configs/tmpibfs_fnw']
galaxy.jobs.runners DEBUG 2025-07-13 01:34:17,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (108) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/108/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/108/galaxy_108.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:34:17,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [109] prepared (69.599 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:17,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:17,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:34:17,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/109/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/109/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/109/configs/tmpmbsd414c']
galaxy.jobs.runners DEBUG 2025-07-13 01:34:17,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (109) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/109/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/109/galaxy_109.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:17,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:17,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:18,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:18,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:28,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lr46q with k8s id: gxy-lr46q succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:34:28,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 109: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:29,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dm28k with k8s id: gxy-dm28k succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:34:29,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 108: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:34:36,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 109 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:34:36,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/109/working/data_fetch_upload_h12m1uj9', 'object_id': 118}]}]}]
galaxy.jobs INFO 2025-07-13 01:34:37,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 109 in /galaxy/server/database/jobs_directory/000/109
galaxy.jobs DEBUG 2025-07-13 01:34:37,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 109 executed (147.166 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:37,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:34:37,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 108 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:34:37,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/108/working/gxupload_0', 'object_id': 117}]}]}]
galaxy.jobs INFO 2025-07-13 01:34:37,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 108 in /galaxy/server/database/jobs_directory/000/108
galaxy.jobs DEBUG 2025-07-13 01:34:37,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 108 executed (189.241 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:37,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:34:38,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 110
tpv.core.entities DEBUG 2025-07-13 01:34:38,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:34:38,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:34:38,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:34:38,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:34:38,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:34:38,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Working directory for job is: /galaxy/server/database/jobs_directory/000/110
galaxy.jobs.runners DEBUG 2025-07-13 01:34:38,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [110] queued (41.920 ms)
galaxy.jobs.handler INFO 2025-07-13 01:34:38,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:39,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 110
galaxy.jobs DEBUG 2025-07-13 01:34:39,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [110] prepared (62.026 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:34:39,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:34:39,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:34:39,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:34:39,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/110/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/110/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/4/d/8/dataset_4d891634-6a56-47b2-8829-ab8088e80191.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/1/4/d/dataset_14dad059-1142-4838-bd99-0ec0c2e0b4e8.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/7/b/b/metadata_7bb3a3de-bcd9-437e-9b3f-4e7a1c3e2d4f.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 14 --skip-coverage 0 --limit-coverage 0   --haplotype-length 0 --min-alternate-count 1 --min-alternate-fraction 0.05 --pooled-continuous --report-monomorphic --standard-filters  "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/5/9/3/dataset_593f35cd-7ec7-47d6-93fa-399cd30f3472.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:34:39,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (110) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/110/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/110/galaxy_110.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:39,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:34:39,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:34:39,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:34:39,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:39,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:40,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:44,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ht67t with k8s id: gxy-ht67t succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:34:44,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 110: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:34:52,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 110 finished
galaxy.model.metadata DEBUG 2025-07-13 01:34:52,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 119
galaxy.jobs INFO 2025-07-13 01:34:52,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 110 in /galaxy/server/database/jobs_directory/000/110
galaxy.jobs DEBUG 2025-07-13 01:34:52,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 110 executed (116.936 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:52,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:34:53,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 112, 111
tpv.core.entities DEBUG 2025-07-13 01:34:53,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:34:53,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:34:53,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:34:53,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:34:53,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:34:53,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Working directory for job is: /galaxy/server/database/jobs_directory/000/111
galaxy.jobs.runners DEBUG 2025-07-13 01:34:53,419 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [111] queued (33.008 ms)
galaxy.jobs.handler INFO 2025-07-13 01:34:53,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:53,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 111
tpv.core.entities DEBUG 2025-07-13 01:34:53,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:34:53,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:34:53,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:34:53,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:34:53,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:34:53,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Working directory for job is: /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-07-13 01:34:53,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [112] queued (54.284 ms)
galaxy.jobs.handler INFO 2025-07-13 01:34:53,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:53,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 112
galaxy.jobs DEBUG 2025-07-13 01:34:53,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [111] prepared (89.766 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:34:53,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/111/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/111/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/111/configs/tmpwj3bhx35']
galaxy.jobs.runners DEBUG 2025-07-13 01:34:53,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (111) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/111/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/111/galaxy_111.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:53,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:34:53,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [112] prepared (82.925 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:53,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:34:53,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/112/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/112/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/112/configs/tmp3gpmup71']
galaxy.jobs.runners DEBUG 2025-07-13 01:34:53,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (112) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/112/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/112/galaxy_112.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:53,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:53,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:54,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:34:54,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:03,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j65kj with k8s id: gxy-j65kj succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:35:04,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 112: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:04,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8mx2c with k8s id: gxy-8mx2c succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:35:05,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 111: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:35:12,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 112 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:35:12,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/112/working/data_fetch_upload_ov_t3ab_', 'object_id': 121}]}]}]
galaxy.jobs INFO 2025-07-13 01:35:12,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.jobs DEBUG 2025-07-13 01:35:12,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 112 executed (142.958 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:12,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:35:13,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 111 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:35:13,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/111/working/gxupload_0', 'object_id': 120}]}]}]
galaxy.jobs INFO 2025-07-13 01:35:13,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.jobs DEBUG 2025-07-13 01:35:13,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 111 executed (144.637 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:13,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:35:13,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 113
tpv.core.entities DEBUG 2025-07-13 01:35:13,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:35:13,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:35:13,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:35:13,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:35:14,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:35:14,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Working directory for job is: /galaxy/server/database/jobs_directory/000/113
galaxy.jobs.runners DEBUG 2025-07-13 01:35:14,031 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [113] queued (41.846 ms)
galaxy.jobs.handler INFO 2025-07-13 01:35:14,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:14,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 113
galaxy.jobs DEBUG 2025-07-13 01:35:14,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [113] prepared (62.112 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:14,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:35:14,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:14,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:35:14,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/113/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/113/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/4/7/5/dataset_475404b8-11cd-4a61-a536-c67e31add0c4.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/6/8/5/dataset_685b6d41-4148-4cbc-9b17-a8d032bf235b.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/1/b/5/metadata_1b525d38-6be7-48a4-ba9e-60632a156eaf.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 14 --skip-coverage 0 --limit-coverage 0   --haplotype-length 0 --min-alternate-count 1 --min-alternate-fraction 0.05 --pooled-continuous --report-monomorphic --standard-filters  "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/c/c/a/dataset_cca51946-57ea-423e-8ef0-4143f434368a.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:35:14,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (113) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/113/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/113/galaxy_113.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:14,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:14,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:35:14,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:14,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:14,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:15,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:19,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8k7sk with k8s id: gxy-8k7sk succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:35:19,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 113: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:35:26,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 113 finished
galaxy.model.metadata DEBUG 2025-07-13 01:35:26,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 122
galaxy.jobs INFO 2025-07-13 01:35:26,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 113 in /galaxy/server/database/jobs_directory/000/113
galaxy.jobs DEBUG 2025-07-13 01:35:27,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 113 executed (109.049 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:27,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:35:28,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 115, 114
tpv.core.entities DEBUG 2025-07-13 01:35:28,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:35:28,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:35:28,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:35:28,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:35:28,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:35:28,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Working directory for job is: /galaxy/server/database/jobs_directory/000/114
galaxy.jobs.runners DEBUG 2025-07-13 01:35:28,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [114] queued (38.432 ms)
galaxy.jobs.handler INFO 2025-07-13 01:35:28,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:28,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 114
tpv.core.entities DEBUG 2025-07-13 01:35:28,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:35:28,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:35:28,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:35:28,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:35:28,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:35:28,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Working directory for job is: /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-07-13 01:35:28,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [115] queued (50.828 ms)
galaxy.jobs.handler INFO 2025-07-13 01:35:28,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:28,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 115
galaxy.jobs DEBUG 2025-07-13 01:35:28,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [114] prepared (81.282 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:35:28,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/114/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/114/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/114/configs/tmpqip394wx']
galaxy.jobs.runners DEBUG 2025-07-13 01:35:28,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (114) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/114/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/114/galaxy_114.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:28,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:35:28,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [115] prepared (80.264 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:28,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:35:28,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/115/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/115/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/115/configs/tmp09vo_84a']
galaxy.jobs.runners DEBUG 2025-07-13 01:35:28,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (115) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/115/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/115/galaxy_115.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:28,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:28,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:29,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:29,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:38,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-45mvz with k8s id: gxy-45mvz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:39,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fsk2p with k8s id: gxy-fsk2p succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:35:39,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 114: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:35:39,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 115: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:35:47,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 114 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:35:47,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/114/working/gxupload_0', 'object_id': 123}]}]}]
galaxy.jobs INFO 2025-07-13 01:35:47,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 114 in /galaxy/server/database/jobs_directory/000/114
galaxy.jobs DEBUG 2025-07-13 01:35:47,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 114 executed (165.408 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:47,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:35:47,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 115 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:35:47,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/115/working/data_fetch_upload_z0yz0k6q', 'object_id': 124}]}]}]
galaxy.jobs INFO 2025-07-13 01:35:47,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs DEBUG 2025-07-13 01:35:47,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 115 executed (134.353 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:47,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:35:47,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 116
tpv.core.entities DEBUG 2025-07-13 01:35:47,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:35:47,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:35:47,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:35:47,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:35:47,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:35:47,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Working directory for job is: /galaxy/server/database/jobs_directory/000/116
galaxy.jobs.runners DEBUG 2025-07-13 01:35:47,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [116] queued (45.945 ms)
galaxy.jobs.handler INFO 2025-07-13 01:35:47,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:47,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 116
galaxy.jobs DEBUG 2025-07-13 01:35:48,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [116] prepared (63.956 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:48,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:35:48,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:48,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:35:48,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/116/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/116/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/b/2/d/dataset_b2d406e8-9b92-4cde-abc1-14e576e7790b.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/f/a/a/dataset_faa7a241-f3d0-471d-ad9e-fb39da6dc028.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/b/d/e/metadata_bde3cda7-e711-4a84-bb76-73ba98901e60.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'    --theta 0.001 --ploidy 1            "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/a/3/0/dataset_a303592e-4abd-49b3-bf24-8b64e00f24ad.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:35:48,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/116/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/116/galaxy_116.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:48,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:48,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:35:48,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:35:48,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:48,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:49,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:35:53,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d4m8t with k8s id: gxy-d4m8t succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:35:53,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 116: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:36:01,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 116 finished
galaxy.model.metadata DEBUG 2025-07-13 01:36:01,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 125
galaxy.jobs INFO 2025-07-13 01:36:01,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 116 in /galaxy/server/database/jobs_directory/000/116
galaxy.jobs DEBUG 2025-07-13 01:36:01,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 116 executed (109.897 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:01,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:36:02,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 117
tpv.core.entities DEBUG 2025-07-13 01:36:02,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:36:02,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:36:02,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:36:02,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:36:02,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:36:02,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Working directory for job is: /galaxy/server/database/jobs_directory/000/117
galaxy.jobs.runners DEBUG 2025-07-13 01:36:02,403 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [117] queued (38.670 ms)
galaxy.jobs.handler INFO 2025-07-13 01:36:02,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:02,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 117
galaxy.jobs DEBUG 2025-07-13 01:36:02,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [117] prepared (62.793 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:36:02,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/117/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/117/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/117/configs/tmp8on7v24h']
galaxy.jobs.runners DEBUG 2025-07-13 01:36:02,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (117) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/117/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/117/galaxy_117.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:02,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:02,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:03,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:36:03,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 118
tpv.core.entities DEBUG 2025-07-13 01:36:03,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:36:03,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:36:03,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:36:03,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:36:03,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:36:03,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Working directory for job is: /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-07-13 01:36:03,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [118] queued (34.602 ms)
galaxy.jobs.handler INFO 2025-07-13 01:36:03,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:03,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 118
galaxy.jobs DEBUG 2025-07-13 01:36:03,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [118] prepared (64.224 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:36:03,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/118/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/118/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/118/configs/tmpkl2vfbfn']
galaxy.jobs.runners DEBUG 2025-07-13 01:36:03,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (118) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/118/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/118/galaxy_118.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:03,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:03,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:04,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:13,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9n7mw with k8s id: gxy-9n7mw succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:13,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dqm2m with k8s id: gxy-dqm2m succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:36:13,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 117: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:36:14,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 118: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:36:22,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 118 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:36:22,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/118/working/data_fetch_upload_sl3osf4a', 'object_id': 127}]}]}]
galaxy.jobs INFO 2025-07-13 01:36:22,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 118 in /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-07-13 01:36:22,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 117 finished
galaxy.jobs DEBUG 2025-07-13 01:36:22,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 118 executed (153.852 ms)
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:36:22,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/117/working/gxupload_0', 'object_id': 126}]}]}]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:22,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:36:22,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 117 in /galaxy/server/database/jobs_directory/000/117
galaxy.jobs DEBUG 2025-07-13 01:36:22,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 117 executed (160.077 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:22,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:36:22,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 119
tpv.core.entities DEBUG 2025-07-13 01:36:22,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:36:22,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:36:22,966 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:36:22,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:36:22,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:36:22,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Working directory for job is: /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners DEBUG 2025-07-13 01:36:23,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [119] queued (32.766 ms)
galaxy.jobs.handler INFO 2025-07-13 01:36:23,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:23,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 119
galaxy.jobs DEBUG 2025-07-13 01:36:23,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [119] prepared (45.933 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:23,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:36:23,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:23,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:36:23,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/119/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/119/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/c/6/8/dataset_c687c056-287c-4bfd-ad0c-e26a529052a8.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/7/6/7/dataset_767a5dfa-57d8-45a7-bc5e-ea90b1df6050.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/0/9/8/metadata_0982da9a-9c8e-4fed-8bc4-438220062e06.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 250 --skip-coverage 0 --limit-coverage 0    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/5/b/2/dataset_5b25b9c9-1288-44d4-a80c-d66c2172e68f.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:36:23,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (119) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/119/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/119/galaxy_119.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:23,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:23,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:36:23,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:23,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:23,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:23,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:28,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v6wrh with k8s id: gxy-v6wrh succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:36:28,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:36:35,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 119 finished
galaxy.model.metadata DEBUG 2025-07-13 01:36:35,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 128
galaxy.jobs INFO 2025-07-13 01:36:35,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs DEBUG 2025-07-13 01:36:35,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 119 executed (102.415 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:35,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:36:37,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 121, 120
tpv.core.entities DEBUG 2025-07-13 01:36:37,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:36:37,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:36:37,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:36:37,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:36:37,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:36:37,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Working directory for job is: /galaxy/server/database/jobs_directory/000/120
galaxy.jobs.runners DEBUG 2025-07-13 01:36:37,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [120] queued (36.532 ms)
galaxy.jobs.handler INFO 2025-07-13 01:36:37,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:37,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 120
tpv.core.entities DEBUG 2025-07-13 01:36:37,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:36:37,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:36:37,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:36:37,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:36:37,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:36:37,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Working directory for job is: /galaxy/server/database/jobs_directory/000/121
galaxy.jobs.runners DEBUG 2025-07-13 01:36:37,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [121] queued (51.368 ms)
galaxy.jobs.handler INFO 2025-07-13 01:36:37,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:37,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 121
galaxy.jobs DEBUG 2025-07-13 01:36:37,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [120] prepared (91.879 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:36:37,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/120/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/120/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/120/configs/tmpx1wxv4g4']
galaxy.jobs.runners DEBUG 2025-07-13 01:36:37,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (120) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/120/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/120/galaxy_120.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:36:37,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [121] prepared (77.315 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:37,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:37,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:36:37,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/121/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/121/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/121/configs/tmpdfjnnmx0']
galaxy.jobs.runners DEBUG 2025-07-13 01:36:37,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (121) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/121/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/121/galaxy_121.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:37,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:37,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:38,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:38,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:48,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-js895 with k8s id: gxy-js895 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:48,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bhbxc with k8s id: gxy-bhbxc succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:36:48,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 120: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:36:48,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 121: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:36:56,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 120 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:36:56,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/120/working/gxupload_0', 'object_id': 129}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:36:56,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 121 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:36:57,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/121/working/data_fetch_upload_sedbx1et', 'object_id': 130}]}]}]
galaxy.jobs INFO 2025-07-13 01:36:57,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 120 in /galaxy/server/database/jobs_directory/000/120
galaxy.jobs INFO 2025-07-13 01:36:57,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 121 in /galaxy/server/database/jobs_directory/000/121
galaxy.jobs DEBUG 2025-07-13 01:36:57,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 120 executed (207.840 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:57,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:36:57,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 121 executed (171.169 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:57,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:36:57,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 122
tpv.core.entities DEBUG 2025-07-13 01:36:57,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:36:57,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:36:57,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:36:57,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:36:57,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:36:57,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Working directory for job is: /galaxy/server/database/jobs_directory/000/122
galaxy.jobs.runners DEBUG 2025-07-13 01:36:57,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [122] queued (37.559 ms)
galaxy.jobs.handler INFO 2025-07-13 01:36:57,897 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:57,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 122
galaxy.jobs DEBUG 2025-07-13 01:36:57,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [122] prepared (45.146 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:57,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:36:57,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:57,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:36:58,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/122/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/122/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/0/c/c/dataset_0cc25585-9820-4e00-8f3f-25b2b2a93679.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/b/9/3/dataset_b9360d01-06b8-48dc-933e-9c594057ba11.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/3/2/6/metadata_326e116a-acac-467b-a6c7-7f2cc62ca8d0.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 0 --skip-coverage 0 --limit-coverage 400    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/4/6/8/dataset_468f75b2-b12b-4d16-b92d-776f0ea5d0ba.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:36:58,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (122) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/122/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/122/galaxy_122.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:58,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:58,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:36:58,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:36:58,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:58,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:36:58,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:03,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lfcpj with k8s id: gxy-lfcpj succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:37:04,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 122: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:37:11,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 122 finished
galaxy.model.metadata DEBUG 2025-07-13 01:37:11,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 131
galaxy.jobs INFO 2025-07-13 01:37:11,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 122 in /galaxy/server/database/jobs_directory/000/122
galaxy.jobs DEBUG 2025-07-13 01:37:11,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 122 executed (96.525 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:11,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:37:13,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 123, 124
tpv.core.entities DEBUG 2025-07-13 01:37:13,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:37:13,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:37:13,262 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:37:13,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:37:13,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:37:13,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Working directory for job is: /galaxy/server/database/jobs_directory/000/123
galaxy.jobs.runners DEBUG 2025-07-13 01:37:13,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [123] queued (36.252 ms)
galaxy.jobs.handler INFO 2025-07-13 01:37:13,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:13,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 123
tpv.core.entities DEBUG 2025-07-13 01:37:13,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:37:13,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:37:13,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:37:13,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:37:13,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:37:13,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Working directory for job is: /galaxy/server/database/jobs_directory/000/124
galaxy.jobs.runners DEBUG 2025-07-13 01:37:13,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [124] queued (50.491 ms)
galaxy.jobs.handler INFO 2025-07-13 01:37:13,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:13,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 124
galaxy.jobs DEBUG 2025-07-13 01:37:13,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [123] prepared (83.158 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:37:13,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/123/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/123/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/123/configs/tmp4l3dcs3v']
galaxy.jobs.runners DEBUG 2025-07-13 01:37:13,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (123) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/123/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/123/galaxy_123.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:13,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:37:13,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [124] prepared (71.878 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:13,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:37:13,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/124/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/124/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/124/configs/tmpt3rjmsbh']
galaxy.jobs.runners DEBUG 2025-07-13 01:37:13,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (124) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/124/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/124/galaxy_124.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:13,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:13,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:13,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:14,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:23,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mqh9t with k8s id: gxy-mqh9t succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:37:23,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 124: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:24,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kfn9l with k8s id: gxy-kfn9l succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:37:24,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 123: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:37:32,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 124 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:37:32,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/124/working/data_fetch_upload_rk7txs_b', 'object_id': 133}]}]}]
galaxy.jobs INFO 2025-07-13 01:37:32,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 124 in /galaxy/server/database/jobs_directory/000/124
galaxy.jobs DEBUG 2025-07-13 01:37:32,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 124 executed (137.519 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:32,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:37:33,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 123 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:37:33,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-hxb2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/123/working/gxupload_0', 'object_id': 132}]}]}]
galaxy.jobs INFO 2025-07-13 01:37:33,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 123 in /galaxy/server/database/jobs_directory/000/123
galaxy.jobs DEBUG 2025-07-13 01:37:33,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 123 executed (142.347 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:33,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:37:34,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 125
tpv.core.entities DEBUG 2025-07-13 01:37:34,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:37:34,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:37:34,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:37:34,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:37:34,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:37:34,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Working directory for job is: /galaxy/server/database/jobs_directory/000/125
galaxy.jobs.runners DEBUG 2025-07-13 01:37:34,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [125] queued (40.115 ms)
galaxy.jobs.handler INFO 2025-07-13 01:37:34,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:34,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 125
galaxy.jobs DEBUG 2025-07-13 01:37:34,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [125] prepared (60.593 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:37:34,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:37:34,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:37:34,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:37:34,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/125/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/125/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/f/b/a/dataset_fba78f16-9f88-4ed0-9586-f00ac178c734.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/6/8/1/dataset_6814b225-aa9b-4471-84b2-68bfa739152e.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/e/2/6/metadata_e26bf178-5f36-4ac7-923f-e25bffbe89d2.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 0 --skip-coverage 100 --limit-coverage 0    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/4/d/c/dataset_4dc1f785-bea9-4adc-b825-593a1bcaeadc.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:37:34,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (125) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/125/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/125/galaxy_125.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:34,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:37:34,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:37:34,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:37:34,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:34,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:34,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:39,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qf44t with k8s id: gxy-qf44t succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:37:40,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 125: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:37:47,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 125 finished
galaxy.model.metadata DEBUG 2025-07-13 01:37:47,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.jobs INFO 2025-07-13 01:37:47,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 125 in /galaxy/server/database/jobs_directory/000/125
galaxy.jobs DEBUG 2025-07-13 01:37:47,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 125 executed (121.240 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:47,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:37:49,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 126, 127
tpv.core.entities DEBUG 2025-07-13 01:37:49,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:37:49,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:37:49,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:37:49,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:37:49,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:37:49,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Working directory for job is: /galaxy/server/database/jobs_directory/000/126
galaxy.jobs.runners DEBUG 2025-07-13 01:37:49,458 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [126] queued (42.381 ms)
galaxy.jobs.handler INFO 2025-07-13 01:37:49,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:49,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 126
tpv.core.entities DEBUG 2025-07-13 01:37:49,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:37:49,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:37:49,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:37:49,481 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:37:49,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:37:49,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Working directory for job is: /galaxy/server/database/jobs_directory/000/127
galaxy.jobs.runners DEBUG 2025-07-13 01:37:49,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [127] queued (50.146 ms)
galaxy.jobs.handler INFO 2025-07-13 01:37:49,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:49,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 127
galaxy.jobs DEBUG 2025-07-13 01:37:49,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [126] prepared (88.095 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:37:49,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/126/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/126/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/126/configs/tmp2sozqeso']
galaxy.jobs.runners DEBUG 2025-07-13 01:37:49,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (126) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/126/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/126/galaxy_126.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:49,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:37:49,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [127] prepared (78.251 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:49,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:37:49,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/127/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/127/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/127/configs/tmptcennhpd']
galaxy.jobs.runners DEBUG 2025-07-13 01:37:49,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (127) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/127/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/127/galaxy_127.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:49,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:49,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:50,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:50,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:37:59,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zmqb9 with k8s id: gxy-zmqb9 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:37:59,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 126: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:00,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h5wdv with k8s id: gxy-h5wdv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:38:00,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 127: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:38:07,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 126 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:38:07,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.cram', 'dbkey': '?', 'ext': 'cram', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded cram file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/126/working/gxupload_0', 'object_id': 135}]}]}]
galaxy.jobs INFO 2025-07-13 01:38:07,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 126 in /galaxy/server/database/jobs_directory/000/126
galaxy.jobs DEBUG 2025-07-13 01:38:07,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 126 executed (179.282 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:07,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:38:08,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 127 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:38:08,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'freebayes-phix174.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/127/working/data_fetch_upload_f09nsxcl', 'object_id': 136}]}]}]
galaxy.jobs INFO 2025-07-13 01:38:08,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 127 in /galaxy/server/database/jobs_directory/000/127
galaxy.jobs DEBUG 2025-07-13 01:38:08,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 127 executed (166.833 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:08,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:38:08,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 128
tpv.core.entities DEBUG 2025-07-13 01:38:08,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:38:08,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:38:08,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:38:08,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:38:08,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:38:08,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Working directory for job is: /galaxy/server/database/jobs_directory/000/128
galaxy.jobs.runners DEBUG 2025-07-13 01:38:08,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [128] queued (39.533 ms)
galaxy.jobs.handler INFO 2025-07-13 01:38:08,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:08,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 128
galaxy.jobs DEBUG 2025-07-13 01:38:09,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [128] prepared (68.824 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:09,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:38:09,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:09,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:38:09,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/128/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/128/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/6/a/8/dataset_6a8996f0-a045-4df6-8fa5-2ef4a2af8c78.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/7/4/8/dataset_748016df-ce29-47c4-a771-a49adadb688b.dat' 'b_0.cram' && ln -s -f '/galaxy/server/database/objects/_metadata_files/1/3/0/metadata_130c90f8-116e-4d26-bafc-010dab7f7b5b.dat' 'b_0.cram.crai' &&   samtools view -H b_0.cram| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.cram' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/8/2/9/dataset_8292965c-99d0-4f14-b3ea-6b8ce574c46e.dat']
galaxy.jobs.runners DEBUG 2025-07-13 01:38:09,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (128) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/128/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/128/galaxy_128.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:09,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:09,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:38:09,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:09,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:09,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:09,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:14,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5dhkx with k8s id: gxy-5dhkx succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:38:14,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 128: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:38:22,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 128 finished
galaxy.model.metadata DEBUG 2025-07-13 01:38:22,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 137
galaxy.jobs INFO 2025-07-13 01:38:22,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 128 in /galaxy/server/database/jobs_directory/000/128
galaxy.jobs DEBUG 2025-07-13 01:38:22,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 128 executed (100.147 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:22,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:38:25,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 130, 129
tpv.core.entities DEBUG 2025-07-13 01:38:25,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:38:25,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:38:25,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:38:25,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:38:25,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:38:25,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Working directory for job is: /galaxy/server/database/jobs_directory/000/129
galaxy.jobs.runners DEBUG 2025-07-13 01:38:25,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [129] queued (42.736 ms)
galaxy.jobs.handler INFO 2025-07-13 01:38:25,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:25,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 129
tpv.core.entities DEBUG 2025-07-13 01:38:25,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:38:25,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:38:25,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:38:25,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:38:25,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:38:25,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Working directory for job is: /galaxy/server/database/jobs_directory/000/130
galaxy.jobs.runners DEBUG 2025-07-13 01:38:25,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [130] queued (46.986 ms)
galaxy.jobs.handler INFO 2025-07-13 01:38:25,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:25,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 130
galaxy.jobs DEBUG 2025-07-13 01:38:25,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [129] prepared (79.957 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:38:25,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/129/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/129/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/129/configs/tmpwupz4vza']
galaxy.jobs.runners DEBUG 2025-07-13 01:38:25,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (129) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/129/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/129/galaxy_129.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:38:25,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [130] prepared (69.489 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:25,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:38:25,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/130/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/130/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/130/configs/tmpmvcwv63h']
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:25,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:38:25,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (130) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/130/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/130/galaxy_130.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:25,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:25,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:25,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:26,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:38:26,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 131
tpv.core.entities DEBUG 2025-07-13 01:38:26,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:38:26,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:38:26,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:38:26,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:38:26,486 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:38:26,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Working directory for job is: /galaxy/server/database/jobs_directory/000/131
galaxy.jobs.runners DEBUG 2025-07-13 01:38:26,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [131] queued (39.188 ms)
galaxy.jobs.handler INFO 2025-07-13 01:38:26,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:26,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 131
galaxy.jobs DEBUG 2025-07-13 01:38:26,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [131] prepared (69.886 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:38:26,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/131/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/131/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/131/configs/tmpwype7k08']
galaxy.jobs.runners DEBUG 2025-07-13 01:38:26,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (131) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/131/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/131/galaxy_131.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:26,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:26,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:27,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:35,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xm84k with k8s id: gxy-xm84k succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:35,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6vjvv with k8s id: gxy-6vjvv succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:38:36,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 129: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:38:36,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 130: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:38,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6p4nb with k8s id: gxy-6p4nb succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:38:38,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 131: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:38:47,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 129 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:38:47,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/129/working/data_fetch_upload_f58h8xpw', 'object_id': 138}]}]}]
galaxy.jobs INFO 2025-07-13 01:38:47,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 129 in /galaxy/server/database/jobs_directory/000/129
galaxy.jobs.runners DEBUG 2025-07-13 01:38:47,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 130 finished
galaxy.jobs DEBUG 2025-07-13 01:38:47,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 129 executed (204.118 ms)
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:38:47,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/130/working/data_fetch_upload_1a8qvclf', 'object_id': 139}]}]}]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:47,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:38:47,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 130 in /galaxy/server/database/jobs_directory/000/130
galaxy.jobs DEBUG 2025-07-13 01:38:47,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 130 executed (137.332 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:47,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:38:49,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 131 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:38:49,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/131/working/data_fetch_upload_78xpqlqp', 'object_id': 140}]}]}]
galaxy.jobs INFO 2025-07-13 01:38:49,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 131 in /galaxy/server/database/jobs_directory/000/131
galaxy.jobs DEBUG 2025-07-13 01:38:49,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 131 executed (129.993 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:49,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:38:50,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 132
tpv.core.entities DEBUG 2025-07-13 01:38:50,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:38:50,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:38:50,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:38:50,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:38:50,207 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:38:50,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Working directory for job is: /galaxy/server/database/jobs_directory/000/132
galaxy.jobs.runners DEBUG 2025-07-13 01:38:50,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [132] queued (44.577 ms)
galaxy.jobs.handler INFO 2025-07-13 01:38:50,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:50,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 132
galaxy.jobs DEBUG 2025-07-13 01:38:50,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [132] prepared (92.820 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:50,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:38:50,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:50,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:38:50,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/132/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/c/8/e/dataset_c8e3520e-47f7-473b-8e46-34c32ecea806.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/0/7/8/dataset_0789a5e0-ee65-4c86-b005-8d0c34cda572.dat' '/galaxy/server/database/objects/c/2/d/dataset_c2d60648-d4aa-487f-8ced-0cfabaf00814.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-13 01:38:50,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (132) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/132/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/132/galaxy_132.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/132/working/abundance.h5" -a -f "/galaxy/server/database/objects/6/d/2/dataset_6d22e1fa-680c-4c9f-a6d1-a8954b8ba2bf.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/132/working/abundance.h5" "/galaxy/server/database/objects/6/d/2/dataset_6d22e1fa-680c-4c9f-a6d1-a8954b8ba2bf.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/132/working/abundance.tsv" -a -f "/galaxy/server/database/objects/5/2/3/dataset_52333c34-d70c-48f7-9c53-1598a9656004.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/132/working/abundance.tsv" "/galaxy/server/database/objects/5/2/3/dataset_52333c34-d70c-48f7-9c53-1598a9656004.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:50,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:50,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:38:50,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:38:50,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:50,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:51,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:38:58,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9bpls with k8s id: gxy-9bpls succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:38:58,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 132: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:39:06,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 132 finished
galaxy.model.metadata DEBUG 2025-07-13 01:39:06,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.model.metadata DEBUG 2025-07-13 01:39:06,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 142
galaxy.util WARNING 2025-07-13 01:39:06,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/d/2/dataset_6d22e1fa-680c-4c9f-a6d1-a8954b8ba2bf.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/d/2/dataset_6d22e1fa-680c-4c9f-a6d1-a8954b8ba2bf.dat'
galaxy.util WARNING 2025-07-13 01:39:06,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/2/3/dataset_52333c34-d70c-48f7-9c53-1598a9656004.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/2/3/dataset_52333c34-d70c-48f7-9c53-1598a9656004.dat'
galaxy.jobs INFO 2025-07-13 01:39:06,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 132 in /galaxy/server/database/jobs_directory/000/132
galaxy.jobs DEBUG 2025-07-13 01:39:06,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 132 executed (156.067 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:06,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:39:07,563 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 133
tpv.core.entities DEBUG 2025-07-13 01:39:07,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:39:07,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:39:07,589 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:39:07,592 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:39:07,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:39:07,621 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Working directory for job is: /galaxy/server/database/jobs_directory/000/133
galaxy.jobs.runners DEBUG 2025-07-13 01:39:07,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [133] queued (37.891 ms)
galaxy.jobs.handler INFO 2025-07-13 01:39:07,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:07,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 133
galaxy.jobs DEBUG 2025-07-13 01:39:07,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [133] prepared (68.536 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:39:07,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/133/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/133/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/133/configs/tmpanelk008']
galaxy.jobs.runners DEBUG 2025-07-13 01:39:07,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (133) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/133/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/133/galaxy_133.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:07,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:07,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:08,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:39:08,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 135, 134
tpv.core.entities DEBUG 2025-07-13 01:39:08,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:39:08,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:39:08,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:39:08,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:39:08,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:39:08,695 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Working directory for job is: /galaxy/server/database/jobs_directory/000/134
galaxy.jobs.runners DEBUG 2025-07-13 01:39:08,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [134] queued (40.425 ms)
galaxy.jobs.handler INFO 2025-07-13 01:39:08,705 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:08,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 134
tpv.core.entities DEBUG 2025-07-13 01:39:08,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:39:08,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:39:08,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:39:08,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:39:08,740 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:39:08,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Working directory for job is: /galaxy/server/database/jobs_directory/000/135
galaxy.jobs.runners DEBUG 2025-07-13 01:39:08,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [135] queued (44.050 ms)
galaxy.jobs.handler INFO 2025-07-13 01:39:08,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:08,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 135
galaxy.jobs DEBUG 2025-07-13 01:39:08,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [134] prepared (74.870 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:39:08,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/134/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/134/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/134/configs/tmpipgkr8or']
galaxy.jobs.runners DEBUG 2025-07-13 01:39:08,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (134) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/134/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/134/galaxy_134.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:08,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:39:08,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [135] prepared (75.634 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:08,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:39:08,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/135/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/135/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/135/configs/tmpj5sifwv_']
galaxy.jobs.runners DEBUG 2025-07-13 01:39:08,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (135) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/135/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/135/galaxy_135.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:08,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:08,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:09,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:09,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:18,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6t4kt with k8s id: gxy-6t4kt succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:39:18,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 133: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:19,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6gqhf with k8s id: gxy-6gqhf succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:39:19,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 135: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:20,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nm5xm with k8s id: gxy-nm5xm succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:39:20,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 134: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:39:29,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 133 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:39:29,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/133/working/data_fetch_upload_72r1_ynw', 'object_id': 143}]}]}]
galaxy.jobs INFO 2025-07-13 01:39:29,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 133 in /galaxy/server/database/jobs_directory/000/133
galaxy.jobs DEBUG 2025-07-13 01:39:29,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 133 executed (195.139 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:29,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:39:31,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 135 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:39:31,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/135/working/data_fetch_upload_8n75umwu', 'object_id': 145}]}]}]
galaxy.jobs INFO 2025-07-13 01:39:31,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 135 in /galaxy/server/database/jobs_directory/000/135
galaxy.jobs DEBUG 2025-07-13 01:39:31,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 135 executed (154.411 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:31,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:39:32,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 134 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:39:32,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/134/working/data_fetch_upload_oob11fvs', 'object_id': 144}]}]}]
galaxy.jobs INFO 2025-07-13 01:39:32,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 134 in /galaxy/server/database/jobs_directory/000/134
galaxy.jobs DEBUG 2025-07-13 01:39:32,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 134 executed (171.863 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:32,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:39:33,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 136
tpv.core.entities DEBUG 2025-07-13 01:39:33,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:39:33,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:39:33,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:39:33,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:39:33,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:39:33,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Working directory for job is: /galaxy/server/database/jobs_directory/000/136
galaxy.jobs.runners DEBUG 2025-07-13 01:39:33,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [136] queued (48.886 ms)
galaxy.jobs.handler INFO 2025-07-13 01:39:33,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:33,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 136
galaxy.jobs DEBUG 2025-07-13 01:39:33,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [136] prepared (67.903 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:39:33,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:39:33,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:39:33,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:39:33,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/136/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/c/2/e/dataset_c2e5ae0e-60bd-4b12-aa46-40ef99b6f70f.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o . --fr-stranded '/galaxy/server/database/objects/4/6/2/dataset_462f78dc-4b51-47b2-a1f0-a0debfdf3c3b.dat' '/galaxy/server/database/objects/5/f/6/dataset_5f64dc7a-e57b-4e72-a1a9-5be296979896.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-13 01:39:33,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (136) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/136/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/136/galaxy_136.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/136/working/abundance.h5" -a -f "/galaxy/server/database/objects/a/6/9/dataset_a696f32b-879d-454e-8123-5c5148d71f40.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/136/working/abundance.h5" "/galaxy/server/database/objects/a/6/9/dataset_a696f32b-879d-454e-8123-5c5148d71f40.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/136/working/abundance.tsv" -a -f "/galaxy/server/database/objects/1/3/4/dataset_134acd6e-687f-4264-bd69-af91f2fdb5f7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/136/working/abundance.tsv" "/galaxy/server/database/objects/1/3/4/dataset_134acd6e-687f-4264-bd69-af91f2fdb5f7.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:33,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:39:33,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:39:33,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:39:33,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:33,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:34,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:38,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tfcrd with k8s id: gxy-tfcrd succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:39:38,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 136: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:39:46,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 136 finished
galaxy.model.metadata DEBUG 2025-07-13 01:39:46,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 146
galaxy.model.metadata DEBUG 2025-07-13 01:39:46,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 147
galaxy.util WARNING 2025-07-13 01:39:46,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/6/9/dataset_a696f32b-879d-454e-8123-5c5148d71f40.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/6/9/dataset_a696f32b-879d-454e-8123-5c5148d71f40.dat'
galaxy.util WARNING 2025-07-13 01:39:46,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/3/4/dataset_134acd6e-687f-4264-bd69-af91f2fdb5f7.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/3/4/dataset_134acd6e-687f-4264-bd69-af91f2fdb5f7.dat'
galaxy.jobs INFO 2025-07-13 01:39:46,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 136 in /galaxy/server/database/jobs_directory/000/136
galaxy.jobs DEBUG 2025-07-13 01:39:46,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 136 executed (144.372 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:46,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:39:47,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 138, 139, 137
tpv.core.entities DEBUG 2025-07-13 01:39:47,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:39:47,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:39:47,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:39:47,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:39:47,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:39:47,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Working directory for job is: /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-07-13 01:39:47,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [137] queued (41.852 ms)
galaxy.jobs.handler INFO 2025-07-13 01:39:47,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:47,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 137
tpv.core.entities DEBUG 2025-07-13 01:39:47,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:39:47,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:39:47,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:39:47,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:39:47,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:39:47,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Working directory for job is: /galaxy/server/database/jobs_directory/000/138
galaxy.jobs.runners DEBUG 2025-07-13 01:39:47,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [138] queued (62.245 ms)
galaxy.jobs.handler INFO 2025-07-13 01:39:47,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:47,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 138
tpv.core.entities DEBUG 2025-07-13 01:39:48,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:39:48,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:39:48,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:39:48,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:39:48,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [137] prepared (114.021 ms)
galaxy.jobs DEBUG 2025-07-13 01:39:48,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-07-13 01:39:48,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/137/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/137/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/137/configs/tmpti60bpwf']
galaxy.jobs DEBUG 2025-07-13 01:39:48,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Working directory for job is: /galaxy/server/database/jobs_directory/000/139
galaxy.jobs.runners DEBUG 2025-07-13 01:39:48,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [139] queued (68.375 ms)
galaxy.jobs.handler INFO 2025-07-13 01:39:48,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Job dispatched
galaxy.jobs.runners DEBUG 2025-07-13 01:39:48,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (137) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/137/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/137/galaxy_137.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 139
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:39:48,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [138] prepared (119.931 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:39:48,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/138/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/138/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/138/configs/tmp5w5ypo51']
galaxy.jobs.runners DEBUG 2025-07-13 01:39:48,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (138) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/138/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/138/galaxy_138.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:39:48,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [139] prepared (99.336 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:39:48,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/139/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/139/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/139/configs/tmpyeyi9_2z']
galaxy.jobs.runners DEBUG 2025-07-13 01:39:48,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (139) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/139/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/139/galaxy_139.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:48,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:49,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:49,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:59,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h7x6d with k8s id: gxy-h7x6d succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:59,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gr5l2 with k8s id: gxy-gr5l2 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:39:59,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v8z5q with k8s id: gxy-v8z5q succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:39:59,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 137: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:39:59,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 138: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:39:59,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 139: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:40:12,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 137 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:40:12,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/137/working/data_fetch_upload_pr_1dyl8', 'object_id': 148}]}]}]
galaxy.jobs INFO 2025-07-13 01:40:12,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 137 in /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-07-13 01:40:12,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 138 finished
galaxy.jobs DEBUG 2025-07-13 01:40:12,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 137 executed (215.356 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:12,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:40:12,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/138/working/data_fetch_upload_akpg_32y', 'object_id': 149}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:40:12,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 139 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:40:12,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/139/working/data_fetch_upload_hms1w_nv', 'object_id': 150}]}]}]
galaxy.jobs INFO 2025-07-13 01:40:12,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 138 in /galaxy/server/database/jobs_directory/000/138
galaxy.jobs INFO 2025-07-13 01:40:12,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 139 in /galaxy/server/database/jobs_directory/000/139
galaxy.jobs DEBUG 2025-07-13 01:40:12,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 138 executed (169.890 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:12,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:40:12,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 139 executed (164.455 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:12,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:40:13,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 140
tpv.core.entities DEBUG 2025-07-13 01:40:13,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:40:13,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:40:13,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:40:13,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:40:13,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:40:13,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Working directory for job is: /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-07-13 01:40:13,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [140] queued (39.841 ms)
galaxy.jobs.handler INFO 2025-07-13 01:40:13,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:13,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 140
galaxy.jobs DEBUG 2025-07-13 01:40:14,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [140] prepared (79.278 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:14,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:40:14,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:14,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:40:14,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/140/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/d/d/e/dataset_dde908ce-4113-4152-8f8b-6732db6c77d5.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/6/b/b/dataset_6bbdb342-b407-429e-bae1-40cd7f55039d.dat' '/galaxy/server/database/objects/9/0/c/dataset_90c2ebcf-aa08-47e3-a824-da635e0790ee.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-13 01:40:14,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (140) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/140/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/140/galaxy_140.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/140/working/abundance.h5" -a -f "/galaxy/server/database/objects/b/7/2/dataset_b723c318-21bf-4f9b-8c46-5ecccc5f9fed.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/140/working/abundance.h5" "/galaxy/server/database/objects/b/7/2/dataset_b723c318-21bf-4f9b-8c46-5ecccc5f9fed.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/140/working/abundance.tsv" -a -f "/galaxy/server/database/objects/4/4/7/dataset_4471437c-3b79-4b57-8811-d68e800112cd.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/140/working/abundance.tsv" "/galaxy/server/database/objects/4/4/7/dataset_4471437c-3b79-4b57-8811-d68e800112cd.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:14,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:14,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:40:14,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:14,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:14,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:14,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:18,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kd5sk with k8s id: gxy-kd5sk succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:40:18,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 140: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:40:26,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 140 finished
galaxy.model.metadata DEBUG 2025-07-13 01:40:26,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 151
galaxy.model.metadata DEBUG 2025-07-13 01:40:26,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 152
galaxy.util WARNING 2025-07-13 01:40:26,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/7/2/dataset_b723c318-21bf-4f9b-8c46-5ecccc5f9fed.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/7/2/dataset_b723c318-21bf-4f9b-8c46-5ecccc5f9fed.dat'
galaxy.util WARNING 2025-07-13 01:40:26,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/4/7/dataset_4471437c-3b79-4b57-8811-d68e800112cd.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/4/7/dataset_4471437c-3b79-4b57-8811-d68e800112cd.dat'
galaxy.jobs INFO 2025-07-13 01:40:26,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 140 in /galaxy/server/database/jobs_directory/000/140
galaxy.jobs DEBUG 2025-07-13 01:40:26,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 140 executed (146.999 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:26,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:40:28,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 142, 141
tpv.core.entities DEBUG 2025-07-13 01:40:28,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:40:28,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:40:28,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:40:28,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:40:28,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:40:28,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Working directory for job is: /galaxy/server/database/jobs_directory/000/141
galaxy.jobs.runners DEBUG 2025-07-13 01:40:28,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [141] queued (39.858 ms)
galaxy.jobs.handler INFO 2025-07-13 01:40:28,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 141
tpv.core.entities DEBUG 2025-07-13 01:40:28,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:40:28,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:40:28,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:40:28,301 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:40:28,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:40:28,347 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Working directory for job is: /galaxy/server/database/jobs_directory/000/142
galaxy.jobs.runners DEBUG 2025-07-13 01:40:28,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [142] queued (53.662 ms)
galaxy.jobs.handler INFO 2025-07-13 01:40:28,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 142
galaxy.jobs DEBUG 2025-07-13 01:40:28,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [141] prepared (96.213 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:40:28,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/141/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/141/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/141/configs/tmpga91sp1g']
galaxy.jobs.runners DEBUG 2025-07-13 01:40:28,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (141) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/141/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/141/galaxy_141.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:40:28,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [142] prepared (91.134 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:40:28,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/142/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/142/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/142/configs/tmpbh6ia5o1']
galaxy.jobs.runners DEBUG 2025-07-13 01:40:28,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (142) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/142/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/142/galaxy_142.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:28,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:38,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-82hwr with k8s id: gxy-82hwr succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:40:38,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 141: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:39,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h7cj4 with k8s id: gxy-h7cj4 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:40:39,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 142: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:40:46,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 141 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:40:46,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/141/working/data_fetch_upload_9a79n_9h', 'object_id': 153}]}]}]
galaxy.jobs INFO 2025-07-13 01:40:46,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 141 in /galaxy/server/database/jobs_directory/000/141
galaxy.jobs DEBUG 2025-07-13 01:40:46,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 141 executed (123.860 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:46,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:40:47,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 142 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:40:47,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/142/working/data_fetch_upload_fvuc2hve', 'object_id': 154}]}]}]
galaxy.jobs INFO 2025-07-13 01:40:47,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 142 in /galaxy/server/database/jobs_directory/000/142
galaxy.jobs DEBUG 2025-07-13 01:40:47,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 142 executed (130.230 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:47,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:40:48,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 143
tpv.core.entities DEBUG 2025-07-13 01:40:48,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:40:48,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:40:48,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:40:48,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:40:48,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:40:48,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Working directory for job is: /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-07-13 01:40:48,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [143] queued (47.407 ms)
galaxy.jobs.handler INFO 2025-07-13 01:40:48,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:48,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 143
galaxy.jobs DEBUG 2025-07-13 01:40:48,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [143] prepared (67.234 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:48,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:40:48,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:48,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:40:48,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/143/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/8/3/1/dataset_8310e215-2874-43c5-acdf-9e1028db08f3.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o . --single --fragment-length 200 --sd 20 '/galaxy/server/database/objects/4/7/0/dataset_47045913-3172-4fb6-923e-5e778b1f8f88.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-13 01:40:48,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (143) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/143/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/143/galaxy_143.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/143/working/abundance.h5" -a -f "/galaxy/server/database/objects/b/e/c/dataset_becf1b8e-35de-4fce-a41e-efda67b5a071.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/143/working/abundance.h5" "/galaxy/server/database/objects/b/e/c/dataset_becf1b8e-35de-4fce-a41e-efda67b5a071.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/143/working/abundance.tsv" -a -f "/galaxy/server/database/objects/e/1/c/dataset_e1c607c1-0c06-438b-bdc1-89e8777cc6e4.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/143/working/abundance.tsv" "/galaxy/server/database/objects/e/1/c/dataset_e1c607c1-0c06-438b-bdc1-89e8777cc6e4.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:48,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:48,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:40:48,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:40:49,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:49,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:50,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:40:53,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9crs4 with k8s id: gxy-9crs4 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:40:53,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 143: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:41:01,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 143 finished
galaxy.model.metadata DEBUG 2025-07-13 01:41:01,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 155
galaxy.model.metadata DEBUG 2025-07-13 01:41:01,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 156
galaxy.util WARNING 2025-07-13 01:41:01,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/e/c/dataset_becf1b8e-35de-4fce-a41e-efda67b5a071.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/e/c/dataset_becf1b8e-35de-4fce-a41e-efda67b5a071.dat'
galaxy.util WARNING 2025-07-13 01:41:01,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/1/c/dataset_e1c607c1-0c06-438b-bdc1-89e8777cc6e4.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/1/c/dataset_e1c607c1-0c06-438b-bdc1-89e8777cc6e4.dat'
galaxy.jobs INFO 2025-07-13 01:41:01,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 143 in /galaxy/server/database/jobs_directory/000/143
galaxy.jobs DEBUG 2025-07-13 01:41:01,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 143 executed (158.065 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:01,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:41:03,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 145, 144, 146
tpv.core.entities DEBUG 2025-07-13 01:41:03,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:41:03,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:41:03,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:41:03,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:41:03,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:41:03,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Working directory for job is: /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-07-13 01:41:03,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [144] queued (38.404 ms)
galaxy.jobs.handler INFO 2025-07-13 01:41:03,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 144
tpv.core.entities DEBUG 2025-07-13 01:41:03,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:41:03,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:41:03,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:41:03,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:41:03,219 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:41:03,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Working directory for job is: /galaxy/server/database/jobs_directory/000/145
galaxy.jobs.runners DEBUG 2025-07-13 01:41:03,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [145] queued (42.793 ms)
galaxy.jobs.handler INFO 2025-07-13 01:41:03,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 145
tpv.core.entities DEBUG 2025-07-13 01:41:03,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:41:03,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:41:03,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:41:03,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:41:03,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [144] prepared (90.846 ms)
galaxy.jobs DEBUG 2025-07-13 01:41:03,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-07-13 01:41:03,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/144/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/144/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/144/configs/tmp_5g76a7r']
galaxy.jobs DEBUG 2025-07-13 01:41:03,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Working directory for job is: /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners DEBUG 2025-07-13 01:41:03,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [146] queued (56.113 ms)
galaxy.jobs.handler INFO 2025-07-13 01:41:03,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Job dispatched
galaxy.jobs.runners DEBUG 2025-07-13 01:41:03,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (144) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/144/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/144/galaxy_144.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 146
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:41:03,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [145] prepared (93.840 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:41:03,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/145/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/145/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/145/configs/tmp9nnmyizm']
galaxy.jobs.runners DEBUG 2025-07-13 01:41:03,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (145) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/145/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/145/galaxy_145.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:41:03,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [146] prepared (94.638 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:41:03,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/146/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/146/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/146/configs/tmpyewpb653']
galaxy.jobs.runners DEBUG 2025-07-13 01:41:03,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (146) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/146/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/146/galaxy_146.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:03,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:05,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:05,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:05,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:13,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2cvbz with k8s id: gxy-2cvbz succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:41:13,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 144: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:14,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dwrdg with k8s id: gxy-dwrdg succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:14,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2ch79 with k8s id: gxy-2ch79 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:41:14,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 145: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:41:15,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 146: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:41:25,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 144 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:41:25,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/144/working/data_fetch_upload_sw7eby_q', 'object_id': 157}]}]}]
galaxy.jobs INFO 2025-07-13 01:41:25,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 144 in /galaxy/server/database/jobs_directory/000/144
galaxy.jobs DEBUG 2025-07-13 01:41:25,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 144 executed (177.027 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:25,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:41:26,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 145 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:41:26,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM_F.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/145/working/data_fetch_upload_fnuygxur', 'object_id': 158}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:41:26,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 146 finished
galaxy.jobs INFO 2025-07-13 01:41:26,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 145 in /galaxy/server/database/jobs_directory/000/145
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:41:26,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM_R.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/146/working/data_fetch_upload_mh95y4q5', 'object_id': 159}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:41:26,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 145 executed (157.938 ms)
galaxy.jobs INFO 2025-07-13 01:41:26,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 146 in /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:26,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:41:26,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 146 executed (136.223 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:26,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:41:28,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 147
tpv.core.entities DEBUG 2025-07-13 01:41:28,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:41:28,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:41:28,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:41:28,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:41:28,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:41:28,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Working directory for job is: /galaxy/server/database/jobs_directory/000/147
galaxy.jobs.runners DEBUG 2025-07-13 01:41:28,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [147] queued (52.147 ms)
galaxy.jobs.handler INFO 2025-07-13 01:41:28,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:28,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 147
galaxy.jobs DEBUG 2025-07-13 01:41:28,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [147] prepared (55.796 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:41:28,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:41:28,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:41:28,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:41:28,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/147/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/2/f/9/dataset_2f95d83a-03e0-4795-8fa2-6f3884b96a6e.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42   --pseudobam -o .  '/galaxy/server/database/objects/6/3/6/dataset_6363b29b-ad34-4499-ab86-3bd51cde3ee2.dat' '/galaxy/server/database/objects/c/0/e/dataset_c0eba902-f7e8-45ac-823f-1111f1beb219.dat'  && samtools sort --no-PG -O bam -@ ${GALAXY_SLOTS:-1} -T "${TMPDIR:-.}" -o '/galaxy/server/database/objects/b/1/c/dataset_b1cd691f-6249-4465-9ba1-87004fb51a77.dat' pseudoalignments.bam && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-13 01:41:28,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (147) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/147/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/147/galaxy_147.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/147/working/abundance.h5" -a -f "/galaxy/server/database/objects/f/4/8/dataset_f48460fb-34bf-4dd9-9753-9da27aeed00f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/147/working/abundance.h5" "/galaxy/server/database/objects/f/4/8/dataset_f48460fb-34bf-4dd9-9753-9da27aeed00f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/147/working/abundance.tsv" -a -f "/galaxy/server/database/objects/0/a/d/dataset_0ad342ad-2f0a-4a56-b31c-c09a9cd9e1cc.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/147/working/abundance.tsv" "/galaxy/server/database/objects/0/a/d/dataset_0ad342ad-2f0a-4a56-b31c-c09a9cd9e1cc.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/147/working/abundance.txt" -a -f "/galaxy/server/database/objects/b/1/c/dataset_b1cd691f-6249-4465-9ba1-87004fb51a77.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/147/working/abundance.txt" "/galaxy/server/database/objects/b/1/c/dataset_b1cd691f-6249-4465-9ba1-87004fb51a77.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:28,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:41:28,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:41:28,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:41:28,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:28,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:28,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:33,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4qdqj with k8s id: gxy-4qdqj succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:41:33,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 147: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:41:41,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 147 finished
galaxy.model.metadata DEBUG 2025-07-13 01:41:41,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 160
galaxy.model.metadata DEBUG 2025-07-13 01:41:41,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 161
galaxy.model.metadata DEBUG 2025-07-13 01:41:41,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 162
galaxy.util WARNING 2025-07-13 01:41:41,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/4/8/dataset_f48460fb-34bf-4dd9-9753-9da27aeed00f.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/4/8/dataset_f48460fb-34bf-4dd9-9753-9da27aeed00f.dat'
galaxy.util WARNING 2025-07-13 01:41:41,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/a/d/dataset_0ad342ad-2f0a-4a56-b31c-c09a9cd9e1cc.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/a/d/dataset_0ad342ad-2f0a-4a56-b31c-c09a9cd9e1cc.dat'
galaxy.jobs INFO 2025-07-13 01:41:41,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 147 in /galaxy/server/database/jobs_directory/000/147
galaxy.jobs DEBUG 2025-07-13 01:41:41,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 147 executed (157.907 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:41,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:41:43,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 149, 148
tpv.core.entities DEBUG 2025-07-13 01:41:43,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:41:43,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:41:43,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:41:43,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:41:43,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:41:43,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Working directory for job is: /galaxy/server/database/jobs_directory/000/148
galaxy.jobs.runners DEBUG 2025-07-13 01:41:43,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [148] queued (32.531 ms)
galaxy.jobs.handler INFO 2025-07-13 01:41:43,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:43,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 148
tpv.core.entities DEBUG 2025-07-13 01:41:43,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:41:43,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:41:43,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:41:43,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:41:43,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:41:43,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Working directory for job is: /galaxy/server/database/jobs_directory/000/149
galaxy.jobs.runners DEBUG 2025-07-13 01:41:43,583 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [149] queued (48.570 ms)
galaxy.jobs.handler INFO 2025-07-13 01:41:43,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:43,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 149
galaxy.jobs DEBUG 2025-07-13 01:41:43,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [148] prepared (81.527 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:41:43,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/148/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/148/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/148/configs/tmpytdxbw_b']
galaxy.jobs.runners DEBUG 2025-07-13 01:41:43,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/148/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/148/galaxy_148.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:43,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:41:43,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [149] prepared (76.984 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:43,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:41:43,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/149/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/149/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/149/configs/tmpmatrswa8']
galaxy.jobs.runners DEBUG 2025-07-13 01:41:43,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (149) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/149/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/149/galaxy_149.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:43,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:43,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:44,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:44,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:53,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h48vr with k8s id: gxy-h48vr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:41:53,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hfbg9 with k8s id: gxy-hfbg9 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:41:53,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 148: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:41:53,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 149: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:42:01,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 149 finished
galaxy.jobs.runners DEBUG 2025-07-13 01:42:01,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 148 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:42:01,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sacCer2_chrM_R.fq', 'dbkey': 'sacCer2', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/149/working/data_fetch_upload_wz8r1j3s', 'object_id': 164}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:42:01,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sacCer2_chrM_F.fq.gz', 'dbkey': 'sacCer2', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/148/working/data_fetch_upload_hqtowa1q', 'object_id': 163}]}]}]
galaxy.jobs INFO 2025-07-13 01:42:02,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 149 in /galaxy/server/database/jobs_directory/000/149
galaxy.jobs INFO 2025-07-13 01:42:02,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 148 in /galaxy/server/database/jobs_directory/000/148
galaxy.jobs DEBUG 2025-07-13 01:42:02,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 149 executed (194.828 ms)
galaxy.jobs DEBUG 2025-07-13 01:42:02,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 148 executed (180.772 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:02,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:02,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:42:03,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 150
tpv.core.entities DEBUG 2025-07-13 01:42:03,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:03,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:03,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:03,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:03,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:03,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Working directory for job is: /galaxy/server/database/jobs_directory/000/150
galaxy.jobs.runners DEBUG 2025-07-13 01:42:03,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [150] queued (38.166 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:03,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:03,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 150
galaxy.jobs DEBUG 2025-07-13 01:42:03,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [150] prepared (67.480 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:42:03,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/150/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/150/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/150/configs/tmpdmdw4hte']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:03,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (150) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/150/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/150/galaxy_150.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:03,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:03,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:03,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:42:04,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 152, 151
tpv.core.entities DEBUG 2025-07-13 01:42:04,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:04,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:04,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:04,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:04,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:04,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Working directory for job is: /galaxy/server/database/jobs_directory/000/151
galaxy.jobs.runners DEBUG 2025-07-13 01:42:04,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [151] queued (32.181 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:04,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 151
tpv.core.entities DEBUG 2025-07-13 01:42:04,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:04,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:04,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:04,193 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:04,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:04,234 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Working directory for job is: /galaxy/server/database/jobs_directory/000/152
galaxy.jobs.runners DEBUG 2025-07-13 01:42:04,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [152] queued (48.583 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:04,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 152
galaxy.jobs DEBUG 2025-07-13 01:42:04,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [151] prepared (81.494 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:42:04,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/151/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/151/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/151/configs/tmpy7841q_b']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:04,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/151/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/151/galaxy_151.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:42:04,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [152] prepared (85.104 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:42:04,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/152/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/152/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/152/configs/tmp6fbhpa_k']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:04,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (152) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/152/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/152/galaxy_152.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:04,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:13,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-529bf with k8s id: gxy-529bf succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:42:13,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 150: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:14,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-br8zx with k8s id: gxy-br8zx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:14,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q4cz2 with k8s id: gxy-q4cz2 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:42:14,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 151: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:42:14,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 152: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:42:24,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 150 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:42:24,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'hg38_transcripts.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/150/working/data_fetch_upload_8k9w0ruy', 'object_id': 165}]}]}]
galaxy.jobs INFO 2025-07-13 01:42:25,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 150 in /galaxy/server/database/jobs_directory/000/150
galaxy.jobs DEBUG 2025-07-13 01:42:25,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 150 executed (182.054 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:25,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:42:26,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 151 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:42:26,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'hg38_F.fq.gz', 'dbkey': 'hg38', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/151/working/data_fetch_upload_y0h43lvn', 'object_id': 166}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:42:26,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 152 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:42:26,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'hg38_R.fq.gz', 'dbkey': 'hg38', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/152/working/data_fetch_upload__oli77xd', 'object_id': 167}]}]}]
galaxy.jobs INFO 2025-07-13 01:42:26,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 151 in /galaxy/server/database/jobs_directory/000/151
galaxy.jobs INFO 2025-07-13 01:42:26,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 152 in /galaxy/server/database/jobs_directory/000/152
galaxy.jobs DEBUG 2025-07-13 01:42:26,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 151 executed (171.576 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:26,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:42:26,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 152 executed (152.932 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:26,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:42:26,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 153
tpv.core.entities DEBUG 2025-07-13 01:42:26,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:26,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:26,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:26,885 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:26,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:26,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Working directory for job is: /galaxy/server/database/jobs_directory/000/153
galaxy.jobs.runners DEBUG 2025-07-13 01:42:26,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [153] queued (38.273 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:26,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:26,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 153
galaxy.jobs DEBUG 2025-07-13 01:42:26,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [153] prepared (44.589 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:42:26,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:42:26,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:42:27,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:42:27,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/153/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/3/8/f/dataset_38fc60a7-5e97-485e-8224-6f90ad06c1b3.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --fusion --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/b/7/1/dataset_b711f729-9262-4111-9eaa-b9ef8767c383.dat' '/galaxy/server/database/objects/8/7/d/dataset_87d09b81-b6d1-4fcb-99e8-74a0c6d2436e.dat'  && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-13 01:42:27,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/153/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/153/galaxy_153.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/153/working/abundance.h5" -a -f "/galaxy/server/database/objects/8/3/2/dataset_83258232-4a98-4cd4-8c5b-9799dfd6b9ce.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/153/working/abundance.h5" "/galaxy/server/database/objects/8/3/2/dataset_83258232-4a98-4cd4-8c5b-9799dfd6b9ce.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/153/working/abundance.tsv" -a -f "/galaxy/server/database/objects/1/f/0/dataset_1f097caf-3916-47f6-9919-f5efe2512844.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/153/working/abundance.tsv" "/galaxy/server/database/objects/1/f/0/dataset_1f097caf-3916-47f6-9919-f5efe2512844.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/153/working/fusion.txt" -a -f "/galaxy/server/database/objects/d/3/e/dataset_d3ec39a4-81b7-486d-907b-8aea30b29c6f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/153/working/fusion.txt" "/galaxy/server/database/objects/d/3/e/dataset_d3ec39a4-81b7-486d-907b-8aea30b29c6f.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:27,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:42:27,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:42:27,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:42:27,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:27,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:27,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:31,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-49bn6 with k8s id: gxy-49bn6 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:42:31,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 153: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:42:39,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 153 finished
galaxy.model.metadata DEBUG 2025-07-13 01:42:39,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 168
galaxy.model.metadata DEBUG 2025-07-13 01:42:39,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 169
galaxy.model.metadata DEBUG 2025-07-13 01:42:39,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 170
galaxy.util WARNING 2025-07-13 01:42:39,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/3/2/dataset_83258232-4a98-4cd4-8c5b-9799dfd6b9ce.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/3/2/dataset_83258232-4a98-4cd4-8c5b-9799dfd6b9ce.dat'
galaxy.util WARNING 2025-07-13 01:42:39,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/f/0/dataset_1f097caf-3916-47f6-9919-f5efe2512844.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/f/0/dataset_1f097caf-3916-47f6-9919-f5efe2512844.dat'
galaxy.util WARNING 2025-07-13 01:42:39,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/3/e/dataset_d3ec39a4-81b7-486d-907b-8aea30b29c6f.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/3/e/dataset_d3ec39a4-81b7-486d-907b-8aea30b29c6f.dat'
galaxy.jobs INFO 2025-07-13 01:42:39,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 153 in /galaxy/server/database/jobs_directory/000/153
galaxy.jobs DEBUG 2025-07-13 01:42:39,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 153 executed (151.983 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:39,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:42:41,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 155, 156, 154
tpv.core.entities DEBUG 2025-07-13 01:42:41,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:41,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:41,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:41,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:41,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:41,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Working directory for job is: /galaxy/server/database/jobs_directory/000/154
galaxy.jobs.runners DEBUG 2025-07-13 01:42:41,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [154] queued (38.420 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:41,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 154
tpv.core.entities DEBUG 2025-07-13 01:42:41,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:41,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:41,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:41,298 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:41,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:41,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Working directory for job is: /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-07-13 01:42:41,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [155] queued (49.316 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:41,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 155
tpv.core.entities DEBUG 2025-07-13 01:42:41,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:41,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:41,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:41,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:41,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:41,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [154] prepared (103.727 ms)
galaxy.jobs DEBUG 2025-07-13 01:42:41,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Working directory for job is: /galaxy/server/database/jobs_directory/000/156
galaxy.jobs.command_factory INFO 2025-07-13 01:42:41,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/154/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/154/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/154/configs/tmp97ct8b69']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:41,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [156] queued (59.449 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:41,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 156
galaxy.jobs.runners DEBUG 2025-07-13 01:42:41,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (154) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/154/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/154/galaxy_154.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:42:41,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [155] prepared (103.072 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:42:41,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/155/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/155/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/155/configs/tmp25_r9sl_']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:41,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (155) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/155/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/155/galaxy_155.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:42:41,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [156] prepared (90.081 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:42:41,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/156/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/156/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/156/configs/tmp13682uf3']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:41,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (156) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/156/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/156/galaxy_156.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:41,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-07-13 01:42:42,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 158, 157
tpv.core.entities DEBUG 2025-07-13 01:42:42,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:42,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:42,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:42,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:42,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:42,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Working directory for job is: /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners DEBUG 2025-07-13 01:42:42,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [157] queued (37.006 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:42,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:42,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 157
tpv.core.entities DEBUG 2025-07-13 01:42:42,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:42:42,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:42:42,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:42:42,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:42:42,560 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:42:42,584 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Working directory for job is: /galaxy/server/database/jobs_directory/000/158
galaxy.jobs.runners DEBUG 2025-07-13 01:42:42,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [158] queued (56.784 ms)
galaxy.jobs.handler INFO 2025-07-13 01:42:42,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:42,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 158
galaxy.jobs DEBUG 2025-07-13 01:42:42,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [157] prepared (90.841 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:42:42,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/157/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/157/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/157/configs/tmp8ir2632e']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:42,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (157) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/157/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/157/galaxy_157.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:42,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:42:42,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [158] prepared (81.611 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:42,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:42:42,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/158/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/158/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/158/configs/tmpqtileau0']
galaxy.jobs.runners DEBUG 2025-07-13 01:42:42,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (158) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/158/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/158/galaxy_158.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:42,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:42,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:43,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:43,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:43,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:52,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w76kr with k8s id: gxy-w76kr succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:42:52,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 154: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:53,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zc2xl with k8s id: gxy-zc2xl succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:53,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4v26b with k8s id: gxy-4v26b succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:53,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rlnx6 with k8s id: gxy-rlnx6 succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:42:53,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 155: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:42:53,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 156: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:42:53,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 157: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:42:54,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rn7bk with k8s id: gxy-rn7bk succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:43:07,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 154 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:43:07,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'transcripts.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/154/working/data_fetch_upload_yud3524k', 'object_id': 171}]}]}]
galaxy.jobs INFO 2025-07-13 01:43:07,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 154 in /galaxy/server/database/jobs_directory/000/154
galaxy.jobs DEBUG 2025-07-13 01:43:07,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 154 executed (282.835 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:07,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:43:08,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 158: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:43:09,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 155 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:43:09,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reads_forward.fastq.gz', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/155/working/data_fetch_upload_daexo3v8', 'object_id': 172}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:43:09,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 157 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:43:09,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'annotation.gtf.gz', 'dbkey': '?', 'ext': 'gtf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gtf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/157/working/data_fetch_upload_h6jwnf3_', 'object_id': 174}]}]}]
galaxy.jobs INFO 2025-07-13 01:43:09,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 155 in /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-07-13 01:43:09,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 156 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:43:09,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'reads_reverse.fastq.gz', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/156/working/data_fetch_upload_5u46padw', 'object_id': 173}]}]}]
galaxy.jobs DEBUG 2025-07-13 01:43:10,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 155 executed (1010.273 ms)
galaxy.jobs INFO 2025-07-13 01:43:10,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 157 in /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:10,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-07-13 01:43:10,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 156 in /galaxy/server/database/jobs_directory/000/156
galaxy.jobs DEBUG 2025-07-13 01:43:10,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 157 executed (931.197 ms)
galaxy.jobs DEBUG 2025-07-13 01:43:10,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 156 executed (847.554 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:10,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:10,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-07-13 01:43:16,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 158 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:43:17,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'chromosome_size.tabular', 'dbkey': '?', 'ext': 'txt', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded txt file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/158/working/data_fetch_upload_f5gbi_yv', 'object_id': 175}]}]}]
galaxy.jobs INFO 2025-07-13 01:43:17,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 158 in /galaxy/server/database/jobs_directory/000/158
galaxy.jobs DEBUG 2025-07-13 01:43:17,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 158 executed (153.314 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:17,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:43:17,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 159
tpv.core.entities DEBUG 2025-07-13 01:43:17,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:43:17,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:43:17,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:43:17,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:43:17,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:43:17,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Working directory for job is: /galaxy/server/database/jobs_directory/000/159
galaxy.jobs.runners DEBUG 2025-07-13 01:43:17,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [159] queued (43.359 ms)
galaxy.jobs.handler INFO 2025-07-13 01:43:17,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:17,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 159
galaxy.jobs DEBUG 2025-07-13 01:43:17,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [159] prepared (88.825 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:17,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:43:17,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:18,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:43:18,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/159/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/3/d/6/dataset_3d6f27c2-0594-49f8-b2d0-535cf1125514.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto quant -i 'reference.kallisto' --bootstrap-samples 0 --seed 42 --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/8/3/a/dataset_83a624bd-9ce5-44c6-8ef1-1d630e5ce672.dat' '/galaxy/server/database/objects/3/c/a/dataset_3ca5fc1b-b439-4f37-97f9-8fff4315a9c1.dat'  --genomebam --gtf /galaxy/server/database/objects/d/9/6/dataset_d96888d5-7021-4a26-83ba-aeb0bbcc0ee0.dat --chromosomes /galaxy/server/database/objects/3/e/4/dataset_3e4e7c8e-0b49-4462-b40f-8efc8cb9c2ad.dat && cat run_info.json]
galaxy.jobs.runners DEBUG 2025-07-13 01:43:18,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (159) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/159/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/159/galaxy_159.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/159/working/abundance.h5" -a -f "/galaxy/server/database/objects/7/e/9/dataset_7e953b90-da3a-4156-8c07-7da2bfe265bf.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/159/working/abundance.h5" "/galaxy/server/database/objects/7/e/9/dataset_7e953b90-da3a-4156-8c07-7da2bfe265bf.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/159/working/abundance.tsv" -a -f "/galaxy/server/database/objects/f/9/1/dataset_f91c928c-0013-42e1-ace6-43426df1476e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/159/working/abundance.tsv" "/galaxy/server/database/objects/f/9/1/dataset_f91c928c-0013-42e1-ace6-43426df1476e.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:18,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:18,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:43:18,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_quant/kallisto_quant/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:18,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:18,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:18,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:22,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2mgkb with k8s id: gxy-2mgkb succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:43:22,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 159: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:43:30,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 159 finished
galaxy.model.metadata DEBUG 2025-07-13 01:43:30,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 176
galaxy.model.metadata DEBUG 2025-07-13 01:43:30,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 177
galaxy.util WARNING 2025-07-13 01:43:30,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/e/9/dataset_7e953b90-da3a-4156-8c07-7da2bfe265bf.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/e/9/dataset_7e953b90-da3a-4156-8c07-7da2bfe265bf.dat'
galaxy.util WARNING 2025-07-13 01:43:30,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/9/1/dataset_f91c928c-0013-42e1-ace6-43426df1476e.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/9/1/dataset_f91c928c-0013-42e1-ace6-43426df1476e.dat'
galaxy.jobs INFO 2025-07-13 01:43:30,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 159 in /galaxy/server/database/jobs_directory/000/159
galaxy.jobs DEBUG 2025-07-13 01:43:30,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 159 executed (136.488 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:30,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:43:34,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 161, 160
tpv.core.entities DEBUG 2025-07-13 01:43:34,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:43:34,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:43:34,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:43:34,193 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:43:34,207 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:43:34,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Working directory for job is: /galaxy/server/database/jobs_directory/000/160
galaxy.jobs.runners DEBUG 2025-07-13 01:43:34,233 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [160] queued (39.802 ms)
galaxy.jobs.handler INFO 2025-07-13 01:43:34,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 160
tpv.core.entities DEBUG 2025-07-13 01:43:34,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:43:34,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:43:34,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:43:34,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:43:34,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:43:34,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Working directory for job is: /galaxy/server/database/jobs_directory/000/161
galaxy.jobs.runners DEBUG 2025-07-13 01:43:34,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [161] queued (40.315 ms)
galaxy.jobs.handler INFO 2025-07-13 01:43:34,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 161
galaxy.jobs DEBUG 2025-07-13 01:43:34,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [160] prepared (86.993 ms)
galaxy.jobs.command_factory INFO 2025-07-13 01:43:34,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/160/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/160/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/160/configs/tmph5as1poc']
galaxy.jobs.runners DEBUG 2025-07-13 01:43:34,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (160) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/160/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/160/galaxy_160.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-07-13 01:43:34,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [161] prepared (78.107 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-07-13 01:43:34,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/161/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/161/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/161/configs/tmphe77uki7']
galaxy.jobs.runners DEBUG 2025-07-13 01:43:34,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (161) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/161/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/161/galaxy_161.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:34,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:45,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tnwjn with k8s id: gxy-tnwjn succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:45,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-66f7m with k8s id: gxy-66f7m succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:43:45,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 160: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:43:45,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 161: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:43:53,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 160 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:43:53,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'human_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/160/working/data_fetch_upload_jb6z3ugq', 'object_id': 178}]}]}]
galaxy.jobs.runners DEBUG 2025-07-13 01:43:53,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 161 finished
galaxy.tool_util.provided_metadata DEBUG 2025-07-13 01:43:53,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mouse_aqp3.fasta', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/161/working/data_fetch_upload_w1_tqq0b', 'object_id': 179}]}]}]
galaxy.jobs INFO 2025-07-13 01:43:53,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 160 in /galaxy/server/database/jobs_directory/000/160
galaxy.jobs INFO 2025-07-13 01:43:53,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 161 in /galaxy/server/database/jobs_directory/000/161
galaxy.jobs DEBUG 2025-07-13 01:43:53,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 160 executed (162.781 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:53,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-07-13 01:43:53,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 161 executed (157.087 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:53,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-07-13 01:43:54,683 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 162
tpv.core.entities DEBUG 2025-07-13 01:43:54,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-07-13 01:43:54,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-07-13 01:43:54,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-07-13 01:43:54,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-07-13 01:43:54,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-07-13 01:43:54,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Working directory for job is: /galaxy/server/database/jobs_directory/000/162
galaxy.jobs.runners DEBUG 2025-07-13 01:43:54,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [162] queued (56.885 ms)
galaxy.jobs.handler INFO 2025-07-13 01:43:54,784 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:54,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 162
galaxy.jobs DEBUG 2025-07-13 01:43:54,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [162] prepared (75.374 ms)
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:54,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:43:54,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_dnadiff/mummer_dnadiff/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:55,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-07-13 01:43:55,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/162/tool_script.sh] for tool command [ln -s /galaxy/server/database/objects/8/6/8/dataset_868653bc-22e0-40b2-ab61-3d66fd08c992.dat reference.fa && ln -s /galaxy/server/database/objects/8/8/9/dataset_8899a4b8-0a9a-49a2-ac8f-2e4d69b7ae47.dat query.fa && dnadiff 'reference.fa' 'query.fa']
galaxy.jobs.runners DEBUG 2025-07-13 01:43:55,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (162) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/162/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/162/galaxy_162.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.report" -a -f "/galaxy/server/database/objects/8/1/9/dataset_8196076d-90bc-444d-93b1-dd989e588d6b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.report" "/galaxy/server/database/objects/8/1/9/dataset_8196076d-90bc-444d-93b1-dd989e588d6b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.delta" -a -f "/galaxy/server/database/objects/8/3/8/dataset_838ca6ac-2cc1-4510-9433-0f0f607d1bdf.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.delta" "/galaxy/server/database/objects/8/3/8/dataset_838ca6ac-2cc1-4510-9433-0f0f607d1bdf.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.1delta" -a -f "/galaxy/server/database/objects/5/8/d/dataset_58d97ee9-b4f7-46d8-95b2-472c43184576.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.1delta" "/galaxy/server/database/objects/5/8/d/dataset_58d97ee9-b4f7-46d8-95b2-472c43184576.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.mdelta" -a -f "/galaxy/server/database/objects/7/e/0/dataset_7e0e64a5-bfce-47b7-aa7a-df231ac8a5b3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.mdelta" "/galaxy/server/database/objects/7/e/0/dataset_7e0e64a5-bfce-47b7-aa7a-df231ac8a5b3.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.1coords" -a -f "/galaxy/server/database/objects/c/e/b/dataset_ceb0a9d0-a4a9-468d-9e00-0050ae92381f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.1coords" "/galaxy/server/database/objects/c/e/b/dataset_ceb0a9d0-a4a9-468d-9e00-0050ae92381f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.mcoords" -a -f "/galaxy/server/database/objects/7/c/c/dataset_7ccef19a-e6c6-483e-aad5-48313798fa31.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.mcoords" "/galaxy/server/database/objects/7/c/c/dataset_7ccef19a-e6c6-483e-aad5-48313798fa31.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.snps" -a -f "/galaxy/server/database/objects/9/7/e/dataset_97ede2fe-8538-40ae-a781-56121a6800e1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.snps" "/galaxy/server/database/objects/9/7/e/dataset_97ede2fe-8538-40ae-a781-56121a6800e1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.rdiff" -a -f "/galaxy/server/database/objects/3/9/2/dataset_39283493-e25e-41f0-b819-85f501caf8ae.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.rdiff" "/galaxy/server/database/objects/3/9/2/dataset_39283493-e25e-41f0-b819-85f501caf8ae.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/out.qdiff" -a -f "/galaxy/server/database/objects/1/4/e/dataset_14e67d15-44ca-4e3c-acd6-87927a797d2f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/out.qdiff" "/galaxy/server/database/objects/1/4/e/dataset_14e67d15-44ca-4e3c-acd6-87927a797d2f.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:55,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:55,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-07-13 01:43:55,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_dnadiff/mummer_dnadiff/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-07-13 01:43:55,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:55,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:43:56,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:44:07,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-thjmm with k8s id: gxy-thjmm succeeded
galaxy.jobs.runners DEBUG 2025-07-13 01:44:08,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 162: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-07-13 01:44:15,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 162 finished
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 180
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 181
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 182
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 183
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 184
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 185
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 186
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 187
galaxy.model.metadata DEBUG 2025-07-13 01:44:15,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 188
galaxy.util WARNING 2025-07-13 01:44:15,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/1/9/dataset_8196076d-90bc-444d-93b1-dd989e588d6b.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/1/9/dataset_8196076d-90bc-444d-93b1-dd989e588d6b.dat'
galaxy.util WARNING 2025-07-13 01:44:15,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/3/8/dataset_838ca6ac-2cc1-4510-9433-0f0f607d1bdf.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/3/8/dataset_838ca6ac-2cc1-4510-9433-0f0f607d1bdf.dat'
galaxy.util WARNING 2025-07-13 01:44:15,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/8/d/dataset_58d97ee9-b4f7-46d8-95b2-472c43184576.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/8/d/dataset_58d97ee9-b4f7-46d8-95b2-472c43184576.dat'
galaxy.util WARNING 2025-07-13 01:44:15,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/e/0/dataset_7e0e64a5-bfce-47b7-aa7a-df231ac8a5b3.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/e/0/dataset_7e0e64a5-bfce-47b7-aa7a-df231ac8a5b3.dat'
galaxy.util WARNING 2025-07-13 01:44:15,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/e/b/dataset_ceb0a9d0-a4a9-468d-9e00-0050ae92381f.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/e/b/dataset_ceb0a9d0-a4a9-468d-9e00-0050ae92381f.dat'
galaxy.util WARNING 2025-07-13 01:44:15,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/c/c/dataset_7ccef19a-e6c6-483e-aad5-48313798fa31.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/c/c/dataset_7ccef19a-e6c6-483e-aad5-48313798fa31.dat'
galaxy.util WARNING 2025-07-13 01:44:15,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/7/e/dataset_97ede2fe-8538-40ae-a781-56121a6800e1.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/7/e/dataset_97ede2fe-8538-40ae-a781-56121a6800e1.dat'
galaxy.util WARNING 2025-07-13 01:44:15,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/9/2/dataset_39283493-e25e-41f0-b819-85f501caf8ae.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/9/2/dataset_39283493-e25e-41f0-b819-85f501caf8ae.dat'
galaxy.util WARNING 2025-07-13 01:44:15,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/4/e/dataset_14e67d15-44ca-4e3c-acd6-87927a797d2f.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/4/e/dataset_14e67d15-44ca-4e3c-acd6-87927a797d2f.dat'
galaxy.jobs INFO 2025-07-13 01:44:15,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 162 in /galaxy/server/database/jobs_directory/000/162
galaxy.jobs DEBUG 2025-07-13 01:44:16,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 162 executed (288.130 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-07-13 01:44:16,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
