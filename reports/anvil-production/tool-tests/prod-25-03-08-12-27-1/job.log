galaxy.jobs.runners DEBUG 2025-03-08 12:45:53,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 6 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:45:53,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/6/working/gxupload_0', 'object_id': 7}]}]}]
galaxy.jobs INFO 2025-03-08 12:45:53,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 6 in /galaxy/server/database/jobs_directory/000/6
galaxy.jobs DEBUG 2025-03-08 12:45:53,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 6 executed (126.979 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:45:53,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 6 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:45:54,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 7
tpv.core.entities DEBUG 2025-03-08 12:45:54,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:45:54,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:45:54,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:45:54,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:45:54,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Working directory for job is: /galaxy/server/database/jobs_directory/000/7
galaxy.jobs.runners DEBUG 2025-03-08 12:45:54,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [7] queued (34.684 ms)
galaxy.jobs.handler INFO 2025-03-08 12:45:54,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (7) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:45:54,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 7
galaxy.jobs DEBUG 2025-03-08 12:45:54,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [7] prepared (43.430 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:45:54,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:45:54,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:45:54,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:45:54,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/7/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/7/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/2/5/c/dataset_25c9aae7-c5a1-4011-8f05-359417c8b14b.dat' input_1.fastq.gz &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33    --output_dir ./   --illumina   input_1.fastq.gz    && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi   && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:45:54,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (7) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/7/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/7/galaxy_7.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/7/working/input_1_trimmed.fq" -a -f "/galaxy/server/database/objects/2/1/0/dataset_2105e094-4d31-4615-b26e-13c2206591bd.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/7/working/input_1_trimmed.fq" "/galaxy/server/database/objects/2/1/0/dataset_2105e094-4d31-4615-b26e-13c2206591bd.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:45:54,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:45:54,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:45:54,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:45:54,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:45:54,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:45:54,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:05,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rw6rn with k8s id: gxy-rw6rn succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:46:05,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 7: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:46:12,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 7 finished
galaxy.model.metadata DEBUG 2025-03-08 12:46:12,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 8
galaxy.util WARNING 2025-03-08 12:46:12,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/1/0/dataset_2105e094-4d31-4615-b26e-13c2206591bd.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/1/0/dataset_2105e094-4d31-4615-b26e-13c2206591bd.dat'
galaxy.jobs INFO 2025-03-08 12:46:12,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 7 in /galaxy/server/database/jobs_directory/000/7
galaxy.jobs DEBUG 2025-03-08 12:46:12,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 7 executed (106.926 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:12,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 7 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:46:13,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 8
tpv.core.entities DEBUG 2025-03-08 12:46:13,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:46:13,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:46:13,803 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:46:13,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:46:13,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Working directory for job is: /galaxy/server/database/jobs_directory/000/8
galaxy.jobs.runners DEBUG 2025-03-08 12:46:13,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [8] queued (30.271 ms)
galaxy.jobs.handler INFO 2025-03-08 12:46:13,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (8) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:13,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 8
galaxy.jobs DEBUG 2025-03-08 12:46:13,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [8] prepared (59.496 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:46:13,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/8/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/8/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/8/configs/tmpvwl6li98']
galaxy.jobs.runners DEBUG 2025-03-08 12:46:13,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (8) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/8/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/8/galaxy_8.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:13,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:13,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:14,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:23,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fqg97 with k8s id: gxy-fqg97 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:46:23,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 8: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:46:30,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 8 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:46:30,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastqsanger', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/8/working/data_fetch_upload_ypnr__8i', 'object_id': 9}]}]}]
galaxy.jobs INFO 2025-03-08 12:46:30,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 8 in /galaxy/server/database/jobs_directory/000/8
galaxy.jobs DEBUG 2025-03-08 12:46:30,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 8 executed (140.581 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:30,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 8 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:46:31,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 9
tpv.core.entities DEBUG 2025-03-08 12:46:31,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:46:31,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:46:31,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:46:31,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:46:31,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Working directory for job is: /galaxy/server/database/jobs_directory/000/9
galaxy.jobs.runners DEBUG 2025-03-08 12:46:31,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [9] queued (35.400 ms)
galaxy.jobs.handler INFO 2025-03-08 12:46:31,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (9) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:31,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 9
galaxy.jobs DEBUG 2025-03-08 12:46:31,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [9] prepared (49.153 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:46:31,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:46:31,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:46:31,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:46:31,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/9/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/9/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/a/1/5/dataset_a1509e80-f3c1-4bbf-a373-fa24a3219032.dat' input_1.fastq &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33    --output_dir ./      input_1.fastq  --dont_gzip   && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi   && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:46:31,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (9) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/9/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/9/galaxy_9.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/9/working/input_1_trimmed.fq" -a -f "/galaxy/server/database/objects/2/e/7/dataset_2e720536-55fe-4dc2-aa46-e1ad4224162a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/9/working/input_1_trimmed.fq" "/galaxy/server/database/objects/2/e/7/dataset_2e720536-55fe-4dc2-aa46-e1ad4224162a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:31,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:46:31,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:46:31,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:46:31,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:31,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:32,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:36,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r87b5 with k8s id: gxy-r87b5 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:46:36,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 9: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:46:43,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 9 finished
galaxy.model.metadata DEBUG 2025-03-08 12:46:43,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 10
galaxy.util WARNING 2025-03-08 12:46:43,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/e/7/dataset_2e720536-55fe-4dc2-aa46-e1ad4224162a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/e/7/dataset_2e720536-55fe-4dc2-aa46-e1ad4224162a.dat'
galaxy.jobs INFO 2025-03-08 12:46:43,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 9 in /galaxy/server/database/jobs_directory/000/9
galaxy.jobs DEBUG 2025-03-08 12:46:44,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 9 executed (111.793 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:44,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 9 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:46:45,587 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 10
tpv.core.entities DEBUG 2025-03-08 12:46:45,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:46:45,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:46:45,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:46:45,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:46:45,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Working directory for job is: /galaxy/server/database/jobs_directory/000/10
galaxy.jobs.runners DEBUG 2025-03-08 12:46:45,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [10] queued (31.742 ms)
galaxy.jobs.handler INFO 2025-03-08 12:46:45,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (10) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:45,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 10
galaxy.jobs DEBUG 2025-03-08 12:46:45,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [10] prepared (59.082 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:46:45,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/10/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/10/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/10/configs/tmphn0bunu5']
galaxy.jobs.runners DEBUG 2025-03-08 12:46:45,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (10) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/10/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/10/galaxy_10.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:45,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:45,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:46,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:46:54,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-65gmb with k8s id: gxy-65gmb succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:46:54,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 10: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:47:02,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 10 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:47:02,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'sanger_full_range_original_sanger.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/10/working/gxupload_0', 'object_id': 11}]}]}]
galaxy.jobs INFO 2025-03-08 12:47:02,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 10 in /galaxy/server/database/jobs_directory/000/10
galaxy.jobs DEBUG 2025-03-08 12:47:02,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 10 executed (119.110 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:02,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 10 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:47:02,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 11
tpv.core.entities DEBUG 2025-03-08 12:47:02,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:47:02,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:47:02,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:47:02,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:47:02,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Working directory for job is: /galaxy/server/database/jobs_directory/000/11
galaxy.jobs.runners DEBUG 2025-03-08 12:47:03,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [11] queued (37.106 ms)
galaxy.jobs.handler INFO 2025-03-08 12:47:03,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (11) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:03,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 11
galaxy.jobs DEBUG 2025-03-08 12:47:03,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [11] prepared (41.147 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:03,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:47:03,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:03,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:47:03,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/11/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/11/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/b/c/d/dataset_bcdf65c5-c952-4862-beff-8ba7d971bbd5.dat' input_1.fastq.gz &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33    --output_dir ./      input_1.fastq.gz    && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi   && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:47:03,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (11) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/11/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/11/galaxy_11.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/11/working/input_1_trimmed.fq" -a -f "/galaxy/server/database/objects/0/c/9/dataset_0c9808e6-3919-422b-b5e8-a304c03ef536.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/11/working/input_1_trimmed.fq" "/galaxy/server/database/objects/0/c/9/dataset_0c9808e6-3919-422b-b5e8-a304c03ef536.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:03,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:03,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:47:03,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:03,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:03,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:03,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:07,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lptfz with k8s id: gxy-lptfz succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:47:08,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 11: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:47:15,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 11 finished
galaxy.model.metadata DEBUG 2025-03-08 12:47:15,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 12
galaxy.util WARNING 2025-03-08 12:47:15,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/c/9/dataset_0c9808e6-3919-422b-b5e8-a304c03ef536.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/c/9/dataset_0c9808e6-3919-422b-b5e8-a304c03ef536.dat'
galaxy.jobs INFO 2025-03-08 12:47:15,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 11 in /galaxy/server/database/jobs_directory/000/11
galaxy.jobs DEBUG 2025-03-08 12:47:15,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 11 executed (104.573 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:15,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 11 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:47:17,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 12, 13
tpv.core.entities DEBUG 2025-03-08 12:47:17,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:47:17,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:47:17,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:47:17,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:47:17,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Working directory for job is: /galaxy/server/database/jobs_directory/000/12
galaxy.jobs.runners DEBUG 2025-03-08 12:47:17,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [12] queued (32.380 ms)
galaxy.jobs.handler INFO 2025-03-08 12:47:17,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (12) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:17,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 12
tpv.core.entities DEBUG 2025-03-08 12:47:17,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:47:17,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:47:17,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:47:17,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:47:17,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Working directory for job is: /galaxy/server/database/jobs_directory/000/13
galaxy.jobs.runners DEBUG 2025-03-08 12:47:17,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [13] queued (42.082 ms)
galaxy.jobs.handler INFO 2025-03-08 12:47:17,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (13) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:17,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 13
galaxy.jobs DEBUG 2025-03-08 12:47:17,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [12] prepared (81.612 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:47:17,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/12/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/12/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/12/configs/tmphaay0j5s']
galaxy.jobs.runners DEBUG 2025-03-08 12:47:17,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (12) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/12/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/12/galaxy_12.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:17,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 12:47:17,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [13] prepared (71.671 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:17,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 12:47:17,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/13/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/13/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/13/configs/tmpzxb95ci2']
galaxy.jobs.runners DEBUG 2025-03-08 12:47:17,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (13) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/13/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/13/galaxy_13.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:17,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:17,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:18,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:18,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:27,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vznkz with k8s id: gxy-vznkz succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:47:27,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 12: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:28,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2p28h with k8s id: gxy-2p28h succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:47:28,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 13: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:47:35,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 12 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:47:35,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/12/working/data_fetch_upload_yyhwkm98', 'object_id': 13}]}]}]
galaxy.jobs INFO 2025-03-08 12:47:35,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 12 in /galaxy/server/database/jobs_directory/000/12
galaxy.jobs DEBUG 2025-03-08 12:47:35,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 12 executed (117.529 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:35,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 12 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 12:47:35,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 13 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:47:35,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/13/working/data_fetch_upload_mwp2clay', 'object_id': 14}]}]}]
galaxy.jobs INFO 2025-03-08 12:47:36,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 13 in /galaxy/server/database/jobs_directory/000/13
galaxy.jobs DEBUG 2025-03-08 12:47:36,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 13 executed (117.651 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:36,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 13 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:47:36,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 14
tpv.core.entities DEBUG 2025-03-08 12:47:36,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:47:36,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:47:36,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:47:36,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:47:36,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Working directory for job is: /galaxy/server/database/jobs_directory/000/14
galaxy.jobs.runners DEBUG 2025-03-08 12:47:36,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [14] queued (38.212 ms)
galaxy.jobs.handler INFO 2025-03-08 12:47:36,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (14) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:36,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 14
galaxy.jobs DEBUG 2025-03-08 12:47:36,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [14] prepared (58.373 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:36,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:47:36,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:36,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:47:36,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/14/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/14/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/5/7/b/dataset_57bdafe2-402d-4be0-b9f6-e66421c356ed.dat' input_1.fastq &&  ln -s '/galaxy/server/database/objects/a/4/9/dataset_a49ea712-75d9-4c84-a88a-68681be80d1b.dat' input_2.fastq &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33   --quality 20  --stringency 1  -e 0.1  --length 20       --output_dir ./      --paired      input_1.fastq input_2.fastq  --dont_gzip   && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi   && cat ./*_trimming_report.txt > '/galaxy/server/database/objects/2/0/1/dataset_201a5e23-368d-46d8-b31b-eff515a9b817.dat' && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:47:36,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (14) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/14/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/14/galaxy_14.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/14/working/input_2_val_2.fq" -a -f "/galaxy/server/database/objects/0/3/7/dataset_03787782-b049-4c3e-92b7-d6cac27ef230.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/14/working/input_2_val_2.fq" "/galaxy/server/database/objects/0/3/7/dataset_03787782-b049-4c3e-92b7-d6cac27ef230.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/14/working/input_1_val_1.fq" -a -f "/galaxy/server/database/objects/c/2/7/dataset_c27da1ae-c6e6-4ad4-b545-eabcd62826df.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/14/working/input_1_val_1.fq" "/galaxy/server/database/objects/c/2/7/dataset_c27da1ae-c6e6-4ad4-b545-eabcd62826df.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:36,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:36,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:47:36,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:47:36,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:36,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:37,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:41,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wqdb5 with k8s id: gxy-wqdb5 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:47:41,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 14: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:47:49,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 14 finished
galaxy.model.metadata DEBUG 2025-03-08 12:47:49,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 17
galaxy.model.metadata DEBUG 2025-03-08 12:47:49,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 16
galaxy.model.metadata DEBUG 2025-03-08 12:47:49,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 15
galaxy.util WARNING 2025-03-08 12:47:49,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/3/7/dataset_03787782-b049-4c3e-92b7-d6cac27ef230.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/3/7/dataset_03787782-b049-4c3e-92b7-d6cac27ef230.dat'
galaxy.util WARNING 2025-03-08 12:47:49,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/2/7/dataset_c27da1ae-c6e6-4ad4-b545-eabcd62826df.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/2/7/dataset_c27da1ae-c6e6-4ad4-b545-eabcd62826df.dat'
galaxy.jobs INFO 2025-03-08 12:47:49,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 14 in /galaxy/server/database/jobs_directory/000/14
galaxy.jobs DEBUG 2025-03-08 12:47:49,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 14 executed (155.790 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:49,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 14 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:47:51,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 15
tpv.core.entities DEBUG 2025-03-08 12:47:51,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:47:51,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:47:51,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:47:51,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:47:51,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Working directory for job is: /galaxy/server/database/jobs_directory/000/15
galaxy.jobs.runners DEBUG 2025-03-08 12:47:51,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [15] queued (28.960 ms)
galaxy.jobs.handler INFO 2025-03-08 12:47:51,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (15) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:51,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 15
galaxy.jobs DEBUG 2025-03-08 12:47:51,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [15] prepared (72.541 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:47:51,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/15/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/15/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/15/configs/tmpeh9k15ma']
galaxy.jobs.runners DEBUG 2025-03-08 12:47:51,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (15) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/15/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/15/galaxy_15.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:51,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:51,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:51,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-08 12:47:52,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 16
tpv.core.entities DEBUG 2025-03-08 12:47:52,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:47:52,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:47:52,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:47:52,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:47:52,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Working directory for job is: /galaxy/server/database/jobs_directory/000/16
galaxy.jobs.runners DEBUG 2025-03-08 12:47:52,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [16] queued (44.076 ms)
galaxy.jobs.handler INFO 2025-03-08 12:47:52,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (16) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:52,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 16
galaxy.jobs DEBUG 2025-03-08 12:47:52,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [16] prepared (69.448 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:47:52,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/16/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/16/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/16/configs/tmpow_50ei2']
galaxy.jobs.runners DEBUG 2025-03-08 12:47:52,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (16) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/16/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/16/galaxy_16.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:52,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:52,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:47:52,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:01,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4dhrr with k8s id: gxy-4dhrr succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:48:01,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 15: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:02,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lbvj4 with k8s id: gxy-lbvj4 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:48:02,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 16: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:48:09,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 15 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:48:09,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/15/working/gxupload_0', 'object_id': 18}]}]}]
galaxy.jobs INFO 2025-03-08 12:48:09,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 15 in /galaxy/server/database/jobs_directory/000/15
galaxy.jobs DEBUG 2025-03-08 12:48:09,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 15 executed (118.032 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:09,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 15 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 12:48:10,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 16 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:48:10,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/16/working/gxupload_0', 'object_id': 19}]}]}]
galaxy.jobs INFO 2025-03-08 12:48:10,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 16 in /galaxy/server/database/jobs_directory/000/16
galaxy.jobs DEBUG 2025-03-08 12:48:10,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 16 executed (120.190 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:10,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 16 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:48:11,532 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 17
tpv.core.entities DEBUG 2025-03-08 12:48:11,561 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:48:11,561 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:48:11,565 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:48:11,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:48:11,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Working directory for job is: /galaxy/server/database/jobs_directory/000/17
galaxy.jobs.runners DEBUG 2025-03-08 12:48:11,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [17] queued (43.571 ms)
galaxy.jobs.handler INFO 2025-03-08 12:48:11,611 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (17) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:11,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 17
galaxy.jobs DEBUG 2025-03-08 12:48:11,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [17] prepared (52.798 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:11,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:48:11,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:11,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:48:11,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/17/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/17/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/4/c/e/dataset_4ceb3d15-77d5-4ba8-87b1-d271829a73c8.dat' input_1.fastq.gz &&  ln -s '/galaxy/server/database/objects/1/4/e/dataset_14ebd13e-ec71-4094-9ca0-7f153a542e13.dat' input_2.fastq.gz &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33   --quality 20  --stringency 1  -e 0.1  --length 20       --output_dir ./      --paired      input_1.fastq.gz input_2.fastq.gz    && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi   && cat ./*_trimming_report.txt > '/galaxy/server/database/objects/c/3/7/dataset_c3700214-2e52-4ced-8b4e-1e4e2a14684b.dat' && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:48:11,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (17) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/17/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/17/galaxy_17.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/17/working/input_2_val_2.fq" -a -f "/galaxy/server/database/objects/1/e/2/dataset_1e20ba7c-6695-4ca4-b76d-d13e29feae0a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/17/working/input_2_val_2.fq" "/galaxy/server/database/objects/1/e/2/dataset_1e20ba7c-6695-4ca4-b76d-d13e29feae0a.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/17/working/input_1_val_1.fq" -a -f "/galaxy/server/database/objects/9/a/c/dataset_9ac5376e-57f3-4c91-bea6-9cf6aea457d0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/17/working/input_1_val_1.fq" "/galaxy/server/database/objects/9/a/c/dataset_9ac5376e-57f3-4c91-bea6-9cf6aea457d0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:11,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:11,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:48:11,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:11,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:11,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:12,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:17,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-shhhd with k8s id: gxy-shhhd succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:48:17,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 17: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:48:24,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 17 finished
galaxy.model.metadata DEBUG 2025-03-08 12:48:24,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 20
galaxy.model.metadata DEBUG 2025-03-08 12:48:24,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 21
galaxy.model.metadata DEBUG 2025-03-08 12:48:24,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 22
galaxy.util WARNING 2025-03-08 12:48:24,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/a/c/dataset_9ac5376e-57f3-4c91-bea6-9cf6aea457d0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/a/c/dataset_9ac5376e-57f3-4c91-bea6-9cf6aea457d0.dat'
galaxy.util WARNING 2025-03-08 12:48:24,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/e/2/dataset_1e20ba7c-6695-4ca4-b76d-d13e29feae0a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/e/2/dataset_1e20ba7c-6695-4ca4-b76d-d13e29feae0a.dat'
galaxy.jobs INFO 2025-03-08 12:48:24,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 17 in /galaxy/server/database/jobs_directory/000/17
galaxy.jobs DEBUG 2025-03-08 12:48:24,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 17 executed (145.096 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:24,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 17 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:48:26,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 19, 18
tpv.core.entities DEBUG 2025-03-08 12:48:26,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:48:26,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:48:26,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:48:26,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:48:26,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Working directory for job is: /galaxy/server/database/jobs_directory/000/19
galaxy.jobs.runners DEBUG 2025-03-08 12:48:26,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [19] queued (36.969 ms)
galaxy.jobs.handler INFO 2025-03-08 12:48:26,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (19) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:26,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 19
tpv.core.entities DEBUG 2025-03-08 12:48:26,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:48:26,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:48:26,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:48:26,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:48:26,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Working directory for job is: /galaxy/server/database/jobs_directory/000/18
galaxy.jobs.runners DEBUG 2025-03-08 12:48:26,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [18] queued (51.620 ms)
galaxy.jobs.handler INFO 2025-03-08 12:48:26,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (18) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:26,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 18
galaxy.jobs DEBUG 2025-03-08 12:48:27,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [19] prepared (85.597 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:48:27,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/19/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/19/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/19/configs/tmp4uuhfgcn']
galaxy.jobs.runners DEBUG 2025-03-08 12:48:27,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (19) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/19/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/19/galaxy_19.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:27,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 12:48:27,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [18] prepared (74.065 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:27,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 12:48:27,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/18/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/18/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/18/configs/tmpjpkxi7ub']
galaxy.jobs.runners DEBUG 2025-03-08 12:48:27,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (18) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/18/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/18/galaxy_18.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:27,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:27,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:28,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:28,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:37,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-47g2z with k8s id: gxy-47g2z succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:37,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-697xk with k8s id: gxy-697xk succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:48:37,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 19: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:48:37,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 18: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:48:45,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 19 finished
galaxy.jobs.runners DEBUG 2025-03-08 12:48:45,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 18 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:48:45,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/19/working/data_fetch_upload__cig6p41', 'object_id': 24}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:48:45,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/18/working/data_fetch_upload_cjabya84', 'object_id': 23}]}]}]
galaxy.jobs INFO 2025-03-08 12:48:45,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 19 in /galaxy/server/database/jobs_directory/000/19
galaxy.jobs INFO 2025-03-08 12:48:45,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 18 in /galaxy/server/database/jobs_directory/000/18
galaxy.jobs DEBUG 2025-03-08 12:48:45,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 19 executed (168.002 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:45,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 19 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 12:48:45,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 18 executed (159.564 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:45,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 18 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:48:46,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 20
tpv.core.entities DEBUG 2025-03-08 12:48:46,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:48:46,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:48:46,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:48:46,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:48:46,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Working directory for job is: /galaxy/server/database/jobs_directory/000/20
galaxy.jobs.runners DEBUG 2025-03-08 12:48:46,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [20] queued (45.162 ms)
galaxy.jobs.handler INFO 2025-03-08 12:48:46,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (20) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:46,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 20
galaxy.tools.evaluation INFO 2025-03-08 12:48:46,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Updating param_dict for forward with /galaxy/server/database/objects/9/4/b/dataset_94b19b72-8007-4718-8371-92146bce6676.dat
galaxy.tools.evaluation INFO 2025-03-08 12:48:46,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Updating param_dict for reverse with /galaxy/server/database/objects/d/7/9/dataset_d79a53f6-28cb-4a46-8e70-81514e9d3862.dat
galaxy.tools.evaluation INFO 2025-03-08 12:48:46,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Updating param_dict for forward with /galaxy/server/database/objects/c/6/7/dataset_c6718ea3-5549-4e4a-8ed7-5e54c08cd9f0.dat
galaxy.tools.evaluation INFO 2025-03-08 12:48:46,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Updating param_dict for reverse with /galaxy/server/database/objects/a/1/0/dataset_a1099022-efe2-4a2e-bb07-b91a55b1b699.dat
galaxy.jobs DEBUG 2025-03-08 12:48:46,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [20] prepared (97.564 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:46,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:48:46,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:46,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:48:46,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/20/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/20/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/8/e/9/dataset_8e9f707a-5174-406d-b2a3-cb5edcdb382f.dat' input_1.fastq &&  ln -s '/galaxy/server/database/objects/4/e/f/dataset_4ef5c8f0-b677-4c32-8d54-33fe288da6b7.dat' input_2.fastq &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33   --quality 20  --stringency 1  -e 0.1  --length 20    --retain_unpaired --length_1 35 --length_2 35    --output_dir ./      --paired      input_1.fastq input_2.fastq  --dont_gzip   && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi   && cat ./*_trimming_report.txt > '/galaxy/server/database/objects/0/b/4/dataset_0b47ac2a-b70b-4390-95d7-4e3aa9acd4f3.dat' && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:48:46,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (20) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/20/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/20/galaxy_20.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/20/working/input_1_val_1.fq" -a -f "/galaxy/server/database/objects/c/6/7/dataset_c6718ea3-5549-4e4a-8ed7-5e54c08cd9f0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/20/working/input_1_val_1.fq" "/galaxy/server/database/objects/c/6/7/dataset_c6718ea3-5549-4e4a-8ed7-5e54c08cd9f0.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/20/working/input_2_val_2.fq" -a -f "/galaxy/server/database/objects/a/1/0/dataset_a1099022-efe2-4a2e-bb07-b91a55b1b699.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/20/working/input_2_val_2.fq" "/galaxy/server/database/objects/a/1/0/dataset_a1099022-efe2-4a2e-bb07-b91a55b1b699.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/20/working/input_1_unpaired_1.fq" -a -f "/galaxy/server/database/objects/9/4/b/dataset_94b19b72-8007-4718-8371-92146bce6676.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/20/working/input_1_unpaired_1.fq" "/galaxy/server/database/objects/9/4/b/dataset_94b19b72-8007-4718-8371-92146bce6676.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/20/working/input_2_unpaired_2.fq" -a -f "/galaxy/server/database/objects/d/7/9/dataset_d79a53f6-28cb-4a46-8e70-81514e9d3862.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/20/working/input_2_unpaired_2.fq" "/galaxy/server/database/objects/d/7/9/dataset_d79a53f6-28cb-4a46-8e70-81514e9d3862.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:46,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:46,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:48:46,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:48:46,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:46,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:47,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:48:52,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rbb2s with k8s id: gxy-rbb2s succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:48:52,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 20: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:49:00,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 20 finished
galaxy.model.metadata DEBUG 2025-03-08 12:49:00,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 25
galaxy.model.metadata DEBUG 2025-03-08 12:49:00,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 26
galaxy.model.metadata DEBUG 2025-03-08 12:49:00,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 27
galaxy.model.metadata DEBUG 2025-03-08 12:49:00,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 28
galaxy.model.metadata DEBUG 2025-03-08 12:49:00,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 29
galaxy.util WARNING 2025-03-08 12:49:00,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/6/7/dataset_c6718ea3-5549-4e4a-8ed7-5e54c08cd9f0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/6/7/dataset_c6718ea3-5549-4e4a-8ed7-5e54c08cd9f0.dat'
galaxy.util WARNING 2025-03-08 12:49:00,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/1/0/dataset_a1099022-efe2-4a2e-bb07-b91a55b1b699.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/1/0/dataset_a1099022-efe2-4a2e-bb07-b91a55b1b699.dat'
galaxy.util WARNING 2025-03-08 12:49:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/4/b/dataset_94b19b72-8007-4718-8371-92146bce6676.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/4/b/dataset_94b19b72-8007-4718-8371-92146bce6676.dat'
galaxy.util WARNING 2025-03-08 12:49:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/7/9/dataset_d79a53f6-28cb-4a46-8e70-81514e9d3862.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/7/9/dataset_d79a53f6-28cb-4a46-8e70-81514e9d3862.dat'
galaxy.jobs INFO 2025-03-08 12:49:00,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 20 in /galaxy/server/database/jobs_directory/000/20
galaxy.jobs DEBUG 2025-03-08 12:49:00,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 20 executed (250.866 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:00,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 20 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:49:02,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 22, 21
tpv.core.entities DEBUG 2025-03-08 12:49:02,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:49:02,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:49:02,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:49:02,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:49:02,791 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Working directory for job is: /galaxy/server/database/jobs_directory/000/22
galaxy.jobs.runners DEBUG 2025-03-08 12:49:02,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [22] queued (35.182 ms)
galaxy.jobs.handler INFO 2025-03-08 12:49:02,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (22) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:02,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 22
tpv.core.entities DEBUG 2025-03-08 12:49:02,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:49:02,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:49:02,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:49:02,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:49:02,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Working directory for job is: /galaxy/server/database/jobs_directory/000/21
galaxy.jobs.runners DEBUG 2025-03-08 12:49:02,862 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [21] queued (44.286 ms)
galaxy.jobs.handler INFO 2025-03-08 12:49:02,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (21) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:02,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 21
galaxy.jobs DEBUG 2025-03-08 12:49:02,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [22] prepared (78.575 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:49:02,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/22/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/22/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/22/configs/tmp9dhyqlau']
galaxy.jobs.runners DEBUG 2025-03-08 12:49:02,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (22) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/22/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/22/galaxy_22.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:02,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 12:49:02,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [21] prepared (77.760 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:02,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 12:49:02,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/21/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/21/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/21/configs/tmpfs_5f7jr']
galaxy.jobs.runners DEBUG 2025-03-08 12:49:02,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (21) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/21/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/21/galaxy_21.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:03,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:03,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:03,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:03,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:13,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kbjmn with k8s id: gxy-kbjmn succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:13,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8qdj6 with k8s id: gxy-8qdj6 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:49:13,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 22: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:49:13,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 21: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:49:21,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 21 finished
galaxy.jobs.runners DEBUG 2025-03-08 12:49:21,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 22 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:49:21,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/21/working/gxupload_0', 'object_id': 30}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:49:21,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/22/working/gxupload_0', 'object_id': 31}]}]}]
galaxy.jobs INFO 2025-03-08 12:49:21,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 21 in /galaxy/server/database/jobs_directory/000/21
galaxy.jobs INFO 2025-03-08 12:49:21,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 22 in /galaxy/server/database/jobs_directory/000/22
galaxy.jobs DEBUG 2025-03-08 12:49:21,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 21 executed (138.169 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:21,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 21 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 12:49:21,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 22 executed (137.973 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:21,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 22 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:49:23,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 23
tpv.core.entities DEBUG 2025-03-08 12:49:23,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:49:23,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:49:23,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:49:23,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:49:23,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Working directory for job is: /galaxy/server/database/jobs_directory/000/23
galaxy.jobs.runners DEBUG 2025-03-08 12:49:23,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [23] queued (58.095 ms)
galaxy.jobs.handler INFO 2025-03-08 12:49:23,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (23) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:23,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 23
galaxy.tools.evaluation INFO 2025-03-08 12:49:23,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Updating param_dict for forward with /galaxy/server/database/objects/f/0/c/dataset_f0c7f0f6-663e-47e7-8c32-db98aa8efee5.dat
galaxy.tools.evaluation INFO 2025-03-08 12:49:23,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Updating param_dict for reverse with /galaxy/server/database/objects/3/0/d/dataset_30d8a4c8-fcf2-4fe3-b85b-cb0ff79ecec6.dat
galaxy.tools.evaluation INFO 2025-03-08 12:49:23,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Updating param_dict for forward with /galaxy/server/database/objects/4/1/0/dataset_4107f5eb-2c4a-494d-a9ed-78ca6fde083c.dat
galaxy.tools.evaluation INFO 2025-03-08 12:49:23,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Updating param_dict for reverse with /galaxy/server/database/objects/e/3/4/dataset_e34a2754-676b-4ed1-aae9-809203ae92a2.dat
galaxy.jobs DEBUG 2025-03-08 12:49:23,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [23] prepared (81.148 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:49:23,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:49:23,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:49:23,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:49:23,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/23/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/23/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/c/4/6/dataset_c4603225-3a72-4da2-b32a-e51900253e0a.dat' input_1.fastq.gz &&  ln -s '/galaxy/server/database/objects/3/2/6/dataset_32659529-e449-4c8c-b68d-78437ad9e87a.dat' input_2.fastq.gz &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33   --quality 20  --stringency 1  -e 0.1  --length 20    --retain_unpaired --length_1 35 --length_2 35    --output_dir ./      --paired      input_1.fastq.gz input_2.fastq.gz    && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi   && cat ./*_trimming_report.txt > '/galaxy/server/database/objects/f/1/5/dataset_f151e9fb-f3de-486b-acb8-13d01c7bff21.dat' && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:49:23,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (23) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/23/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/23/galaxy_23.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/23/working/input_1_unpaired_1.fq" -a -f "/galaxy/server/database/objects/f/0/c/dataset_f0c7f0f6-663e-47e7-8c32-db98aa8efee5.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/23/working/input_1_unpaired_1.fq" "/galaxy/server/database/objects/f/0/c/dataset_f0c7f0f6-663e-47e7-8c32-db98aa8efee5.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/23/working/input_2_unpaired_2.fq" -a -f "/galaxy/server/database/objects/3/0/d/dataset_30d8a4c8-fcf2-4fe3-b85b-cb0ff79ecec6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/23/working/input_2_unpaired_2.fq" "/galaxy/server/database/objects/3/0/d/dataset_30d8a4c8-fcf2-4fe3-b85b-cb0ff79ecec6.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/23/working/input_1_val_1.fq" -a -f "/galaxy/server/database/objects/4/1/0/dataset_4107f5eb-2c4a-494d-a9ed-78ca6fde083c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/23/working/input_1_val_1.fq" "/galaxy/server/database/objects/4/1/0/dataset_4107f5eb-2c4a-494d-a9ed-78ca6fde083c.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/23/working/input_2_val_2.fq" -a -f "/galaxy/server/database/objects/e/3/4/dataset_e34a2754-676b-4ed1-aae9-809203ae92a2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/23/working/input_2_val_2.fq" "/galaxy/server/database/objects/e/3/4/dataset_e34a2754-676b-4ed1-aae9-809203ae92a2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:23,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:49:23,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:49:23,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:49:23,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:23,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:24,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:28,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-22nkq with k8s id: gxy-22nkq succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:49:28,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 23: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:49:36,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 23 finished
galaxy.model.metadata DEBUG 2025-03-08 12:49:36,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 34
galaxy.model.metadata DEBUG 2025-03-08 12:49:36,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 35
galaxy.model.metadata DEBUG 2025-03-08 12:49:36,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 36
galaxy.model.metadata DEBUG 2025-03-08 12:49:36,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 32
galaxy.model.metadata DEBUG 2025-03-08 12:49:36,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 33
galaxy.util WARNING 2025-03-08 12:49:36,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/0/c/dataset_f0c7f0f6-663e-47e7-8c32-db98aa8efee5.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/0/c/dataset_f0c7f0f6-663e-47e7-8c32-db98aa8efee5.dat'
galaxy.util WARNING 2025-03-08 12:49:36,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/0/d/dataset_30d8a4c8-fcf2-4fe3-b85b-cb0ff79ecec6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/0/d/dataset_30d8a4c8-fcf2-4fe3-b85b-cb0ff79ecec6.dat'
galaxy.util WARNING 2025-03-08 12:49:36,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/1/0/dataset_4107f5eb-2c4a-494d-a9ed-78ca6fde083c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/1/0/dataset_4107f5eb-2c4a-494d-a9ed-78ca6fde083c.dat'
galaxy.util WARNING 2025-03-08 12:49:36,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/3/4/dataset_e34a2754-676b-4ed1-aae9-809203ae92a2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/3/4/dataset_e34a2754-676b-4ed1-aae9-809203ae92a2.dat'
galaxy.jobs INFO 2025-03-08 12:49:36,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 23 in /galaxy/server/database/jobs_directory/000/23
galaxy.jobs DEBUG 2025-03-08 12:49:36,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 23 executed (189.895 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:36,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 23 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:49:38,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 24, 25
tpv.core.entities DEBUG 2025-03-08 12:49:38,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:49:38,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:49:38,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:49:38,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:49:38,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Working directory for job is: /galaxy/server/database/jobs_directory/000/24
galaxy.jobs.runners DEBUG 2025-03-08 12:49:38,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [24] queued (36.622 ms)
galaxy.jobs.handler INFO 2025-03-08 12:49:38,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (24) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:38,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 24
tpv.core.entities DEBUG 2025-03-08 12:49:38,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:49:38,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:49:38,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:49:38,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:49:38,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Working directory for job is: /galaxy/server/database/jobs_directory/000/25
galaxy.jobs.runners DEBUG 2025-03-08 12:49:38,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [25] queued (51.655 ms)
galaxy.jobs.handler INFO 2025-03-08 12:49:38,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (25) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:38,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 25
galaxy.jobs DEBUG 2025-03-08 12:49:38,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [24] prepared (88.297 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:49:38,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/24/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/24/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/24/configs/tmppv3vtlzv']
galaxy.jobs.runners DEBUG 2025-03-08 12:49:38,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (24) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/24/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/24/galaxy_24.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-08 12:49:38,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [25] prepared (69.314 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:38,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:38,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 12:49:38,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/25/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/25/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/25/configs/tmp14s9d7p4']
galaxy.jobs.runners DEBUG 2025-03-08 12:49:38,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (25) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/25/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/25/galaxy_25.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:38,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:38,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:39,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:39,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2dldj failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-2dldj.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 12:49:48,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-2dldj

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-2dldj": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-x2sbd with k8s id: gxy-x2sbd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (24/gxy-2dldj) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (24/gxy-2dldj) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (24/gxy-2dldj) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (24/gxy-2dldj) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-2dldj.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 24 (gxy-2dldj)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-2dldj to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 24 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:48,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (24/gxy-2dldj) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-03-08 12:49:48,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 25: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-08 12:49:48,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 26
tpv.core.entities DEBUG 2025-03-08 12:49:49,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:49:49,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:49:49,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:49:49,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:49:49,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Working directory for job is: /galaxy/server/database/jobs_directory/000/26
galaxy.jobs.runners DEBUG 2025-03-08 12:49:49,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [26] queued (35.453 ms)
galaxy.jobs.handler INFO 2025-03-08 12:49:49,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (26) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:49,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 26
galaxy.jobs DEBUG 2025-03-08 12:49:49,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [26] prepared (68.755 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:49:49,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/26/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/26/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/26/configs/tmp_9779bce']
galaxy.jobs.runners DEBUG 2025-03-08 12:49:49,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (26) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/26/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/26/galaxy_26.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:49,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:49,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:49:50,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 27
tpv.core.entities DEBUG 2025-03-08 12:49:50,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:49:50,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:49:50,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:49:50,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:49:50,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Working directory for job is: /galaxy/server/database/jobs_directory/000/27
galaxy.jobs.runners DEBUG 2025-03-08 12:49:50,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [27] queued (33.740 ms)
galaxy.jobs.handler INFO 2025-03-08 12:49:50,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (27) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:50,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 27
galaxy.jobs DEBUG 2025-03-08 12:49:50,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [27] prepared (70.935 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:49:50,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/27/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/27/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/27/configs/tmp0mw12jr7']
galaxy.jobs.runners DEBUG 2025-03-08 12:49:50,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (27) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/27/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/27/galaxy_27.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:50,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:50,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:50,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:51,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-08 12:49:55,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 25 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:49:55,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/25/working/data_fetch_upload__osnsoc9', 'object_id': 38}]}]}]
galaxy.jobs INFO 2025-03-08 12:49:55,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 25 in /galaxy/server/database/jobs_directory/000/25
galaxy.jobs DEBUG 2025-03-08 12:49:55,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 25 executed (125.579 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:55,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 25 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:49:59,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pl66f with k8s id: gxy-pl66f succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:49:59,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 26: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:00,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s8dxl with k8s id: gxy-s8dxl succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:50:01,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 27: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:50:07,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 26 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:50:07,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/26/working/data_fetch_upload_d6aqxe9i', 'object_id': 39}]}]}]
galaxy.jobs INFO 2025-03-08 12:50:07,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 26 in /galaxy/server/database/jobs_directory/000/26
galaxy.jobs DEBUG 2025-03-08 12:50:07,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 26 executed (128.587 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:07,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 26 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 12:50:09,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 27 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:50:09,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/27/working/data_fetch_upload_0scrysbg', 'object_id': 40}]}]}]
galaxy.jobs INFO 2025-03-08 12:50:09,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 27 in /galaxy/server/database/jobs_directory/000/27
galaxy.jobs DEBUG 2025-03-08 12:50:09,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 27 executed (134.711 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:09,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 27 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:50:09,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 28
tpv.core.entities DEBUG 2025-03-08 12:50:09,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:50:09,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:50:09,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:50:09,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:50:09,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Working directory for job is: /galaxy/server/database/jobs_directory/000/28
galaxy.jobs.runners DEBUG 2025-03-08 12:50:09,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [28] queued (39.069 ms)
galaxy.jobs.handler INFO 2025-03-08 12:50:09,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (28) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:09,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 28
galaxy.jobs DEBUG 2025-03-08 12:50:09,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [28] prepared (63.498 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:09,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:50:09,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:09,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:50:09,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/28/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/28/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/1/f/9/dataset_1f9e002f-9425-4a7c-a97a-a212a907b535.dat' input_1.fastq &&  ln -s '/galaxy/server/database/objects/3/e/e/dataset_3eee72d8-0300-44c5-b302-fc9f2988efab.dat' input_2.fastq &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33    --output_dir ./      --paired      input_1.fastq input_2.fastq  --dont_gzip  --hardtrim5 20    && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi  && if [ -f input_1.20bp_5prime.fq.gz ] ; then mv input_1.20bp_5prime.fq.gz input_1_hardtrim.fq ; fi && if [ -f input_2.20bp_5prime.fq.gz ] ; then mv input_2.20bp_5prime.fq.gz input_2_hardtrim.fq ; fi && if [ -f input_1.bp_3prime.fq.gz ] ; then mv input_1.bp_3prime.fq.gz input_1_hardtrim.fq ; fi && if [ -f input_2.bp_3prime.fq.gz ] ; then mv input_2.bp_3prime.fq.gz input_2_hardtrim.fq ; fi && if [ -f input_1.20bp_5prime.fq ] ; then mv input_1.20bp_5prime.fq input_1_hardtrim.fq ; fi && if [ -f input_2.20bp_5prime.fq ] ; then mv input_2.20bp_5prime.fq input_2_hardtrim.fq ; fi && if [ -f input_1.bp_3prime.fq ] ; then mv input_1.bp_3prime.fq input_1_hardtrim.fq ; fi && if [ -f input_2.bp_3prime.fq ] ; then mv input_2.bp_3prime.fq input_2_hardtrim.fq ; fi  && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:50:09,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (28) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/28/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/28/galaxy_28.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/28/working/input_2_hardtrim.fq" -a -f "/galaxy/server/database/objects/e/5/1/dataset_e51edaf9-f19a-4bbf-a2ad-c50c879e1b58.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/28/working/input_2_hardtrim.fq" "/galaxy/server/database/objects/e/5/1/dataset_e51edaf9-f19a-4bbf-a2ad-c50c879e1b58.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/28/working/input_1_hardtrim.fq" -a -f "/galaxy/server/database/objects/0/a/0/dataset_0a092b6d-9a5c-4c89-abae-3c205fec92f3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/28/working/input_1_hardtrim.fq" "/galaxy/server/database/objects/0/a/0/dataset_0a092b6d-9a5c-4c89-abae-3c205fec92f3.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:09,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:09,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:50:09,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:09,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:09,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:11,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:16,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8p7w5 with k8s id: gxy-8p7w5 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:50:16,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 28: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:50:23,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 28 finished
galaxy.model.metadata DEBUG 2025-03-08 12:50:23,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 42
galaxy.model.metadata DEBUG 2025-03-08 12:50:23,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 41
galaxy.util WARNING 2025-03-08 12:50:23,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/5/1/dataset_e51edaf9-f19a-4bbf-a2ad-c50c879e1b58.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/5/1/dataset_e51edaf9-f19a-4bbf-a2ad-c50c879e1b58.dat'
galaxy.util WARNING 2025-03-08 12:50:23,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/a/0/dataset_0a092b6d-9a5c-4c89-abae-3c205fec92f3.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/a/0/dataset_0a092b6d-9a5c-4c89-abae-3c205fec92f3.dat'
galaxy.jobs INFO 2025-03-08 12:50:23,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 28 in /galaxy/server/database/jobs_directory/000/28
galaxy.jobs DEBUG 2025-03-08 12:50:23,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 28 executed (115.986 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:23,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 28 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:50:25,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 29, 30
tpv.core.entities DEBUG 2025-03-08 12:50:25,855 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:50:25,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:50:25,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:50:25,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:50:25,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Working directory for job is: /galaxy/server/database/jobs_directory/000/29
galaxy.jobs.runners DEBUG 2025-03-08 12:50:25,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [29] queued (28.927 ms)
galaxy.jobs.handler INFO 2025-03-08 12:50:25,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (29) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:25,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 29
tpv.core.entities DEBUG 2025-03-08 12:50:25,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:50:25,903 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:50:25,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:50:25,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:50:25,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Working directory for job is: /galaxy/server/database/jobs_directory/000/30
galaxy.jobs.runners DEBUG 2025-03-08 12:50:25,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [30] queued (36.981 ms)
galaxy.jobs.handler INFO 2025-03-08 12:50:25,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (30) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:25,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 30
galaxy.jobs DEBUG 2025-03-08 12:50:25,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [29] prepared (68.410 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:50:25,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/29/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/29/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/29/configs/tmpob8xf6qw']
galaxy.jobs.runners DEBUG 2025-03-08 12:50:26,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (29) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/29/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/29/galaxy_29.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-08 12:50:26,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [30] prepared (66.436 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:26,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 12:50:26,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/30/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/30/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/30/configs/tmpqeha12k3']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:26,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 12:50:26,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (30) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/30/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/30/galaxy_30.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:26,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:26,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:27,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:27,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:35,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z9h2t with k8s id: gxy-z9h2t succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:50:35,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 29: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:36,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pmd76 with k8s id: gxy-pmd76 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:50:36,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 30: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:50:43,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 29 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:50:43,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/29/working/data_fetch_upload_hga7ifir', 'object_id': 43}]}]}]
galaxy.jobs INFO 2025-03-08 12:50:43,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 29 in /galaxy/server/database/jobs_directory/000/29
galaxy.jobs DEBUG 2025-03-08 12:50:43,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 29 executed (117.487 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:43,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 29 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 12:50:44,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 30 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:50:44,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq2.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/30/working/data_fetch_upload_qcay75xu', 'object_id': 44}]}]}]
galaxy.jobs INFO 2025-03-08 12:50:44,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 30 in /galaxy/server/database/jobs_directory/000/30
galaxy.jobs DEBUG 2025-03-08 12:50:44,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 30 executed (103.494 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:44,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 30 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:50:45,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 31
tpv.core.entities DEBUG 2025-03-08 12:50:45,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/.*, abstract=False, cores=5, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:50:45,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:50:45,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:50:45,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:50:45,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Working directory for job is: /galaxy/server/database/jobs_directory/000/31
galaxy.jobs.runners DEBUG 2025-03-08 12:50:45,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [31] queued (33.220 ms)
galaxy.jobs.handler INFO 2025-03-08 12:50:45,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (31) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:45,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 31
galaxy.jobs DEBUG 2025-03-08 12:50:45,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [31] prepared (49.249 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:45,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:50:45,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:45,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:50:45,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/31/tool_script.sh] for tool command [trim_galore --version > /galaxy/server/database/jobs_directory/000/31/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/4/4/2/dataset_44220fec-374e-42fb-9171-8d801a136384.dat' input_1.fastq &&  ln -s '/galaxy/server/database/objects/3/8/c/dataset_38cad744-42d8-42de-accc-545f3e9e1f4f.dat' input_2.fastq &&  trim_galore  --cores ${GALAXY_SLOTS:-4}  --phred33    --output_dir ./      --paired      input_1.fastq input_2.fastq  --dont_gzip  --clock   && if [ -f input_1_trimmed.fq.gz ] ; then mv input_1_trimmed.fq.gz input_1_trimmed.fq ; fi && if [ -f input_1_val_1.fq.gz ] ; then mv input_1_val_1.fq.gz input_1_val_1.fq ; fi && if [ -f input_2_val_2.fq.gz ] ; then mv input_2_val_2.fq.gz input_2_val_2.fq ; fi && if [ -f input_1_unpaired_1.fq.gz ] ; then mv input_1_unpaired_1.fq.gz input_1_unpaired_1.fq ; fi && if [ -f input_2_unpaired_2.fq.gz ] ; then mv input_2_unpaired_2.fq.gz input_2_unpaired_2.fq ; fi && if [ -f input_1.clock_UMI.R1.fq.gz ] ; then mv input_1.clock_UMI.R1.fq.gz input_1.clock_UMI.R1.fq ; fi && if [ -f input_2.clock_UMI.R2.fq.gz ] ; then mv input_2.clock_UMI.R2.fq.gz input_2.clock_UMI.R2.fq ; fi  && if [ -f input_1.bp_5prime.fq.gz ] ; then mv input_1.bp_5prime.fq.gz input_1_hardtrim.fq ; fi && if [ -f input_2.bp_5prime.fq.gz ] ; then mv input_2.bp_5prime.fq.gz input_2_hardtrim.fq ; fi && if [ -f input_1.bp_3prime.fq.gz ] ; then mv input_1.bp_3prime.fq.gz input_1_hardtrim.fq ; fi && if [ -f input_2.bp_3prime.fq.gz ] ; then mv input_2.bp_3prime.fq.gz input_2_hardtrim.fq ; fi && if [ -f input_1.bp_5prime.fq ] ; then mv input_1.bp_5prime.fq input_1_hardtrim.fq ; fi && if [ -f input_2.bp_5prime.fq ] ; then mv input_2.bp_5prime.fq input_2_hardtrim.fq ; fi && if [ -f input_1.bp_3prime.fq ] ; then mv input_1.bp_3prime.fq input_1_hardtrim.fq ; fi && if [ -f input_2.bp_3prime.fq ] ; then mv input_2.bp_3prime.fq input_2_hardtrim.fq ; fi  && ls -lah]
galaxy.jobs.runners DEBUG 2025-03-08 12:50:45,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (31) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/31/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/31/galaxy_31.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/31/working/input_2.clock_UMI.R2.fq" -a -f "/galaxy/server/database/objects/0/d/0/dataset_0d0332de-8448-4e85-88c5-814eb56ddde7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/31/working/input_2.clock_UMI.R2.fq" "/galaxy/server/database/objects/0/d/0/dataset_0d0332de-8448-4e85-88c5-814eb56ddde7.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/31/working/input_1.clock_UMI.R1.fq" -a -f "/galaxy/server/database/objects/3/0/f/dataset_30fb8384-12c4-446e-82eb-4af3a75ef2a0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/31/working/input_1.clock_UMI.R1.fq" "/galaxy/server/database/objects/3/0/f/dataset_30fb8384-12c4-446e-82eb-4af3a75ef2a0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:45,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:45,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:50:45,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/trim_galore/trim_galore/0.6.7+galaxy0: trim-galore:0.6.7
galaxy.tool_util.deps.containers INFO 2025-03-08 12:50:45,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/trim-galore:0.6.7--hdfd78af_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:45,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:46,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:50,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vtrdc with k8s id: gxy-vtrdc succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:50:50,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 31: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:50:58,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 31 finished
galaxy.model.metadata DEBUG 2025-03-08 12:50:58,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 46
galaxy.model.metadata DEBUG 2025-03-08 12:50:58,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 45
galaxy.util WARNING 2025-03-08 12:50:58,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/d/0/dataset_0d0332de-8448-4e85-88c5-814eb56ddde7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/d/0/dataset_0d0332de-8448-4e85-88c5-814eb56ddde7.dat'
galaxy.util WARNING 2025-03-08 12:50:58,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/0/f/dataset_30fb8384-12c4-446e-82eb-4af3a75ef2a0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/0/f/dataset_30fb8384-12c4-446e-82eb-4af3a75ef2a0.dat'
galaxy.jobs INFO 2025-03-08 12:50:58,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 31 in /galaxy/server/database/jobs_directory/000/31
galaxy.jobs DEBUG 2025-03-08 12:50:58,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 31 executed (127.543 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:58,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 31 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:50:59,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 32, 33
tpv.core.entities DEBUG 2025-03-08 12:50:59,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:50:59,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:50:59,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:50:59,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:50:59,740 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Working directory for job is: /galaxy/server/database/jobs_directory/000/32
galaxy.jobs.runners DEBUG 2025-03-08 12:50:59,747 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [32] queued (30.636 ms)
galaxy.jobs.handler INFO 2025-03-08 12:50:59,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (32) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:59,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 32
tpv.core.entities DEBUG 2025-03-08 12:50:59,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:50:59,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:50:59,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:50:59,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:50:59,802 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Working directory for job is: /galaxy/server/database/jobs_directory/000/33
galaxy.jobs.runners DEBUG 2025-03-08 12:50:59,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [33] queued (46.489 ms)
galaxy.jobs.handler INFO 2025-03-08 12:50:59,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (33) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:59,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 33
galaxy.jobs DEBUG 2025-03-08 12:50:59,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [32] prepared (75.301 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:50:59,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/32/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/32/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/32/configs/tmp_zhw1if_']
galaxy.jobs.runners DEBUG 2025-03-08 12:50:59,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (32) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/32/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/32/galaxy_32.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-08 12:50:59,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [33] prepared (64.899 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:59,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:59,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 12:50:59,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/33/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/33/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/33/configs/tmp1kmqkxga']
galaxy.jobs.runners DEBUG 2025-03-08 12:50:59,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (33) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/33/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/33/galaxy_33.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:59,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:50:59,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:00,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:00,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:09,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gwpk6 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:09,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:09,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gwpk6.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:09,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 12:51:10,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-gwpk6

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-gwpk6": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (33/gxy-gwpk6) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (33/gxy-gwpk6) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (33/gxy-gwpk6) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (33/gxy-gwpk6) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gwpk6.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 33 (gxy-gwpk6)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-gwpk6 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 33 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:10,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (33/gxy-gwpk6) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:11,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-g4xx2 with k8s id: gxy-g4xx2 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:51:11,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 32: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:51:18,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 32 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:51:18,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bwa-mem-fastq1.fq', 'dbkey': '?', 'ext': 'fastqsanger', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/32/working/data_fetch_upload_jyqka9uw', 'object_id': 47}]}]}]
galaxy.jobs INFO 2025-03-08 12:51:18,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 32 in /galaxy/server/database/jobs_directory/000/32
galaxy.jobs DEBUG 2025-03-08 12:51:18,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 32 executed (134.195 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:18,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 32 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:51:21,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 34
tpv.core.entities DEBUG 2025-03-08 12:51:21,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:51:21,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:51:21,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:51:21,365 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:51:21,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Working directory for job is: /galaxy/server/database/jobs_directory/000/34
galaxy.jobs.runners DEBUG 2025-03-08 12:51:21,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [34] queued (28.481 ms)
galaxy.jobs.handler INFO 2025-03-08 12:51:21,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:21,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 34
galaxy.jobs DEBUG 2025-03-08 12:51:21,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [34] prepared (65.955 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:51:21,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/34/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/34/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/34/configs/tmpotszk_5m']
galaxy.jobs.runners DEBUG 2025-03-08 12:51:21,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (34) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/34/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/34/galaxy_34.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:21,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:21,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:22,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:31,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v8k5j with k8s id: gxy-v8k5j succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:51:31,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 34: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:51:38,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 34 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:51:38,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'samtools_fastx-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/34/working/gxupload_0', 'object_id': 49}]}]}]
galaxy.jobs INFO 2025-03-08 12:51:38,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 34 in /galaxy/server/database/jobs_directory/000/34
galaxy.jobs DEBUG 2025-03-08 12:51:39,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 34 executed (135.475 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:39,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 34 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:51:39,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 35
tpv.core.entities DEBUG 2025-03-08 12:51:39,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:51:39,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:51:39,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:51:39,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:51:39,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Working directory for job is: /galaxy/server/database/jobs_directory/000/35
galaxy.jobs.runners DEBUG 2025-03-08 12:51:39,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [35] queued (38.058 ms)
galaxy.jobs.handler INFO 2025-03-08 12:51:39,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:39,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 35
galaxy.jobs DEBUG 2025-03-08 12:51:39,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [35] prepared (59.526 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:51:39,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:51:39,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:51:40,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:51:40,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/35/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/35/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/d/e/b/dataset_deb136a5-f60d-4453-b5fd-da7450a3abed.dat' > input &&   samtools fasta    input  > output.fasta && ln -s output.fasta output]
galaxy.jobs.runners DEBUG 2025-03-08 12:51:40,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (35) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/35/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/35/galaxy_35.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/35/working/output" -a -f "/galaxy/server/database/objects/6/b/5/dataset_6b53f138-b05a-4e11-8226-420ed3a00545.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/35/working/output" "/galaxy/server/database/objects/6/b/5/dataset_6b53f138-b05a-4e11-8226-420ed3a00545.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:40,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:51:40,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:51:40,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:51:40,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:40,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:40,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:48,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6fxt9 with k8s id: gxy-6fxt9 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:51:48,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 35: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:51:55,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 35 finished
galaxy.model.metadata DEBUG 2025-03-08 12:51:55,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 50
galaxy.util WARNING 2025-03-08 12:51:55,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/b/5/dataset_6b53f138-b05a-4e11-8226-420ed3a00545.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/b/5/dataset_6b53f138-b05a-4e11-8226-420ed3a00545.dat'
galaxy.jobs INFO 2025-03-08 12:51:56,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 35 in /galaxy/server/database/jobs_directory/000/35
galaxy.jobs DEBUG 2025-03-08 12:51:56,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 35 executed (108.886 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:56,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 35 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:51:57,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 36
tpv.core.entities DEBUG 2025-03-08 12:51:57,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:51:57,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:51:57,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:51:57,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:51:57,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Working directory for job is: /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-03-08 12:51:57,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [36] queued (27.134 ms)
galaxy.jobs.handler INFO 2025-03-08 12:51:57,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:57,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 36
galaxy.jobs DEBUG 2025-03-08 12:51:57,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [36] prepared (77.053 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:51:57,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/36/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/36/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/36/configs/tmp_61t66ls']
galaxy.jobs.runners DEBUG 2025-03-08 12:51:57,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (36) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/36/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/36/galaxy_36.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:57,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:57,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:51:57,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:08,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w5jvc with k8s id: gxy-w5jvc succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:52:08,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 36: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:52:15,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 36 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:52:15,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'samtools_fastx-in2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/36/working/gxupload_0', 'object_id': 51}]}]}]
galaxy.jobs INFO 2025-03-08 12:52:15,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs DEBUG 2025-03-08 12:52:15,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 36 executed (135.404 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:15,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 36 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:52:16,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 37
tpv.core.entities DEBUG 2025-03-08 12:52:16,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:52:16,500 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:52:16,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:52:16,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:52:16,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.runners DEBUG 2025-03-08 12:52:16,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (48.330 ms)
galaxy.jobs.handler INFO 2025-03-08 12:52:16,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:16,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 37
galaxy.jobs DEBUG 2025-03-08 12:52:16,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [37] prepared (59.005 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:16,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:52:16,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:16,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:52:16,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/37/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/7/4/7/dataset_747e330e-bde3-49a7-8c3a-9348e842a6ed.dat' > input &&   samtools fastq     -0 nonspecific.fastqsanger -1 forward.fastqsanger -2 reverse.fastqsanger  input  > /dev/null && ln -s nonspecific.fastqsanger nonspecific && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse]
galaxy.jobs.runners DEBUG 2025-03-08 12:52:16,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/37/working/nonspecific" -a -f "/galaxy/server/database/objects/7/b/9/dataset_7b9ce7ec-8d87-48af-95ea-fea4f4f32c0e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/37/working/nonspecific" "/galaxy/server/database/objects/7/b/9/dataset_7b9ce7ec-8d87-48af-95ea-fea4f4f32c0e.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/37/working/forward" -a -f "/galaxy/server/database/objects/7/7/a/dataset_77a7bace-ba93-44ea-bcdd-5c372c0fe461.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/37/working/forward" "/galaxy/server/database/objects/7/7/a/dataset_77a7bace-ba93-44ea-bcdd-5c372c0fe461.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/37/working/reverse" -a -f "/galaxy/server/database/objects/e/f/4/dataset_ef4e7b29-ff90-4753-9b77-5b456e714fa8.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/37/working/reverse" "/galaxy/server/database/objects/e/f/4/dataset_ef4e7b29-ff90-4753-9b77-5b456e714fa8.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:16,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:16,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:52:16,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:16,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:16,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:17,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:21,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5s2tp with k8s id: gxy-5s2tp succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:52:21,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 37: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:52:29,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 37 finished
galaxy.model.metadata DEBUG 2025-03-08 12:52:29,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 52
galaxy.model.metadata DEBUG 2025-03-08 12:52:29,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 53
galaxy.model.metadata DEBUG 2025-03-08 12:52:29,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 54
galaxy.util WARNING 2025-03-08 12:52:29,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/b/9/dataset_7b9ce7ec-8d87-48af-95ea-fea4f4f32c0e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/b/9/dataset_7b9ce7ec-8d87-48af-95ea-fea4f4f32c0e.dat'
galaxy.util WARNING 2025-03-08 12:52:29,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/7/a/dataset_77a7bace-ba93-44ea-bcdd-5c372c0fe461.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/7/a/dataset_77a7bace-ba93-44ea-bcdd-5c372c0fe461.dat'
galaxy.util WARNING 2025-03-08 12:52:29,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/f/4/dataset_ef4e7b29-ff90-4753-9b77-5b456e714fa8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/f/4/dataset_ef4e7b29-ff90-4753-9b77-5b456e714fa8.dat'
galaxy.jobs INFO 2025-03-08 12:52:29,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 37 in /galaxy/server/database/jobs_directory/000/37
galaxy.jobs DEBUG 2025-03-08 12:52:29,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 37 executed (198.660 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:29,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 37 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:52:31,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38
tpv.core.entities DEBUG 2025-03-08 12:52:31,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:52:31,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:52:31,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:52:31,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:52:31,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-03-08 12:52:31,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (36.525 ms)
galaxy.jobs.handler INFO 2025-03-08 12:52:31,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:31,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 38
galaxy.jobs DEBUG 2025-03-08 12:52:31,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [38] prepared (60.805 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:52:31,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/38/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/38/configs/tmpnjqgr1t7']
galaxy.jobs.runners DEBUG 2025-03-08 12:52:31,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:32,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:32,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:32,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:41,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kmljb with k8s id: gxy-kmljb succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:52:41,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 38: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:52:49,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 38 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:52:49,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'samtools_fastx-in3.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/38/working/data_fetch_upload_u24bokqn', 'object_id': 55}]}]}]
galaxy.jobs INFO 2025-03-08 12:52:49,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 38 in /galaxy/server/database/jobs_directory/000/38
galaxy.jobs DEBUG 2025-03-08 12:52:49,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 38 executed (111.410 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:49,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 38 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:52:50,218 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 39
tpv.core.entities DEBUG 2025-03-08 12:52:50,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:52:50,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:52:50,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:52:50,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:52:50,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.runners DEBUG 2025-03-08 12:52:50,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (54.250 ms)
galaxy.jobs.handler INFO 2025-03-08 12:52:50,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:50,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 39
galaxy.jobs DEBUG 2025-03-08 12:52:50,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [39] prepared (61.889 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:50,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:52:50,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:50,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:52:50,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/39/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/2/1/e/dataset_21e4b169-bc45-4a12-8ccf-dcefe5457002.dat' > input &&   samtools fasta   -0 nonspecific.fasta -1 forward.fasta -2 reverse.fasta  input  > /dev/null && ln -s nonspecific.fasta nonspecific && ln -s forward.fasta forward && ln -s reverse.fasta reverse]
galaxy.jobs.runners DEBUG 2025-03-08 12:52:50,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/39/working/nonspecific" -a -f "/galaxy/server/database/objects/f/f/d/dataset_ffdf8449-089a-4915-b409-2f96c31778eb.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/39/working/nonspecific" "/galaxy/server/database/objects/f/f/d/dataset_ffdf8449-089a-4915-b409-2f96c31778eb.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/39/working/forward" -a -f "/galaxy/server/database/objects/7/8/0/dataset_780e8a32-e118-4f72-a9fc-c2d8b6ce9bfd.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/39/working/forward" "/galaxy/server/database/objects/7/8/0/dataset_780e8a32-e118-4f72-a9fc-c2d8b6ce9bfd.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/39/working/reverse" -a -f "/galaxy/server/database/objects/9/1/b/dataset_91b12613-9167-4892-ac57-cb27bd1e73e9.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/39/working/reverse" "/galaxy/server/database/objects/9/1/b/dataset_91b12613-9167-4892-ac57-cb27bd1e73e9.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:50,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:50,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:52:50,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:52:50,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:50,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:50,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:52:55,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jfd4s with k8s id: gxy-jfd4s succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:52:55,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 39: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:53:02,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 39 finished
galaxy.model.metadata DEBUG 2025-03-08 12:53:02,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 56
galaxy.model.metadata DEBUG 2025-03-08 12:53:02,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 57
galaxy.model.metadata DEBUG 2025-03-08 12:53:02,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 58
galaxy.util WARNING 2025-03-08 12:53:02,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/f/d/dataset_ffdf8449-089a-4915-b409-2f96c31778eb.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/f/d/dataset_ffdf8449-089a-4915-b409-2f96c31778eb.dat'
galaxy.util WARNING 2025-03-08 12:53:02,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/8/0/dataset_780e8a32-e118-4f72-a9fc-c2d8b6ce9bfd.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/8/0/dataset_780e8a32-e118-4f72-a9fc-c2d8b6ce9bfd.dat'
galaxy.util WARNING 2025-03-08 12:53:02,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/1/b/dataset_91b12613-9167-4892-ac57-cb27bd1e73e9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/1/b/dataset_91b12613-9167-4892-ac57-cb27bd1e73e9.dat'
galaxy.jobs INFO 2025-03-08 12:53:02,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 39 in /galaxy/server/database/jobs_directory/000/39
galaxy.jobs DEBUG 2025-03-08 12:53:02,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 39 executed (209.332 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:02,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 39 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:53:05,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 40
tpv.core.entities DEBUG 2025-03-08 12:53:05,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:53:05,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:53:05,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:53:05,608 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:53:05,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-03-08 12:53:05,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (37.789 ms)
galaxy.jobs.handler INFO 2025-03-08 12:53:05,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:05,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 40
galaxy.jobs DEBUG 2025-03-08 12:53:05,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [40] prepared (80.725 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:53:05,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/40/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/40/configs/tmpkmkjyj7h']
galaxy.jobs.runners DEBUG 2025-03-08 12:53:05,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:05,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:05,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:06,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:16,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mbbjb with k8s id: gxy-mbbjb succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:53:16,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 40: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:53:23,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 40 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:53:23,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'samtools_fastx-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/40/working/gxupload_0', 'object_id': 59}]}]}]
galaxy.jobs INFO 2025-03-08 12:53:23,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 40 in /galaxy/server/database/jobs_directory/000/40
galaxy.jobs DEBUG 2025-03-08 12:53:23,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 40 executed (135.695 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:23,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 40 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:53:24,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-03-08 12:53:24,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:53:24,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:53:24,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:53:25,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:53:25,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-03-08 12:53:25,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (30.310 ms)
galaxy.jobs.handler INFO 2025-03-08 12:53:25,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:25,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 41
galaxy.jobs DEBUG 2025-03-08 12:53:25,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [41] prepared (41.091 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:25,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:53:25,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:25,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:53:25,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/41/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/a/6/7/dataset_a67c4610-f385-4945-9ead-5f6b1481997c.dat' > input &&   samtools fasta    input  | gzip - > output.fasta.gz && ln -s output.fasta.gz output]
galaxy.jobs.runners DEBUG 2025-03-08 12:53:25,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/41/working/output" -a -f "/galaxy/server/database/objects/e/5/4/dataset_e549c2ae-a51b-4d31-9044-1a7c0362bd45.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/41/working/output" "/galaxy/server/database/objects/e/5/4/dataset_e549c2ae-a51b-4d31-9044-1a7c0362bd45.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:25,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:25,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:53:25,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:25,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:25,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:25,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:29,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s2f4z with k8s id: gxy-s2f4z succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:53:29,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:53:36,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 41 finished
galaxy.model.metadata DEBUG 2025-03-08 12:53:36,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 60
galaxy.util WARNING 2025-03-08 12:53:36,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/5/4/dataset_e549c2ae-a51b-4d31-9044-1a7c0362bd45.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/5/4/dataset_e549c2ae-a51b-4d31-9044-1a7c0362bd45.dat'
galaxy.jobs INFO 2025-03-08 12:53:36,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-03-08 12:53:36,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 41 executed (91.031 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:36,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 41 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:53:38,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 42
tpv.core.entities DEBUG 2025-03-08 12:53:38,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:53:38,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:53:38,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:53:38,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:53:38,298 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-03-08 12:53:38,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (32.288 ms)
galaxy.jobs.handler INFO 2025-03-08 12:53:38,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:38,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 42
galaxy.jobs DEBUG 2025-03-08 12:53:38,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [42] prepared (62.845 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:53:38,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/42/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/42/configs/tmpssphgnaa']
galaxy.jobs.runners DEBUG 2025-03-08 12:53:38,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:38,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:38,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:39,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:49,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-76ln7 with k8s id: gxy-76ln7 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:53:49,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:53:56,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 42 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:53:56,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'samtools_fastx-in2.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/42/working/gxupload_0', 'object_id': 61}]}]}]
galaxy.jobs INFO 2025-03-08 12:53:57,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs DEBUG 2025-03-08 12:53:57,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 42 executed (114.644 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:57,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 42 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:53:57,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43
tpv.core.entities DEBUG 2025-03-08 12:53:57,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:53:57,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:53:57,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:53:57,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:53:57,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-03-08 12:53:57,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (41.168 ms)
galaxy.jobs.handler INFO 2025-03-08 12:53:57,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:57,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 43
galaxy.jobs DEBUG 2025-03-08 12:53:57,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [43] prepared (49.026 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:57,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:53:57,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:57,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:53:57,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/43/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/1/6/c/dataset_16ca7305-e03d-48c6-b8c6-9ad00f9f0e2e.dat' > input &&   samtools fastq     -0 nonspecific.fastqsanger.gz -1 forward.fastqsanger.gz -2 reverse.fastqsanger.gz  input  > /dev/null && ln -s nonspecific.fastqsanger.gz nonspecific && ln -s forward.fastqsanger.gz forward && ln -s reverse.fastqsanger.gz reverse]
galaxy.jobs.runners DEBUG 2025-03-08 12:53:57,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/43/working/nonspecific" -a -f "/galaxy/server/database/objects/c/b/c/dataset_cbce043f-5162-435f-a89c-238822e51c29.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/43/working/nonspecific" "/galaxy/server/database/objects/c/b/c/dataset_cbce043f-5162-435f-a89c-238822e51c29.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/43/working/forward" -a -f "/galaxy/server/database/objects/d/c/4/dataset_dc4731da-5a3c-4f97-840b-50868a7c101a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/43/working/forward" "/galaxy/server/database/objects/d/c/4/dataset_dc4731da-5a3c-4f97-840b-50868a7c101a.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/43/working/reverse" -a -f "/galaxy/server/database/objects/7/c/9/dataset_7c9db9bb-f4d6-4248-a9a7-c7a43306bde0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/43/working/reverse" "/galaxy/server/database/objects/7/c/9/dataset_7c9db9bb-f4d6-4248-a9a7-c7a43306bde0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:57,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:57,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:53:57,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:53:57,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:57,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:53:58,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:02,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8qgnz with k8s id: gxy-8qgnz succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:54:03,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:54:10,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 43 finished
galaxy.model.metadata DEBUG 2025-03-08 12:54:10,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 62
galaxy.model.metadata DEBUG 2025-03-08 12:54:10,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 63
galaxy.model.metadata DEBUG 2025-03-08 12:54:10,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 64
galaxy.util WARNING 2025-03-08 12:54:10,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/b/c/dataset_cbce043f-5162-435f-a89c-238822e51c29.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/b/c/dataset_cbce043f-5162-435f-a89c-238822e51c29.dat'
galaxy.util WARNING 2025-03-08 12:54:10,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/c/4/dataset_dc4731da-5a3c-4f97-840b-50868a7c101a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/c/4/dataset_dc4731da-5a3c-4f97-840b-50868a7c101a.dat'
galaxy.util WARNING 2025-03-08 12:54:10,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/c/9/dataset_7c9db9bb-f4d6-4248-a9a7-c7a43306bde0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/c/9/dataset_7c9db9bb-f4d6-4248-a9a7-c7a43306bde0.dat'
galaxy.jobs INFO 2025-03-08 12:54:10,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-03-08 12:54:10,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 43 executed (142.508 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:10,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 43 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:54:12,005 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 44
tpv.core.entities DEBUG 2025-03-08 12:54:12,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:54:12,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:54:12,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:54:12,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:54:12,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.runners DEBUG 2025-03-08 12:54:12,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (37.269 ms)
galaxy.jobs.handler INFO 2025-03-08 12:54:12,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:12,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 44
galaxy.jobs DEBUG 2025-03-08 12:54:12,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [44] prepared (72.511 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:54:12,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/44/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/44/configs/tmp69zxhcor']
galaxy.jobs.runners DEBUG 2025-03-08 12:54:12,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:12,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:12,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:12,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:21,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-29mzq with k8s id: gxy-29mzq succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:54:21,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:54:28,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 44 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:54:28,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'samtools_fastx-in3.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/44/working/data_fetch_upload_9d7gcnab', 'object_id': 65}]}]}]
galaxy.jobs INFO 2025-03-08 12:54:28,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-03-08 12:54:28,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 44 executed (129.582 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:28,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:54:29,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-03-08 12:54:29,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:54:29,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:54:29,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:54:29,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:54:29,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-03-08 12:54:29,465 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (39.782 ms)
galaxy.jobs.handler INFO 2025-03-08 12:54:29,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:29,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-03-08 12:54:29,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [45] prepared (47.722 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:29,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:54:29,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:29,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:54:29,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/45/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/4/0/7/dataset_407d26bc-16b7-472f-b2c4-86bd6c642717.dat' > input &&   samtools fasta   -0 nonspecific.fasta.gz -1 forward.fasta.gz -2 reverse.fasta.gz  input  > /dev/null && ln -s nonspecific.fasta.gz nonspecific && ln -s forward.fasta.gz forward && ln -s reverse.fasta.gz reverse]
galaxy.jobs.runners DEBUG 2025-03-08 12:54:29,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/45/working/nonspecific" -a -f "/galaxy/server/database/objects/d/b/3/dataset_db395e10-f9d6-415e-becc-448637fc4ca7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/45/working/nonspecific" "/galaxy/server/database/objects/d/b/3/dataset_db395e10-f9d6-415e-becc-448637fc4ca7.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/45/working/forward" -a -f "/galaxy/server/database/objects/6/a/1/dataset_6a144c65-169e-4453-a57c-19835e7f6292.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/45/working/forward" "/galaxy/server/database/objects/6/a/1/dataset_6a144c65-169e-4453-a57c-19835e7f6292.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/45/working/reverse" -a -f "/galaxy/server/database/objects/e/5/7/dataset_e575098d-7b58-473c-85fd-854881fc3d25.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/45/working/reverse" "/galaxy/server/database/objects/e/5/7/dataset_e575098d-7b58-473c-85fd-854881fc3d25.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:29,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:29,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:54:29,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:29,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:29,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:30,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7cx6t failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-7cx6t.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 12:54:34,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-7cx6t

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-7cx6t": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (45/gxy-7cx6t) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (45/gxy-7cx6t) tool_stderr: [M::bam2fq_mainloop] discarded 0 singletons
[M::bam2fq_mainloop] processed 396 reads

galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (45/gxy-7cx6t) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (45/gxy-7cx6t) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-7cx6t.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 45 (gxy-7cx6t)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-7cx6t to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:34,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (45/gxy-7cx6t) Terminated at user's request
galaxy.util WARNING 2025-03-08 12:54:34,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/b/3/dataset_db395e10-f9d6-415e-becc-448637fc4ca7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/b/3/dataset_db395e10-f9d6-415e-becc-448637fc4ca7.dat'
galaxy.util WARNING 2025-03-08 12:54:34,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/a/1/dataset_6a144c65-169e-4453-a57c-19835e7f6292.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/a/1/dataset_6a144c65-169e-4453-a57c-19835e7f6292.dat'
galaxy.util WARNING 2025-03-08 12:54:34,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/5/7/dataset_e575098d-7b58-473c-85fd-854881fc3d25.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/5/7/dataset_e575098d-7b58-473c-85fd-854881fc3d25.dat'
galaxy.jobs.handler DEBUG 2025-03-08 12:54:35,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-03-08 12:54:35,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:54:35,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:54:35,582 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:54:35,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:54:35,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-03-08 12:54:35,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (31.299 ms)
galaxy.jobs.handler INFO 2025-03-08 12:54:35,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:35,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-03-08 12:54:35,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [46] prepared (54.273 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:54:35,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/46/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/46/configs/tmpk0s2lw2k']
galaxy.jobs.runners DEBUG 2025-03-08 12:54:35,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:35,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:35,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:36,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:45,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zwqgf with k8s id: gxy-zwqgf succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:54:45,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:54:52,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 46 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:54:52,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.001.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/46/working/data_fetch_upload_o1pkw0hu', 'object_id': 69}]}]}]
galaxy.jobs INFO 2025-03-08 12:54:53,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-03-08 12:54:53,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 46 executed (130.960 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:53,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:54:53,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-03-08 12:54:53,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:54:53,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:54:53,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:54:53,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:54:53,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-03-08 12:54:53,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (40.688 ms)
galaxy.jobs.handler INFO 2025-03-08 12:54:53,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:53,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 47
galaxy.jobs DEBUG 2025-03-08 12:54:54,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [47] prepared (56.122 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:54,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:54:54,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:54,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:54:54,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/47/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/c/6/d/dataset_c6d4909f-5524-48ba-b5d2-956a30f50fa3.dat' > input &&   samtools fastq     -1 forward.fastqsanger -2 reverse.fastqsanger  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse]
galaxy.jobs.runners DEBUG 2025-03-08 12:54:54,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/forward" -a -f "/galaxy/server/database/objects/6/b/c/dataset_6bc9d795-9697-4db8-b53e-e6f43c3b087f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/forward" "/galaxy/server/database/objects/6/b/c/dataset_6bc9d795-9697-4db8-b53e-e6f43c3b087f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/reverse" -a -f "/galaxy/server/database/objects/b/e/3/dataset_be31434d-987a-45e8-a3d1-e77160e23981.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/reverse" "/galaxy/server/database/objects/b/e/3/dataset_be31434d-987a-45e8-a3d1-e77160e23981.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output" -a -f "/galaxy/server/database/objects/4/7/c/dataset_47c031be-8e24-4cff-9c99-e5c9bbd704e2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output" "/galaxy/server/database/objects/4/7/c/dataset_47c031be-8e24-4cff-9c99-e5c9bbd704e2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:54,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:54,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:54:54,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:54:54,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:54,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:54,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:54:58,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-29vkh with k8s id: gxy-29vkh succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:54:58,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:55:06,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 47 finished
galaxy.model.metadata DEBUG 2025-03-08 12:55:06,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 70
galaxy.model.metadata DEBUG 2025-03-08 12:55:06,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 71
galaxy.model.metadata DEBUG 2025-03-08 12:55:06,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 72
galaxy.util WARNING 2025-03-08 12:55:06,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/b/c/dataset_6bc9d795-9697-4db8-b53e-e6f43c3b087f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/b/c/dataset_6bc9d795-9697-4db8-b53e-e6f43c3b087f.dat'
galaxy.util WARNING 2025-03-08 12:55:06,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/e/3/dataset_be31434d-987a-45e8-a3d1-e77160e23981.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/e/3/dataset_be31434d-987a-45e8-a3d1-e77160e23981.dat'
galaxy.util WARNING 2025-03-08 12:55:06,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/7/c/dataset_47c031be-8e24-4cff-9c99-e5c9bbd704e2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/7/c/dataset_47c031be-8e24-4cff-9c99-e5c9bbd704e2.dat'
galaxy.jobs INFO 2025-03-08 12:55:06,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs DEBUG 2025-03-08 12:55:06,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 47 executed (196.374 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:06,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:55:08,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 48
tpv.core.entities DEBUG 2025-03-08 12:55:08,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:55:08,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:55:08,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:55:08,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:55:08,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-03-08 12:55:08,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (26.174 ms)
galaxy.jobs.handler INFO 2025-03-08 12:55:08,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:08,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 48
galaxy.jobs DEBUG 2025-03-08 12:55:08,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [48] prepared (60.419 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:55:08,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/48/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/48/configs/tmp11u2ah4k']
galaxy.jobs.runners DEBUG 2025-03-08 12:55:08,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:08,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:08,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:09,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:18,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4v7hw with k8s id: gxy-4v7hw succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:55:19,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:55:26,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 48 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:55:26,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.001.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/48/working/data_fetch_upload_bot5rmgt', 'object_id': 73}]}]}]
galaxy.jobs INFO 2025-03-08 12:55:26,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs DEBUG 2025-03-08 12:55:26,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 48 executed (128.582 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:26,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:55:26,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 49
tpv.core.entities DEBUG 2025-03-08 12:55:26,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:55:26,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:55:26,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:55:26,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:55:26,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-03-08 12:55:26,685 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (44.913 ms)
galaxy.jobs.handler INFO 2025-03-08 12:55:26,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:26,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-03-08 12:55:26,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [49] prepared (51.020 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:55:26,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:55:26,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:55:26,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:55:26,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/49/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/3/4/d/dataset_34d59ce9-3cd3-428d-b786-66d8836f8b7c.dat' > input &&   samtools fastq     -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons]
galaxy.jobs.runners DEBUG 2025-03-08 12:55:26,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/49/working/forward" -a -f "/galaxy/server/database/objects/a/b/b/dataset_abb99b33-dcce-46b8-adb8-fbce2f45e0b1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/49/working/forward" "/galaxy/server/database/objects/a/b/b/dataset_abb99b33-dcce-46b8-adb8-fbce2f45e0b1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/49/working/reverse" -a -f "/galaxy/server/database/objects/4/5/0/dataset_450f10f3-6be0-440b-881f-5cc795393803.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/49/working/reverse" "/galaxy/server/database/objects/4/5/0/dataset_450f10f3-6be0-440b-881f-5cc795393803.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/49/working/singletons" -a -f "/galaxy/server/database/objects/7/5/e/dataset_75ea7550-4b35-4651-b387-63c228e9ff95.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/49/working/singletons" "/galaxy/server/database/objects/7/5/e/dataset_75ea7550-4b35-4651-b387-63c228e9ff95.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/49/working/output" -a -f "/galaxy/server/database/objects/7/0/7/dataset_70717978-b969-418e-8088-93b986145a14.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/49/working/output" "/galaxy/server/database/objects/7/0/7/dataset_70717978-b969-418e-8088-93b986145a14.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:26,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:55:26,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:55:26,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:55:26,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:26,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:27,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:32,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s75hv with k8s id: gxy-s75hv succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:55:32,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:55:39,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 49 finished
galaxy.model.metadata DEBUG 2025-03-08 12:55:39,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 74
galaxy.model.metadata DEBUG 2025-03-08 12:55:39,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 75
galaxy.model.metadata DEBUG 2025-03-08 12:55:39,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 76
galaxy.model.metadata DEBUG 2025-03-08 12:55:39,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 77
galaxy.util WARNING 2025-03-08 12:55:39,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/b/b/dataset_abb99b33-dcce-46b8-adb8-fbce2f45e0b1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/b/b/dataset_abb99b33-dcce-46b8-adb8-fbce2f45e0b1.dat'
galaxy.util WARNING 2025-03-08 12:55:39,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/5/0/dataset_450f10f3-6be0-440b-881f-5cc795393803.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/5/0/dataset_450f10f3-6be0-440b-881f-5cc795393803.dat'
galaxy.util WARNING 2025-03-08 12:55:39,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/5/e/dataset_75ea7550-4b35-4651-b387-63c228e9ff95.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/5/e/dataset_75ea7550-4b35-4651-b387-63c228e9ff95.dat'
galaxy.util WARNING 2025-03-08 12:55:39,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/0/7/dataset_70717978-b969-418e-8088-93b986145a14.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/0/7/dataset_70717978-b969-418e-8088-93b986145a14.dat'
galaxy.jobs INFO 2025-03-08 12:55:39,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs DEBUG 2025-03-08 12:55:39,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 49 executed (245.493 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:39,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:55:42,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-03-08 12:55:42,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:55:42,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:55:42,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:55:42,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:55:42,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-03-08 12:55:42,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (29.574 ms)
galaxy.jobs.handler INFO 2025-03-08 12:55:42,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:42,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-03-08 12:55:42,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [50] prepared (60.600 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:55:42,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/50/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/50/configs/tmpn4tsze84']
galaxy.jobs.runners DEBUG 2025-03-08 12:55:42,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:42,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:42,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:43,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:52,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4mz5j with k8s id: gxy-4mz5j succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:55:52,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:55:59,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 50 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:55:59,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.002.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/50/working/data_fetch_upload_zlbrn9ap', 'object_id': 78}]}]}]
galaxy.jobs INFO 2025-03-08 12:55:59,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-03-08 12:55:59,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 50 executed (119.651 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:55:59,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:56:00,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 51
tpv.core.entities DEBUG 2025-03-08 12:56:00,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:56:00,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:56:00,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:56:00,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:56:00,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-03-08 12:56:00,485 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (41.631 ms)
galaxy.jobs.handler INFO 2025-03-08 12:56:00,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:00,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 51
galaxy.jobs DEBUG 2025-03-08 12:56:00,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [51] prepared (37.726 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:00,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:56:00,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:00,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:56:00,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/51/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/2/5/d/dataset_25dabb3b-9809-4833-bd61-53bf02ede971.dat' > input &&   samtools fastq     -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons]
galaxy.jobs.runners DEBUG 2025-03-08 12:56:00,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/51/working/forward" -a -f "/galaxy/server/database/objects/d/c/3/dataset_dc36fada-60aa-457a-84c1-628e10eae65c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/51/working/forward" "/galaxy/server/database/objects/d/c/3/dataset_dc36fada-60aa-457a-84c1-628e10eae65c.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/51/working/reverse" -a -f "/galaxy/server/database/objects/b/8/c/dataset_b8cf44ad-101c-4b18-b36b-85b05048c00f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/51/working/reverse" "/galaxy/server/database/objects/b/8/c/dataset_b8cf44ad-101c-4b18-b36b-85b05048c00f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/51/working/singletons" -a -f "/galaxy/server/database/objects/a/d/3/dataset_ad3444d1-f7f8-469a-85cb-b5ffa918ceca.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/51/working/singletons" "/galaxy/server/database/objects/a/d/3/dataset_ad3444d1-f7f8-469a-85cb-b5ffa918ceca.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/51/working/output" -a -f "/galaxy/server/database/objects/8/7/4/dataset_874600a6-5aa3-48f7-9668-6e15d0033c4a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/51/working/output" "/galaxy/server/database/objects/8/7/4/dataset_874600a6-5aa3-48f7-9668-6e15d0033c4a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:00,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:00,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:56:00,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:00,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:00,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:01,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:05,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vvqjq with k8s id: gxy-vvqjq succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:56:05,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:56:12,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 51 finished
galaxy.model.metadata DEBUG 2025-03-08 12:56:13,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 79
galaxy.model.metadata DEBUG 2025-03-08 12:56:13,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 80
galaxy.model.metadata DEBUG 2025-03-08 12:56:13,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 81
galaxy.model.metadata DEBUG 2025-03-08 12:56:13,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 82
galaxy.util WARNING 2025-03-08 12:56:13,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/c/3/dataset_dc36fada-60aa-457a-84c1-628e10eae65c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/c/3/dataset_dc36fada-60aa-457a-84c1-628e10eae65c.dat'
galaxy.util WARNING 2025-03-08 12:56:13,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/8/c/dataset_b8cf44ad-101c-4b18-b36b-85b05048c00f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/8/c/dataset_b8cf44ad-101c-4b18-b36b-85b05048c00f.dat'
galaxy.util WARNING 2025-03-08 12:56:13,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/d/3/dataset_ad3444d1-f7f8-469a-85cb-b5ffa918ceca.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/d/3/dataset_ad3444d1-f7f8-469a-85cb-b5ffa918ceca.dat'
galaxy.util WARNING 2025-03-08 12:56:13,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/7/4/dataset_874600a6-5aa3-48f7-9668-6e15d0033c4a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/7/4/dataset_874600a6-5aa3-48f7-9668-6e15d0033c4a.dat'
galaxy.jobs INFO 2025-03-08 12:56:13,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs DEBUG 2025-03-08 12:56:13,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 51 executed (210.307 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:13,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:56:15,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52
tpv.core.entities DEBUG 2025-03-08 12:56:15,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:56:15,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:56:15,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:56:15,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:56:15,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-03-08 12:56:15,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (34.431 ms)
galaxy.jobs.handler INFO 2025-03-08 12:56:15,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:15,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-03-08 12:56:15,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [52] prepared (60.213 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:56:15,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/52/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/52/configs/tmpdd1eybzg']
galaxy.jobs.runners DEBUG 2025-03-08 12:56:15,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:15,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:15,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:16,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:25,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s2t49 with k8s id: gxy-s2t49 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:56:25,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:56:32,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 52 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:56:32,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.003.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/52/working/data_fetch_upload_wv1mfonj', 'object_id': 83}]}]}]
galaxy.jobs INFO 2025-03-08 12:56:33,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-03-08 12:56:33,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 52 executed (119.555 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:33,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:56:34,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 53
tpv.core.entities DEBUG 2025-03-08 12:56:34,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:56:34,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:56:34,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:56:34,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:56:34,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-03-08 12:56:34,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (44.544 ms)
galaxy.jobs.handler INFO 2025-03-08 12:56:34,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:34,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 53
galaxy.jobs DEBUG 2025-03-08 12:56:34,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [53] prepared (46.954 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:34,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:56:34,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:34,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:56:34,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/53/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/b/4/5/dataset_b45ebee5-9714-48a2-b15c-d9e98cf9fdb8.dat' > input &&   samtools fastq     -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons]
galaxy.jobs.runners DEBUG 2025-03-08 12:56:34,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/53/working/forward" -a -f "/galaxy/server/database/objects/e/a/f/dataset_eaf1e7ea-9d91-4cd7-934f-e9f1312c4e76.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/53/working/forward" "/galaxy/server/database/objects/e/a/f/dataset_eaf1e7ea-9d91-4cd7-934f-e9f1312c4e76.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/53/working/reverse" -a -f "/galaxy/server/database/objects/f/2/e/dataset_f2e17a6f-e622-476c-b21b-d5fe13827280.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/53/working/reverse" "/galaxy/server/database/objects/f/2/e/dataset_f2e17a6f-e622-476c-b21b-d5fe13827280.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/53/working/singletons" -a -f "/galaxy/server/database/objects/f/8/7/dataset_f87d5614-29dd-48e2-abf0-759e9131ad4f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/53/working/singletons" "/galaxy/server/database/objects/f/8/7/dataset_f87d5614-29dd-48e2-abf0-759e9131ad4f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/53/working/output" -a -f "/galaxy/server/database/objects/7/0/2/dataset_702b931b-b5db-46ab-a940-0efd490c9778.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/53/working/output" "/galaxy/server/database/objects/7/0/2/dataset_702b931b-b5db-46ab-a940-0efd490c9778.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:34,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:34,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:56:34,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:56:34,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:34,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:34,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:38,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q9vjk with k8s id: gxy-q9vjk succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:56:38,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:56:46,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 53 finished
galaxy.model.metadata DEBUG 2025-03-08 12:56:46,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 84
galaxy.model.metadata DEBUG 2025-03-08 12:56:46,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 85
galaxy.model.metadata DEBUG 2025-03-08 12:56:46,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 86
galaxy.model.metadata DEBUG 2025-03-08 12:56:46,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 87
galaxy.util WARNING 2025-03-08 12:56:46,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/a/f/dataset_eaf1e7ea-9d91-4cd7-934f-e9f1312c4e76.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/a/f/dataset_eaf1e7ea-9d91-4cd7-934f-e9f1312c4e76.dat'
galaxy.util WARNING 2025-03-08 12:56:46,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/2/e/dataset_f2e17a6f-e622-476c-b21b-d5fe13827280.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/2/e/dataset_f2e17a6f-e622-476c-b21b-d5fe13827280.dat'
galaxy.util WARNING 2025-03-08 12:56:46,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/8/7/dataset_f87d5614-29dd-48e2-abf0-759e9131ad4f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/8/7/dataset_f87d5614-29dd-48e2-abf0-759e9131ad4f.dat'
galaxy.util WARNING 2025-03-08 12:56:46,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/0/2/dataset_702b931b-b5db-46ab-a940-0efd490c9778.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/0/2/dataset_702b931b-b5db-46ab-a940-0efd490c9778.dat'
galaxy.jobs INFO 2025-03-08 12:56:46,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs DEBUG 2025-03-08 12:56:46,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 53 executed (190.842 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:46,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:56:48,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54
tpv.core.entities DEBUG 2025-03-08 12:56:48,457 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:56:48,457 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:56:48,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:56:48,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:56:48,485 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-03-08 12:56:48,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (30.300 ms)
galaxy.jobs.handler INFO 2025-03-08 12:56:48,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:48,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 54
galaxy.jobs DEBUG 2025-03-08 12:56:48,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [54] prepared (56.541 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:56:48,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/54/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/54/configs/tmp7ukw9m3g']
galaxy.jobs.runners DEBUG 2025-03-08 12:56:48,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:48,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:48,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:48,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:56:58,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bxt2w with k8s id: gxy-bxt2w succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:56:58,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 54: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:57:05,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 54 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:57:05,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.004.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/54/working/data_fetch_upload_o8bdle0s', 'object_id': 88}]}]}]
galaxy.jobs INFO 2025-03-08 12:57:05,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 54 in /galaxy/server/database/jobs_directory/000/54
galaxy.jobs DEBUG 2025-03-08 12:57:05,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 54 executed (111.233 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:05,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:57:06,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 55
tpv.core.entities DEBUG 2025-03-08 12:57:06,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:57:06,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:57:06,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:57:06,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:57:06,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-03-08 12:57:06,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (46.715 ms)
galaxy.jobs.handler INFO 2025-03-08 12:57:06,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:06,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 55
galaxy.jobs DEBUG 2025-03-08 12:57:06,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [55] prepared (58.254 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:06,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:57:06,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:06,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:57:06,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/55/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/f/e/0/dataset_fe0b64b6-902f-4977-8798-262c37a20773.dat' > input &&   samtools fastq     -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger --i1 i1.fastqsanger --i2 i2.fastqsanger --index-format n2i2 --barcode-tag BC  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons && ln -s i1.fastqsanger i1 && ln -s i2.fastqsanger i2]
galaxy.jobs.runners DEBUG 2025-03-08 12:57:07,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/55/working/forward" -a -f "/galaxy/server/database/objects/9/7/4/dataset_97491b1a-baef-433f-9748-b13c867c9a1b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/55/working/forward" "/galaxy/server/database/objects/9/7/4/dataset_97491b1a-baef-433f-9748-b13c867c9a1b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/55/working/reverse" -a -f "/galaxy/server/database/objects/9/f/1/dataset_9f1d8658-0cc5-4e3c-ac9c-c6832861e863.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/55/working/reverse" "/galaxy/server/database/objects/9/f/1/dataset_9f1d8658-0cc5-4e3c-ac9c-c6832861e863.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/55/working/singletons" -a -f "/galaxy/server/database/objects/7/6/9/dataset_7697ca80-8c7a-4475-ba41-1ee4f2cf519f.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/55/working/singletons" "/galaxy/server/database/objects/7/6/9/dataset_7697ca80-8c7a-4475-ba41-1ee4f2cf519f.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/55/working/output" -a -f "/galaxy/server/database/objects/1/e/a/dataset_1eaf3d9d-8844-4969-a55e-5290bd513ba6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/55/working/output" "/galaxy/server/database/objects/1/e/a/dataset_1eaf3d9d-8844-4969-a55e-5290bd513ba6.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/55/working/i1" -a -f "/galaxy/server/database/objects/e/5/e/dataset_e5e371b0-9daf-4287-8777-5c789319ae93.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/55/working/i1" "/galaxy/server/database/objects/e/5/e/dataset_e5e371b0-9daf-4287-8777-5c789319ae93.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/55/working/i2" -a -f "/galaxy/server/database/objects/7/3/b/dataset_73b1faeb-54c7-40e6-93b6-0d88fbefdd82.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/55/working/i2" "/galaxy/server/database/objects/7/3/b/dataset_73b1faeb-54c7-40e6-93b6-0d88fbefdd82.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:07,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:07,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:57:07,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:07,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:07,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:07,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:11,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pwksw with k8s id: gxy-pwksw succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:57:11,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:57:18,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 55 finished
galaxy.model.metadata DEBUG 2025-03-08 12:57:19,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 89
galaxy.model.metadata DEBUG 2025-03-08 12:57:19,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 90
galaxy.model.metadata DEBUG 2025-03-08 12:57:19,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 91
galaxy.model.metadata DEBUG 2025-03-08 12:57:19,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 92
galaxy.model.metadata DEBUG 2025-03-08 12:57:19,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 93
galaxy.model.metadata DEBUG 2025-03-08 12:57:19,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 94
galaxy.util WARNING 2025-03-08 12:57:19,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/7/4/dataset_97491b1a-baef-433f-9748-b13c867c9a1b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/7/4/dataset_97491b1a-baef-433f-9748-b13c867c9a1b.dat'
galaxy.util WARNING 2025-03-08 12:57:19,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/f/1/dataset_9f1d8658-0cc5-4e3c-ac9c-c6832861e863.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/f/1/dataset_9f1d8658-0cc5-4e3c-ac9c-c6832861e863.dat'
galaxy.util WARNING 2025-03-08 12:57:19,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/6/9/dataset_7697ca80-8c7a-4475-ba41-1ee4f2cf519f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/6/9/dataset_7697ca80-8c7a-4475-ba41-1ee4f2cf519f.dat'
galaxy.util WARNING 2025-03-08 12:57:19,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/e/a/dataset_1eaf3d9d-8844-4969-a55e-5290bd513ba6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/e/a/dataset_1eaf3d9d-8844-4969-a55e-5290bd513ba6.dat'
galaxy.util WARNING 2025-03-08 12:57:19,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/5/e/dataset_e5e371b0-9daf-4287-8777-5c789319ae93.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/5/e/dataset_e5e371b0-9daf-4287-8777-5c789319ae93.dat'
galaxy.util WARNING 2025-03-08 12:57:19,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/3/b/dataset_73b1faeb-54c7-40e6-93b6-0d88fbefdd82.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/3/b/dataset_73b1faeb-54c7-40e6-93b6-0d88fbefdd82.dat'
galaxy.jobs INFO 2025-03-08 12:57:19,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-03-08 12:57:19,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 55 executed (258.599 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:19,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:57:22,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56
tpv.core.entities DEBUG 2025-03-08 12:57:22,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:57:22,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:57:22,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:57:22,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:57:22,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-03-08 12:57:22,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (32.495 ms)
galaxy.jobs.handler INFO 2025-03-08 12:57:22,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:22,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 56
galaxy.jobs DEBUG 2025-03-08 12:57:22,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [56] prepared (62.223 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:57:22,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/56/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/56/configs/tmpalexlibj']
galaxy.jobs.runners DEBUG 2025-03-08 12:57:22,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:22,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:22,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:22,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:31,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hrhwn with k8s id: gxy-hrhwn succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:57:31,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 56: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:57:38,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 56 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:57:39,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.010.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/56/working/data_fetch_upload_29g3itq0', 'object_id': 95}]}]}]
galaxy.jobs INFO 2025-03-08 12:57:39,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 56 in /galaxy/server/database/jobs_directory/000/56
galaxy.jobs DEBUG 2025-03-08 12:57:39,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 56 executed (125.951 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:39,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:57:39,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 57
tpv.core.entities DEBUG 2025-03-08 12:57:39,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:57:39,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:57:39,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:57:39,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:57:39,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-03-08 12:57:39,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (47.219 ms)
galaxy.jobs.handler INFO 2025-03-08 12:57:39,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:39,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 57
galaxy.jobs DEBUG 2025-03-08 12:57:39,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [57] prepared (55.040 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:39,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:57:39,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:39,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:57:39,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/57/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/6/0/d/dataset_60d2d0c4-52cc-4cb7-95cb-01d9dc003001.dat' > input &&   samtools fastq -O    -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger --i1 i1.fastqsanger --i2 i2.fastqsanger --index-format n2i2 --barcode-tag BC  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons && ln -s i1.fastqsanger i1 && ln -s i2.fastqsanger i2]
galaxy.jobs.runners DEBUG 2025-03-08 12:57:39,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/57/working/forward" -a -f "/galaxy/server/database/objects/4/5/1/dataset_4515dbeb-7b7a-42e9-9d3a-c236f0a481e2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/57/working/forward" "/galaxy/server/database/objects/4/5/1/dataset_4515dbeb-7b7a-42e9-9d3a-c236f0a481e2.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/57/working/reverse" -a -f "/galaxy/server/database/objects/2/f/d/dataset_2fd46107-1e8e-401a-b9ce-94f8619d6749.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/57/working/reverse" "/galaxy/server/database/objects/2/f/d/dataset_2fd46107-1e8e-401a-b9ce-94f8619d6749.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/57/working/singletons" -a -f "/galaxy/server/database/objects/5/8/0/dataset_580888e1-c6e7-4505-bad6-53a906b9a923.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/57/working/singletons" "/galaxy/server/database/objects/5/8/0/dataset_580888e1-c6e7-4505-bad6-53a906b9a923.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/57/working/output" -a -f "/galaxy/server/database/objects/f/d/2/dataset_fd2f77eb-bb91-4b7a-90c8-961ac54719e1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/57/working/output" "/galaxy/server/database/objects/f/d/2/dataset_fd2f77eb-bb91-4b7a-90c8-961ac54719e1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/57/working/i1" -a -f "/galaxy/server/database/objects/c/7/3/dataset_c7312fee-878b-4a44-a7b5-81a5b6b080f0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/57/working/i1" "/galaxy/server/database/objects/c/7/3/dataset_c7312fee-878b-4a44-a7b5-81a5b6b080f0.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/57/working/i2" -a -f "/galaxy/server/database/objects/e/a/1/dataset_ea1d42d5-f304-4317-9d07-3c92ce7a316a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/57/working/i2" "/galaxy/server/database/objects/e/a/1/dataset_ea1d42d5-f304-4317-9d07-3c92ce7a316a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:39,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:39,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:57:39,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:57:39,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:39,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:40,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gdswr failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gdswr.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 12:57:43,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-gdswr

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-gdswr": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (57/gxy-gdswr) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (57/gxy-gdswr) tool_stderr: [M::bam2fq_mainloop] discarded 2 singletons
[M::bam2fq_mainloop] processed 36 reads

galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (57/gxy-gdswr) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (57/gxy-gdswr) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gdswr.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 57 (gxy-gdswr)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-gdswr to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:43,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:44,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (57/gxy-gdswr) Terminated at user's request
galaxy.util WARNING 2025-03-08 12:57:44,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/5/1/dataset_4515dbeb-7b7a-42e9-9d3a-c236f0a481e2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/5/1/dataset_4515dbeb-7b7a-42e9-9d3a-c236f0a481e2.dat'
galaxy.util WARNING 2025-03-08 12:57:44,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/f/d/dataset_2fd46107-1e8e-401a-b9ce-94f8619d6749.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/f/d/dataset_2fd46107-1e8e-401a-b9ce-94f8619d6749.dat'
galaxy.util WARNING 2025-03-08 12:57:44,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/8/0/dataset_580888e1-c6e7-4505-bad6-53a906b9a923.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/8/0/dataset_580888e1-c6e7-4505-bad6-53a906b9a923.dat'
galaxy.util WARNING 2025-03-08 12:57:44,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/d/2/dataset_fd2f77eb-bb91-4b7a-90c8-961ac54719e1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/d/2/dataset_fd2f77eb-bb91-4b7a-90c8-961ac54719e1.dat'
galaxy.util WARNING 2025-03-08 12:57:44,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/7/3/dataset_c7312fee-878b-4a44-a7b5-81a5b6b080f0.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/7/3/dataset_c7312fee-878b-4a44-a7b5-81a5b6b080f0.dat'
galaxy.util WARNING 2025-03-08 12:57:44,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/a/1/dataset_ea1d42d5-f304-4317-9d07-3c92ce7a316a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/a/1/dataset_ea1d42d5-f304-4317-9d07-3c92ce7a316a.dat'
galaxy.jobs.handler DEBUG 2025-03-08 12:57:45,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58
tpv.core.entities DEBUG 2025-03-08 12:57:45,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:57:45,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:57:45,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:57:45,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:57:45,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-03-08 12:57:45,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (34.159 ms)
galaxy.jobs.handler INFO 2025-03-08 12:57:45,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:45,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-03-08 12:57:45,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [58] prepared (54.888 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:57:45,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/58/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/58/configs/tmpkwx3npw7']
galaxy.jobs.runners DEBUG 2025-03-08 12:57:45,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:46,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:46,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:46,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:57:56,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-slgq9 with k8s id: gxy-slgq9 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:57:56,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:58:03,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 58 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:58:03,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.010.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/58/working/data_fetch_upload_wv6xlyon', 'object_id': 102}]}]}]
galaxy.jobs INFO 2025-03-08 12:58:03,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.jobs DEBUG 2025-03-08 12:58:03,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 58 executed (110.445 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:03,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:58:04,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-03-08 12:58:04,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:58:04,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:58:04,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:58:04,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:58:04,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-03-08 12:58:04,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (45.316 ms)
galaxy.jobs.handler INFO 2025-03-08 12:58:04,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:04,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-03-08 12:58:04,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [59] prepared (55.190 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:04,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:58:04,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:04,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:58:04,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/59/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/3/8/7/dataset_38791235-1cb4-4d8e-af44-86dea14d092b.dat' > input &&   samtools fastq -O    -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger --i1 i1.fastqsanger --i2 i2.fastqsanger --index-format n2i2 --barcode-tag BC  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons && ln -s i1.fastqsanger i1 && ln -s i2.fastqsanger i2]
galaxy.jobs.runners DEBUG 2025-03-08 12:58:04,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/59/working/forward" -a -f "/galaxy/server/database/objects/2/4/4/dataset_24404453-5a36-45b2-8edb-2738161950ad.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/59/working/forward" "/galaxy/server/database/objects/2/4/4/dataset_24404453-5a36-45b2-8edb-2738161950ad.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/59/working/reverse" -a -f "/galaxy/server/database/objects/a/4/9/dataset_a491bbd2-29b9-494e-ad54-08f923f8be70.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/59/working/reverse" "/galaxy/server/database/objects/a/4/9/dataset_a491bbd2-29b9-494e-ad54-08f923f8be70.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/59/working/singletons" -a -f "/galaxy/server/database/objects/1/7/b/dataset_17b2be57-c01e-42b8-8396-811b25b3889d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/59/working/singletons" "/galaxy/server/database/objects/1/7/b/dataset_17b2be57-c01e-42b8-8396-811b25b3889d.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/59/working/output" -a -f "/galaxy/server/database/objects/a/d/1/dataset_ad1c7f6b-75e8-4ca9-abff-1bc49f8eed4b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/59/working/output" "/galaxy/server/database/objects/a/d/1/dataset_ad1c7f6b-75e8-4ca9-abff-1bc49f8eed4b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/59/working/i1" -a -f "/galaxy/server/database/objects/a/d/b/dataset_adb38a3d-17d6-44f5-80a0-44b7deddb56d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/59/working/i1" "/galaxy/server/database/objects/a/d/b/dataset_adb38a3d-17d6-44f5-80a0-44b7deddb56d.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/59/working/i2" -a -f "/galaxy/server/database/objects/f/0/8/dataset_f086be9d-688c-4868-9f0a-d7f14343ba4e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/59/working/i2" "/galaxy/server/database/objects/f/0/8/dataset_f086be9d-688c-4868-9f0a-d7f14343ba4e.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:04,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:04,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:58:04,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:04,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:04,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:05,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:09,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9j25r with k8s id: gxy-9j25r succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:58:09,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:58:16,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 59 finished
galaxy.model.metadata DEBUG 2025-03-08 12:58:17,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 103
galaxy.model.metadata DEBUG 2025-03-08 12:58:17,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 104
galaxy.model.metadata DEBUG 2025-03-08 12:58:17,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 105
galaxy.model.metadata DEBUG 2025-03-08 12:58:17,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 106
galaxy.model.metadata DEBUG 2025-03-08 12:58:17,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 107
galaxy.model.metadata DEBUG 2025-03-08 12:58:17,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 108
galaxy.util WARNING 2025-03-08 12:58:17,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/4/4/dataset_24404453-5a36-45b2-8edb-2738161950ad.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/4/4/dataset_24404453-5a36-45b2-8edb-2738161950ad.dat'
galaxy.util WARNING 2025-03-08 12:58:17,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/4/9/dataset_a491bbd2-29b9-494e-ad54-08f923f8be70.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/4/9/dataset_a491bbd2-29b9-494e-ad54-08f923f8be70.dat'
galaxy.util WARNING 2025-03-08 12:58:17,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/7/b/dataset_17b2be57-c01e-42b8-8396-811b25b3889d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/7/b/dataset_17b2be57-c01e-42b8-8396-811b25b3889d.dat'
galaxy.util WARNING 2025-03-08 12:58:17,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/d/1/dataset_ad1c7f6b-75e8-4ca9-abff-1bc49f8eed4b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/d/1/dataset_ad1c7f6b-75e8-4ca9-abff-1bc49f8eed4b.dat'
galaxy.util WARNING 2025-03-08 12:58:17,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/d/b/dataset_adb38a3d-17d6-44f5-80a0-44b7deddb56d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/d/b/dataset_adb38a3d-17d6-44f5-80a0-44b7deddb56d.dat'
galaxy.util WARNING 2025-03-08 12:58:17,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/0/8/dataset_f086be9d-688c-4868-9f0a-d7f14343ba4e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/0/8/dataset_f086be9d-688c-4868-9f0a-d7f14343ba4e.dat'
galaxy.jobs INFO 2025-03-08 12:58:17,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-03-08 12:58:17,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 59 executed (314.995 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:17,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:58:19,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-03-08 12:58:19,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:58:19,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:58:19,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:58:19,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:58:19,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-03-08 12:58:19,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (34.622 ms)
galaxy.jobs.handler INFO 2025-03-08 12:58:19,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:19,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-03-08 12:58:19,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [60] prepared (58.395 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:58:19,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/60/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/60/configs/tmpgczjpkvd']
galaxy.jobs.runners DEBUG 2025-03-08 12:58:19,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:19,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:19,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:20,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:29,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c9dk2 with k8s id: gxy-c9dk2 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:58:29,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:58:36,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 60 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:58:36,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.005.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/60/working/data_fetch_upload_4r1frjgq', 'object_id': 109}]}]}]
galaxy.jobs INFO 2025-03-08 12:58:36,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-03-08 12:58:37,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 60 executed (121.248 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:37,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:58:37,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-03-08 12:58:38,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:58:38,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:58:38,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:58:38,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:58:38,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-03-08 12:58:38,070 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (48.139 ms)
galaxy.jobs.handler INFO 2025-03-08 12:58:38,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:38,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-03-08 12:58:38,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [61] prepared (70.354 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:38,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:58:38,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:38,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:58:38,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/61/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/5/8/d/dataset_58de55b7-8742-46ba-ac23-bf3263491a2a.dat' > input &&   samtools fastq    -N -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger --i1 i1.fastqsanger --i2 i2.fastqsanger --index-format n*i* --barcode-tag BC  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons && ln -s i1.fastqsanger i1 && ln -s i2.fastqsanger i2]
galaxy.jobs.runners DEBUG 2025-03-08 12:58:38,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/61/working/forward" -a -f "/galaxy/server/database/objects/c/1/4/dataset_c1404f10-3c10-4138-a475-64837cfc8e77.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/61/working/forward" "/galaxy/server/database/objects/c/1/4/dataset_c1404f10-3c10-4138-a475-64837cfc8e77.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/61/working/reverse" -a -f "/galaxy/server/database/objects/d/f/0/dataset_df0526c5-2e65-436f-9c73-dbf2f08bc2bd.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/61/working/reverse" "/galaxy/server/database/objects/d/f/0/dataset_df0526c5-2e65-436f-9c73-dbf2f08bc2bd.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/61/working/singletons" -a -f "/galaxy/server/database/objects/f/5/d/dataset_f5da03e5-c097-4296-8392-4c9b18e32f6b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/61/working/singletons" "/galaxy/server/database/objects/f/5/d/dataset_f5da03e5-c097-4296-8392-4c9b18e32f6b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/61/working/output" -a -f "/galaxy/server/database/objects/3/1/7/dataset_31763674-d3db-402e-969a-ee7935900ccb.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/61/working/output" "/galaxy/server/database/objects/3/1/7/dataset_31763674-d3db-402e-969a-ee7935900ccb.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/61/working/i1" -a -f "/galaxy/server/database/objects/5/6/8/dataset_56820e67-d8ea-4c9c-aff6-db96ebfc4328.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/61/working/i1" "/galaxy/server/database/objects/5/6/8/dataset_56820e67-d8ea-4c9c-aff6-db96ebfc4328.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/61/working/i2" -a -f "/galaxy/server/database/objects/4/d/f/dataset_4dfacd36-6017-4750-9a1c-2e7a85cc2110.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/61/working/i2" "/galaxy/server/database/objects/4/d/f/dataset_4dfacd36-6017-4750-9a1c-2e7a85cc2110.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:38,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:38,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:58:38,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:58:38,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:38,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:39,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:43,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p99k2 with k8s id: gxy-p99k2 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:58:44,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 61: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:58:51,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 61 finished
galaxy.model.metadata DEBUG 2025-03-08 12:58:51,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 110
galaxy.model.metadata DEBUG 2025-03-08 12:58:51,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 111
galaxy.model.metadata DEBUG 2025-03-08 12:58:51,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 112
galaxy.model.metadata DEBUG 2025-03-08 12:58:51,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 113
galaxy.model.metadata DEBUG 2025-03-08 12:58:51,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 114
galaxy.model.metadata DEBUG 2025-03-08 12:58:51,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 115
galaxy.util WARNING 2025-03-08 12:58:51,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/1/4/dataset_c1404f10-3c10-4138-a475-64837cfc8e77.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/1/4/dataset_c1404f10-3c10-4138-a475-64837cfc8e77.dat'
galaxy.util WARNING 2025-03-08 12:58:51,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/f/0/dataset_df0526c5-2e65-436f-9c73-dbf2f08bc2bd.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/f/0/dataset_df0526c5-2e65-436f-9c73-dbf2f08bc2bd.dat'
galaxy.util WARNING 2025-03-08 12:58:51,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/5/d/dataset_f5da03e5-c097-4296-8392-4c9b18e32f6b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/5/d/dataset_f5da03e5-c097-4296-8392-4c9b18e32f6b.dat'
galaxy.util WARNING 2025-03-08 12:58:51,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/1/7/dataset_31763674-d3db-402e-969a-ee7935900ccb.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/1/7/dataset_31763674-d3db-402e-969a-ee7935900ccb.dat'
galaxy.util WARNING 2025-03-08 12:58:51,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/6/8/dataset_56820e67-d8ea-4c9c-aff6-db96ebfc4328.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/6/8/dataset_56820e67-d8ea-4c9c-aff6-db96ebfc4328.dat'
galaxy.util WARNING 2025-03-08 12:58:51,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/d/f/dataset_4dfacd36-6017-4750-9a1c-2e7a85cc2110.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/d/f/dataset_4dfacd36-6017-4750-9a1c-2e7a85cc2110.dat'
galaxy.jobs INFO 2025-03-08 12:58:51,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 61 in /galaxy/server/database/jobs_directory/000/61
galaxy.jobs DEBUG 2025-03-08 12:58:51,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 61 executed (278.644 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:51,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:58:54,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-03-08 12:58:54,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:58:54,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:58:54,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:58:54,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:58:54,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-03-08 12:58:54,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (33.058 ms)
galaxy.jobs.handler INFO 2025-03-08 12:58:54,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:54,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-03-08 12:58:54,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [62] prepared (58.615 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:58:54,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/62/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/62/configs/tmpwfcniz0z']
galaxy.jobs.runners DEBUG 2025-03-08 12:58:54,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:54,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:54,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:58:54,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:04,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j88th with k8s id: gxy-j88th succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:59:04,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:59:11,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 62 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:59:11,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.005.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/62/working/data_fetch_upload_917f7sqa', 'object_id': 116}]}]}]
galaxy.jobs INFO 2025-03-08 12:59:11,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.jobs DEBUG 2025-03-08 12:59:11,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 62 executed (123.203 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:11,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:59:12,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-03-08 12:59:12,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:59:12,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:59:12,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:59:12,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:59:12,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-03-08 12:59:12,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (44.439 ms)
galaxy.jobs.handler INFO 2025-03-08 12:59:12,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:12,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-03-08 12:59:12,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [63] prepared (50.753 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:12,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:59:12,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:12,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:59:12,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/63/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/f/c/7/dataset_fc7c9c98-d3bd-4882-99ba-3cb78c02242a.dat' > input &&   samtools fastq   -t -N -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons]
galaxy.jobs.runners DEBUG 2025-03-08 12:59:12,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/forward" -a -f "/galaxy/server/database/objects/9/8/2/dataset_982dc22d-0522-4c9d-b7ba-e1a0ac26c43a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/forward" "/galaxy/server/database/objects/9/8/2/dataset_982dc22d-0522-4c9d-b7ba-e1a0ac26c43a.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/reverse" -a -f "/galaxy/server/database/objects/2/8/c/dataset_28c24d92-1dad-449a-aed1-891573cbcec6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/reverse" "/galaxy/server/database/objects/2/8/c/dataset_28c24d92-1dad-449a-aed1-891573cbcec6.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/singletons" -a -f "/galaxy/server/database/objects/2/f/6/dataset_2f691ee3-d9bd-4d8c-9712-ce5cf0a4c6ad.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/singletons" "/galaxy/server/database/objects/2/f/6/dataset_2f691ee3-d9bd-4d8c-9712-ce5cf0a4c6ad.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/output" -a -f "/galaxy/server/database/objects/3/5/5/dataset_3555252c-9d27-46ad-ae17-f9fd7c9ec29d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/output" "/galaxy/server/database/objects/3/5/5/dataset_3555252c-9d27-46ad-ae17-f9fd7c9ec29d.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:12,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:12,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:59:12,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:13,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:13,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:14,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s4r8v failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-s4r8v.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 12:59:17,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-s4r8v

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-s4r8v": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-s4r8v) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-s4r8v) tool_stderr: [M::bam2fq_mainloop] discarded 2 singletons
[M::bam2fq_mainloop] processed 36 reads

galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-s4r8v) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-s4r8v) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-s4r8v.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 63 (gxy-s4r8v)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-s4r8v to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:17,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63/gxy-s4r8v) Terminated at user's request
galaxy.util WARNING 2025-03-08 12:59:17,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/8/2/dataset_982dc22d-0522-4c9d-b7ba-e1a0ac26c43a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/8/2/dataset_982dc22d-0522-4c9d-b7ba-e1a0ac26c43a.dat'
galaxy.util WARNING 2025-03-08 12:59:17,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/8/c/dataset_28c24d92-1dad-449a-aed1-891573cbcec6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/8/c/dataset_28c24d92-1dad-449a-aed1-891573cbcec6.dat'
galaxy.util WARNING 2025-03-08 12:59:17,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/f/6/dataset_2f691ee3-d9bd-4d8c-9712-ce5cf0a4c6ad.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/f/6/dataset_2f691ee3-d9bd-4d8c-9712-ce5cf0a4c6ad.dat'
galaxy.util WARNING 2025-03-08 12:59:17,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/5/5/dataset_3555252c-9d27-46ad-ae17-f9fd7c9ec29d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/5/5/dataset_3555252c-9d27-46ad-ae17-f9fd7c9ec29d.dat'
galaxy.jobs.handler DEBUG 2025-03-08 12:59:18,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-03-08 12:59:18,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:59:18,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:59:18,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:59:18,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:59:18,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-03-08 12:59:18,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (37.339 ms)
galaxy.jobs.handler INFO 2025-03-08 12:59:19,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:19,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-03-08 12:59:19,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [64] prepared (61.841 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:59:19,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/64/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/64/configs/tmpomyqx4dy']
galaxy.jobs.runners DEBUG 2025-03-08 12:59:19,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:19,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:19,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:19,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:28,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xt4qw with k8s id: gxy-xt4qw succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:59:28,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:59:35,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 64 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 12:59:35,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.005.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/64/working/data_fetch_upload_6ggd23_i', 'object_id': 121}]}]}]
galaxy.jobs INFO 2025-03-08 12:59:35,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.jobs DEBUG 2025-03-08 12:59:35,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 64 executed (119.295 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:35,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:59:36,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65
tpv.core.entities DEBUG 2025-03-08 12:59:36,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:59:36,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:59:36,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:59:36,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:59:36,535 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-03-08 12:59:36,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (43.080 ms)
galaxy.jobs.handler INFO 2025-03-08 12:59:36,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:36,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-03-08 12:59:36,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [65] prepared (56.361 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:36,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:59:36,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:36,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 12:59:36,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/65/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/5/2/4/dataset_5240f9fe-3fe0-4cad-8a15-2366b1b68db9.dat' > input &&   samtools fastq   -t -T 'MD,ia' -N -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons]
galaxy.jobs.runners DEBUG 2025-03-08 12:59:36,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/65/working/forward" -a -f "/galaxy/server/database/objects/7/4/a/dataset_74a12adc-6e41-41a3-999b-ba1f7b07b5e1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/65/working/forward" "/galaxy/server/database/objects/7/4/a/dataset_74a12adc-6e41-41a3-999b-ba1f7b07b5e1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/65/working/reverse" -a -f "/galaxy/server/database/objects/2/3/8/dataset_238ed745-33f7-42c8-9a65-c88e0278483a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/65/working/reverse" "/galaxy/server/database/objects/2/3/8/dataset_238ed745-33f7-42c8-9a65-c88e0278483a.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/65/working/singletons" -a -f "/galaxy/server/database/objects/8/1/3/dataset_8137a2ce-d6b8-4bcf-a597-6c162e69a5d2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/65/working/singletons" "/galaxy/server/database/objects/8/1/3/dataset_8137a2ce-d6b8-4bcf-a597-6c162e69a5d2.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/65/working/output" -a -f "/galaxy/server/database/objects/2/2/f/dataset_22f2ac79-d138-4541-b004-47a0f5725f86.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/65/working/output" "/galaxy/server/database/objects/2/2/f/dataset_22f2ac79-d138-4541-b004-47a0f5725f86.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:36,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:36,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 12:59:36,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 12:59:36,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:36,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:37,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:41,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-595sw with k8s id: gxy-595sw succeeded
galaxy.jobs.runners DEBUG 2025-03-08 12:59:41,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 12:59:49,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 65 finished
galaxy.model.metadata DEBUG 2025-03-08 12:59:49,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 122
galaxy.model.metadata DEBUG 2025-03-08 12:59:49,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 123
galaxy.model.metadata DEBUG 2025-03-08 12:59:49,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 124
galaxy.model.metadata DEBUG 2025-03-08 12:59:49,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 125
galaxy.util WARNING 2025-03-08 12:59:49,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/4/a/dataset_74a12adc-6e41-41a3-999b-ba1f7b07b5e1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/4/a/dataset_74a12adc-6e41-41a3-999b-ba1f7b07b5e1.dat'
galaxy.util WARNING 2025-03-08 12:59:49,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/3/8/dataset_238ed745-33f7-42c8-9a65-c88e0278483a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/3/8/dataset_238ed745-33f7-42c8-9a65-c88e0278483a.dat'
galaxy.util WARNING 2025-03-08 12:59:49,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/1/3/dataset_8137a2ce-d6b8-4bcf-a597-6c162e69a5d2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/1/3/dataset_8137a2ce-d6b8-4bcf-a597-6c162e69a5d2.dat'
galaxy.util WARNING 2025-03-08 12:59:49,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/2/f/dataset_22f2ac79-d138-4541-b004-47a0f5725f86.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/2/f/dataset_22f2ac79-d138-4541-b004-47a0f5725f86.dat'
galaxy.jobs INFO 2025-03-08 12:59:49,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-03-08 12:59:49,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 65 executed (212.388 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:49,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 12:59:51,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-03-08 12:59:51,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 12:59:51,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 12:59:51,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 12:59:51,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 12:59:51,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-03-08 12:59:51,875 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (34.262 ms)
galaxy.jobs.handler INFO 2025-03-08 12:59:51,878 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:51,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-03-08 12:59:51,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [66] prepared (59.271 ms)
galaxy.jobs.command_factory INFO 2025-03-08 12:59:51,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/66/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/66/configs/tmp5uo7nuwt']
galaxy.jobs.runners DEBUG 2025-03-08 12:59:51,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:51,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:52,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 12:59:52,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:01,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-62h9v with k8s id: gxy-62h9v succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:00:01,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:00:09,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 66 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:00:09,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.005.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/66/working/data_fetch_upload_4uqpzitg', 'object_id': 126}]}]}]
galaxy.jobs INFO 2025-03-08 13:00:09,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.jobs DEBUG 2025-03-08 13:00:09,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 66 executed (124.215 ms)
galaxy.jobs.handler DEBUG 2025-03-08 13:00:10,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67
tpv.core.entities DEBUG 2025-03-08 13:00:10,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:00:10,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:00:10,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:00:10,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:00:10,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-03-08 13:00:10,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (52.402 ms)
galaxy.jobs.handler INFO 2025-03-08 13:00:10,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:10,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 67
galaxy.jobs DEBUG 2025-03-08 13:00:10,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [67] prepared (65.901 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:10,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:00:10,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:10,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:00:10,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/67/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/c/b/8/dataset_cb8322c9-af52-466e-989e-72c27deb8664.dat' > input &&   samtools fastq  -i -t -T 'MD,ia' -N -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons]
galaxy.jobs.runners DEBUG 2025-03-08 13:00:10,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/67/working/forward" -a -f "/galaxy/server/database/objects/4/f/9/dataset_4f9fadcd-4c13-4218-9943-6f29d789339d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/67/working/forward" "/galaxy/server/database/objects/4/f/9/dataset_4f9fadcd-4c13-4218-9943-6f29d789339d.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/67/working/reverse" -a -f "/galaxy/server/database/objects/8/2/4/dataset_8249eeb6-ee4f-45aa-a027-1d410bd35d97.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/67/working/reverse" "/galaxy/server/database/objects/8/2/4/dataset_8249eeb6-ee4f-45aa-a027-1d410bd35d97.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/67/working/singletons" -a -f "/galaxy/server/database/objects/b/8/f/dataset_b8f8cc20-6988-4b90-b27d-6791d7b31d54.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/67/working/singletons" "/galaxy/server/database/objects/b/8/f/dataset_b8f8cc20-6988-4b90-b27d-6791d7b31d54.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/67/working/output" -a -f "/galaxy/server/database/objects/9/2/0/dataset_920bdee0-233e-4c1e-a342-a2780db35c1e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/67/working/output" "/galaxy/server/database/objects/9/2/0/dataset_920bdee0-233e-4c1e-a342-a2780db35c1e.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:10,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:10,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:00:10,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:10,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:10,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:10,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:13,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:16,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vgqkr with k8s id: gxy-vgqkr succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:00:16,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:00:24,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 67 finished
galaxy.model.metadata DEBUG 2025-03-08 13:00:24,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 127
galaxy.model.metadata DEBUG 2025-03-08 13:00:24,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 128
galaxy.model.metadata DEBUG 2025-03-08 13:00:24,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 129
galaxy.model.metadata DEBUG 2025-03-08 13:00:24,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 130
galaxy.util WARNING 2025-03-08 13:00:24,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/f/9/dataset_4f9fadcd-4c13-4218-9943-6f29d789339d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/f/9/dataset_4f9fadcd-4c13-4218-9943-6f29d789339d.dat'
galaxy.util WARNING 2025-03-08 13:00:24,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/2/4/dataset_8249eeb6-ee4f-45aa-a027-1d410bd35d97.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/2/4/dataset_8249eeb6-ee4f-45aa-a027-1d410bd35d97.dat'
galaxy.util WARNING 2025-03-08 13:00:24,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/8/f/dataset_b8f8cc20-6988-4b90-b27d-6791d7b31d54.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/8/f/dataset_b8f8cc20-6988-4b90-b27d-6791d7b31d54.dat'
galaxy.util WARNING 2025-03-08 13:00:24,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/2/0/dataset_920bdee0-233e-4c1e-a342-a2780db35c1e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/2/0/dataset_920bdee0-233e-4c1e-a342-a2780db35c1e.dat'
galaxy.jobs INFO 2025-03-08 13:00:24,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs DEBUG 2025-03-08 13:00:24,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 67 executed (217.824 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:24,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:00:25,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 68
tpv.core.entities DEBUG 2025-03-08 13:00:25,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:00:25,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:00:25,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:00:25,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:00:25,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-03-08 13:00:25,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (31.609 ms)
galaxy.jobs.handler INFO 2025-03-08 13:00:25,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:25,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-03-08 13:00:25,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [68] prepared (64.788 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:00:25,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/68/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/68/configs/tmpk6x6tadr']
galaxy.jobs.runners DEBUG 2025-03-08 13:00:25,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:25,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:25,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:26,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:35,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m5sjn with k8s id: gxy-m5sjn succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:00:35,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 68: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:00:43,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 68 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:00:43,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam2fq.004.sam', 'dbkey': '?', 'ext': 'sam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/68/working/data_fetch_upload_idf64v4p', 'object_id': 131}]}]}]
galaxy.jobs INFO 2025-03-08 13:00:43,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 68 in /galaxy/server/database/jobs_directory/000/68
galaxy.jobs DEBUG 2025-03-08 13:00:43,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 68 executed (116.088 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:43,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:00:44,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69
tpv.core.entities DEBUG 2025-03-08 13:00:44,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:00:44,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:00:44,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:00:44,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:00:44,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-03-08 13:00:44,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (57.970 ms)
galaxy.jobs.handler INFO 2025-03-08 13:00:44,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:44,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 69
galaxy.jobs DEBUG 2025-03-08 13:00:44,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [69] prepared (63.833 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:44,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:00:44,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:44,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:00:44,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/69/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools sort -@ $addthreads -n '/galaxy/server/database/objects/3/0/b/dataset_30b180b8-388b-4708-a147-0967578ec791.dat' > input &&   samtools fastq  -i   -1 forward.fastqsanger -2 reverse.fastqsanger -s singletons.fastqsanger --i1 i1.fastqsanger --i2 i2.fastqsanger --index-format n2i2 --barcode-tag BC  input  > output.fastqsanger && ln -s output.fastqsanger output && ln -s forward.fastqsanger forward && ln -s reverse.fastqsanger reverse && ln -s singletons.fastqsanger singletons && ln -s i1.fastqsanger i1 && ln -s i2.fastqsanger i2]
galaxy.jobs.runners DEBUG 2025-03-08 13:00:44,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/69/working/forward" -a -f "/galaxy/server/database/objects/7/1/4/dataset_714974cc-66f1-4084-a5cc-99a94e2fd352.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/69/working/forward" "/galaxy/server/database/objects/7/1/4/dataset_714974cc-66f1-4084-a5cc-99a94e2fd352.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/69/working/reverse" -a -f "/galaxy/server/database/objects/1/4/c/dataset_14c33546-309c-4cb4-a05d-ec64c7d257e8.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/69/working/reverse" "/galaxy/server/database/objects/1/4/c/dataset_14c33546-309c-4cb4-a05d-ec64c7d257e8.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/69/working/singletons" -a -f "/galaxy/server/database/objects/f/b/f/dataset_fbfc4a60-c353-49b1-b382-e5aa052e24e1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/69/working/singletons" "/galaxy/server/database/objects/f/b/f/dataset_fbfc4a60-c353-49b1-b382-e5aa052e24e1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/69/working/output" -a -f "/galaxy/server/database/objects/0/8/c/dataset_08c2f758-7df5-4c64-a7e0-6050157a0851.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/69/working/output" "/galaxy/server/database/objects/0/8/c/dataset_08c2f758-7df5-4c64-a7e0-6050157a0851.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/69/working/i1" -a -f "/galaxy/server/database/objects/d/4/8/dataset_d483a4ba-5649-44f5-bec3-8e15307b328c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/69/working/i1" "/galaxy/server/database/objects/d/4/8/dataset_d483a4ba-5649-44f5-bec3-8e15307b328c.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/69/working/i2" -a -f "/galaxy/server/database/objects/2/d/9/dataset_2d905569-a88a-458d-a272-896eacb64dee.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/69/working/i2" "/galaxy/server/database/objects/2/d/9/dataset_2d905569-a88a-458d-a272-896eacb64dee.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:44,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:44,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:00:44,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_fastx/samtools_fastx/1.9+galaxy1: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-08 13:00:44,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:44,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:44,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:48,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-brblg with k8s id: gxy-brblg succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:00:49,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:00:56,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 69 finished
galaxy.model.metadata DEBUG 2025-03-08 13:00:56,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 132
galaxy.model.metadata DEBUG 2025-03-08 13:00:56,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 133
galaxy.model.metadata DEBUG 2025-03-08 13:00:56,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.model.metadata DEBUG 2025-03-08 13:00:56,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 135
galaxy.model.metadata DEBUG 2025-03-08 13:00:56,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 136
galaxy.model.metadata DEBUG 2025-03-08 13:00:56,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 137
galaxy.util WARNING 2025-03-08 13:00:56,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/1/4/dataset_714974cc-66f1-4084-a5cc-99a94e2fd352.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/1/4/dataset_714974cc-66f1-4084-a5cc-99a94e2fd352.dat'
galaxy.util WARNING 2025-03-08 13:00:56,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/4/c/dataset_14c33546-309c-4cb4-a05d-ec64c7d257e8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/4/c/dataset_14c33546-309c-4cb4-a05d-ec64c7d257e8.dat'
galaxy.util WARNING 2025-03-08 13:00:56,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/b/f/dataset_fbfc4a60-c353-49b1-b382-e5aa052e24e1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/b/f/dataset_fbfc4a60-c353-49b1-b382-e5aa052e24e1.dat'
galaxy.util WARNING 2025-03-08 13:00:56,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/8/c/dataset_08c2f758-7df5-4c64-a7e0-6050157a0851.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/8/c/dataset_08c2f758-7df5-4c64-a7e0-6050157a0851.dat'
galaxy.util WARNING 2025-03-08 13:00:56,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/4/8/dataset_d483a4ba-5649-44f5-bec3-8e15307b328c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/4/8/dataset_d483a4ba-5649-44f5-bec3-8e15307b328c.dat'
galaxy.util WARNING 2025-03-08 13:00:56,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/d/9/dataset_2d905569-a88a-458d-a272-896eacb64dee.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/d/9/dataset_2d905569-a88a-458d-a272-896eacb64dee.dat'
galaxy.jobs INFO 2025-03-08 13:00:56,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs DEBUG 2025-03-08 13:00:56,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 69 executed (284.334 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:56,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:00:59,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 70
tpv.core.entities DEBUG 2025-03-08 13:00:59,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:00:59,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:00:59,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:00:59,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:00:59,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-03-08 13:00:59,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (31.181 ms)
galaxy.jobs.handler INFO 2025-03-08 13:00:59,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:59,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 70
galaxy.jobs DEBUG 2025-03-08 13:00:59,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [70] prepared (64.539 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:00:59,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/70/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/70/configs/tmpdlnj96zw']
galaxy.jobs.runners DEBUG 2025-03-08 13:00:59,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:59,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:59,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:00:59,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-08 13:01:00,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71
tpv.core.entities DEBUG 2025-03-08 13:01:00,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:01:00,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:01:00,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:01:00,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:01:00,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-03-08 13:01:00,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (38.774 ms)
galaxy.jobs.handler INFO 2025-03-08 13:01:00,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:00,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-03-08 13:01:00,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [71] prepared (69.219 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:01:00,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/71/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/71/configs/tmpfks7l3dx']
galaxy.jobs.runners DEBUG 2025-03-08 13:01:00,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:00,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:00,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:01,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ncz9j with k8s id: gxy-ncz9j succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jmz2l failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jmz2l.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:01:09,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-jmz2l

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-jmz2l": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners DEBUG 2025-03-08 13:01:09,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (71/gxy-jmz2l) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (71/gxy-jmz2l) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (71/gxy-jmz2l) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (71/gxy-jmz2l) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-jmz2l.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 71 (gxy-jmz2l)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-jmz2l to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:09,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (71/gxy-jmz2l) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-03-08 13:01:16,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 70 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:01:16,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam-slice-input.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/70/working/gxupload_0', 'object_id': 138}]}]}]
galaxy.jobs INFO 2025-03-08 13:01:16,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs DEBUG 2025-03-08 13:01:17,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 70 executed (128.180 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:17,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:01:17,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72
tpv.core.entities DEBUG 2025-03-08 13:01:17,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:01:17,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:01:17,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:01:17,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:01:17,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-03-08 13:01:17,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (34.874 ms)
galaxy.jobs.handler INFO 2025-03-08 13:01:17,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:17,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 72
galaxy.jobs DEBUG 2025-03-08 13:01:17,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [72] prepared (60.664 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:01:17,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/72/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/72/configs/tmp__9_vvwz']
galaxy.jobs.runners DEBUG 2025-03-08 13:01:17,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:18,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:18,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:18,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:28,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jq9lm with k8s id: gxy-jq9lm succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:01:28,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:01:35,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 72 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:01:35,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bam-slice-input.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/72/working/gxupload_0', 'object_id': 140}]}]}]
galaxy.jobs INFO 2025-03-08 13:01:35,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs DEBUG 2025-03-08 13:01:35,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 72 executed (132.324 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:35,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:01:36,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 73
tpv.core.entities DEBUG 2025-03-08 13:01:36,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:01:36,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:01:36,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:01:36,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:01:36,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-03-08 13:01:36,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (35.236 ms)
galaxy.jobs.handler INFO 2025-03-08 13:01:36,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:36,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-03-08 13:01:36,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [73] prepared (54.409 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:01:36,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:01:36,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_slice_bam/samtools_slice_bam/2.0.3: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-03-08 13:01:36,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:01:36,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/73/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/a/f/0/dataset_af0f2aaa-daea-4467-a543-c3b38c357564.dat' temp_input.bam && ln -s '/galaxy/server/database/objects/_metadata_files/7/f/6/metadata_7f6abd52-6f17-4049-9dd0-1bd298e6476f.dat' temp_input.bam.bai &&  samtools view -@ ${GALAXY_SLOTS:-1} -b -o unsorted_output.bam temp_input.bam "chrM" &&  samtools sort -O bam -T "${TMPDIR:-.}" -@ ${GALAXY_SLOTS:-1} -o '/galaxy/server/database/objects/2/1/c/dataset_21c779c4-e5d3-4d65-a953-e5cbac275a8c.dat' unsorted_output.bam]
galaxy.jobs.runners DEBUG 2025-03-08 13:01:36,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:36,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:01:36,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:01:36,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_slice_bam/samtools_slice_bam/2.0.3: samtools:1.20
galaxy.tool_util.deps.containers INFO 2025-03-08 13:01:36,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.20--h50ea8bc_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:36,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:37,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:44,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-l7n2c with k8s id: gxy-l7n2c succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:01:44,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:01:51,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 73 finished
galaxy.model.metadata DEBUG 2025-03-08 13:01:51,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.jobs INFO 2025-03-08 13:01:51,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs DEBUG 2025-03-08 13:01:51,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 73 executed (114.808 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:51,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:01:53,589 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-03-08 13:01:53,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:01:53,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:01:53,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:01:53,633 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:01:53,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-03-08 13:01:53,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (37.807 ms)
galaxy.jobs.handler INFO 2025-03-08 13:01:53,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:53,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-03-08 13:01:53,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [74] prepared (63.705 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:01:53,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/74/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/74/configs/tmp6fcwex2e']
galaxy.jobs.runners DEBUG 2025-03-08 13:01:53,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:53,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:53,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:01:54,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bwvkg failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-bwvkg.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:02:03,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-bwvkg

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-bwvkg": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (74/gxy-bwvkg) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (74/gxy-bwvkg) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (74/gxy-bwvkg) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (74/gxy-bwvkg) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-bwvkg.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 74 (gxy-bwvkg)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-bwvkg to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:03,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:04,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (74/gxy-bwvkg) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-08 13:02:05,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 75
tpv.core.entities DEBUG 2025-03-08 13:02:05,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:02:05,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:02:05,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:02:05,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:02:05,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-03-08 13:02:05,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (30.248 ms)
galaxy.jobs.handler INFO 2025-03-08 13:02:05,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:05,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 75
galaxy.jobs DEBUG 2025-03-08 13:02:06,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [75] prepared (70.128 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:02:06,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/75/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/75/configs/tmpmev5be2f']
galaxy.jobs.runners DEBUG 2025-03-08 13:02:06,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:06,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:06,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:06,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:16,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jsl5s with k8s id: gxy-jsl5s succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:02:16,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:02:23,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 75 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:02:23,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/75/working/data_fetch_upload_ka2u11dr', 'object_id': 143}]}]}]
galaxy.jobs INFO 2025-03-08 13:02:23,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-03-08 13:02:23,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 75 executed (123.608 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:23,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:02:24,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76
tpv.core.entities DEBUG 2025-03-08 13:02:24,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:02:24,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:02:24,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:02:24,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:02:24,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-03-08 13:02:24,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (36.766 ms)
galaxy.jobs.handler INFO 2025-03-08 13:02:24,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:24,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-03-08 13:02:24,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [76] prepared (73.694 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:02:24,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:02:24,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_counts/bcftools_plugin_counts/1.15.1+galaxy4: mulled-v2-4046a3b419fc1d57d709b7a143645d599908d9e4:611c5ed00120482fe681399c9f103740803fdcb2
galaxy.tool_util.deps.containers INFO 2025-03-08 13:02:24,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-4046a3b419fc1d57d709b7a143645d599908d9e4:611c5ed00120482fe681399c9f103740803fdcb2-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:02:24,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/76/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/7/5/8/dataset_758b3378-9c03-4c26-b433-0762f9eff264.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools plugin counts                 input.vcf.gz | tee counts_file.txt && sed 's/^.* \([a-zA-Z]*\):[ ]*\([0-9]*\)$/\1 \2/' counts_file.txt | python /galaxy/server/database/jobs_directory/000/76/configs/tmppsgu1scf > '/galaxy/server/database/objects/c/d/3/dataset_cd3c18ac-f25b-4f91-acb4-b5fc4a44af85.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:02:24,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:24,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:02:24,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:02:24,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_counts/bcftools_plugin_counts/1.15.1+galaxy4: mulled-v2-4046a3b419fc1d57d709b7a143645d599908d9e4:611c5ed00120482fe681399c9f103740803fdcb2
galaxy.tool_util.deps.containers INFO 2025-03-08 13:02:24,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-4046a3b419fc1d57d709b7a143645d599908d9e4:611c5ed00120482fe681399c9f103740803fdcb2-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:24,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:25,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:39,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z26q4 with k8s id: gxy-z26q4 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:02:39,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:02:46,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 76 finished
galaxy.model.metadata DEBUG 2025-03-08 13:02:46,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 144
galaxy.jobs INFO 2025-03-08 13:02:46,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.jobs DEBUG 2025-03-08 13:02:46,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 76 executed (99.648 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:46,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:02:49,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-03-08 13:02:49,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:02:49,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:02:49,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:02:49,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:02:49,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-03-08 13:02:49,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (32.651 ms)
galaxy.jobs.handler INFO 2025-03-08 13:02:49,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:49,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-03-08 13:02:49,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [77] prepared (64.448 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:02:49,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/77/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/77/configs/tmpwjaz6vxs']
galaxy.jobs.runners DEBUG 2025-03-08 13:02:49,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:49,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:49,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:50,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:02:59,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vpb2j with k8s id: gxy-vpb2j succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:03:00,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:03:07,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 77 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:03:07,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/77/working/data_fetch_upload_94q1pihf', 'object_id': 145}]}]}]
galaxy.jobs INFO 2025-03-08 13:03:07,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-03-08 13:03:07,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 77 executed (153.177 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:07,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:03:08,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 78
tpv.core.entities DEBUG 2025-03-08 13:03:08,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:03:08,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:03:08,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:03:08,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:03:08,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-03-08 13:03:08,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (41.376 ms)
galaxy.jobs.handler INFO 2025-03-08 13:03:08,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:08,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 78
galaxy.jobs DEBUG 2025-03-08 13:03:08,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [78] prepared (74.156 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:08,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:03:08,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:08,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:03:08,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/78/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/4/e/5/dataset_4e59cfe5-fb1a-4b0c-a05d-5a1c9a9616fc.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view     --samples 'NA00002'     --min-ac 1 --max-ac 1 --types "snps"                     --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/9/a/9/dataset_9a93205c-b811-4c59-8adb-49adf2bff2e9.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:03:08,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:08,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:08,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:03:08,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:08,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:08,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:09,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:19,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fvdhq with k8s id: gxy-fvdhq succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:03:19,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:03:26,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 78 finished
galaxy.model.metadata DEBUG 2025-03-08 13:03:26,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 146
galaxy.jobs INFO 2025-03-08 13:03:26,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.jobs DEBUG 2025-03-08 13:03:26,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 78 executed (114.002 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:26,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:03:27,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79
tpv.core.entities DEBUG 2025-03-08 13:03:27,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:03:27,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:03:27,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:03:27,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:03:27,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-03-08 13:03:27,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (31.649 ms)
galaxy.jobs.handler INFO 2025-03-08 13:03:27,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:27,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-03-08 13:03:27,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [79] prepared (73.768 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:03:27,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/79/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/79/configs/tmp8qiqak64']
galaxy.jobs.runners DEBUG 2025-03-08 13:03:27,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:27,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:27,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:28,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:37,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dmhpb with k8s id: gxy-dmhpb succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:03:37,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:03:44,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 79 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:03:44,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.bcf', 'dbkey': '?', 'ext': 'bcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/79/working/gxupload_0', 'object_id': 147}]}]}]
galaxy.jobs INFO 2025-03-08 13:03:45,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-03-08 13:03:45,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 79 executed (366.447 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:45,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:03:45,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 80
tpv.core.entities DEBUG 2025-03-08 13:03:45,897 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:03:45,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:03:45,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:03:45,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:03:45,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-03-08 13:03:45,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (34.439 ms)
galaxy.jobs.handler INFO 2025-03-08 13:03:45,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:45,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 80
galaxy.jobs DEBUG 2025-03-08 13:03:46,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [80] prepared (49.937 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:46,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:03:46,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:46,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:03:46,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/80/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     ln -s '/galaxy/server/database/objects/5/3/d/dataset_53df519e-a36d-4cfb-b4f4-81fa7ac38d45.dat' input.bcf && ln -s '/galaxy/server/database/objects/_metadata_files/e/f/c/metadata_efcbc140-57da-4aca-b992-afceb94a99ac.dat' input.bcf.csi &&            bcftools view     --samples 'NA00002'     --min-ac 1 --max-ac 1 --types "snps"                     --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.bcf  > '/galaxy/server/database/objects/4/6/6/dataset_4661e122-ef7d-4fd0-8570-93df8ab44942.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:03:46,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:46,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:46,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:03:46,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:03:46,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:46,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:46,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:50,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-95bht with k8s id: gxy-95bht succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:03:50,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:03:58,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 80 finished
galaxy.model.metadata DEBUG 2025-03-08 13:03:58,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 148
galaxy.jobs INFO 2025-03-08 13:03:58,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-03-08 13:03:58,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 80 executed (116.957 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:58,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:03:59,200 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81
tpv.core.entities DEBUG 2025-03-08 13:03:59,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:03:59,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:03:59,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:03:59,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:03:59,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-03-08 13:03:59,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (32.709 ms)
galaxy.jobs.handler INFO 2025-03-08 13:03:59,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:59,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-03-08 13:03:59,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [81] prepared (66.576 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:03:59,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/81/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/81/configs/tmpyc6cvjgl']
galaxy.jobs.runners DEBUG 2025-03-08 13:03:59,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:59,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:59,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:03:59,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:10,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xc6mq with k8s id: gxy-xc6mq succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:04:10,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:04:17,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 81 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:04:17,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/81/working/data_fetch_upload__ftjt6r4', 'object_id': 149}]}]}]
galaxy.jobs INFO 2025-03-08 13:04:17,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-03-08 13:04:17,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 81 executed (136.123 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:17,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:04:18,596 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-03-08 13:04:18,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:04:18,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:04:18,633 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:04:18,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:04:18,664 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-03-08 13:04:18,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (39.940 ms)
galaxy.jobs.handler INFO 2025-03-08 13:04:18,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:18,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-03-08 13:04:18,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [82] prepared (48.384 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:18,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:04:18,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:18,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:04:18,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/82/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/8/3/dataset_b83045d5-ace1-4844-b649-b39bc480572c.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view     --samples 'NA00003'     --novel --exclude-private                 --regions 'Y:20-'          --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/5/c/d/dataset_5cd80502-bfcc-4174-bf44-86757c58df03.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:04:18,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:18,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:18,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:04:18,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:18,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:18,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:19,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:24,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pxflx with k8s id: gxy-pxflx succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:04:24,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 82: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:04:31,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 82 finished
galaxy.model.metadata DEBUG 2025-03-08 13:04:31,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 150
galaxy.jobs INFO 2025-03-08 13:04:31,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 82 in /galaxy/server/database/jobs_directory/000/82
galaxy.jobs DEBUG 2025-03-08 13:04:31,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 82 executed (111.431 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:31,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:04:32,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83
tpv.core.entities DEBUG 2025-03-08 13:04:32,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:04:32,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:04:32,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:04:32,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:04:32,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-03-08 13:04:32,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (28.332 ms)
galaxy.jobs.handler INFO 2025-03-08 13:04:32,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:32,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 83
galaxy.jobs DEBUG 2025-03-08 13:04:33,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [83] prepared (62.159 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:04:33,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/83/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/83/configs/tmp9tq36y9r']
galaxy.jobs.runners DEBUG 2025-03-08 13:04:33,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:33,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:33,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:33,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:42,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fcgmq with k8s id: gxy-fcgmq succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:04:42,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:04:50,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 83 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:04:50,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/83/working/data_fetch_upload_98wtflja', 'object_id': 151}]}]}]
galaxy.jobs INFO 2025-03-08 13:04:50,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.jobs DEBUG 2025-03-08 13:04:50,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 83 executed (146.552 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:50,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:04:51,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 84
tpv.core.entities DEBUG 2025-03-08 13:04:51,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:04:51,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:04:51,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:04:51,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:04:51,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-03-08 13:04:51,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (34.859 ms)
galaxy.jobs.handler INFO 2025-03-08 13:04:51,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:51,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-03-08 13:04:51,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [84] prepared (52.567 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:51,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:04:51,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:51,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:04:51,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/84/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/f/0/1/dataset_f0151ca7-d0e0-438c-95da-9dfbc34042e2.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view     --samples 'NA00003'     --known --exclude-private     --apply-filters 'PASS'             --regions 'Y:20-'          --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/a/e/c/dataset_aecfdf59-e393-4714-b145-6858d4830809.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:04:51,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:51,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:51,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:04:51,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:04:51,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:51,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:52,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:04:56,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-878t9 with k8s id: gxy-878t9 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:04:56,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:05:04,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 84 finished
galaxy.model.metadata DEBUG 2025-03-08 13:05:04,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 152
galaxy.jobs INFO 2025-03-08 13:05:04,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-03-08 13:05:04,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 84 executed (98.242 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:04,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:05:05,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-03-08 13:05:05,697 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:05:05,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:05:05,701 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:05:05,711 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:05:05,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-03-08 13:05:05,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (31.104 ms)
galaxy.jobs.handler INFO 2025-03-08 13:05:05,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:05,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-03-08 13:05:05,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [85] prepared (60.674 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:05:05,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/85/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/85/configs/tmpfi0zwklt']
galaxy.jobs.runners DEBUG 2025-03-08 13:05:05,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:05,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:05,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:06,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:15,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jjg4l with k8s id: gxy-jjg4l succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:05:16,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:05:23,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 85 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:05:23,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/85/working/data_fetch_upload_0rfdowap', 'object_id': 153}]}]}]
galaxy.jobs INFO 2025-03-08 13:05:23,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-03-08 13:05:23,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 85 executed (124.693 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:23,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:05:24,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 86
tpv.core.entities DEBUG 2025-03-08 13:05:24,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:05:24,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:05:24,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:05:24,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:05:24,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-03-08 13:05:24,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (42.827 ms)
galaxy.jobs.handler INFO 2025-03-08 13:05:24,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:24,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 86
galaxy.jobs DEBUG 2025-03-08 13:05:24,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [86] prepared (50.240 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:05:24,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:05:24,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:05:24,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:05:24,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/86/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/c/9/6/dataset_c967d3c1-035a-43c2-ac4c-f637f9b8a2de.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view     --samples 'NA00003'     --private                     --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/3/0/4/dataset_304e99c9-e338-4bd2-a10f-a93445a99fb1.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:05:24,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:24,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:05:24,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:05:24,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:05:24,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:24,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:25,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:30,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lfpsf with k8s id: gxy-lfpsf succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:05:30,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:05:37,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 86 finished
galaxy.model.metadata DEBUG 2025-03-08 13:05:37,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 154
galaxy.jobs INFO 2025-03-08 13:05:37,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.jobs DEBUG 2025-03-08 13:05:37,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 86 executed (96.708 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:37,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:05:38,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87
tpv.core.entities DEBUG 2025-03-08 13:05:38,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:05:38,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:05:38,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:05:38,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:05:38,463 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-03-08 13:05:38,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (29.506 ms)
galaxy.jobs.handler INFO 2025-03-08 13:05:38,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:38,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-03-08 13:05:38,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [87] prepared (57.911 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:05:38,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/87/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/87/configs/tmpsyfp4hi2']
galaxy.jobs.runners DEBUG 2025-03-08 13:05:38,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:38,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:38,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:39,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nvxfl failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-nvxfl.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:05:48,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-nvxfl

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-nvxfl": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87/gxy-nvxfl) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87/gxy-nvxfl) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87/gxy-nvxfl) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87/gxy-nvxfl) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-nvxfl.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 87 (gxy-nvxfl)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-nvxfl to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:48,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87/gxy-nvxfl) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-08 13:05:49,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-03-08 13:05:49,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:05:49,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:05:49,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:05:49,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:05:49,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-03-08 13:05:49,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (37.383 ms)
galaxy.jobs.handler INFO 2025-03-08 13:05:49,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:49,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-03-08 13:05:49,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [88] prepared (51.074 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:05:49,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/88/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/88/configs/tmpdp7k_c30']
galaxy.jobs.runners DEBUG 2025-03-08 13:05:49,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:49,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:49,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:50,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:05:59,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bzxts with k8s id: gxy-bzxts succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:05:59,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:06:07,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 88 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:06:07,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/88/working/data_fetch_upload_mtyzb3wm', 'object_id': 156}]}]}]
galaxy.jobs INFO 2025-03-08 13:06:07,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-03-08 13:06:07,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 88 executed (178.036 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:07,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:06:08,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 89
tpv.core.entities DEBUG 2025-03-08 13:06:08,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:06:08,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:06:08,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:06:08,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:06:08,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-03-08 13:06:08,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (31.475 ms)
galaxy.jobs.handler INFO 2025-03-08 13:06:08,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:08,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 89
galaxy.jobs DEBUG 2025-03-08 13:06:08,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [89] prepared (63.301 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:08,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:06:08,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:08,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:06:08,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/89/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/1/3/dataset_3130d9c0-58ed-4d0b-87a8-480680be1edb.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view         --phased                     --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/c/3/1/dataset_c3194a78-973d-417b-92ab-edd3f2a775c4.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:06:08,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:08,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:08,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:06:08,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:08,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:08,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:09,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:13,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6qgp6 with k8s id: gxy-6qgp6 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:06:14,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:06:21,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 89 finished
galaxy.model.metadata DEBUG 2025-03-08 13:06:21,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 157
galaxy.jobs INFO 2025-03-08 13:06:21,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-03-08 13:06:21,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 89 executed (118.662 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:21,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:06:22,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90
tpv.core.entities DEBUG 2025-03-08 13:06:22,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:06:22,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:06:22,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:06:22,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:06:22,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-03-08 13:06:22,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (38.528 ms)
galaxy.jobs.handler INFO 2025-03-08 13:06:22,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:22,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-03-08 13:06:22,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [90] prepared (71.121 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:06:22,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/90/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/90/configs/tmp8sbmb0td']
galaxy.jobs.runners DEBUG 2025-03-08 13:06:22,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:22,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:22,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:22,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:32,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5xl2w with k8s id: gxy-5xl2w succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:06:32,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:06:39,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 90 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:06:39,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/90/working/data_fetch_upload_7ufenzdw', 'object_id': 158}]}]}]
galaxy.jobs INFO 2025-03-08 13:06:39,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-03-08 13:06:39,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 90 executed (143.665 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:39,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:06:40,766 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-03-08 13:06:40,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:06:40,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:06:40,803 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:06:40,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:06:40,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-03-08 13:06:40,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (37.079 ms)
galaxy.jobs.handler INFO 2025-03-08 13:06:40,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:40,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-03-08 13:06:40,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [91] prepared (47.514 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:40,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:06:40,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:40,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:06:40,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/91/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/8/8/5/dataset_8854c027-606c-49bd-aa48-eda81692c889.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view         --exclude-phased                     --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/2/4/2/dataset_242a7892-e8a0-4099-a454-c3ae815e7c3c.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:06:40,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:40,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:40,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:06:40,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:06:40,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:41,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:42,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:46,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s96w9 with k8s id: gxy-s96w9 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:06:46,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:06:53,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 91 finished
galaxy.model.metadata DEBUG 2025-03-08 13:06:53,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 159
galaxy.jobs INFO 2025-03-08 13:06:53,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-03-08 13:06:54,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 91 executed (108.481 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:54,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:06:55,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92
tpv.core.entities DEBUG 2025-03-08 13:06:55,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:06:55,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:06:55,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:06:55,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:06:55,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-03-08 13:06:55,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (48.989 ms)
galaxy.jobs.handler INFO 2025-03-08 13:06:55,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:55,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 92
galaxy.jobs DEBUG 2025-03-08 13:06:55,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [92] prepared (73.936 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:06:55,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/92/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/92/configs/tmpeggpchhk']
galaxy.jobs.runners DEBUG 2025-03-08 13:06:55,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:55,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:55,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:06:56,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:05,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-srkjd with k8s id: gxy-srkjd succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:07:05,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:07:13,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 92 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:07:13,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/92/working/data_fetch_upload_gs8nfv4g', 'object_id': 160}]}]}]
galaxy.jobs INFO 2025-03-08 13:07:13,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs DEBUG 2025-03-08 13:07:13,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 92 executed (146.412 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:13,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:07:13,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 93
tpv.core.entities DEBUG 2025-03-08 13:07:13,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:07:13,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:07:13,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:07:13,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:07:13,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-03-08 13:07:13,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (35.718 ms)
galaxy.jobs.handler INFO 2025-03-08 13:07:13,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:13,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 93
galaxy.jobs DEBUG 2025-03-08 13:07:13,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [93] prepared (50.987 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:13,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:07:13,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:13,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:07:13,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/93/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/c/1/0/dataset_c10d56b4-2236-449c-9357-acf0984c457f.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view         --min-alleles 2 --max-alleles 2 --min-af 0.3 --max-af 0.7   --header-only                   --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/c/a/c/dataset_cacfcc3b-6fbf-4ac6-9acd-ca3160b12048.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:07:13,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:13,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:13,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:07:13,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:13,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:13,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:14,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:18,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-27w5n with k8s id: gxy-27w5n succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:07:18,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:07:25,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 93 finished
galaxy.model.metadata DEBUG 2025-03-08 13:07:25,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 161
galaxy.jobs INFO 2025-03-08 13:07:25,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs DEBUG 2025-03-08 13:07:26,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 93 executed (114.487 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:26,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:07:26,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94
tpv.core.entities DEBUG 2025-03-08 13:07:26,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:07:26,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:07:26,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:07:26,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:07:26,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-03-08 13:07:26,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (38.512 ms)
galaxy.jobs.handler INFO 2025-03-08 13:07:26,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:26,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-03-08 13:07:27,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [94] prepared (61.943 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:07:27,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/94/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/94/configs/tmpnplai23b']
galaxy.jobs.runners DEBUG 2025-03-08 13:07:27,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:27,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:27,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:27,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:36,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vfbsq with k8s id: gxy-vfbsq succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:07:36,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:07:44,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 94 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:07:44,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/94/working/data_fetch_upload_2odslt18', 'object_id': 162}]}]}]
galaxy.jobs INFO 2025-03-08 13:07:44,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.jobs DEBUG 2025-03-08 13:07:44,597 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 94 executed (137.908 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:44,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:07:45,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95
tpv.core.entities DEBUG 2025-03-08 13:07:45,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:07:45,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:07:45,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:07:45,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:07:45,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-03-08 13:07:45,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (33.268 ms)
galaxy.jobs.handler INFO 2025-03-08 13:07:45,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:45,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 95
galaxy.jobs DEBUG 2025-03-08 13:07:45,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [95] prepared (48.167 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:45,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:07:45,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:45,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:07:45,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/95/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/e/8/b/dataset_e8bf8111-e798-408b-8fa5-d8efede9e9a6.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view         --uncalled   --no-header                   --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/d/1/b/dataset_d1b57ab3-2c82-41fe-a0e1-c4ca03f25cfb.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:07:45,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:45,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:45,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:07:45,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:07:45,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:45,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:45,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:51,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5tq9j with k8s id: gxy-5tq9j succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:07:51,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:07:58,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 95 finished
galaxy.model.metadata DEBUG 2025-03-08 13:07:58,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 163
galaxy.jobs INFO 2025-03-08 13:07:58,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-03-08 13:07:58,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 95 executed (141.855 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:58,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:07:59,610 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 96
tpv.core.entities DEBUG 2025-03-08 13:07:59,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:07:59,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:07:59,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:07:59,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:07:59,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-03-08 13:07:59,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (41.259 ms)
galaxy.jobs.handler INFO 2025-03-08 13:07:59,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:59,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 96
galaxy.jobs DEBUG 2025-03-08 13:07:59,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [96] prepared (74.400 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:07:59,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/96/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/96/configs/tmp76fb3i6y']
galaxy.jobs.runners DEBUG 2025-03-08 13:07:59,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:59,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:07:59,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:00,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:09,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hjm9c with k8s id: gxy-hjm9c succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:08:09,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:08:16,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 96 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:08:16,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/96/working/data_fetch_upload_mcz7_mnf', 'object_id': 164}]}]}]
galaxy.jobs INFO 2025-03-08 13:08:17,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs DEBUG 2025-03-08 13:08:17,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 96 executed (142.396 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:17,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:08:18,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97
tpv.core.entities DEBUG 2025-03-08 13:08:18,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:08:18,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:08:18,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:08:18,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:08:18,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-03-08 13:08:18,097 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (34.579 ms)
galaxy.jobs.handler INFO 2025-03-08 13:08:18,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:18,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-03-08 13:08:18,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [97] prepared (57.901 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:18,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:08:18,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:18,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:08:18,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/97/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/4/b/6/dataset_4b69e1ad-6ccd-4533-a734-a38f26d47731.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view         --exclude-types "snps"  --drop-genotypes                   --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/e/2/b/dataset_e2b14585-248d-4379-abd2-e3e02e3835a9.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:08:18,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:18,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:18,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:08:18,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:18,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:18,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:18,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:22,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dpp9z with k8s id: gxy-dpp9z succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:08:22,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:08:30,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 97 finished
galaxy.model.metadata DEBUG 2025-03-08 13:08:30,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 165
galaxy.jobs INFO 2025-03-08 13:08:30,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs DEBUG 2025-03-08 13:08:30,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 97 executed (114.806 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:30,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:08:32,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98
tpv.core.entities DEBUG 2025-03-08 13:08:32,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:08:32,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:08:32,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:08:32,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:08:32,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-03-08 13:08:32,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (41.382 ms)
galaxy.jobs.handler INFO 2025-03-08 13:08:32,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:32,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 98
galaxy.jobs DEBUG 2025-03-08 13:08:32,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [98] prepared (81.530 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:08:32,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/98/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/98/configs/tmpgfncf3ta']
galaxy.jobs.runners DEBUG 2025-03-08 13:08:32,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:32,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:32,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:33,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:42,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rrbzc with k8s id: gxy-rrbzc succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:08:42,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:08:51,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 98 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:08:51,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/98/working/data_fetch_upload_wd1s48xx', 'object_id': 166}]}]}]
galaxy.jobs INFO 2025-03-08 13:08:51,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs DEBUG 2025-03-08 13:08:51,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 98 executed (173.086 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:51,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:08:52,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 99
tpv.core.entities DEBUG 2025-03-08 13:08:52,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:08:52,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:08:52,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:08:52,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:08:52,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-03-08 13:08:52,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (49.682 ms)
galaxy.jobs.handler INFO 2025-03-08 13:08:52,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:52,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 99
galaxy.jobs DEBUG 2025-03-08 13:08:53,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [99] prepared (69.427 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:53,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:08:53,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:53,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:08:53,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/99/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/f/e/7/dataset_fe79dc30-168b-4834-8aaa-21fd5a86e51a.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view         --genotype "^het" --phased                     --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/e/b/6/dataset_eb652600-714c-4775-a737-b05a474acc1b.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:08:53,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:53,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:53,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:08:53,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:08:53,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:53,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:53,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:08:59,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-frmbd with k8s id: gxy-frmbd succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:08:59,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:09:06,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 99 finished
galaxy.model.metadata DEBUG 2025-03-08 13:09:06,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 167
galaxy.jobs INFO 2025-03-08 13:09:06,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs DEBUG 2025-03-08 13:09:06,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 99 executed (118.600 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:06,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:09:08,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100
tpv.core.entities DEBUG 2025-03-08 13:09:08,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:09:08,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:09:08,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:09:08,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:09:08,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-03-08 13:09:08,289 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (30.787 ms)
galaxy.jobs.handler INFO 2025-03-08 13:09:08,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:08,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-03-08 13:09:08,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [100] prepared (71.667 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:09:08,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/100/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/100/configs/tmptjnhfzt0']
galaxy.jobs.runners DEBUG 2025-03-08 13:09:08,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:08,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:08,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:10,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:19,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cfmsf with k8s id: gxy-cfmsf succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:09:19,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:09:26,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 100 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:09:26,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'view.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/100/working/data_fetch_upload_c8gqcap7', 'object_id': 168}]}]}]
galaxy.jobs INFO 2025-03-08 13:09:27,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-03-08 13:09:27,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 100 executed (133.450 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:27,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:09:27,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 101
tpv.core.entities DEBUG 2025-03-08 13:09:27,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:09:27,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:09:27,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:09:27,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:09:27,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-03-08 13:09:27,719 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (40.421 ms)
galaxy.jobs.handler INFO 2025-03-08 13:09:27,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:27,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 101
galaxy.jobs DEBUG 2025-03-08 13:09:27,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [101] prepared (50.166 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:09:27,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:09:27,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:09:27,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:09:27,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/101/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/0/8/f/dataset_08fbcc13-7abb-4538-96ce-e58e98dc805f.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools view     --samples 'NA00002'     --min-ac 1 --max-ac 1 --types "snps"            --regions-overlap 1          --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/6/9/9/dataset_699a0f9b-8054-45f8-a545-f866b1cd9e11.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:09:27,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:27,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:09:27,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:09:27,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_view/bcftools_view/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-08 13:09:27,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:27,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:28,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:32,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5ghkl with k8s id: gxy-5ghkl succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:09:32,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:09:40,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 101 finished
galaxy.model.metadata DEBUG 2025-03-08 13:09:40,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 169
galaxy.jobs INFO 2025-03-08 13:09:40,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-03-08 13:09:40,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 101 executed (122.611 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:40,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:09:41,966 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 102
tpv.core.entities DEBUG 2025-03-08 13:09:41,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:09:41,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:09:41,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:09:42,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:09:42,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-03-08 13:09:42,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (33.230 ms)
galaxy.jobs.handler INFO 2025-03-08 13:09:42,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:42,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 102
galaxy.jobs DEBUG 2025-03-08 13:09:42,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [102] prepared (58.095 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:09:42,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/102/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/102/configs/tmpiao09sp4']
galaxy.jobs.runners DEBUG 2025-03-08 13:09:42,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:42,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:42,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:42,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-08 13:09:43,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 103
tpv.core.entities DEBUG 2025-03-08 13:09:43,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:09:43,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:09:43,067 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:09:43,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:09:43,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-03-08 13:09:43,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (37.155 ms)
galaxy.jobs.handler INFO 2025-03-08 13:09:43,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:43,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 103
galaxy.jobs DEBUG 2025-03-08 13:09:43,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [103] prepared (62.782 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:09:43,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/103/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/103/configs/tmpcq_r3eev']
galaxy.jobs.runners DEBUG 2025-03-08 13:09:43,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:43,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:43,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:43,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:53,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6nsmr with k8s id: gxy-6nsmr succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:09:53,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 102: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:09:53,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gw98h with k8s id: gxy-gw98h succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:09:53,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:10:01,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 102 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:10:01,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib-test-genome-phix.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/102/working/data_fetch_upload_gz7vntsf', 'object_id': 170}]}]}]
galaxy.jobs INFO 2025-03-08 13:10:01,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 102 in /galaxy/server/database/jobs_directory/000/102
galaxy.jobs DEBUG 2025-03-08 13:10:01,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 102 executed (198.552 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:01,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 13:10:02,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 103 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:10:02,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib-phix.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/103/working/data_fetch_upload_2x4spv6n', 'object_id': 171}]}]}]
galaxy.jobs INFO 2025-03-08 13:10:02,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs DEBUG 2025-03-08 13:10:02,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 103 executed (189.125 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:02,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:10:02,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 104
tpv.core.entities DEBUG 2025-03-08 13:10:02,695 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:10:02,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:10:02,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:10:02,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:10:02,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Working directory for job is: /galaxy/server/database/jobs_directory/000/104
galaxy.jobs.runners DEBUG 2025-03-08 13:10:02,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [104] queued (45.444 ms)
galaxy.jobs.handler INFO 2025-03-08 13:10:02,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:02,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 104
galaxy.jobs DEBUG 2025-03-08 13:10:02,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [104] prepared (70.606 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:02,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:10:02,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfcheck/vcfcheck/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:03,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:10:03,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/104/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/1/d/8/dataset_1d846e4c-bcbb-47ff-a183-4d69e3cccdef.dat' 'localref.fa' && vcfcheck -x -f 'localref.fa' '/galaxy/server/database/objects/6/e/4/dataset_6e4dcfe0-a3c6-47e0-8a34-b05c07c5c743.dat' > '/galaxy/server/database/objects/1/0/6/dataset_1065c2be-a03c-4b2f-91de-1f0f5d4e7713.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:10:03,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (104) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/104/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/104/galaxy_104.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:03,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:03,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:10:03,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfcheck/vcfcheck/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:03,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:03,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:04,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:14,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jfhvd with k8s id: gxy-jfhvd succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:10:14,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 104: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:10:22,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 104 finished
galaxy.model.metadata DEBUG 2025-03-08 13:10:22,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 172
galaxy.jobs INFO 2025-03-08 13:10:22,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 104 in /galaxy/server/database/jobs_directory/000/104
galaxy.jobs DEBUG 2025-03-08 13:10:22,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 104 executed (128.641 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:22,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:10:25,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 105
tpv.core.entities DEBUG 2025-03-08 13:10:25,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:10:25,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:10:25,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:10:25,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:10:25,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Working directory for job is: /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-03-08 13:10:25,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [105] queued (44.399 ms)
galaxy.jobs.handler INFO 2025-03-08 13:10:25,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:25,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 105
galaxy.jobs DEBUG 2025-03-08 13:10:25,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [105] prepared (69.793 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:10:25,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/105/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/105/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/105/configs/tmpogshis_o']
galaxy.jobs.runners DEBUG 2025-03-08 13:10:25,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (105) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/105/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/105/galaxy_105.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:25,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:25,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:25,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:34,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ct84d with k8s id: gxy-ct84d succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:10:35,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 105: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:10:42,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 105 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:10:42,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'vcflib.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/105/working/data_fetch_upload_le6x69jy', 'object_id': 173}]}]}]
galaxy.jobs INFO 2025-03-08 13:10:42,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 105 in /galaxy/server/database/jobs_directory/000/105
galaxy.jobs DEBUG 2025-03-08 13:10:42,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 105 executed (131.295 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:42,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:10:43,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 106
tpv.core.entities DEBUG 2025-03-08 13:10:43,621 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:10:43,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:10:43,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:10:43,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:10:43,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Working directory for job is: /galaxy/server/database/jobs_directory/000/106
galaxy.jobs.runners DEBUG 2025-03-08 13:10:43,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [106] queued (33.185 ms)
galaxy.jobs.handler INFO 2025-03-08 13:10:43,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:43,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 106
galaxy.jobs DEBUG 2025-03-08 13:10:43,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [106] prepared (41.689 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:43,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:10:43,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfdistance/vcfdistance/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:43,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:10:43,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/106/tool_script.sh] for tool command [cat '/galaxy/server/database/objects/b/1/a/dataset_b1abab48-50cd-41e7-be5f-b007a08ad263.dat' | vcfdistance > '/galaxy/server/database/objects/4/7/4/dataset_4742b7eb-fd5b-4cdb-9e23-2198fdf7810a.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:10:43,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (106) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/106/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/106/galaxy_106.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:43,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:43,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:10:43,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfdistance/vcfdistance/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-08 13:10:43,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:43,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:44,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:48,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bzxql with k8s id: gxy-bzxql succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:10:48,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 106: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:10:55,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 106 finished
galaxy.model.metadata DEBUG 2025-03-08 13:10:55,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 174
galaxy.jobs INFO 2025-03-08 13:10:55,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 106 in /galaxy/server/database/jobs_directory/000/106
galaxy.jobs DEBUG 2025-03-08 13:10:55,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 106 executed (115.552 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:55,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:10:58,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 107
tpv.core.entities DEBUG 2025-03-08 13:10:58,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:10:58,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:10:58,978 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:10:58,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:10:59,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Working directory for job is: /galaxy/server/database/jobs_directory/000/107
galaxy.jobs.runners DEBUG 2025-03-08 13:10:59,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [107] queued (48.834 ms)
galaxy.jobs.handler INFO 2025-03-08 13:10:59,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:59,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 107
galaxy.jobs DEBUG 2025-03-08 13:10:59,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [107] prepared (77.885 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:10:59,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/107/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/107/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/107/configs/tmpq_bi8hmm']
galaxy.jobs.runners DEBUG 2025-03-08 13:10:59,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (107) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/107/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/107/galaxy_107.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:59,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:10:59,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:00,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:10,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zp6tv with k8s id: gxy-zp6tv succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:11:10,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 107: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:11:18,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 107 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:11:18,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/107/working/gxupload_0', 'object_id': 175}]}]}]
galaxy.jobs INFO 2025-03-08 13:11:18,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 107 in /galaxy/server/database/jobs_directory/000/107
galaxy.jobs DEBUG 2025-03-08 13:11:18,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 107 executed (299.479 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:18,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:11:19,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 108
tpv.core.entities DEBUG 2025-03-08 13:11:19,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualpos/poretools_qualpos/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:11:19,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:11:19,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:11:19,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:11:19,565 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Working directory for job is: /galaxy/server/database/jobs_directory/000/108
galaxy.jobs.runners DEBUG 2025-03-08 13:11:19,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [108] queued (41.628 ms)
galaxy.jobs.handler INFO 2025-03-08 13:11:19,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:19,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 108
galaxy.jobs DEBUG 2025-03-08 13:11:19,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [108] prepared (55.160 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:11:19,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:11:19,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualpos/poretools_qualpos/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-08 13:11:19,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:11:19,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/108/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools qualpos '/galaxy/server/database/objects/0/a/a/dataset_0aa23706-ccd7-400b-97c2-6f8d9d8a3652.dat' --saveas qualpos.png --min-length 0 --max-length 1000000000 --bin-width 1000  && mv qualpos.png '/galaxy/server/database/objects/e/7/2/dataset_e72cc454-8c87-4d22-ba0b-6b3e293ca2fa.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:11:19,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (108) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/108/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/108/galaxy_108.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:19,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:11:19,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:11:19,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualpos/poretools_qualpos/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-08 13:11:19,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:20,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:20,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:51,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5lj79 with k8s id: gxy-5lj79 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:11:51,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 108: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:11:58,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 108 finished
galaxy.model.metadata DEBUG 2025-03-08 13:11:58,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 176
galaxy.util WARNING 2025-03-08 13:11:58,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/7/2/dataset_e72cc454-8c87-4d22-ba0b-6b3e293ca2fa.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/7/2/dataset_e72cc454-8c87-4d22-ba0b-6b3e293ca2fa.dat'
galaxy.jobs INFO 2025-03-08 13:11:58,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 108 in /galaxy/server/database/jobs_directory/000/108
galaxy.jobs DEBUG 2025-03-08 13:11:58,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 108 executed (84.508 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:11:58,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:12:00,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 109
tpv.core.entities DEBUG 2025-03-08 13:12:00,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:12:00,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:12:00,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:12:00,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:12:00,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Working directory for job is: /galaxy/server/database/jobs_directory/000/109
galaxy.jobs.runners DEBUG 2025-03-08 13:12:00,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [109] queued (41.541 ms)
galaxy.jobs.handler INFO 2025-03-08 13:12:00,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:00,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 109
galaxy.jobs DEBUG 2025-03-08 13:12:00,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [109] prepared (64.591 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:12:00,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/109/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/109/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/109/configs/tmp3xih6s00']
galaxy.jobs.runners DEBUG 2025-03-08 13:12:00,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (109) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/109/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/109/galaxy_109.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:00,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:00,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:01,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:10,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f7dq6 with k8s id: gxy-f7dq6 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:12:10,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 109: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:12:17,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 109 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:12:18,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'poretools-in1.tar.bz2', 'dbkey': '?', 'ext': 'fast5.tar', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fast5.tar file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/109/working/gxupload_0', 'object_id': 177}]}]}]
galaxy.jobs INFO 2025-03-08 13:12:23,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 109 in /galaxy/server/database/jobs_directory/000/109
galaxy.jobs DEBUG 2025-03-08 13:12:23,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 109 executed (5666.498 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:23,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:12:24,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 110
tpv.core.entities DEBUG 2025-03-08 13:12:24,897 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualpos/poretools_qualpos/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:12:24,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:12:24,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:12:24,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:12:24,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Working directory for job is: /galaxy/server/database/jobs_directory/000/110
galaxy.jobs.runners DEBUG 2025-03-08 13:12:24,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [110] queued (42.782 ms)
galaxy.jobs.handler INFO 2025-03-08 13:12:24,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:24,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 110
galaxy.jobs DEBUG 2025-03-08 13:12:25,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [110] prepared (46.170 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:12:25,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:12:25,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualpos/poretools_qualpos/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-08 13:12:25,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:12:25,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/110/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools qualpos '/galaxy/server/database/objects/1/2/a/dataset_12a96549-f55f-44b6-b16d-7a846b16a178.dat' --saveas qualpos.pdf --min-length 0 --max-length 1000000000 --bin-width 1000  && mv qualpos.pdf '/galaxy/server/database/objects/f/7/e/dataset_f7e7aa4b-1afc-42ff-99b0-9e9c63b4b0ff.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:12:25,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (110) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/110/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/110/galaxy_110.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:25,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:12:25,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:12:25,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_qualpos/poretools_qualpos/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-08 13:12:25,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:25,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:26,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:31,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cr8f4 with k8s id: gxy-cr8f4 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:12:31,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 110: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:12:39,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 110 finished
galaxy.model.metadata DEBUG 2025-03-08 13:12:39,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 178
galaxy.util WARNING 2025-03-08 13:12:39,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/7/e/dataset_f7e7aa4b-1afc-42ff-99b0-9e9c63b4b0ff.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/7/e/dataset_f7e7aa4b-1afc-42ff-99b0-9e9c63b4b0ff.dat'
galaxy.jobs INFO 2025-03-08 13:12:39,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 110 in /galaxy/server/database/jobs_directory/000/110
galaxy.jobs DEBUG 2025-03-08 13:12:39,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 110 executed (96.121 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:39,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:12:42,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 111
tpv.core.entities DEBUG 2025-03-08 13:12:42,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:12:42,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:12:42,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:12:42,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:12:42,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Working directory for job is: /galaxy/server/database/jobs_directory/000/111
galaxy.jobs.runners DEBUG 2025-03-08 13:12:42,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [111] queued (33.298 ms)
galaxy.jobs.handler INFO 2025-03-08 13:12:42,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:42,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 111
galaxy.jobs DEBUG 2025-03-08 13:12:42,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [111] prepared (68.960 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:12:42,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/111/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/111/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/111/configs/tmpyelb04bz']
galaxy.jobs.runners DEBUG 2025-03-08 13:12:42,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (111) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/111/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/111/galaxy_111.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:42,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:42,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:42,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:12:52,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c79vv with k8s id: gxy-c79vv succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:12:53,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 111: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:13:00,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 111 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:13:00,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/111/working/gxupload_0', 'object_id': 179}]}]}]
galaxy.jobs INFO 2025-03-08 13:13:00,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.jobs DEBUG 2025-03-08 13:13:00,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 111 executed (135.452 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:00,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:13:00,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 112
tpv.core.entities DEBUG 2025-03-08 13:13:00,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:13:00,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:13:00,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:13:00,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:13:00,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Working directory for job is: /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-03-08 13:13:00,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [112] queued (35.053 ms)
galaxy.jobs.handler INFO 2025-03-08 13:13:00,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:00,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 112
galaxy.jobs DEBUG 2025-03-08 13:13:00,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [112] prepared (67.553 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:00,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:13:00,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:01,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:13:01,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/112/tool_script.sh] for tool command [bamCompare --version > /galaxy/server/database/jobs_directory/000/112/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/2/4/e/dataset_24e2dcf4-fe9d-42f8-ba19-bba9db88eb4a.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/2/0/7/metadata_207b9276-0da0-4aee-83bd-f46af60be1aa.dat' one.bam.bai && ln -s '/galaxy/server/database/objects/2/4/e/dataset_24e2dcf4-fe9d-42f8-ba19-bba9db88eb4a.dat' two.bam && ln -s '/galaxy/server/database/objects/_metadata_files/2/0/7/metadata_207b9276-0da0-4aee-83bd-f46af60be1aa.dat' two.bam.bai &&  bamCompare --numberOfProcessors "${GALAXY_SLOTS:-4}" --bamfile1 one.bam --bamfile2 two.bam  --outFileName '/galaxy/server/database/objects/b/6/f/dataset_b6f7a842-9872-49a9-8871-caccfc9b2657.dat' --outFileFormat 'bedgraph'  --binSize 5  --scaleFactorsMethod readCount  --operation ratio   --pseudocount 1 1]
galaxy.jobs.runners DEBUG 2025-03-08 13:13:01,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (112) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/112/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/112/galaxy_112.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:01,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:01,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:13:01,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:01,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:01,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:01,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:22,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9rrnk with k8s id: gxy-9rrnk succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:13:22,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 112: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:13:30,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 112 finished
galaxy.model.metadata DEBUG 2025-03-08 13:13:30,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 180
galaxy.util WARNING 2025-03-08 13:13:30,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/6/f/dataset_b6f7a842-9872-49a9-8871-caccfc9b2657.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/6/f/dataset_b6f7a842-9872-49a9-8871-caccfc9b2657.dat'
galaxy.jobs INFO 2025-03-08 13:13:30,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.jobs DEBUG 2025-03-08 13:13:30,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 112 executed (94.260 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:30,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:13:31,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 113
tpv.core.entities DEBUG 2025-03-08 13:13:31,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:13:31,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:13:31,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:13:31,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:13:31,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Working directory for job is: /galaxy/server/database/jobs_directory/000/113
galaxy.jobs.runners DEBUG 2025-03-08 13:13:31,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [113] queued (32.418 ms)
galaxy.jobs.handler INFO 2025-03-08 13:13:31,214 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:31,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 113
galaxy.jobs DEBUG 2025-03-08 13:13:31,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [113] prepared (54.235 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:13:31,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/113/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/113/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/113/configs/tmporg_sz9r']
galaxy.jobs.runners DEBUG 2025-03-08 13:13:31,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (113) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/113/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/113/galaxy_113.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:31,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:31,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:31,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:41,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f4fvc with k8s id: gxy-f4fvc succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:13:41,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 113: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:13:48,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 113 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:13:48,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'bowtie2 test1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/113/working/gxupload_0', 'object_id': 181}]}]}]
galaxy.jobs INFO 2025-03-08 13:13:48,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 113 in /galaxy/server/database/jobs_directory/000/113
galaxy.jobs DEBUG 2025-03-08 13:13:48,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 113 executed (148.145 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:48,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:13:49,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 114
tpv.core.entities DEBUG 2025-03-08 13:13:49,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:13:49,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:13:49,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:13:49,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:13:49,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Working directory for job is: /galaxy/server/database/jobs_directory/000/114
galaxy.jobs.runners DEBUG 2025-03-08 13:13:49,610 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [114] queued (36.487 ms)
galaxy.jobs.handler INFO 2025-03-08 13:13:49,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:49,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 114
galaxy.jobs DEBUG 2025-03-08 13:13:49,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [114] prepared (56.130 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:49,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:13:49,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:49,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:13:49,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/114/tool_script.sh] for tool command [bamCompare --version > /galaxy/server/database/jobs_directory/000/114/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/3/6/7/dataset_367eaa79-7044-4054-ab4b-73586333e726.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/f/f/d/metadata_ffd48b16-6b98-4f17-b331-4712b950732e.dat' one.bam.bai && ln -s '/galaxy/server/database/objects/3/6/7/dataset_367eaa79-7044-4054-ab4b-73586333e726.dat' two.bam && ln -s '/galaxy/server/database/objects/_metadata_files/f/f/d/metadata_ffd48b16-6b98-4f17-b331-4712b950732e.dat' two.bam.bai &&  bamCompare --numberOfProcessors "${GALAXY_SLOTS:-4}" --bamfile1 one.bam --bamfile2 two.bam  --outFileName '/galaxy/server/database/objects/d/5/4/dataset_d54a12ef-3bc1-4edf-a41a-2b8d86556bbc.dat' --outFileFormat 'bigwig'  --binSize 10  --scaleFactorsMethod readCount  --operation ratio   --pseudocount 1 1       --minMappingQuality '1']
galaxy.jobs.runners DEBUG 2025-03-08 13:13:49,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (114) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/114/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/114/galaxy_114.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:49,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:49,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:13:49,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_compare/deeptools_bam_compare/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-03-08 13:13:49,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:49,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:50,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vcqp5 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-vcqp5.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:13:54,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-vcqp5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-vcqp5": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114/gxy-vcqp5) tool_stdout: Due to filtering, 83.33333333333334% of the aforementioned alignments will be used 39.16666666666667
Due to filtering, 83.33333333333334% of the aforementioned alignments will be used 39.16666666666667

galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114/gxy-vcqp5) tool_stderr: bamFilesList: ['one.bam', 'two.bam']
binLength: 10
numberOfSamples: 0
blackListFileName: None
skipZeroOverZero: False
bed_and_bin: False
genomeChunkSize: None
defaultFragmentLength: read length
numberOfProcessors: 8
verbose: False
region: None
bedFile: None
minMappingQuality: 1
ignoreDuplicates: False
chrsToSkip: []
stepSize: 10
center_read: False
samFlag_include: None
samFlag_exclude: None
minFragmentLength: 0
maxFragmentLength: 0
zerosToNans: False
smoothLength: None
save_data: False
out_file_for_raw_data: None
maxPairedFragmentLength: 1000

galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114/gxy-vcqp5) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114/gxy-vcqp5) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-vcqp5.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 114 (gxy-vcqp5)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-vcqp5 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:54,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114/gxy-vcqp5) Terminated at user's request
galaxy.util WARNING 2025-03-08 13:13:54,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/5/4/dataset_d54a12ef-3bc1-4edf-a41a-2b8d86556bbc.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/5/4/dataset_d54a12ef-3bc1-4edf-a41a-2b8d86556bbc.dat'
galaxy.jobs.handler DEBUG 2025-03-08 13:13:58,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 115, 116
tpv.core.entities DEBUG 2025-03-08 13:13:58,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:13:58,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:13:58,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:13:58,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:13:58,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Working directory for job is: /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-03-08 13:13:58,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [115] queued (37.450 ms)
galaxy.jobs.handler INFO 2025-03-08 13:13:58,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:58,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 115
tpv.core.entities DEBUG 2025-03-08 13:13:58,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:13:58,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:13:58,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:13:58,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:13:58,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Working directory for job is: /galaxy/server/database/jobs_directory/000/116
galaxy.jobs.runners DEBUG 2025-03-08 13:13:58,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [116] queued (52.813 ms)
galaxy.jobs.handler INFO 2025-03-08 13:13:58,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:58,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 116
galaxy.jobs DEBUG 2025-03-08 13:13:58,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [115] prepared (84.794 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:13:58,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/115/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/115/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/115/configs/tmp012lhzsk']
galaxy.jobs.runners DEBUG 2025-03-08 13:13:58,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (115) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/115/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/115/galaxy_115.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:58,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:13:59,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [116] prepared (81.964 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:59,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:13:59,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/116/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/116/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/116/configs/tmplr4szebm']
galaxy.jobs.runners DEBUG 2025-03-08 13:13:59,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (116) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/116/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/116/galaxy_116.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:59,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:59,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:59,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:13:59,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:09,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9pzvh with k8s id: gxy-9pzvh succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:14:09,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 116: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:10,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nfzk8 with k8s id: gxy-nfzk8 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:14:10,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 115: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:14:16,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 116 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:14:17,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pBR322.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/116/working/data_fetch_upload_c9h5akqo', 'object_id': 184}]}]}]
galaxy.jobs INFO 2025-03-08 13:14:17,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 116 in /galaxy/server/database/jobs_directory/000/116
galaxy.jobs DEBUG 2025-03-08 13:14:17,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 116 executed (180.563 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:17,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 13:14:18,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 115 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:14:18,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/115/working/gxupload_0', 'object_id': 183}]}]}]
galaxy.jobs INFO 2025-03-08 13:14:18,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs DEBUG 2025-03-08 13:14:18,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 115 executed (155.506 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:18,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:14:19,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 117
tpv.core.entities DEBUG 2025-03-08 13:14:19,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:14:19,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:14:19,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:14:19,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:14:19,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Working directory for job is: /galaxy/server/database/jobs_directory/000/117
galaxy.jobs.runners DEBUG 2025-03-08 13:14:19,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [117] queued (38.109 ms)
galaxy.jobs.handler INFO 2025-03-08 13:14:19,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:19,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 117
galaxy.jobs DEBUG 2025-03-08 13:14:19,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [117] prepared (41.226 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:14:19,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:14:19,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:14:19,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:14:19,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/117/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/117/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/f/8/e/dataset_f8e58e0d-6dbd-4647-8344-f5ca0ad3dc65.dat' reference.fa && lofreq faidx reference.fa 2>&1 || echo "Error running samtools faidx for indexing fasta reference for lofreq" >&2 &&  lofreq alnqual -b    '/galaxy/server/database/objects/e/1/5/dataset_e15502b4-c993-40ef-b8a5-ef410be75820.dat' 'reference.fa' > '/galaxy/server/database/objects/e/e/c/dataset_eec44a9f-b625-4be7-a2ab-924dad277b28.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:14:19,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (117) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/117/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/117/galaxy_117.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:19,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:14:19,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:14:19,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:14:19,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:19,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:20,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:33,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wqls8 with k8s id: gxy-wqls8 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:14:33,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 117: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:14:41,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 117 finished
galaxy.model.metadata DEBUG 2025-03-08 13:14:41,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 185
galaxy.jobs INFO 2025-03-08 13:14:41,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 117 in /galaxy/server/database/jobs_directory/000/117
galaxy.jobs DEBUG 2025-03-08 13:14:41,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 117 executed (133.044 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:41,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:14:42,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 119, 118
tpv.core.entities DEBUG 2025-03-08 13:14:42,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:14:42,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:14:42,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:14:42,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:14:42,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Working directory for job is: /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-03-08 13:14:42,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [118] queued (35.674 ms)
galaxy.jobs.handler INFO 2025-03-08 13:14:42,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:42,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 118
tpv.core.entities DEBUG 2025-03-08 13:14:42,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:14:42,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:14:42,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:14:42,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:14:42,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Working directory for job is: /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners DEBUG 2025-03-08 13:14:42,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [119] queued (48.095 ms)
galaxy.jobs.handler INFO 2025-03-08 13:14:42,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:42,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 119
galaxy.jobs DEBUG 2025-03-08 13:14:42,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [118] prepared (80.729 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:14:43,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/118/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/118/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/118/configs/tmpsim50av0']
galaxy.jobs.runners DEBUG 2025-03-08 13:14:43,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (118) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/118/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/118/galaxy_118.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:43,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:14:43,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [119] prepared (80.221 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:43,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:14:43,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/119/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/119/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/119/configs/tmp5t5ywwal']
galaxy.jobs.runners DEBUG 2025-03-08 13:14:43,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (119) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/119/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/119/galaxy_119.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:43,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:43,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:43,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:43,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:52,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8tzpr with k8s id: gxy-8tzpr succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:14:53,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tvh25 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-tvh25.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:14:54,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-tvh25

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-tvh25": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (118/gxy-tvh25) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (118/gxy-tvh25) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (118/gxy-tvh25) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (118/gxy-tvh25) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-tvh25.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 118 (gxy-tvh25)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-tvh25 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:54,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (118/gxy-tvh25) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-08 13:14:56,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 121, 120
tpv.core.entities DEBUG 2025-03-08 13:14:56,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:14:56,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:14:56,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:14:56,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:14:56,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Working directory for job is: /galaxy/server/database/jobs_directory/000/120
galaxy.jobs.runners DEBUG 2025-03-08 13:14:56,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [120] queued (38.287 ms)
galaxy.jobs.handler INFO 2025-03-08 13:14:56,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:56,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 120
tpv.core.entities DEBUG 2025-03-08 13:14:56,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:14:56,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:14:56,288 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:14:56,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:14:56,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Working directory for job is: /galaxy/server/database/jobs_directory/000/121
galaxy.jobs.runners DEBUG 2025-03-08 13:14:56,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [121] queued (49.761 ms)
galaxy.jobs.handler INFO 2025-03-08 13:14:56,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:56,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 121
galaxy.jobs DEBUG 2025-03-08 13:14:56,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [120] prepared (84.099 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:14:56,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/120/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/120/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/120/configs/tmp16z8k_he']
galaxy.jobs.runners DEBUG 2025-03-08 13:14:56,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (120) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/120/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/120/galaxy_120.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:56,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:14:56,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [121] prepared (73.346 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:56,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:14:56,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/121/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/121/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/121/configs/tmp_0rf52_s']
galaxy.jobs.runners DEBUG 2025-03-08 13:14:56,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (121) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/121/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/121/galaxy_121.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:56,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:56,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:57,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:14:57,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-08 13:15:00,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 119 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:15:00,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pBR322.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/119/working/data_fetch_upload_zerp6pmn', 'object_id': 187}]}]}]
galaxy.jobs INFO 2025-03-08 13:15:00,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs DEBUG 2025-03-08 13:15:00,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 119 executed (128.171 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:00,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:06,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hq2mj with k8s id: gxy-hq2mj succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:06,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vj65t with k8s id: gxy-vj65t succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:15:06,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 120: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:15:06,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 121: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:15:15,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 121 finished
galaxy.jobs.runners DEBUG 2025-03-08 13:15:15,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 120 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:15:15,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pBR322.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/121/working/data_fetch_upload_5u2zk81l', 'object_id': 189}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:15:15,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/120/working/gxupload_0', 'object_id': 188}]}]}]
galaxy.jobs INFO 2025-03-08 13:15:15,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 121 in /galaxy/server/database/jobs_directory/000/121
galaxy.jobs INFO 2025-03-08 13:15:15,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 120 in /galaxy/server/database/jobs_directory/000/120
galaxy.jobs DEBUG 2025-03-08 13:15:15,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 121 executed (182.383 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:15,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:15:15,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 120 executed (178.618 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:15,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:15:16,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 122
tpv.core.entities DEBUG 2025-03-08 13:15:16,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:15:16,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:15:16,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:15:16,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:15:16,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Working directory for job is: /galaxy/server/database/jobs_directory/000/122
galaxy.jobs.runners DEBUG 2025-03-08 13:15:16,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [122] queued (36.096 ms)
galaxy.jobs.handler INFO 2025-03-08 13:15:16,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:16,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 122
galaxy.jobs DEBUG 2025-03-08 13:15:16,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [122] prepared (49.893 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:16,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:15:16,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:17,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:15:17,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/122/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/122/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/f/f/9/dataset_ff95650d-9032-4141-a358-a92f844db645.dat' reference.fa && lofreq faidx reference.fa 2>&1 || echo "Error running samtools faidx for indexing fasta reference for lofreq" >&2 &&  lofreq alnqual -b  -A  '/galaxy/server/database/objects/7/8/1/dataset_781b830d-06f9-48ed-8b9c-3f67de73a726.dat' 'reference.fa' > '/galaxy/server/database/objects/7/d/b/dataset_7db477ee-ed24-4e55-a36e-0affe864cf7c.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:15:17,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (122) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/122/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/122/galaxy_122.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:17,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:17,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:15:17,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:17,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:17,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:17,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:21,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5mbqx with k8s id: gxy-5mbqx succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:15:22,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 122: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:15:29,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 122 finished
galaxy.model.metadata DEBUG 2025-03-08 13:15:29,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 190
galaxy.jobs INFO 2025-03-08 13:15:29,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 122 in /galaxy/server/database/jobs_directory/000/122
galaxy.jobs DEBUG 2025-03-08 13:15:29,804 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 122 executed (133.350 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:29,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:15:31,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 123
tpv.core.entities DEBUG 2025-03-08 13:15:31,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:15:31,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:15:31,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:15:31,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:15:31,153 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Working directory for job is: /galaxy/server/database/jobs_directory/000/123
galaxy.jobs.runners DEBUG 2025-03-08 13:15:31,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [123] queued (41.981 ms)
galaxy.jobs.handler INFO 2025-03-08 13:15:31,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:31,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 123
galaxy.jobs DEBUG 2025-03-08 13:15:31,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [123] prepared (66.599 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:15:31,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/123/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/123/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/123/configs/tmpq7bba_ft']
galaxy.jobs.runners DEBUG 2025-03-08 13:15:31,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (123) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/123/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/123/galaxy_123.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:31,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:31,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:31,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-08 13:15:32,169 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 124
tpv.core.entities DEBUG 2025-03-08 13:15:32,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:15:32,200 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:15:32,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:15:32,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:15:32,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Working directory for job is: /galaxy/server/database/jobs_directory/000/124
galaxy.jobs.runners DEBUG 2025-03-08 13:15:32,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [124] queued (43.515 ms)
galaxy.jobs.handler INFO 2025-03-08 13:15:32,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:32,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 124
galaxy.jobs DEBUG 2025-03-08 13:15:32,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [124] prepared (75.509 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:15:32,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/124/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/124/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/124/configs/tmp2gqp0lj5']
galaxy.jobs.runners DEBUG 2025-03-08 13:15:32,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (124) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/124/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/124/galaxy_124.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:32,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:32,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:33,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:42,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2xwwx with k8s id: gxy-2xwwx succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:42,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8x6fc with k8s id: gxy-8x6fc succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:15:42,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 123: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:15:42,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 124: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:15:50,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 123 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:15:50,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/123/working/gxupload_0', 'object_id': 191}]}]}]
galaxy.jobs.runners DEBUG 2025-03-08 13:15:50,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 124 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:15:50,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pBR322.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/124/working/data_fetch_upload_b61guo3k', 'object_id': 192}]}]}]
galaxy.jobs INFO 2025-03-08 13:15:50,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 123 in /galaxy/server/database/jobs_directory/000/123
galaxy.jobs INFO 2025-03-08 13:15:50,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 124 in /galaxy/server/database/jobs_directory/000/124
galaxy.jobs DEBUG 2025-03-08 13:15:50,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 123 executed (192.724 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:50,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:15:50,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 124 executed (148.079 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:50,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:15:51,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 125
tpv.core.entities DEBUG 2025-03-08 13:15:51,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:15:51,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:15:51,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:15:51,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:15:51,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Working directory for job is: /galaxy/server/database/jobs_directory/000/125
galaxy.jobs.runners DEBUG 2025-03-08 13:15:51,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [125] queued (37.247 ms)
galaxy.jobs.handler INFO 2025-03-08 13:15:51,784 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:51,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 125
galaxy.jobs DEBUG 2025-03-08 13:15:51,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [125] prepared (51.670 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:51,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:15:51,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:51,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:15:51,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/125/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/125/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/c/e/8/dataset_ce86c44d-17aa-4dc7-bb30-0e6d2f459185.dat' reference.fa && lofreq faidx reference.fa 2>&1 || echo "Error running samtools faidx for indexing fasta reference for lofreq" >&2 &&  lofreq alnqual -b -e   '/galaxy/server/database/objects/3/5/9/dataset_359717b3-fb74-4365-8560-d8eae19a041e.dat' 'reference.fa' > '/galaxy/server/database/objects/b/7/d/dataset_b7d7f460-6ece-43c7-beab-24a15ec90146.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:15:51,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (125) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/125/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/125/galaxy_125.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:51,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:51,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:15:51,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:15:51,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:51,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:52,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vv8mm failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-vv8mm.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:15:56,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-vv8mm

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-vv8mm": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (125/gxy-vv8mm) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (125/gxy-vv8mm) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (125/gxy-vv8mm) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (125/gxy-vv8mm) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-vv8mm.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 125 (gxy-vv8mm)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-vv8mm to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:56,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:57,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (125/gxy-vv8mm) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-08 13:15:58,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 126, 127
tpv.core.entities DEBUG 2025-03-08 13:15:58,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:15:58,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:15:58,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:15:58,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:15:58,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Working directory for job is: /galaxy/server/database/jobs_directory/000/126
galaxy.jobs.runners DEBUG 2025-03-08 13:15:58,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [126] queued (29.278 ms)
galaxy.jobs.handler INFO 2025-03-08 13:15:58,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:58,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 126
tpv.core.entities DEBUG 2025-03-08 13:15:58,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:15:58,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:15:58,978 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:15:58,998 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:15:59,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Working directory for job is: /galaxy/server/database/jobs_directory/000/127
galaxy.jobs.runners DEBUG 2025-03-08 13:15:59,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [127] queued (50.761 ms)
galaxy.jobs.handler INFO 2025-03-08 13:15:59,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:59,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 127
galaxy.jobs DEBUG 2025-03-08 13:15:59,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [126] prepared (85.487 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:15:59,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/126/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/126/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/126/configs/tmpxz90efox']
galaxy.jobs.runners DEBUG 2025-03-08 13:15:59,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (126) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/126/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/126/galaxy_126.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-08 13:15:59,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [127] prepared (70.572 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:59,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:59,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:15:59,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/127/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/127/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/127/configs/tmpux7zwchj']
galaxy.jobs.runners DEBUG 2025-03-08 13:15:59,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (127) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/127/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/127/galaxy_127.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:59,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:59,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:15:59,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:00,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:09,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dfkzk with k8s id: gxy-dfkzk succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:16:09,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 127: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:10,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jktt4 with k8s id: gxy-jktt4 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:16:10,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 126: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:16:17,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 127 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:16:17,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pBR322.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/127/working/data_fetch_upload_2mvhbt_b', 'object_id': 195}]}]}]
galaxy.jobs INFO 2025-03-08 13:16:17,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 127 in /galaxy/server/database/jobs_directory/000/127
galaxy.jobs DEBUG 2025-03-08 13:16:17,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 127 executed (234.644 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:17,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 13:16:18,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 126 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:16:18,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'lofreq-in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/126/working/gxupload_0', 'object_id': 194}]}]}]
galaxy.jobs INFO 2025-03-08 13:16:18,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 126 in /galaxy/server/database/jobs_directory/000/126
galaxy.jobs DEBUG 2025-03-08 13:16:18,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 126 executed (152.169 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:18,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:16:19,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 128
tpv.core.entities DEBUG 2025-03-08 13:16:19,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:16:19,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:16:19,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:16:19,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:16:19,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Working directory for job is: /galaxy/server/database/jobs_directory/000/128
galaxy.jobs.runners DEBUG 2025-03-08 13:16:19,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [128] queued (44.041 ms)
galaxy.jobs.handler INFO 2025-03-08 13:16:19,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:19,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 128
galaxy.jobs DEBUG 2025-03-08 13:16:19,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [128] prepared (50.305 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:16:19,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:16:19,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:16:19,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:16:19,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/128/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/128/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/6/3/c/dataset_63cb56d1-0213-4f81-a6d0-deefc4bd75d6.dat' reference.fa && lofreq faidx reference.fa 2>&1 || echo "Error running samtools faidx for indexing fasta reference for lofreq" >&2 &&  lofreq alnqual -b    '/galaxy/server/database/objects/9/0/e/dataset_90e7d56b-47c0-4340-a19a-0cc57c899a48.dat' 'reference.fa' > '/galaxy/server/database/objects/c/d/a/dataset_cda63caf-377b-4c4c-ae2e-d981e1190f50.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:16:19,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (128) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/128/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/128/galaxy_128.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:19,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:16:19,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:16:19,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_alnqual/lofreq_alnqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:16:19,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:19,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:20,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:25,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sckg7 with k8s id: gxy-sckg7 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:16:25,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 128: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:16:33,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 128 finished
galaxy.model.metadata DEBUG 2025-03-08 13:16:33,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 196
galaxy.jobs INFO 2025-03-08 13:16:33,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 128 in /galaxy/server/database/jobs_directory/000/128
galaxy.jobs DEBUG 2025-03-08 13:16:33,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 128 executed (130.497 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:33,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:16:35,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 130, 129
tpv.core.entities DEBUG 2025-03-08 13:16:35,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:16:35,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:16:35,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:16:35,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:16:35,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Working directory for job is: /galaxy/server/database/jobs_directory/000/129
galaxy.jobs.runners DEBUG 2025-03-08 13:16:35,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [129] queued (37.296 ms)
galaxy.jobs.handler INFO 2025-03-08 13:16:35,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:35,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 129
tpv.core.entities DEBUG 2025-03-08 13:16:35,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:16:35,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:16:36,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:16:36,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:16:36,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Working directory for job is: /galaxy/server/database/jobs_directory/000/130
galaxy.jobs.runners DEBUG 2025-03-08 13:16:36,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [130] queued (50.896 ms)
galaxy.jobs.handler INFO 2025-03-08 13:16:36,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:36,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 130
galaxy.jobs DEBUG 2025-03-08 13:16:36,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [129] prepared (86.434 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:16:36,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/129/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/129/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/129/configs/tmpalcp8qdr']
galaxy.jobs.runners DEBUG 2025-03-08 13:16:36,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (129) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/129/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/129/galaxy_129.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:36,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:16:36,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [130] prepared (81.422 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:36,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:16:36,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/130/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/130/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/130/configs/tmpry0txyhz']
galaxy.jobs.runners DEBUG 2025-03-08 13:16:36,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (130) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/130/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/130/galaxy_130.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:36,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:36,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:36,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:36,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-08 13:16:37,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 131
tpv.core.entities DEBUG 2025-03-08 13:16:37,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:16:37,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:16:37,095 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:16:37,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:16:37,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Working directory for job is: /galaxy/server/database/jobs_directory/000/131
galaxy.jobs.runners DEBUG 2025-03-08 13:16:37,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [131] queued (38.733 ms)
galaxy.jobs.handler INFO 2025-03-08 13:16:37,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:37,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 131
galaxy.jobs DEBUG 2025-03-08 13:16:37,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [131] prepared (94.762 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:16:37,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/131/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/131/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/131/configs/tmplyfpqd2b']
galaxy.jobs.runners DEBUG 2025-03-08 13:16:37,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (131) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/131/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/131/galaxy_131.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:37,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:37,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:38,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:45,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xntsc failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:45,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:45,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xntsc.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:16:46,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-xntsc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-xntsc": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zrvcb with k8s id: gxy-zrvcb succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (129/gxy-xntsc) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (129/gxy-xntsc) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (129/gxy-xntsc) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (129/gxy-xntsc) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xntsc.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 129 (gxy-xntsc)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-xntsc to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:46,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (129/gxy-xntsc) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-03-08 13:16:46,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 130: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hw7k5 with k8s id: gxy-hw7k5 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:16:47,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 131: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-08 13:16:47,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 134, 133, 132
tpv.core.entities DEBUG 2025-03-08 13:16:47,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:16:47,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:16:47,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:16:47,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:16:47,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Working directory for job is: /galaxy/server/database/jobs_directory/000/132
galaxy.jobs.runners DEBUG 2025-03-08 13:16:47,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [132] queued (39.412 ms)
galaxy.jobs.handler INFO 2025-03-08 13:16:47,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 132
tpv.core.entities DEBUG 2025-03-08 13:16:47,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:16:47,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:16:47,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:16:47,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:16:47,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Working directory for job is: /galaxy/server/database/jobs_directory/000/133
galaxy.jobs.runners DEBUG 2025-03-08 13:16:47,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [133] queued (96.506 ms)
galaxy.jobs.handler INFO 2025-03-08 13:16:47,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 133
tpv.core.entities DEBUG 2025-03-08 13:16:47,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:16:47,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:16:47,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:16:47,695 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:16:47,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [132] prepared (178.749 ms)
galaxy.jobs DEBUG 2025-03-08 13:16:47,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Working directory for job is: /galaxy/server/database/jobs_directory/000/134
galaxy.jobs.command_factory INFO 2025-03-08 13:16:47,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/132/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/132/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/132/configs/tmpzp4c8pgy']
galaxy.jobs.runners DEBUG 2025-03-08 13:16:47,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [134] queued (95.219 ms)
galaxy.jobs.handler INFO 2025-03-08 13:16:47,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Job dispatched
galaxy.jobs.runners DEBUG 2025-03-08 13:16:47,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (132) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/132/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/132/galaxy_132.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:16:47,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [133] prepared (132.027 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:16:47,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/133/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/133/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/133/configs/tmphwpvlngv']
galaxy.jobs.runners DEBUG 2025-03-08 13:16:47,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (133) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/133/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/133/galaxy_133.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 134
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:47,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:16:47,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [134] prepared (103.976 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:16:48,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/134/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/134/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/134/configs/tmpcm1x7hwv']
galaxy.jobs.runners DEBUG 2025-03-08 13:16:48,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (134) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/134/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/134/galaxy_134.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:48,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:48,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:49,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:49,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:49,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-08 13:16:54,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 130 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:16:54,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.umi', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/130/working/data_fetch_upload_p5c2ivc6', 'object_id': 198}]}]}]
galaxy.jobs INFO 2025-03-08 13:16:54,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 130 in /galaxy/server/database/jobs_directory/000/130
galaxy.jobs DEBUG 2025-03-08 13:16:54,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 130 executed (122.232 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:54,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 130 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 13:16:55,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 131 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:16:55,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq.gz', 'dbkey': '?', 'ext': 'fastq.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/131/working/gxupload_0', 'object_id': 199}]}]}]
galaxy.jobs INFO 2025-03-08 13:16:55,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 131 in /galaxy/server/database/jobs_directory/000/131
galaxy.jobs DEBUG 2025-03-08 13:16:55,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 131 executed (124.157 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:55,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:58,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kttzq with k8s id: gxy-kttzq succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:58,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rgzz7 with k8s id: gxy-rgzz7 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:16:58,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 132: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:16:58,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 133: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:16:59,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rtr8k with k8s id: gxy-rtr8k succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:16:59,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 134: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:17:09,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 132 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:17:09,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/132/working/data_fetch_upload_1k0z3ycb', 'object_id': 200}]}]}]
galaxy.jobs INFO 2025-03-08 13:17:09,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 132 in /galaxy/server/database/jobs_directory/000/132
galaxy.jobs.runners DEBUG 2025-03-08 13:17:09,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 133 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:17:09,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq.gz', 'dbkey': '?', 'ext': 'fastq.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/133/working/gxupload_0', 'object_id': 201}]}]}]
galaxy.jobs DEBUG 2025-03-08 13:17:09,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 132 executed (172.760 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:09,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-08 13:17:09,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 133 in /galaxy/server/database/jobs_directory/000/133
galaxy.jobs DEBUG 2025-03-08 13:17:09,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 133 executed (145.156 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:09,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 13:17:10,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 134 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:17:10,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.r.fq.gz', 'dbkey': '?', 'ext': 'fastq.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/134/working/gxupload_0', 'object_id': 202}]}]}]
galaxy.jobs INFO 2025-03-08 13:17:10,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 134 in /galaxy/server/database/jobs_directory/000/134
galaxy.jobs DEBUG 2025-03-08 13:17:10,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 134 executed (126.516 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:10,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 134 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:17:11,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 135
tpv.core.entities DEBUG 2025-03-08 13:17:11,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:17:11,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:17:11,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:17:11,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:17:11,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Working directory for job is: /galaxy/server/database/jobs_directory/000/135
galaxy.jobs.runners DEBUG 2025-03-08 13:17:11,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [135] queued (38.994 ms)
galaxy.jobs.handler INFO 2025-03-08 13:17:11,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:11,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 135
galaxy.jobs DEBUG 2025-03-08 13:17:11,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [135] prepared (87.384 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:11,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:17:11,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_pseudo/kallisto_pseudo/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:11,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:17:11,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/135/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/2/a/c/dataset_2ac8677f-1a29-4b43-8e94-78fe063d0b49.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto pseudo -i 'reference.kallisto' --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/2/7/f/dataset_27f25265-a843-4430-9d0b-ddd969720e57.dat' '/galaxy/server/database/objects/0/5/d/dataset_05d9b552-3bd8-4de5-a49c-6445c9350c22.dat' && if [ -f run_info.json ] ; then cat run_info.json ; fi && mkdir outputs && if [ -f matrix.ec ] ; then mv matrix.ec outputs/Matrix.ec ; fi && if [ -f matrix.tcc.mtx ] ; then mv matrix.tcc.mtx outputs/Matrix.tabular ; fi && if [ -f matrix.cells ] ; then mv matrix.cells outputs/Matrix.cells ; fi && if [ -f pseudoalignments.tsv ] ; then mv pseudoalignments.tsv outputs/Pseudoalignments.tabular ; fi && if [ -f pseudoalignments.ec ] ; then mv pseudoalignments.ec outputs/Pseudoalignments.ec ; fi]
galaxy.jobs.runners DEBUG 2025-03-08 13:17:11,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (135) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/135/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/135/galaxy_135.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:11,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:11,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:17:11,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_pseudo/kallisto_pseudo/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:11,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:11,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:12,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:19,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tzlgk with k8s id: gxy-tzlgk succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:17:19,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 135: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:17:26,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 135 finished
galaxy.model.metadata DEBUG 2025-03-08 13:17:26,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 203
galaxy.jobs INFO 2025-03-08 13:17:26,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 135 in /galaxy/server/database/jobs_directory/000/135
galaxy.jobs DEBUG 2025-03-08 13:17:27,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 135 executed (369.416 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:27,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 135 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:17:28,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 137, 136
tpv.core.entities DEBUG 2025-03-08 13:17:28,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:17:28,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:17:28,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:17:28,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:17:28,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Working directory for job is: /galaxy/server/database/jobs_directory/000/136
galaxy.jobs.runners DEBUG 2025-03-08 13:17:28,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [136] queued (36.306 ms)
galaxy.jobs.handler INFO 2025-03-08 13:17:28,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:28,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 136
tpv.core.entities DEBUG 2025-03-08 13:17:28,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:17:28,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:17:28,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:17:28,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:17:28,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Working directory for job is: /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-03-08 13:17:28,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [137] queued (52.819 ms)
galaxy.jobs.handler INFO 2025-03-08 13:17:28,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:28,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 137
galaxy.jobs DEBUG 2025-03-08 13:17:28,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [136] prepared (87.988 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:17:29,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/136/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/136/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/136/configs/tmp_5lm8_5j']
galaxy.jobs.runners DEBUG 2025-03-08 13:17:29,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (136) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/136/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/136/galaxy_136.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:29,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:17:29,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [137] prepared (77.600 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:29,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:17:29,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/137/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/137/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/137/configs/tmp9n1m2kzu']
galaxy.jobs.runners DEBUG 2025-03-08 13:17:29,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (137) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/137/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/137/galaxy_137.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:29,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:29,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:29,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:29,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:40,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zj4ms with k8s id: gxy-zj4ms succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:40,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qbk62 with k8s id: gxy-qbk62 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:17:40,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 136: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:17:40,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 137: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:17:48,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 136 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:17:48,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/136/working/data_fetch_upload_a8gaomo_', 'object_id': 209}]}]}]
galaxy.jobs INFO 2025-03-08 13:17:48,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 136 in /galaxy/server/database/jobs_directory/000/136
galaxy.jobs.runners DEBUG 2025-03-08 13:17:48,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 137 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:17:48,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'mm10_chrM-1.f.fq.gz', 'dbkey': '?', 'ext': 'fastq.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/137/working/gxupload_0', 'object_id': 210}]}]}]
galaxy.jobs DEBUG 2025-03-08 13:17:48,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 136 executed (151.378 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:48,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-08 13:17:48,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 137 in /galaxy/server/database/jobs_directory/000/137
galaxy.jobs DEBUG 2025-03-08 13:17:48,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 137 executed (143.986 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:48,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:17:49,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 138
tpv.core.entities DEBUG 2025-03-08 13:17:49,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:17:49,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:17:49,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:17:49,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:17:49,563 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Working directory for job is: /galaxy/server/database/jobs_directory/000/138
galaxy.jobs.runners DEBUG 2025-03-08 13:17:49,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [138] queued (39.513 ms)
galaxy.jobs.handler INFO 2025-03-08 13:17:49,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:49,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 138
galaxy.jobs DEBUG 2025-03-08 13:17:49,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [138] prepared (58.442 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:49,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:17:49,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_pseudo/kallisto_pseudo/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:49,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:17:49,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/138/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/e/0/4/dataset_e04e9af9-80d8-4415-b3ee-dbdc82176ad5.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto pseudo -i 'reference.kallisto' --threads ${GALAXY_SLOTS:-1} -o .  --single --fragment-length 200 --sd 20 '/galaxy/server/database/objects/6/3/f/dataset_63f15972-c109-404f-a791-15e6610eb97f.dat' && if [ -f run_info.json ] ; then cat run_info.json ; fi && mkdir outputs && if [ -f matrix.ec ] ; then mv matrix.ec outputs/Matrix.ec ; fi && if [ -f matrix.tcc.mtx ] ; then mv matrix.tcc.mtx outputs/Matrix.tabular ; fi && if [ -f matrix.cells ] ; then mv matrix.cells outputs/Matrix.cells ; fi && if [ -f pseudoalignments.tsv ] ; then mv pseudoalignments.tsv outputs/Pseudoalignments.tabular ; fi && if [ -f pseudoalignments.ec ] ; then mv pseudoalignments.ec outputs/Pseudoalignments.ec ; fi]
galaxy.jobs.runners DEBUG 2025-03-08 13:17:49,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (138) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/138/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/138/galaxy_138.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:49,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:49,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:17:49,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_pseudo/kallisto_pseudo/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-03-08 13:17:49,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:49,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:50,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:17:54,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ldqhn with k8s id: gxy-ldqhn succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:17:54,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 138: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:18:02,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 138 finished
galaxy.model.metadata DEBUG 2025-03-08 13:18:02,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 211
galaxy.jobs INFO 2025-03-08 13:18:02,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 138 in /galaxy/server/database/jobs_directory/000/138
galaxy.jobs DEBUG 2025-03-08 13:18:02,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 138 executed (362.980 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:02,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:18:03,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 139
tpv.core.entities DEBUG 2025-03-08 13:18:03,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:18:03,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:18:03,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:18:03,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:18:03,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Working directory for job is: /galaxy/server/database/jobs_directory/000/139
galaxy.jobs.runners DEBUG 2025-03-08 13:18:03,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [139] queued (36.942 ms)
galaxy.jobs.handler INFO 2025-03-08 13:18:03,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:03,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 139
galaxy.jobs DEBUG 2025-03-08 13:18:03,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [139] prepared (71.586 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:18:04,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/139/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/139/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/139/configs/tmpvplk8ba0']
galaxy.jobs.runners DEBUG 2025-03-08 13:18:04,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (139) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/139/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/139/galaxy_139.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:04,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:04,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:04,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-08 13:18:04,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 141, 140
tpv.core.entities DEBUG 2025-03-08 13:18:04,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:18:04,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:18:04,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:18:04,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:18:04,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Working directory for job is: /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-03-08 13:18:04,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [140] queued (38.535 ms)
galaxy.jobs.handler INFO 2025-03-08 13:18:04,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:04,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 140
tpv.core.entities DEBUG 2025-03-08 13:18:05,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:18:05,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:18:05,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:18:05,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:18:05,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Working directory for job is: /galaxy/server/database/jobs_directory/000/141
galaxy.jobs.runners DEBUG 2025-03-08 13:18:05,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [141] queued (49.466 ms)
galaxy.jobs.handler INFO 2025-03-08 13:18:05,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:05,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 141
galaxy.jobs DEBUG 2025-03-08 13:18:05,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [140] prepared (81.906 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:18:05,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/140/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/140/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/140/configs/tmpept3zzbm']
galaxy.jobs.runners DEBUG 2025-03-08 13:18:05,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (140) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/140/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/140/galaxy_140.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:05,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:18:05,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [141] prepared (81.290 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:05,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-08 13:18:05,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/141/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/141/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/141/configs/tmp4hx_7jzc']
galaxy.jobs.runners DEBUG 2025-03-08 13:18:05,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (141) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/141/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/141/galaxy_141.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:05,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:05,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:05,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:05,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:14,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8dmn7 with k8s id: gxy-8dmn7 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:18:14,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 139: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:15,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-h7dr8 with k8s id: gxy-h7dr8 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:15,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q5flg with k8s id: gxy-q5flg succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:18:15,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 140: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:18:15,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 141: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:18:26,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 139 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:18:26,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM.fa', 'dbkey': '?', 'ext': 'fasta', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/139/working/data_fetch_upload_zja2rxlz', 'object_id': 217}]}]}]
galaxy.jobs INFO 2025-03-08 13:18:26,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 139 in /galaxy/server/database/jobs_directory/000/139
galaxy.jobs DEBUG 2025-03-08 13:18:26,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 139 executed (197.791 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:26,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-08 13:18:27,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 140 finished
galaxy.jobs.runners DEBUG 2025-03-08 13:18:27,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 141 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:18:27,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM_F.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/140/working/data_fetch_upload_cp44owh9', 'object_id': 218}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:18:27,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'felCat8_chrM_R.fq', 'dbkey': '?', 'ext': 'fastq', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastq file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/141/working/data_fetch_upload__xz80wx1', 'object_id': 219}]}]}]
galaxy.jobs INFO 2025-03-08 13:18:27,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 140 in /galaxy/server/database/jobs_directory/000/140
galaxy.jobs INFO 2025-03-08 13:18:27,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 141 in /galaxy/server/database/jobs_directory/000/141
galaxy.jobs DEBUG 2025-03-08 13:18:27,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 140 executed (228.384 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:27,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-08 13:18:27,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 141 executed (178.224 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:27,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:18:28,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 142
tpv.core.entities DEBUG 2025-03-08 13:18:28,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:18:28,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:18:28,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:18:28,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:18:28,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Working directory for job is: /galaxy/server/database/jobs_directory/000/142
galaxy.jobs.runners DEBUG 2025-03-08 13:18:28,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [142] queued (39.347 ms)
galaxy.jobs.handler INFO 2025-03-08 13:18:28,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:28,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 142
galaxy.jobs DEBUG 2025-03-08 13:18:28,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [142] prepared (62.588 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:18:28,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:18:28,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_pseudo/kallisto_pseudo/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-03-08 13:18:28,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:18:28,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/142/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/9/3/4/dataset_934f6ee2-3a14-4ac8-92d5-8f1403b561fb.dat' reference.fa && kallisto index reference.fa -i reference.kallisto && kallisto pseudo -i 'reference.kallisto' --threads ${GALAXY_SLOTS:-1} -o .  '/galaxy/server/database/objects/1/3/b/dataset_13b2700d-527c-410e-b5f3-c06b4f766adb.dat' '/galaxy/server/database/objects/a/a/8/dataset_aa881e6a-af33-471d-9e2f-b6610445f53c.dat' && if [ -f run_info.json ] ; then cat run_info.json ; fi && mkdir outputs && if [ -f matrix.ec ] ; then mv matrix.ec outputs/Matrix.ec ; fi && if [ -f matrix.tcc.mtx ] ; then mv matrix.tcc.mtx outputs/Matrix.tabular ; fi && if [ -f matrix.cells ] ; then mv matrix.cells outputs/Matrix.cells ; fi && if [ -f pseudoalignments.tsv ] ; then mv pseudoalignments.tsv outputs/Pseudoalignments.tabular ; fi && if [ -f pseudoalignments.ec ] ; then mv pseudoalignments.ec outputs/Pseudoalignments.ec ; fi]
galaxy.jobs.runners DEBUG 2025-03-08 13:18:28,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (142) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/142/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/142/galaxy_142.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:28,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:18:28,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:18:28,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/kallisto_pseudo/kallisto_pseudo/0.48.0+galaxy1: mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e
galaxy.tool_util.deps.containers INFO 2025-03-08 13:18:28,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-143e618e2d6eef454186ce14739a2cabc5ecf645:73585c71af93a1f54d7c49f0c36f37d638946b1e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:28,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:29,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:33,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gbdg9 with k8s id: gxy-gbdg9 succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:18:34,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 142: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:18:41,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 142 finished
galaxy.model.metadata DEBUG 2025-03-08 13:18:41,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 220
galaxy.jobs INFO 2025-03-08 13:18:41,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 142 in /galaxy/server/database/jobs_directory/000/142
galaxy.jobs DEBUG 2025-03-08 13:18:41,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 142 executed (348.224 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:41,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:18:42,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 143
tpv.core.entities DEBUG 2025-03-08 13:18:43,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:18:43,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:18:43,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:18:43,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:18:43,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Working directory for job is: /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-03-08 13:18:43,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [143] queued (30.939 ms)
galaxy.jobs.handler INFO 2025-03-08 13:18:43,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:43,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 143
galaxy.jobs DEBUG 2025-03-08 13:18:43,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [143] prepared (58.413 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:18:43,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/143/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/143/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/143/configs/tmpcrirmh28']
galaxy.jobs.runners DEBUG 2025-03-08 13:18:43,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (143) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/143/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/143/galaxy_143.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:43,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:43,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:44,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-08 13:18:44,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 144
tpv.core.entities DEBUG 2025-03-08 13:18:44,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:18:44,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:18:44,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:18:44,095 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:18:44,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Working directory for job is: /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-03-08 13:18:44,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [144] queued (29.058 ms)
galaxy.jobs.handler INFO 2025-03-08 13:18:44,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:44,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 144
galaxy.jobs DEBUG 2025-03-08 13:18:44,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [144] prepared (59.762 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:18:44,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/144/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/144/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/144/configs/tmpsxxr6ktn']
galaxy.jobs.runners DEBUG 2025-03-08 13:18:44,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (144) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/144/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/144/galaxy_144.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:44,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:44,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:45,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fgs7g failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fgs7g.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:18:52,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-fgs7g

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-fgs7g": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (143/gxy-fgs7g) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (143/gxy-fgs7g) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (143/gxy-fgs7g) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (143/gxy-fgs7g) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fgs7g.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 143 (gxy-fgs7g)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-fgs7g to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:52,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (143/gxy-fgs7g) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-08 13:18:54,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 145
tpv.core.entities DEBUG 2025-03-08 13:18:54,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:18:54,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:18:54,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:18:54,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:18:54,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Working directory for job is: /galaxy/server/database/jobs_directory/000/145
galaxy.jobs.runners DEBUG 2025-03-08 13:18:54,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [145] queued (32.722 ms)
galaxy.jobs.handler INFO 2025-03-08 13:18:54,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 145
galaxy.jobs DEBUG 2025-03-08 13:18:54,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [145] prepared (64.030 ms)
galaxy.jobs.command_factory INFO 2025-03-08 13:18:54,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/145/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/145/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/145/configs/tmpcu60gj8b']
galaxy.jobs.runners DEBUG 2025-03-08 13:18:54,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (145) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/145/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/145/galaxy_145.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-czt2g failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-czt2g.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-08 13:18:54,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-03-08-12-27-1/jobs/gxy-czt2g

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-czt2g": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (144/gxy-czt2g) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (144/gxy-czt2g) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (144/gxy-czt2g) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (144/gxy-czt2g) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-czt2g.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 144 (gxy-czt2g)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-czt2g to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:54,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (144/gxy-czt2g) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:18:55,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:04,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9tgxr with k8s id: gxy-9tgxr succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:19:04,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 145: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:19:12,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 145 finished
galaxy.tool_util.provided_metadata DEBUG 2025-03-08 13:19:12,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'nucmer.txt', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/145/working/data_fetch_upload_kgw4ooa1', 'object_id': 228}]}]}]
galaxy.jobs INFO 2025-03-08 13:19:12,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 145 in /galaxy/server/database/jobs_directory/000/145
galaxy.jobs DEBUG 2025-03-08 13:19:12,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 145 executed (139.534 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:12,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-08 13:19:12,766 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 146
tpv.core.entities DEBUG 2025-03-08 13:19:12,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-08 13:19:12,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-08 13:19:12,802 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-08 13:19:12,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-08 13:19:12,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Working directory for job is: /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners DEBUG 2025-03-08 13:19:12,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [146] queued (36.061 ms)
galaxy.jobs.handler INFO 2025-03-08 13:19:12,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:12,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 146
galaxy.jobs DEBUG 2025-03-08 13:19:12,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [146] prepared (46.659 ms)
galaxy.tool_util.deps.containers INFO 2025-03-08 13:19:12,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:19:12,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_show_coords/mummer_show_coords/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:19:13,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-08 13:19:13,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/146/tool_script.sh] for tool command [show-coords  -c -H -I '75.0' -l -L '100'  -q -T '/galaxy/server/database/objects/1/d/d/dataset_1dd3f35c-bcb7-4461-abc8-a406c9bad5c6.dat' > '/galaxy/server/database/objects/3/4/c/dataset_34cd8317-56e9-4aff-918b-f5ea79ea765b.dat']
galaxy.jobs.runners DEBUG 2025-03-08 13:19:13,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (146) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/146/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/146/galaxy_146.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/146/working/show-coords.txt" -a -f "/galaxy/server/database/objects/3/4/c/dataset_34cd8317-56e9-4aff-918b-f5ea79ea765b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/146/working/show-coords.txt" "/galaxy/server/database/objects/3/4/c/dataset_34cd8317-56e9-4aff-918b-f5ea79ea765b.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:13,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-08 13:19:13,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-08 13:19:13,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_show_coords/mummer_show_coords/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-03-08 13:19:13,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:13,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:13,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:23,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-g9ljr with k8s id: gxy-g9ljr succeeded
galaxy.jobs.runners DEBUG 2025-03-08 13:19:23,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 146: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-08 13:19:30,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 146 finished
galaxy.model.metadata DEBUG 2025-03-08 13:19:30,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 229
galaxy.jobs INFO 2025-03-08 13:19:30,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 146 in /galaxy/server/database/jobs_directory/000/146
galaxy.jobs DEBUG 2025-03-08 13:19:30,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 146 executed (91.667 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-08 13:19:30,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
