galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:11:55,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hf654 with k8s id: gxy-hf654 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:11:55,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:12:02,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 44 finished
galaxy.model.metadata DEBUG 2025-11-05 01:12:02,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 52
galaxy.model.metadata DEBUG 2025-11-05 01:12:02,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 53
galaxy.model.metadata DEBUG 2025-11-05 01:12:02,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 54
galaxy.model.metadata DEBUG 2025-11-05 01:12:02,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 55
galaxy.util WARNING 2025-11-05 01:12:02,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/8/2/dataset_4822d1f6-8656-4421-bfb6-eb794e866e9c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/8/2/dataset_4822d1f6-8656-4421-bfb6-eb794e866e9c.dat'
galaxy.util WARNING 2025-11-05 01:12:02,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/6/d/dataset_c6d4e7b4-89e2-40d5-9cc5-ddd06df8f90d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/6/d/dataset_c6d4e7b4-89e2-40d5-9cc5-ddd06df8f90d.dat'
galaxy.util WARNING 2025-11-05 01:12:02,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/6/b/dataset_f6bec426-708f-4932-b136-b0840279e070.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/6/b/dataset_f6bec426-708f-4932-b136-b0840279e070.dat'
galaxy.util WARNING 2025-11-05 01:12:02,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/d/a/dataset_eda9ba38-1bda-4351-977c-6e889856eb19.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/d/a/dataset_eda9ba38-1bda-4351-977c-6e889856eb19.dat'
galaxy.jobs INFO 2025-11-05 01:12:02,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-11-05 01:12:02,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 44 executed (128.876 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:02,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 44 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:12:09,660 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46, 45
tpv.core.entities DEBUG 2025-11-05 01:12:09,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:12:09,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:12:09,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:12:09,691 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:12:09,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:12:09,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-11-05 01:12:09,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (33.984 ms)
galaxy.jobs.handler INFO 2025-11-05 01:12:09,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:09,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 46
tpv.core.entities DEBUG 2025-11-05 01:12:09,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:12:09,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:12:09,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:12:09,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:12:09,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:12:09,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-11-05 01:12:09,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (46.738 ms)
galaxy.jobs.handler INFO 2025-11-05 01:12:09,802 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:09,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-11-05 01:12:09,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [46] prepared (84.111 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:12:09,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/46/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/46/configs/tmp64xe6ddu']
galaxy.jobs.runners DEBUG 2025-11-05 01:12:09,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:09,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:12:09,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [45] prepared (78.962 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:09,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:12:09,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/45/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/45/configs/tmp9ggrlj76']
galaxy.jobs.runners DEBUG 2025-11-05 01:12:09,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:09,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:09,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:11,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:11,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:20,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rknr9 with k8s id: gxy-rknr9 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:20,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mwbsl with k8s id: gxy-mwbsl succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:12:20,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:12:20,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:12:28,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 46 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:12:28,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/46/working/gxupload_0', 'object_id': 57}]}]}]
galaxy.jobs INFO 2025-11-05 01:12:28,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-11-05 01:12:28,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 45 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:12:28,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/45/working/gxupload_0', 'object_id': 56}]}]}]
galaxy.jobs DEBUG 2025-11-05 01:12:28,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 46 executed (191.500 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:28,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 46 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-11-05 01:12:28,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-11-05 01:12:28,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 45 executed (163.699 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:28,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 45 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:12:29,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-11-05 01:12:29,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:12:29,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:12:29,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:12:29,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:12:29,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:12:29,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-11-05 01:12:29,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (64.143 ms)
galaxy.jobs.handler INFO 2025-11-05 01:12:29,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:29,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 47
galaxy.jobs DEBUG 2025-11-05 01:12:29,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [47] prepared (101.204 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:12:29,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:12:29,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:12:29,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:12:29,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/47/outputs/COMMAND_VERSION 2>&1;
mkdir -p paired_reads1 && ln -s '/galaxy/server/database/objects/b/0/b/dataset_b0bbbd27-2113-4c1a-aa83-698a091354df.dat' 'paired_reads1/ecoli_1K1.fastq.gz' &&  ln -s '/galaxy/server/database/objects/d/f/b/dataset_dfbe783a-818c-4bfa-b3f1-77c0ebeb24c5.dat' 'paired_reads1/ecoli_1K2.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py  -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-1 1 'paired_reads1/ecoli_1K1.fastq.gz' --pe-2 1 'paired_reads1/ecoli_1K2.fastq.gz' --pe-or 1 fr        --cov-cutoff auto  -k '33'   --isolate --disable-rr --iontorrent   --phred-offset 33    && (test -f 'output/contigs.fasta' && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/nml/spades/822954de3f59/spades/write_tsv_script.py' < 'output/contigs.fasta' > '/galaxy/server/database/objects/4/6/5/dataset_4652a7aa-2542-4cc2-9a12-de09fcfe8c56.dat' || echo 'No contigs.fasta.') && (test -f 'output/scaffolds.fasta' && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/nml/spades/822954de3f59/spades/write_tsv_script.py' < 'output/scaffolds.fasta' > '/galaxy/server/database/objects/3/c/2/dataset_3c2b24e0-ef8a-48b2-8702-ef157ec3a8ce.dat' || echo 'No scaffolds.fasta.')]
galaxy.jobs.runners DEBUG 2025-11-05 01:12:29,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output/spades.log" -a -f "/galaxy/server/database/objects/7/9/9/dataset_799d9eb4-fe2b-4051-8831-d6e69cb5a018.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output/spades.log" "/galaxy/server/database/objects/7/9/9/dataset_799d9eb4-fe2b-4051-8831-d6e69cb5a018.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/c/4/c/dataset_c4c408e1-b768-4052-8828-6edda68b71c9.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output/scaffolds.fasta" "/galaxy/server/database/objects/c/4/c/dataset_c4c408e1-b768-4052-8828-6edda68b71c9.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output/scaffolds.paths" -a -f "/galaxy/server/database/objects/e/a/e/dataset_eaeed0be-9e33-415e-af2f-3a28f74881b2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output/scaffolds.paths" "/galaxy/server/database/objects/e/a/e/dataset_eaeed0be-9e33-415e-af2f-3a28f74881b2.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/9/a/0/dataset_9a0edce1-ec18-45be-8d07-5c567370651b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/9/a/0/dataset_9a0edce1-ec18-45be-8d07-5c567370651b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/9/2/4/dataset_924f13fa-f360-40fb-8d79-9708a84efef6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/9/2/4/dataset_924f13fa-f360-40fb-8d79-9708a84efef6.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/d/d/9/dataset_dd97a3a3-35cf-4ac3-b77b-524ed5e18425.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output/contigs.fasta" "/galaxy/server/database/objects/d/d/9/dataset_dd97a3a3-35cf-4ac3-b77b-524ed5e18425.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/47/working/output/contigs.paths" -a -f "/galaxy/server/database/objects/b/4/c/dataset_b4c35c82-699a-4fea-b757-fb630c0414d2.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/47/working/output/contigs.paths" "/galaxy/server/database/objects/b/4/c/dataset_b4c35c82-699a-4fea-b757-fb630c0414d2.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:29,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:12:29,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:12:29,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:12:29,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:29,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:30,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:42,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n5gns failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:42,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:42,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-n5gns.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:42,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-11-05 01:12:42,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-11-05-00-42-1/jobs/gxy-n5gns

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-n5gns": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (47/gxy-n5gns) tool_stdout: Command line: /usr/local/bin/spades.py	-o	/galaxy/server/database/jobs_directory/000/47/working/output	-t	8	-m	14	--tmp-dir	/galaxy/server/database/jobs_directory/000/47/tmp	--pe-1	1	/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K1.fastq.gz	--pe-2	1	/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K2.fastq.gz	--pe-or	1	fr	--cov-cutoff	auto	-k	33	--isolate	--disable-rr	--iontorrent	--phred-offset	33	

System information:
  SPAdes version: 4.2.0
  Python version: 3.13.7
  OS: Linux-6.6.97+-x86_64-with-glibc2.36

Output dir: /galaxy/server/database/jobs_directory/000/47/working/output
Mode: read error correction and assembling
Debug mode is turned OFF

Dataset parameters:
  IonTorrent data
  Isolate mode
  Reads:
    Library number: 1, library type: paired-end
      orientation: fr
      left reads: ['/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K1.fastq.gz']
      right reads: ['/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K2.fastq.gz']
      interlaced reads: not specified
      single reads: not specified
      merged reads: not specified
Read error correction parameters:
  Iterations: 1
  PHRED offset: 33
  Corrected reads will be compressed
Assembly parameters:
  k: [33]
  Repeat resolution is DISABLED
  Mismatch careful mode is turned OFF
  MismatchCorrector will be SKIPPED
  Coverage cutoff is turned ON and threshold will be auto-detected
  Assembly graph output will use GFA v1.2 format
Other parameters:
  Dir for temp files: /galaxy/server/database/jobs_directory/000/47/tmp
  Threads: 8
  Memory limit (in Gb): 14


======= SPAdes pipeline started. Log can be found here: /galaxy/server/database/jobs_directory/000/47/working/output/spades.log


===== Before start started. 


===== Read error correction started. 


===== Read error correction started. 


== Running: /usr/local/bin/spades-ionhammer /galaxy/server/database/jobs_directory/000/47/working/output/corrected/configs/ionhammer.cfg

  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  : 292)   Starting IonHammer, built from N/A, git revision N/A
  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  : 296)   Loading config from /galaxy/server/database/jobs_directory/000/47/working/output/corrected/configs/ionhammer.cfg
  0:00:00.003     1M / 22M   INFO    General                 (main.cpp                  : 298)   Maximum # of threads to use (adjusted due to OMP capabilities): 8
  0:00:00.003     1M / 22M   INFO    General                 (memory_limit.cpp          :  55)   Memory limit set to 14 Gb
  0:00:00.004     1M / 22M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 503)   Building kmer index
  0:00:00.005     1M / 22M   INFO    General                 (kmer_index_builder.hpp    : 308)   Splitting kmer instances into 32 files using 8 threads. This might take a while.
  0:00:00.028     1M / 22M   INFO    General                 (file_limit.hpp            :  43)   Open file limit set to 1048576
  0:00:00.028     1M / 22M   INFO    General                 (kmer_splitter.hpp         :  96)   Memory available for splitting buffers: 0.583328 Gb
  0:00:00.028     1M / 22M   INFO    General                 (kmer_splitter.hpp         : 104)   Using cell size of 1048576
  0:00:00.030  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             :  78)   Processing "/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K1.fastq.gz"
  0:00:00.306  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             :  78)   Processing "/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K2.fastq.gz"
  0:00:00.540  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             :  92)   Processed 4108 reads
  0:00:00.541     1M / 22M   INFO    General                 (kmer_index_builder.hpp    : 314)   Starting k-mer counting.
  0:00:00.620     1M / 22M   INFO    General                 (kmer_index_builder.hpp    : 325)   K-mer counting done. There are 2238 kmers in total.
  0:00:00.620     1M / 22M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 460)   Building perfect hash indices
  0:00:00.643     1M / 22M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 496)   Index built. Total 2238 kmers, 26664 bytes occupied (95.3137 bits per kmer).
  0:00:00.643     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 176)   Collecting K-mer information, this takes a while.
  0:00:00.644     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 182)   Processing "/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K1.fastq.gz"
  0:00:00.653     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 182)   Processing "/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K2.fastq.gz"
  0:00:00.662     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 188)   Collection done, postprocessing.
  0:00:00.662     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 201)   Merge done. There are 2238 kmers in total. Among them 210 (9.38338%) are singletons.Among them 0 (0%) are skipped during sampling.
  0:00:00.684     1M / 22M   INFO    General                 (main.cpp                  : 122)   Clustering Hamming graph.
  0:00:00.685     1M / 22M   INFO    General                 (main.cpp                  : 129)   Clustering done. Total clusters: 1426
  0:00:00.685     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 325)   Likelihood first: -630.092
  0:00:00.685     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 329)   Likelihood second: -21.8471
  0:00:00.685     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 330)   First weights: 0.954984
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 325)   Likelihood first: 69.5596
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 329)   Likelihood second: -300.095
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 330)   First weights: 0.64678
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 425)   For kmer length <= 21
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 426)   Median quality -4.9271
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 427)   Sample size 622
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 428)   Genomic dist: -4.91384 0.0414325
  0:00:00.686     1M / 22M   INFO    General                 (normal_quality_model.hpp  : 430)   NonGenomic dist: -3.45252 0.899384
  0:00:00.686     1M / 22M   INFO    General                 (main.cpp                  : 157)   Subclustering.
  0:00:00.689     1M / 22M   INFO    General                 (subcluster.hpp            :  43)   Good kmers: 968
  0:00:00.689     1M / 22M   INFO    General                 (subcluster.hpp            :  44)   Perfect kmers: 0
  0:00:00.689     1M / 22M   INFO    General                 (subcluster.hpp            :  45)   Reasigned by consensus: 0
  0:00:00.689     1M / 22M   INFO    General                 (main.cpp                  : 315)   Correcting reads.
  0:00:00.689     1M / 22M   INFO    General                 (main.cpp                  : 336)   Correcting pair of reads: "/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K1.fastq.gz" and "/galaxy/server/database/jobs_directory/000/47/working/paired_reads1/ecoli_1K2.fastq.gz"
  0:00:00.809     1M / 22M   INFO    General                 (read_corrector_new.hpp    : 172)   Skipped reads count: 170
  0:00:00.824     1M / 22M   INFO    General                 (main.cpp                  : 413)   Saving corrected dataset description to "/galaxy/server/database/jobs_directory/000/47/working/output/corrected/corrected.yaml"

===== Read error correction finished. 


===== corrected reads compression started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/compress_all.py --input_file /galaxy/server/database/jobs_directory/000/47/working/output/corrected/corrected.yaml --ext_python_modules_home /usr/local/share/spades --max_threads 8 --output_dir /galaxy/server/database/jobs_directory/000/47/working/output/corrected --gzip_output

== Compressing corrected reads (with gzip)
== Files to compress: ['/galaxy/server/database/jobs_directory/000/47/working/output/corrected/ecoli_1K1.fastq0_0.cor.fasta', '/galaxy/server/database/jobs_directory/000/47/working/output/corrected/ecoli_1K2.fastq0_0.cor.fasta']
== Files compression is finished
== Dataset yaml file is updated

===== corrected reads compression finished. 


===== Read error correction finished. 


===== Assembling started. 


===== K33 started. 


== Running: /usr/local/bin/spades-core /galaxy/server/database/jobs_directory/000/47/working/output/K33/configs/config.info /galaxy/server/database/jobs_directory/000/47/working/output/K33/configs/isolate_mode.info

  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  :  94)   Loaded config from "/galaxy/server/database/jobs_directory/000/47/working/output/K33/configs/config.info"
  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  :  94)   Loaded config from "/galaxy/server/database/jobs_directory/000/47/working/output/K33/configs/isolate_mode.info"
  0:00:00.000     1M / 22M   INFO    General                 (memory_limit.cpp          :  55)   Memory limit set to 14 Gb
  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  : 102)   Starting SPAdes, built from N/A, git revision N/A
  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  : 103)   Maximum k-mer length: 128
  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  : 104)   Assembling dataset ("/galaxy/server/database/jobs_directory/000/47/working/output/dataset.info") with K=33
  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  : 105)   Maximum # of threads to use (adjusted due to OMP capabilities): 8
  0:00:00.000     1M / 22M   INFO    General                 (pipeline.cpp              : 221)   SPAdes started
  0:00:00.000     1M / 22M   INFO    General                 (pipeline.cpp              : 234)   Starting from stage: read_conversion
  0:00:00.000     1M / 22M   INFO    General                 (pipeline.cpp              : 245)   Two-step repeat resolution disabled
  0:00:00.000     1M / 22M   INFO   GraphCore                (graph_core.hpp            : 689)   Graph created, vertex min_id: 3, edge min_id: 3
  0:00:00.000     1M / 22M   INFO   GraphCore                (graph_core.hpp            : 690)   Vertex size: 48, edge size: 40
  0:00:00.000     1M / 22M   INFO    General                 (edge_index.hpp            : 132)   Size of edge index entries: 12/8
  0:00:00.001     1M / 22M   INFO    General                 (pipeline.cpp              : 256)   Will need read mapping, kmer mapper will be attached
  0:00:00.001     1M / 22M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == Binary Read Conversion (id: read_conversion)
  0:00:00.005     1M / 22M   INFO    General                 (read_converter.cpp        :  78)   Converting reads to binary format for library #0 (takes a while)
  0:00:00.005     1M / 22M   INFO    General                 (read_converter.cpp        :  99)   Converting paired reads
  0:00:00.028    81M / 81M   INFO    General                 (binary_converter.cpp      : 143)   2054 reads written
  0:00:00.030    52M / 52M   INFO    General                 (read_converter.cpp        : 113)   Converting single reads
  0:00:00.042    65M / 65M   INFO    General                 (binary_converter.cpp      : 143)   0 reads written
  0:00:00.045    52M / 52M   INFO    General                 (read_converter.cpp        : 119)   Converting merged reads
  0:00:00.052    65M / 65M   INFO    General                 (binary_converter.cpp      : 143)   0 reads written
  0:00:00.175     1M / 36M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == de Bruijn graph construction (id: construction)
  0:00:00.192     1M / 36M   INFO    General                 (construction.cpp          : 150)   Max read length 100
  0:00:00.192     1M / 36M   INFO    General                 (construction.cpp          : 156)   Average read length 86.1485
  0:00:00.192     1M / 36M   INFO    General                 (stage.cpp                 : 121)   PROCEDURE == k+1-mer counting (id: construction:kpomer_counting)
  0:00:00.192     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 308)   Splitting kmer instances into 80 files using 8 threads. This might take a while.
  0:00:00.252     1M / 36M   INFO    General                 (file_limit.hpp            :  43)   Open file limit set to 1048576
  0:00:00.252     1M / 36M   INFO    General                 (kmer_splitter.hpp         :  96)   Memory available for splitting buffers: 0.583324 Gb
  0:00:00.252     1M / 36M   INFO    General                 (kmer_splitter.hpp         : 104)   Using cell size of 419430
  0:00:00.934     1M / 36M   INFO    General                 (kmer_splitters.hpp        : 134)   Used 8216 reads
  0:00:00.935     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 314)   Starting k-mer counting.
  0:00:01.154     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 325)   K-mer counting done. There are 987 kmers in total.
  0:00:01.154     1M / 36M   INFO    General                 (stage.cpp                 : 121)   PROCEDURE == Extension index construction (id: construction:extension_index_construction)
  0:00:01.201     1M / 36M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 503)   Building kmer index
  0:00:01.202     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 308)   Splitting kmer instances into 80 files using 8 threads. This might take a while.
  0:00:01.265     1M / 36M   INFO    General                 (file_limit.hpp            :  43)   Open file limit set to 1048576
  0:00:01.265     1M / 36M   INFO    General                 (kmer_splitter.hpp         :  96)   Memory available for splitting buffers: 0.583309 Gb
  0:00:01.265     1M / 36M   INFO    General                 (kmer_splitter.hpp         : 104)   Using cell size of 419430
  0:00:01.891  4641M / 4641M INFO    General                 (kmer_splitters.hpp        : 202)   Used 987 kmers.
  0:00:01.891     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 314)   Starting k-mer counting.
  0:00:02.107     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 325)   K-mer counting done. There are 988 kmers in total.
  0:00:02.107     1M / 36M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 460)   Building perfect hash indices
  0:00:02.162     1M / 36M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 496)   Index built. Total 988 kmers, 64056 bytes occupied (518.672 bits per kmer).
  0:00:02.162     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 191)   Merging final buckets.
  0:00:02.339     1M / 36M   INFO   DeBruijnExtensionIndexBu (kmer_extension_index_build: 101)   Building k-mer extensions from k+1-mers
  0:00:02.349     1M / 36M   INFO   DeBruijnExtensionIndexBu (kmer_extension_index_build: 106)   Building k-mer extensions from k+1-mers finished.
  0:00:02.364     1M / 36M   INFO    General                 (stage.cpp                 : 121)   PROCEDURE == Condensing graph (id: construction:graph_condensing)
  0:00:02.365     1M / 36M   INFO   UnbranchingPathExtractor (debruijn_graph_constructor: 324)   Extracting unbranching paths
  0:00:02.365     1M / 36M   INFO   UnbranchingPathExtractor (debruijn_graph_constructor: 348)   Extracting unbranching paths finished.
..
NFO   Simplification           (parallel_processing.hpp   : 168)   Running Low coverage edge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Low coverage edge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO    General                 (simplification.cpp        : 402)   PROCEDURE == Simplification cycle, iteration 7
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Tip clipper
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Tip clipper triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Bulge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Bulge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Low coverage edge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Low coverage edge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO    General                 (simplification.cpp        : 402)   PROCEDURE == Simplification cycle, iteration 8
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Tip clipper
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Tip clipper triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Bulge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Bulge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Low coverage edge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Low coverage edge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO    General                 (simplification.cpp        : 402)   PROCEDURE == Simplification cycle, iteration 9
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Tip clipper
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Tip clipper triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Bulge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Bulge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Low coverage edge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Low coverage edge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO    General                 (simplification.cpp        : 402)   PROCEDURE == Simplification cycle, iteration 10
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Tip clipper
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Tip clipper triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Bulge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Bulge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Low coverage edge remover
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Low coverage edge remover triggered 0 times
  0:00:07.887     1M / 36M   INFO    General                 (simplification.cpp        : 402)   PROCEDURE == Simplification cycle, iteration 11
  0:00:07.887     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Tip clipper
  0:00:07.888     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Tip clipper triggered 0 times
  0:00:07.888     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Bulge remover
  0:00:07.888     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Bulge remover triggered 0 times
  0:00:07.888     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Low coverage edge remover
  0:00:07.888     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Low coverage edge remover triggered 0 times
  0:00:07.888     1M / 36M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == Gap Closer (id: late_gapcloser)
  0:00:07.889    14M / 36M   INFO    General                 (edge_index.hpp            : 132)   Size of edge index entries: 12/8
  0:00:07.889    14M / 36M   INFO    General                 (gap_closer.cpp            : 117)   Total edges in tip neighborhood: 2 out of 2, length: 1934
  0:00:07.889    14M / 36M   INFO    General                 (edge_index.hpp            : 196)   Using small index (max_id = 29)
  0:00:07.889    14M / 36M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 460)   Building perfect hash indices
  0:00:07.890    14M / 36M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 496)   Index built. Total 967 kmers, 1464 bytes occupied (12.1117 bits per kmer).
  0:00:07.890    14M / 36M   INFO    General                 (edge_index_builders.hpp   : 266)   Collecting edge information from graph, this takes a while.
  0:00:07.899    14M / 36M   INFO    General                 (sequence_mapper_notifier.h:  80)   Starting sequence mapping
  0:00:07.904    14M / 36M   INFO    General                 (sequence_mapper_notifier.h: 120)   Total 2054 reads processed
  0:00:07.908    14M / 36M   INFO    General                 (gap_closer.cpp            : 505)   Initializing gap closer
  0:00:07.908    14M / 36M   INFO   GapCloser                (gap_closer.cpp            : 421)   Collecting gap candidates
  0:00:07.908    14M / 36M   INFO   GapCloser                (gap_closer.cpp            : 425)   Total 2 tips collected, total 2 connection candidates
  0:00:07.908    14M / 36M   INFO   GapCloser                (gap_closer.cpp            : 446)   Closing short gaps complete: filled 0 gaps after checking 2 candidates
  0:00:07.908    14M / 36M   INFO    General                 (gap_closer.cpp            : 510)   Gap closer done
  0:00:07.909     1M / 36M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == Simplification Cleanup (id: simplification_cleanup)
  0:00:07.909     1M / 36M   INFO    General                 (simplification.cpp        : 189)   PROCEDURE == Post simplification
  0:00:07.909     1M / 36M   INFO    General                 (graph_simplification.hpp  : 455)   Disconnection of relatively low covered edges disabled
  0:00:07.909     1M / 36M   INFO    General                 (graph_simplification.hpp  : 494)   Complex tip clipping disabled
  0:00:07.909     1M / 36M   INFO    General                 (graph_simplification.hpp  : 646)   Creating parallel br instance
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Tip clipper
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Tip clipper triggered 0 times
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Bulge remover
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Bulge remover triggered 0 times
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Tip clipper
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Tip clipper triggered 0 times
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Bulge remover
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Bulge remover triggered 0 times
  0:00:07.909     1M / 36M   INFO    General                 (simplification.cpp        : 348)   Disrupting self-conjugate edges
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 168)   Running Removing isolated edges
  0:00:07.909     1M / 36M   INFO   Simplification           (parallel_processing.hpp   : 171)   Removing isolated edges triggered 0 times
  0:00:07.909     1M / 36M   INFO    General                 (simplification.cpp        : 508)   After simplification:
  0:00:07.909     1M / 36M   INFO    General                 (simplification.cpp        : 509)     Average coverage = 225.724
  0:00:07.909     1M / 36M   INFO    General                 (simplification.cpp        : 510)     Total length = 967
  0:00:07.910     1M / 36M   INFO    General                 (simplification.cpp        : 511)     Median edge length: 967
  0:00:07.910     1M / 36M   INFO    General                 (simplification.cpp        : 512)     Edges: 2
  0:00:07.910     1M / 36M   INFO    General                 (simplification.cpp        : 513)     Vertices: 4
  0:00:07.910     1M / 36M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == Mismatch Correction (id: mismatch_correction)
  0:00:07.910     1M / 36M   INFO    General                 (graph_pack_helpers.cpp    :  44)   Index refill
  0:00:07.910     1M / 36M   INFO    General                 (edge_index.hpp            : 175)   Using small index (max_id = 29)
  0:00:07.910     1M / 36M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 460)   Building perfect hash indices
  0:00:07.910     1M / 36M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 496)   Index built. Total 967 kmers, 1464 bytes occupied (12.1117 bits per kmer).
  0:00:07.910     1M / 36M   INFO    General                 (edge_index_builders.hpp   : 253)   Collecting edge information from graph, this takes a while.
  0:00:07.910     1M / 36M   INFO    General                 (graph_pack_helpers.cpp    :  54)   Normalizing k-mer map. Total 0 kmers to process
  0:00:07.910     1M / 36M   INFO    General                 (kmer_mapper.hpp           :  87)   Total 0 kmers with non-trivial roots
  0:00:07.910     1M / 36M   INFO    General                 (graph_pack_helpers.cpp    :  56)   Normalizing done
  0:00:07.910     1M / 36M   INFO    General                 (mismatch_correction.cpp   : 421)   Collect potential mismatches
  0:00:07.910     1M / 36M   INFO    General                 (mismatch_correction.cpp   : 213)   Total 0 edges (out of 2) with 0 potential mismatch positions (-nan positions per edge)
  0:00:07.910     1M / 36M   INFO    General                 (mismatch_correction.cpp   : 423)   Potential mismatches collected
  0:00:07.923     1M / 36M   INFO    General                 (sequence_mapper_notifier.h:  80)   Starting sequence mapping
  0:00:07.933     1M / 36M   INFO    General                 (sequence_mapper_notifier.h: 120)   Total 8216 reads processed
  0:00:07.933     1M / 36M   INFO    General                 (mismatch_correction.cpp   : 416)   All edges processed
  0:00:07.933     1M / 36M   INFO    General                 (mismatch_correction.cpp   : 471)   Corrected 0 nucleotides
  0:00:07.933     1M / 36M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == Contig Output (id: contig_output)
  0:00:07.933     1M / 36M   INFO    General                 (contig_output.hpp         :  20)   Outputting contigs to "/galaxy/server/database/jobs_directory/000/47/working/output/K33/before_rr.fasta"
  0:00:07.939     1M / 36M   INFO    General                 (contig_output_stage.cpp   : 159)   Writing GFA graph to "/galaxy/server/database/jobs_directory/000/47/working/output/K33/assembly_graph_after_simplification.gfa"
  0:00:07.942     1M / 36M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == Contig Output (id: contig_output)
  0:00:07.942     1M / 36M   INFO    General                 (contig_output.hpp         :  20)   Outputting contigs to "/galaxy/server/database/jobs_directory/000/47/working/output/K33/before_rr.fasta"
  0:00:07.946     1M / 36M   INFO    General                 (contig_output_stage.cpp   : 159)   Writing GFA graph to "/galaxy/server/database/jobs_directory/000/47/working/output/K33/assembly_graph_with_scaffolds.gfa"
  0:00:07.946     1M / 36M   INFO    General                 (contig_output_stage.cpp   : 175)   Outputting FastG graph to "/galaxy/server/database/jobs_directory/000/47/working/output/K33/assembly_graph.fastg"
  0:00:07.950     1M / 36M   INFO    General                 (contig_output.hpp         :  20)   Outputting contigs to "/galaxy/server/database/jobs_directory/000/47/working/output/K33/final_contigs.fasta"
  0:00:07.962     1M / 36M   INFO    General                 (pipeline.cpp              : 303)   SPAdes finished
  0:00:07.962     1M / 36M   INFO    General                 (main.cpp                  : 131)   Assembling time: 0 hours 0 minutes 7 seconds

===== K33 finished. 


===== Copy files started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/copy_files.py /galaxy/server/database/jobs_directory/000/47/working/output/K33/before_rr.fasta /galaxy/server/database/jobs_directory/000/47/working/output/before_rr.fasta /galaxy/server/database/jobs_directory/000/47/working/output/K33/assembly_graph_after_simplification.gfa /galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph_after_simplification.gfa /galaxy/server/database/jobs_directory/000/47/working/output/K33/final_contigs.fasta /galaxy/server/database/jobs_directory/000/47/working/output/contigs.fasta /galaxy/server/database/jobs_directory/000/47/working/output/K33/first_pe_contigs.fasta /galaxy/server/database/jobs_directory/000/47/working/output/first_pe_contigs.fasta /galaxy/server/database/jobs_directory/000/47/working/output/K33/strain_graph.gfa /galaxy/server/database/jobs_directory/000/47/working/output/strain_graph.gfa /galaxy/server/database/jobs_directory/000/47/working/output/K33/assembly_graph_with_scaffolds.gfa /galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph_with_scaffolds.gfa /galaxy/server/database/jobs_directory/000/47/working/output/K33/assembly_graph.fastg /galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph.fastg /galaxy/server/database/jobs_directory/000/47/working/output/K33/final_contigs.paths /galaxy/server/database/jobs_directory/000/47/working/output/contigs.paths


===== Copy files finished. 


===== Assembling finished. 


===== Breaking scaffolds started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/breaking_scaffolds_script.py --result_scaffolds_filename /galaxy/server/database/jobs_directory/000/47/working/output/scaffolds.fasta --misc_dir /galaxy/server/database/jobs_directory/000/47/working/output/misc --threshold_for_breaking_scaffolds 3


===== Breaking scaffolds finished. 


===== Terminate started. 


===== Terminate finished. 

 * Corrected reads are in /galaxy/server/database/jobs_directory/000/47/working/output/corrected/
 * Assembled contigs are in /galaxy/server/database/jobs_directory/000/47/working/output/contigs.fasta
 * Assembly graph is in /galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph.fastg
 * Assembly graph in GFA format is in /galaxy/server/database/jobs_directory/000/47/working/output/assembly_graph_with_scaffolds.gfa

======= SPAdes pipeline finished.

SPAdes log can be found here: /galaxy/server/database/jobs_directory/000/47/working/output/spades.log

Thank you for using SPAdes! If you use it in your research, please cite:

  Prjibelski, A., Antipov, D., Meleshko, D., Lapidus, A. and Korobeynikov, A., 2020. Using SPAdes de novo assembler. Current protocols in bioinformatics, 70(1), p.e102.
  doi.org/10.1002/cpbi.102

No scaffolds.fasta.

galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (47/gxy-n5gns) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (47/gxy-n5gns) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (47/gxy-n5gns) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-n5gns.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 47 (gxy-n5gns)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-n5gns to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 47 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:43,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (47/gxy-n5gns) Terminated at user's request
galaxy.util WARNING 2025-11-05 01:12:43,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/a/0/dataset_9a0edce1-ec18-45be-8d07-5c567370651b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/a/0/dataset_9a0edce1-ec18-45be-8d07-5c567370651b.dat'
galaxy.util WARNING 2025-11-05 01:12:43,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/2/4/dataset_924f13fa-f360-40fb-8d79-9708a84efef6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/2/4/dataset_924f13fa-f360-40fb-8d79-9708a84efef6.dat'
galaxy.util WARNING 2025-11-05 01:12:43,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/d/9/dataset_dd97a3a3-35cf-4ac3-b77b-524ed5e18425.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/d/9/dataset_dd97a3a3-35cf-4ac3-b77b-524ed5e18425.dat'
galaxy.util WARNING 2025-11-05 01:12:43,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/9/9/dataset_799d9eb4-fe2b-4051-8831-d6e69cb5a018.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/9/9/dataset_799d9eb4-fe2b-4051-8831-d6e69cb5a018.dat'
galaxy.jobs.handler DEBUG 2025-11-05 01:12:45,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 48, 49
tpv.core.entities DEBUG 2025-11-05 01:12:45,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:12:45,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:12:45,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:12:45,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:12:45,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:12:45,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-11-05 01:12:45,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (34.776 ms)
galaxy.jobs.handler INFO 2025-11-05 01:12:45,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:45,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 48
tpv.core.entities DEBUG 2025-11-05 01:12:45,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:12:45,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:12:45,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:12:45,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:12:45,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:12:45,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-11-05 01:12:45,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (37.243 ms)
galaxy.jobs.handler INFO 2025-11-05 01:12:45,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:45,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-11-05 01:12:45,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [48] prepared (76.392 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:12:45,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/48/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/48/configs/tmpuuyq0gdt']
galaxy.jobs.runners DEBUG 2025-11-05 01:12:45,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-11-05 01:12:45,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [49] prepared (71.065 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:45,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:12:45,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/49/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/49/configs/tmp393o_agd']
galaxy.jobs.runners DEBUG 2025-11-05 01:12:45,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:45,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:45,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:45,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:47,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:47,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:56,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m2d7f with k8s id: gxy-m2d7f succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:12:56,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9bxrr with k8s id: gxy-9bxrr succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:12:56,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:12:56,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:13:03,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 48 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:13:04,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/48/working/gxupload_0', 'object_id': 67}]}]}]
galaxy.jobs.runners DEBUG 2025-11-05 01:13:04,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 49 finished
galaxy.jobs INFO 2025-11-05 01:13:04,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:13:04,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/49/working/gxupload_0', 'object_id': 68}]}]}]
galaxy.jobs DEBUG 2025-11-05 01:13:04,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 48 executed (143.918 ms)
galaxy.jobs INFO 2025-11-05 01:13:04,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:04,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 48 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:13:04,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 49 executed (127.616 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:04,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 49 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:13:05,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-11-05 01:13:05,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:13:05,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:13:05,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:13:05,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:13:05,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:13:05,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-11-05 01:13:05,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (35.346 ms)
galaxy.jobs.handler INFO 2025-11-05 01:13:05,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:05,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-11-05 01:13:05,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [50] prepared (72.745 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:13:05,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:13:05,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:13:05,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:13:05,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/50/outputs/COMMAND_VERSION 2>&1;
mkdir -p paired_reads1 && ln -s '/galaxy/server/database/objects/6/7/c/dataset_67c1b2db-3f02-44e1-8249-6b879d4e2ee5.dat' 'paired_reads1/ecoli_1K1.fastq.gz' &&  ln -s '/galaxy/server/database/objects/a/0/e/dataset_a0eb03cc-a5e3-43cb-a8cb-87b0e3a39e71.dat' 'paired_reads1/ecoli_1K2.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py --only-assembler -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-1 1 'paired_reads1/ecoli_1K1.fastq.gz' --pe-2 1 'paired_reads1/ecoli_1K2.fastq.gz' --pe-or 1 fr        --cov-cutoff off]
galaxy.jobs.runners DEBUG 2025-11-05 01:13:05,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/50/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/f/e/e/dataset_fee79099-acf7-4f4a-8f68-f1eb7c985d4d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/50/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/f/e/e/dataset_fee79099-acf7-4f4a-8f68-f1eb7c985d4d.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/50/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/c/d/0/dataset_cd04bca4-b66d-46f1-9fa3-0d3e7d0600ae.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/50/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/c/d/0/dataset_cd04bca4-b66d-46f1-9fa3-0d3e7d0600ae.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/50/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/a/e/2/dataset_ae2ba7c2-db2a-4015-9eed-de5791a48490.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/50/working/output/contigs.fasta" "/galaxy/server/database/objects/a/e/2/dataset_ae2ba7c2-db2a-4015-9eed-de5791a48490.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/50/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/7/4/9/dataset_749c95dc-0fec-4988-b0dc-1dc60e4e95dd.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/50/working/output/scaffolds.fasta" "/galaxy/server/database/objects/7/4/9/dataset_749c95dc-0fec-4988-b0dc-1dc60e4e95dd.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:05,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:13:05,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:13:05,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:13:05,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:05,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:06,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:27,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7zbq9 with k8s id: gxy-7zbq9 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:13:28,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:13:35,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 50 finished
galaxy.model.metadata DEBUG 2025-11-05 01:13:35,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 69
galaxy.model.metadata DEBUG 2025-11-05 01:13:35,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 70
galaxy.model.metadata DEBUG 2025-11-05 01:13:35,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 71
galaxy.model.metadata DEBUG 2025-11-05 01:13:35,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 72
galaxy.util WARNING 2025-11-05 01:13:35,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/e/e/dataset_fee79099-acf7-4f4a-8f68-f1eb7c985d4d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/e/e/dataset_fee79099-acf7-4f4a-8f68-f1eb7c985d4d.dat'
galaxy.util WARNING 2025-11-05 01:13:35,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/d/0/dataset_cd04bca4-b66d-46f1-9fa3-0d3e7d0600ae.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/d/0/dataset_cd04bca4-b66d-46f1-9fa3-0d3e7d0600ae.dat'
galaxy.util WARNING 2025-11-05 01:13:35,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/e/2/dataset_ae2ba7c2-db2a-4015-9eed-de5791a48490.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/e/2/dataset_ae2ba7c2-db2a-4015-9eed-de5791a48490.dat'
galaxy.util WARNING 2025-11-05 01:13:35,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/4/9/dataset_749c95dc-0fec-4988-b0dc-1dc60e4e95dd.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/4/9/dataset_749c95dc-0fec-4988-b0dc-1dc60e4e95dd.dat'
galaxy.jobs INFO 2025-11-05 01:13:35,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-11-05 01:13:35,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 50 executed (142.563 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:35,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 50 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:13:38,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 51, 53, 52
tpv.core.entities DEBUG 2025-11-05 01:13:38,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:13:38,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:13:38,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:13:38,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:13:38,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:13:38,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-11-05 01:13:38,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (36.831 ms)
galaxy.jobs.handler INFO 2025-11-05 01:13:38,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 51
tpv.core.entities DEBUG 2025-11-05 01:13:38,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:13:38,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:13:38,097 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:13:38,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:13:38,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:13:38,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-11-05 01:13:38,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (47.213 ms)
galaxy.jobs.handler INFO 2025-11-05 01:13:38,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 53
tpv.core.entities DEBUG 2025-11-05 01:13:38,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:13:38,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:13:38,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:13:38,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:13:38,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [51] prepared (96.530 ms)
galaxy.jobs DEBUG 2025-11-05 01:13:38,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-11-05 01:13:38,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/51/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/51/configs/tmpww_dgi29']
galaxy.jobs DEBUG 2025-11-05 01:13:38,227 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-11-05 01:13:38,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (61.937 ms)
galaxy.jobs.handler INFO 2025-11-05 01:13:38,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners DEBUG 2025-11-05 01:13:38,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 52
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:13:38,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [53] prepared (95.890 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:13:38,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/53/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/53/configs/tmpfv4f_7sy']
galaxy.jobs.runners DEBUG 2025-11-05 01:13:38,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:13:38,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [52] prepared (71.459 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:13:38,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/52/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/52/configs/tmphiq86u_k']
galaxy.jobs.runners DEBUG 2025-11-05 01:13:38,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:38,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:39,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:39,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:48,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z27xl with k8s id: gxy-z27xl succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:48,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-k6jwz with k8s id: gxy-k6jwz succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:13:48,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:13:48,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-858c2 with k8s id: gxy-858c2 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:13:49,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:13:49,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:14:00,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 51 finished
galaxy.jobs.runners DEBUG 2025-11-05 01:14:00,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 53 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:14:00,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/51/working/gxupload_0', 'object_id': 73}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:14:01,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/53/working/gxupload_0', 'object_id': 75}]}]}]
galaxy.jobs INFO 2025-11-05 01:14:01,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs INFO 2025-11-05 01:14:01,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs DEBUG 2025-11-05 01:14:01,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 51 executed (189.774 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:01,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 51 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:14:01,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 53 executed (185.492 ms)
galaxy.jobs.runners DEBUG 2025-11-05 01:14:01,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 52 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:14:01,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/52/working/gxupload_0', 'object_id': 74}]}]}]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:01,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 53 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-11-05 01:14:01,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-11-05 01:14:01,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 52 executed (129.778 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:01,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 52 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:14:01,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54
tpv.core.entities DEBUG 2025-11-05 01:14:01,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:14:01,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:14:01,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:14:01,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:14:01,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:14:01,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-11-05 01:14:02,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (54.089 ms)
galaxy.jobs.handler INFO 2025-11-05 01:14:02,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:02,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 54
galaxy.jobs DEBUG 2025-11-05 01:14:02,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [54] prepared (81.909 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:14:02,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:14:02,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:14:02,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:14:02,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/54/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/6/d/7/dataset_6d7e5c02-63c7-4420-a0e3-e696bc01c900.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&    mkdir -p paired_reads2 && ln -s '/galaxy/server/database/objects/2/3/5/dataset_235e3426-286d-400f-8d2e-993e472c14b7.dat' 'paired_reads2/ecoli_1K1.fastq.gz' &&  ln -s '/galaxy/server/database/objects/d/2/e/dataset_d2e32a80-5217-4270-bba9-87a60d5d087d.dat' 'paired_reads2/ecoli_1K2.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py  -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --s 1 'reads1/ecoli_1K.fastq.gz.fastq.gz'     --pe-1 1 'paired_reads2/ecoli_1K1.fastq.gz' --pe-2 1 'paired_reads2/ecoli_1K2.fastq.gz' --pe-or 1 fr        --cov-cutoff off    --careful]
galaxy.jobs.runners DEBUG 2025-11-05 01:14:02,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/54/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/d/4/b/dataset_d4bbb218-def5-4500-8236-28be3b608ee7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/54/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/d/4/b/dataset_d4bbb218-def5-4500-8236-28be3b608ee7.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/54/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/7/e/d/dataset_7ede5570-ff36-44a7-b285-0cc22c8885d8.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/54/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/7/e/d/dataset_7ede5570-ff36-44a7-b285-0cc22c8885d8.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/54/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/d/1/9/dataset_d191acee-d706-44af-974c-56b4eba591d7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/54/working/output/contigs.fasta" "/galaxy/server/database/objects/d/1/9/dataset_d191acee-d706-44af-974c-56b4eba591d7.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/54/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/3/8/4/dataset_384c16ae-291c-4566-94ea-2c9c64efc76b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/54/working/output/scaffolds.fasta" "/galaxy/server/database/objects/3/8/4/dataset_384c16ae-291c-4566-94ea-2c9c64efc76b.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:02,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:14:02,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:14:02,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:14:02,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:02,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:03,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:23,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6486d with k8s id: gxy-6486d succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:14:23,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 54: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:14:31,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 54 finished
galaxy.model.metadata DEBUG 2025-11-05 01:14:31,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 76
galaxy.model.metadata DEBUG 2025-11-05 01:14:31,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 77
galaxy.model.metadata DEBUG 2025-11-05 01:14:31,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.model.metadata DEBUG 2025-11-05 01:14:31,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 79
galaxy.util WARNING 2025-11-05 01:14:31,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/4/b/dataset_d4bbb218-def5-4500-8236-28be3b608ee7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/4/b/dataset_d4bbb218-def5-4500-8236-28be3b608ee7.dat'
galaxy.util WARNING 2025-11-05 01:14:31,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/e/d/dataset_7ede5570-ff36-44a7-b285-0cc22c8885d8.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/e/d/dataset_7ede5570-ff36-44a7-b285-0cc22c8885d8.dat'
galaxy.util WARNING 2025-11-05 01:14:31,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/1/9/dataset_d191acee-d706-44af-974c-56b4eba591d7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/1/9/dataset_d191acee-d706-44af-974c-56b4eba591d7.dat'
galaxy.util WARNING 2025-11-05 01:14:31,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/8/4/dataset_384c16ae-291c-4566-94ea-2c9c64efc76b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/8/4/dataset_384c16ae-291c-4566-94ea-2c9c64efc76b.dat'
galaxy.jobs INFO 2025-11-05 01:14:31,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 54 in /galaxy/server/database/jobs_directory/000/54
galaxy.jobs DEBUG 2025-11-05 01:14:31,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 54 executed (169.734 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:31,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 54 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:14:33,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56, 55, 57
tpv.core.entities DEBUG 2025-11-05 01:14:33,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:14:33,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:14:33,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:14:33,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:14:33,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:14:33,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-11-05 01:14:33,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (36.936 ms)
galaxy.jobs.handler INFO 2025-11-05 01:14:33,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:33,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 55
tpv.core.entities DEBUG 2025-11-05 01:14:33,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:14:33,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:14:33,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:14:33,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:14:33,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:14:33,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-11-05 01:14:33,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (41.528 ms)
galaxy.jobs.handler INFO 2025-11-05 01:14:33,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:33,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 56
tpv.core.entities DEBUG 2025-11-05 01:14:33,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:14:33,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:14:33,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:14:33,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:14:33,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:14:33,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [55] prepared (100.209 ms)
galaxy.jobs DEBUG 2025-11-05 01:14:33,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-11-05 01:14:33,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (53.525 ms)
galaxy.jobs.handler INFO 2025-11-05 01:14:33,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.command_factory INFO 2025-11-05 01:14:33,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/55/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/55/configs/tmpjzvx149k']
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:33,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 57
galaxy.jobs.runners DEBUG 2025-11-05 01:14:33,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-11-05 01:14:33,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [56] prepared (94.838 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:33,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:33,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:14:33,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/56/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/56/configs/tmpq00zrauz']
galaxy.jobs.runners DEBUG 2025-11-05 01:14:33,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:33,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:14:33,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [57] prepared (94.792 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:33,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:14:33,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/57/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/57/configs/tmpjfo9b9bn']
galaxy.jobs.runners DEBUG 2025-11-05 01:14:34,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:34,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:34,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:34,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:34,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:34,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7zlwf with k8s id: gxy-7zlwf succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lbf4j failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners DEBUG 2025-11-05 01:14:44,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lbf4j.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-11-05 01:14:44,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-11-05-00-42-1/jobs/gxy-lbf4j

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-lbf4j": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-lbf4j) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-lbf4j) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-lbf4j) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-lbf4j) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lbf4j.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 56 (gxy-lbf4j)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2vhq6 with k8s id: gxy-2vhq6 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-lbf4j to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 56 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-11-05 01:14:44,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 57: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:44,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (56/gxy-lbf4j) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-11-05 01:14:52,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 55 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:14:52,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/55/working/gxupload_0', 'object_id': 80}]}]}]
galaxy.jobs INFO 2025-11-05 01:14:52,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-11-05 01:14:52,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 55 executed (129.620 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:52,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 55 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-11-05 01:14:52,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 57 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:14:52,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/57/working/gxupload_0', 'object_id': 82}]}]}]
galaxy.jobs INFO 2025-11-05 01:14:52,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 57 in /galaxy/server/database/jobs_directory/000/57
galaxy.jobs DEBUG 2025-11-05 01:14:52,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 57 executed (121.901 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:52,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 57 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:14:53,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58
tpv.core.entities DEBUG 2025-11-05 01:14:53,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:14:53,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:14:53,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:14:53,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:14:53,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:14:53,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-11-05 01:14:53,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (34.246 ms)
galaxy.jobs.handler INFO 2025-11-05 01:14:53,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:53,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-11-05 01:14:53,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [58] prepared (54.648 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:14:53,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/58/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/58/configs/tmpjn6wmyv3']
galaxy.jobs.runners DEBUG 2025-11-05 01:14:53,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:53,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:53,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:14:53,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:03,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5gm7f with k8s id: gxy-5gm7f succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:15:04,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:15:11,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 58 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:15:11,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/58/working/gxupload_0', 'object_id': 83}]}]}]
galaxy.jobs INFO 2025-11-05 01:15:11,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.jobs DEBUG 2025-11-05 01:15:11,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 58 executed (120.963 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:11,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 58 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:15:12,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-11-05 01:15:12,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:15:12,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:15:12,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:15:12,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:15:12,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:15:12,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-11-05 01:15:12,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (33.888 ms)
galaxy.jobs.handler INFO 2025-11-05 01:15:12,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:12,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-11-05 01:15:12,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [59] prepared (55.835 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:12,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:15:12,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:12,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:15:12,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/59/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/c/7/a/dataset_c7a8eb58-6d16-4c53-846d-74c37beeaf31.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py --only-error-correction -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-12 1 'reads1/ecoli_1K.fastq.gz.fastq.gz' --pe-or 1 fr        --cov-cutoff off    --careful]
galaxy.jobs.runners DEBUG 2025-11-05 01:15:12,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/59/working/output/spades.log" -a -f "/galaxy/server/database/objects/2/7/d/dataset_27d275f2-970c-4742-8e46-05ab3c92f295.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/59/working/output/spades.log" "/galaxy/server/database/objects/2/7/d/dataset_27d275f2-970c-4742-8e46-05ab3c92f295.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:12,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:12,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:15:12,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:12,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:12,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:13,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:19,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lx45m with k8s id: gxy-lx45m succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:15:19,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:15:26,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 59 finished
galaxy.model.store.discover DEBUG 2025-11-05 01:15:26,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/59/working/output/corrected/ecoli_1K.fastq.gz_100.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz_100.0_0] for output [out_cr] (3.881 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:15:26,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/59/working/output/corrected/ecoli_1K.fastq.gz_200.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz_200.0_0] for output [out_cr] (0.572 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:15:26,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/59/working/output/corrected/ecoli_1K.fastq.gz__unpaired00.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz__unpaired00.0_0] for output [out_cr] (0.450 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:15:26,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (59) Add dynamic collection datasets to history for output [out_cr] (33.029 ms)
galaxy.model.metadata DEBUG 2025-11-05 01:15:26,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 84
galaxy.util WARNING 2025-11-05 01:15:26,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/7/d/dataset_27d275f2-970c-4742-8e46-05ab3c92f295.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/7/d/dataset_27d275f2-970c-4742-8e46-05ab3c92f295.dat'
galaxy.jobs INFO 2025-11-05 01:15:26,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-11-05 01:15:26,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 59 executed (183.115 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:26,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 59 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:15:29,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-11-05 01:15:29,140 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:15:29,140 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:15:29,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:15:29,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:15:29,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:15:29,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-11-05 01:15:29,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (29.409 ms)
galaxy.jobs.handler INFO 2025-11-05 01:15:29,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:29,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-11-05 01:15:29,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [60] prepared (65.954 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:15:29,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/60/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/60/configs/tmp6qwnws2q']
galaxy.jobs.runners DEBUG 2025-11-05 01:15:29,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:29,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:29,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:30,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:40,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pbgk2 with k8s id: gxy-pbgk2 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:15:40,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:15:48,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 60 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:15:48,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/60/working/gxupload_0', 'object_id': 88}]}]}]
galaxy.jobs INFO 2025-11-05 01:15:48,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-11-05 01:15:48,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 60 executed (144.411 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:48,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 60 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:15:49,592 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-11-05 01:15:49,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:15:49,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:15:49,627 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:15:49,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:15:49,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:15:49,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-11-05 01:15:49,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (33.880 ms)
galaxy.jobs.handler INFO 2025-11-05 01:15:49,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:49,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-11-05 01:15:49,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [61] prepared (54.175 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:49,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:15:49,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:49,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:15:49,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/61/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/d/0/e/dataset_d0eae897-dc4b-40ad-b33e-8835188fde8b.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py --only-error-correction -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-12 1 'reads1/ecoli_1K.fastq.gz.fastq.gz' --pe-or 1 fr        --cov-cutoff off    --careful --sc]
galaxy.jobs.runners DEBUG 2025-11-05 01:15:49,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/61/working/output/spades.log" -a -f "/galaxy/server/database/objects/3/9/a/dataset_39a8cda8-e99f-4b32-82d7-17f2dd08563c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/61/working/output/spades.log" "/galaxy/server/database/objects/3/9/a/dataset_39a8cda8-e99f-4b32-82d7-17f2dd08563c.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:49,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:49,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:15:49,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:15:50,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:50,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:50,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6j26n failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-6j26n.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-11-05 01:15:55,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-11-05-00-42-1/jobs/gxy-6j26n

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-6j26n": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61/gxy-6j26n) tool_stdout: Command line: /usr/local/bin/spades.py	--only-error-correction	-o	/galaxy/server/database/jobs_directory/000/61/working/output	-t	8	-m	14	--tmp-dir	/galaxy/server/database/jobs_directory/000/61/tmp	--pe-12	1	/galaxy/server/database/jobs_directory/000/61/working/reads1/ecoli_1K.fastq.gz.fastq.gz	--pe-or	1	fr	--cov-cutoff	off	--careful	--sc	

System information:
  SPAdes version: 4.2.0
  Python version: 3.13.7
  OS: Linux-6.6.97+-x86_64-with-glibc2.36

Output dir: /galaxy/server/database/jobs_directory/000/61/working/output
Mode: ONLY read error correction (without assembling)
Debug mode is turned OFF

Dataset parameters:
  Single-cell mode
  Reads:
    Library number: 1, library type: paired-end
      orientation: fr
      left reads: not specified
      right reads: not specified
      interlaced reads: ['/galaxy/server/database/jobs_directory/000/61/working/reads1/ecoli_1K.fastq.gz.fastq.gz']
      single reads: not specified
      merged reads: not specified
Read error correction parameters:
  Iterations: 1
  PHRED offset will be auto-detected
  Corrected reads will be compressed
Other parameters:
  Dir for temp files: /galaxy/server/database/jobs_directory/000/61/tmp
  Threads: 8
  Memory limit (in Gb): 14


======= SPAdes pipeline started. Log can be found here: /galaxy/server/database/jobs_directory/000/61/working/output/spades.log


===== Before start started. 


===== Preprocess reads started. 


===== Preprocess interlaced reads started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/preprocess_interlaced_reads.py --args_filename /galaxy/server/database/jobs_directory/000/61/working/output/split_input/tmp/interlaced --dst /galaxy/server/database/jobs_directory/000/61/working/output/split_input

== Splitting /galaxy/server/database/jobs_directory/000/61/working/reads1/ecoli_1K.fastq.gz.fastq.gz into left and right reads (in /galaxy/server/database/jobs_directory/000/61/working/output/split_input directory)

===== Preprocess interlaced reads finished. 


===== Preprocess reads finished. 


===== Read error correction started. 


===== Read error correction started. 


== Running: /usr/local/bin/spades-hammer /galaxy/server/database/jobs_directory/000/61/working/output/corrected/configs/config.info

  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  :  76)   Starting BayesHammer, built from N/A, git revision N/A
  0:00:00.000     1M / 22M   INFO    General                 (main.cpp                  :  77)   Loading config from "/galaxy/server/database/jobs_directory/000/61/working/output/corrected/configs/config.info"
  0:00:00.002     1M / 22M   INFO    General                 (main.cpp                  :  79)   Maximum # of threads to use (adjusted due to OMP capabilities): 8
  0:00:00.002     1M / 22M   INFO    General                 (memory_limit.cpp          :  55)   Memory limit set to 14 Gb
  0:00:00.002     1M / 22M   INFO    General                 (main.cpp                  :  87)   Trying to determine PHRED offset
  0:00:00.004     1M / 22M   INFO    General                 (main.cpp                  :  93)   Determined value is 33
  0:00:00.004     1M / 22M   INFO    General                 (hammer_tools.cpp          :  40)   Hamming graph threshold tau=1, k=21, subkmer positions = [ 0 10 ]
  0:00:00.004     1M / 22M   INFO    General                 (main.cpp                  : 114)   Size of aux. kmer data 24 bytes
     === ITERATION 0 begins ===
  0:00:00.005     1M / 22M   INFO    General                 (kmer_index_builder.hpp    : 308)   Splitting kmer instances into 16 files using 8 threads. This might take a while.
  0:00:00.022     1M / 22M   INFO    General                 (file_limit.hpp            :  43)   Open file limit set to 1048576
  0:00:00.022     1M / 22M   INFO    General                 (kmer_splitter.hpp         :  96)   Memory available for splitting buffers: 0.583329 Gb
  0:00:00.022     1M / 22M   INFO    General                 (kmer_splitter.hpp         : 104)   Using cell size of 4194304
  0:00:00.024  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             :  98)   Processing "/galaxy/server/database/jobs_directory/000/61/working/output/split_input/ecoli_1K.fastq.gz_1.fastq"
  0:00:00.155  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             :  98)   Processing "/galaxy/server/database/jobs_directory/000/61/working/output/split_input/ecoli_1K.fastq.gz_2.fastq"
  0:00:00.294  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             : 113)   Total 4108 reads processed
  0:00:00.298     1M / 22M   INFO    General                 (kmer_index_builder.hpp    : 314)   Starting k-mer counting.
  0:00:00.340     1M / 22M   INFO    General                 (kmer_index_builder.hpp    : 325)   K-mer counting done. There are 1974 kmers in total.
  0:00:00.341     1M / 22M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 460)   Building perfect hash indices
  0:00:00.356     1M / 22M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 496)   Index built. Total 1974 kmers, 13944 bytes occupied (56.5106 bits per kmer).
  0:00:00.356     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 355)   Arranging kmers in hash map order
  0:00:00.371     1M / 22M   INFO    General                 (main.cpp                  : 149)   Clustering Hamming graph.
  0:00:00.373     1M / 22M   INFO    General                 (main.cpp                  : 156)   Extracting clusters:
  0:00:00.373     1M / 22M   INFO    General                 (concurrent_dsu.cpp        :  19)   Connecting to root
  0:00:00.374     1M / 22M   INFO    General                 (concurrent_dsu.cpp        :  35)   Calculating counts
  0:00:00.374     1M / 22M   INFO    General                 (concurrent_dsu.cpp        :  64)   Writing down entries
  0:00:00.385     1M / 22M   INFO    General                 (main.cpp                  : 168)   Clustering done. Total clusters: 1960
  0:00:00.385     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 372)   Collecting K-mer information, this takes a while.
  0:00:00.385     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 378)   Processing "/galaxy/server/database/jobs_directory/000/61/working/output/split_input/ecoli_1K.fastq.gz_1.fastq"
  0:00:00.399     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 378)   Processing "/galaxy/server/database/jobs_directory/000/61/working/output/split_input/ecoli_1K.fastq.gz_2.fastq"
  0:00:00.412     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 385)   Collection done, postprocessing.
  0:00:00.412     1M / 22M   INFO   K-mer Counting           (kmer_data.cpp             : 398)   There are 1974 kmers in total. Among them 0 (0%) are singletons.
  0:00:00.412     1M / 22M   INFO    General                 (main.cpp                  : 174)   Subclustering Hamming graph
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 637)   Subclustering done. Total 0 non-read kmers were generated.
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 638)   Subclustering statistics:
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 639)     Total singleton hamming clusters: 1946. Among them 1946 (100%) are good
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 640)     Total singleton subclusters: 8. Among them 8 (100%) are good
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 641)     Total non-singleton subcluster centers: 10. Among them 10 (100%) are good
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 642)     Average size of non-trivial subcluster: 2.8 kmers
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 643)     Average number of sub-clusters per non-singleton cluster: 1.28571
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 644)     Total solid k-mers: 1964
  0:00:00.422     1M / 22M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 645)     Substitution probabilities: (        0.875        0.125            0            0 )
(            0            1            0            0 )
(            0            0            1            0 )
(            0            0        0.125        0.875 )
  0:00:00.423     1M / 22M   INFO    General                 (main.cpp                  : 179)   Finished clustering.
  0:00:00.423     1M / 22M   INFO    General                 (main.cpp                  : 198)   Starting solid k-mers expansion in 8 threads.
  0:00:00.435     1M / 22M   INFO    General                 (main.cpp                  : 219)   Solid k-mers iteration 0 produced 0 new k-mers.
  0:00:00.435     1M / 22M   INFO    General                 (main.cpp                  : 223)   Solid k-mers finalized
  0:00:00.435     1M / 22M   INFO    General                 (hammer_tools.cpp          : 226)   Starting read correction in 8 threads.
  0:00:00.435     1M / 22M   INFO    General                 (hammer_tools.cpp          : 239)   Correcting pair of reads: "/galaxy/server/database/jobs_directory/000/61/working/output/split_input/ecoli_1K.fastq.gz_1.fastq" and "/galaxy/server/database/jobs_directory/000/61/working/output/split_input/ecoli_1K.fastq.gz_2.fastq"
  0:00:00.520   178M / 178M  INFO    General                 (hammer_tools.cpp          : 173)   Prepared batch 0 of 2054 reads.
  0:00:00.523   178M / 178M  INFO    General                 (hammer_tools.cpp          : 180)   Processed batch 0
  0:00:00.526   178M / 178M  INFO    General                 (hammer_tools.cpp          : 190)   Written batch 0
  0:00:00.578     1M / 178M  INFO    General                 (hammer_tools.cpp          : 280)   Correction done. Changed 4 bases in 4 reads.
  0:00:00.578     1M / 178M  INFO    General                 (hammer_tools.cpp          : 281)   Failed to correct 0 bases out of 353915.
  0:00:00.578     1M / 178M  INFO    General                 (main.cpp                  : 256)   Saving corrected dataset description to /galaxy/server/database/jobs_directory/000/61/working/output/corrected/corrected.yaml
  0:00:00.582     1M / 178M  INFO    General                 (main.cpp                  : 263)   All done. Exiting.

===== Read error correction finished. 


===== corrected reads compression started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/compress_all.py --input_file /galaxy/server/database/jobs_directory/000/61/working/output/corrected/corrected.yaml --ext_python_modules_home /usr/local/share/spades --max_threads 8 --output_dir /galaxy/server/database/jobs_directory/000/61/working/output/corrected --gzip_output

== Compressing corrected reads (with gzip)
== Files to compress: ['/galaxy/server/database/jobs_directory/000/61/working/output/corrected/ecoli_1K.fastq.gz_100.0_0.cor.fastq', '/galaxy/server/database/jobs_directory/000/61/working/output/corrected/ecoli_1K.fastq.gz_200.0_0.cor.fastq', '/galaxy/server/database/jobs_directory/000/61/working/output/corrected/ecoli_1K.fastq.gz__unpaired00.0_0.cor.fastq']
== Files compression is finished
== Dataset yaml file is updated

===== corrected reads compression finished. 


===== Read error correction finished. 


===== Breaking scaffolds started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/breaking_scaffolds_script.py --result_scaffolds_filename /galaxy/server/database/jobs_directory/000/61/working/output/scaffolds.fasta --misc_dir /galaxy/server/database/jobs_directory/000/61/working/output/misc --threshold_for_breaking_scaffolds 3


===== Breaking scaffolds finished. 


===== Terminate started. 


===== Terminate finished. 

 * Corrected reads are in /galaxy/server/database/jobs_directory/000/61/working/output/corrected/

======= SPAdes pipeline finished.

SPAdes log can be found here: /galaxy/server/database/jobs_directory/000/61/working/output/spades.log

Thank you for using SPAdes! If you use it in your research, please cite:

  Prjibelski, A., Antipov, D., Meleshko, D., Lapidus, A. and Korobeynikov, A., 2020. Using SPAdes de novo assembler. Current protocols in bioinformatics, 70(1), p.e102.
  doi.org/10.1002/cpbi.102


galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61/gxy-6j26n) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61/gxy-6j26n) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61/gxy-6j26n) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-6j26n.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 61 (gxy-6j26n)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-6j26n to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 61 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:55,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61/gxy-6j26n) Terminated at user's request
galaxy.util WARNING 2025-11-05 01:15:56,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/9/a/dataset_39a8cda8-e99f-4b32-82d7-17f2dd08563c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/9/a/dataset_39a8cda8-e99f-4b32-82d7-17f2dd08563c.dat'
galaxy.jobs.handler DEBUG 2025-11-05 01:15:57,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-11-05 01:15:57,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:15:57,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:15:57,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:15:57,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:15:57,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:15:57,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-11-05 01:15:57,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (36.047 ms)
galaxy.jobs.handler INFO 2025-11-05 01:15:57,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:57,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-11-05 01:15:57,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [62] prepared (61.951 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:15:57,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/62/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/62/configs/tmp1r0lldhp']
galaxy.jobs.runners DEBUG 2025-11-05 01:15:57,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:57,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:58,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:15:58,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:08,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-phfgf with k8s id: gxy-phfgf succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:16:08,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:16:15,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 62 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:16:15,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/62/working/gxupload_0', 'object_id': 90}]}]}]
galaxy.jobs INFO 2025-11-05 01:16:15,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.jobs DEBUG 2025-11-05 01:16:16,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 62 executed (127.517 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:16,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 62 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:16:17,214 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-11-05 01:16:17,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:16:17,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:16:17,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:16:17,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:16:17,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:16:17,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-11-05 01:16:17,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (35.480 ms)
galaxy.jobs.handler INFO 2025-11-05 01:16:17,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:17,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-11-05 01:16:17,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [63] prepared (51.005 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:16:17,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:16:17,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:16:17,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:16:17,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/63/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/3/a/d/dataset_3ad7b9ca-a37e-44a9-aa94-477e4237f3a6.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py --only-assembler -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-12 1 'reads1/ecoli_1K.fastq.gz.fastq.gz' --pe-or 1 fr        --cov-cutoff off    --sc]
galaxy.jobs.runners DEBUG 2025-11-05 01:16:17,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/0/4/f/dataset_04fd8729-9634-441d-9fd7-34d256534837.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/0/4/f/dataset_04fd8729-9634-441d-9fd7-34d256534837.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/d/4/4/dataset_d44a84e6-281b-4682-96a8-7ebe3fda5de6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/d/4/4/dataset_d44a84e6-281b-4682-96a8-7ebe3fda5de6.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/a/6/f/dataset_a6fafb0d-d580-4325-ab5d-3e06ca47fee9.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/output/contigs.fasta" "/galaxy/server/database/objects/a/6/f/dataset_a6fafb0d-d580-4325-ab5d-3e06ca47fee9.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/63/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/0/e/3/dataset_0e35acb5-4902-46f9-98e8-1982a27a6983.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/63/working/output/scaffolds.fasta" "/galaxy/server/database/objects/0/e/3/dataset_0e35acb5-4902-46f9-98e8-1982a27a6983.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:17,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:16:17,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:16:17,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:16:17,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:17,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:18,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:31,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zpbrc with k8s id: gxy-zpbrc succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:16:31,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 63: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:16:39,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 63 finished
galaxy.model.metadata DEBUG 2025-11-05 01:16:39,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 91
galaxy.model.metadata DEBUG 2025-11-05 01:16:39,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 92
galaxy.model.metadata DEBUG 2025-11-05 01:16:39,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 93
galaxy.model.metadata DEBUG 2025-11-05 01:16:39,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 94
galaxy.util WARNING 2025-11-05 01:16:39,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/4/f/dataset_04fd8729-9634-441d-9fd7-34d256534837.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/4/f/dataset_04fd8729-9634-441d-9fd7-34d256534837.dat'
galaxy.util WARNING 2025-11-05 01:16:39,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/4/4/dataset_d44a84e6-281b-4682-96a8-7ebe3fda5de6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/4/4/dataset_d44a84e6-281b-4682-96a8-7ebe3fda5de6.dat'
galaxy.util WARNING 2025-11-05 01:16:39,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/6/f/dataset_a6fafb0d-d580-4325-ab5d-3e06ca47fee9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/6/f/dataset_a6fafb0d-d580-4325-ab5d-3e06ca47fee9.dat'
galaxy.util WARNING 2025-11-05 01:16:39,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/e/3/dataset_0e35acb5-4902-46f9-98e8-1982a27a6983.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/e/3/dataset_0e35acb5-4902-46f9-98e8-1982a27a6983.dat'
galaxy.jobs INFO 2025-11-05 01:16:39,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 63 in /galaxy/server/database/jobs_directory/000/63
galaxy.jobs DEBUG 2025-11-05 01:16:39,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 63 executed (174.531 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:39,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 63 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:16:41,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-11-05 01:16:41,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:16:41,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:16:41,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:16:41,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:16:41,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:16:41,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-11-05 01:16:41,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (31.438 ms)
galaxy.jobs.handler INFO 2025-11-05 01:16:41,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:41,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-11-05 01:16:41,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [64] prepared (57.918 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:16:41,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/64/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/64/configs/tmp54ao_ojc']
galaxy.jobs.runners DEBUG 2025-11-05 01:16:41,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:41,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:41,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:42,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:51,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gcj4g with k8s id: gxy-gcj4g succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:16:52,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:16:59,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 64 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:16:59,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/64/working/gxupload_0', 'object_id': 95}]}]}]
galaxy.jobs INFO 2025-11-05 01:16:59,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.jobs DEBUG 2025-11-05 01:16:59,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 64 executed (128.676 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:16:59,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 64 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:17:00,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65
tpv.core.entities DEBUG 2025-11-05 01:17:00,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:17:00,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:17:00,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:17:00,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:17:00,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:17:00,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-11-05 01:17:00,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (41.111 ms)
galaxy.jobs.handler INFO 2025-11-05 01:17:00,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:00,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-11-05 01:17:00,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [65] prepared (58.747 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:00,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:17:00,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:00,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:17:00,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/65/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/0/9/4/dataset_0942cf95-f851-4f05-80b0-499aed9b6652.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py --only-error-correction -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-12 1 'reads1/ecoli_1K.fastq.gz.fastq.gz' --pe-or 1 fr        --cov-cutoff off]
galaxy.jobs.runners DEBUG 2025-11-05 01:17:00,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/65/working/output/spades.log" -a -f "/galaxy/server/database/objects/1/1/e/dataset_11ed25b3-1f05-43f3-814b-9c775b81ec46.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/65/working/output/spades.log" "/galaxy/server/database/objects/1/1/e/dataset_11ed25b3-1f05-43f3-814b-9c775b81ec46.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:00,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:00,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:17:00,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:00,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:00,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:01,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:06,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dxsgg with k8s id: gxy-dxsgg succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:17:06,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:17:14,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 65 finished
galaxy.model.store.discover DEBUG 2025-11-05 01:17:14,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (65) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/65/working/output/corrected/ecoli_1K.fastq.gz_100.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz_100.0_0] for output [out_cr] (2.497 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:17:14,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (65) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/65/working/output/corrected/ecoli_1K.fastq.gz_200.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz_200.0_0] for output [out_cr] (0.635 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:17:14,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (65) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/65/working/output/corrected/ecoli_1K.fastq.gz__unpaired00.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz__unpaired00.0_0] for output [out_cr] (0.568 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:17:14,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (65) Add dynamic collection datasets to history for output [out_cr] (37.036 ms)
galaxy.model.metadata DEBUG 2025-11-05 01:17:14,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 96
galaxy.util WARNING 2025-11-05 01:17:14,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/1/e/dataset_11ed25b3-1f05-43f3-814b-9c775b81ec46.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/1/e/dataset_11ed25b3-1f05-43f3-814b-9c775b81ec46.dat'
galaxy.jobs INFO 2025-11-05 01:17:14,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-11-05 01:17:14,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 65 executed (189.207 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:14,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 65 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:17:15,522 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-11-05 01:17:15,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:17:15,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:17:15,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:17:15,559 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:17:15,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:17:15,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-11-05 01:17:15,595 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (35.935 ms)
galaxy.jobs.handler INFO 2025-11-05 01:17:15,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:15,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-11-05 01:17:15,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [66] prepared (51.007 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:17:15,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/66/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/66/configs/tmpeni7m_vq']
galaxy.jobs.runners DEBUG 2025-11-05 01:17:15,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:15,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:15,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:16,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:26,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-42gjc with k8s id: gxy-42gjc succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:17:26,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:17:34,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 66 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:17:34,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/66/working/gxupload_0', 'object_id': 100}]}]}]
galaxy.jobs INFO 2025-11-05 01:17:34,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.jobs DEBUG 2025-11-05 01:17:34,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 66 executed (142.322 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:34,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 66 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:17:34,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67
tpv.core.entities DEBUG 2025-11-05 01:17:34,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:17:34,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:17:35,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:17:35,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:17:35,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:17:35,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-11-05 01:17:35,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (45.151 ms)
galaxy.jobs.handler INFO 2025-11-05 01:17:35,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:35,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 67
galaxy.jobs DEBUG 2025-11-05 01:17:35,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [67] prepared (57.473 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:35,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:17:35,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:35,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:17:35,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/67/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/0/e/8/dataset_0e8b7369-aec4-4793-a389-f205e552e616.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py --only-error-correction -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-12 1 'reads1/ecoli_1K.fastq.gz.fastq.gz' --pe-or 1 fr        --cov-cutoff off    --sc]
galaxy.jobs.runners DEBUG 2025-11-05 01:17:35,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/67/working/output/spades.log" -a -f "/galaxy/server/database/objects/c/2/d/dataset_c2d51c7b-eae8-482e-bfd8-ff87eaa04892.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/67/working/output/spades.log" "/galaxy/server/database/objects/c/2/d/dataset_c2d51c7b-eae8-482e-bfd8-ff87eaa04892.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:35,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:35,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:17:35,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:17:35,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:35,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:35,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:40,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fnh4v with k8s id: gxy-fnh4v succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:17:40,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:17:48,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 67 finished
galaxy.model.store.discover DEBUG 2025-11-05 01:17:48,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/67/working/output/corrected/ecoli_1K.fastq.gz_100.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz_100.0_0] for output [out_cr] (2.799 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:17:48,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/67/working/output/corrected/ecoli_1K.fastq.gz_200.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz_200.0_0] for output [out_cr] (0.624 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:17:48,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/67/working/output/corrected/ecoli_1K.fastq.gz__unpaired00.0_0.cor.fastq.gz] with element identifier [ecoli_1K.fastq.gz__unpaired00.0_0] for output [out_cr] (0.964 ms)
galaxy.model.store.discover DEBUG 2025-11-05 01:17:48,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (67) Add dynamic collection datasets to history for output [out_cr] (40.233 ms)
galaxy.model.metadata DEBUG 2025-11-05 01:17:48,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 101
galaxy.util WARNING 2025-11-05 01:17:48,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/2/d/dataset_c2d51c7b-eae8-482e-bfd8-ff87eaa04892.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/2/d/dataset_c2d51c7b-eae8-482e-bfd8-ff87eaa04892.dat'
galaxy.jobs INFO 2025-11-05 01:17:48,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs DEBUG 2025-11-05 01:17:48,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 67 executed (180.072 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:48,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 67 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:17:50,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 68
tpv.core.entities DEBUG 2025-11-05 01:17:50,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:17:50,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:17:50,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:17:50,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:17:50,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:17:50,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-11-05 01:17:50,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (37.682 ms)
galaxy.jobs.handler INFO 2025-11-05 01:17:50,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:50,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-11-05 01:17:50,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [68] prepared (84.606 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:17:50,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/68/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/68/configs/tmpxxyn3px4']
galaxy.jobs.runners DEBUG 2025-11-05 01:17:50,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:50,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:50,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:17:50,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6nfgv failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-6nfgv.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-11-05 01:18:00,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-11-05-00-42-1/jobs/gxy-6nfgv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-6nfgv": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (68/gxy-6nfgv) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (68/gxy-6nfgv) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (68/gxy-6nfgv) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (68/gxy-6nfgv) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-6nfgv.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 68 (gxy-6nfgv)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-6nfgv to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 68 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:00,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (68/gxy-6nfgv) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-11-05 01:18:01,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69, 70
tpv.core.entities DEBUG 2025-11-05 01:18:01,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:18:01,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:18:01,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:18:01,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:18:01,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:18:01,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-11-05 01:18:01,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (37.501 ms)
galaxy.jobs.handler INFO 2025-11-05 01:18:01,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:01,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 69
tpv.core.entities DEBUG 2025-11-05 01:18:01,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:18:01,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:18:01,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:18:01,685 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:18:01,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:18:01,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-11-05 01:18:01,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (46.303 ms)
galaxy.jobs.handler INFO 2025-11-05 01:18:01,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:01,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 70
galaxy.jobs DEBUG 2025-11-05 01:18:01,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [69] prepared (81.832 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:18:01,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/69/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/69/configs/tmpb6r10cz2']
galaxy.jobs.runners DEBUG 2025-11-05 01:18:01,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-11-05 01:18:01,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [70] prepared (88.277 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:01,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:18:01,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/70/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/70/configs/tmpstdl9d5u']
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:01,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-11-05 01:18:01,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:01,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:01,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:03,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:03,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:12,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gvwt5 with k8s id: gxy-gvwt5 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:12,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hfkcp with k8s id: gxy-hfkcp succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:18:12,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:18:13,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:18:21,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 69 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:18:21,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/69/working/gxupload_0', 'object_id': 106}]}]}]
galaxy.jobs.runners DEBUG 2025-11-05 01:18:21,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 70 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:18:21,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/70/working/gxupload_0', 'object_id': 107}]}]}]
galaxy.jobs INFO 2025-11-05 01:18:21,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs INFO 2025-11-05 01:18:21,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs DEBUG 2025-11-05 01:18:21,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 69 executed (195.671 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:21,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 69 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:18:21,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 70 executed (170.816 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:21,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 70 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:18:22,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71
tpv.core.entities DEBUG 2025-11-05 01:18:22,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:18:22,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:18:22,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:18:22,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:18:22,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:18:22,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-11-05 01:18:22,240 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (54.755 ms)
galaxy.jobs.handler INFO 2025-11-05 01:18:22,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:22,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-11-05 01:18:22,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [71] prepared (77.489 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:22,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:18:22,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:22,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:18:22,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/71/outputs/COMMAND_VERSION 2>&1;
mkdir -p paired_reads1 && ln -s '/galaxy/server/database/objects/b/8/c/dataset_b8c0951d-65d6-42e5-a456-2fbc4bff1dc3.dat' 'paired_reads1/ecoli_1K1.fastq.gz' &&  ln -s '/galaxy/server/database/objects/4/c/8/dataset_4c8fbc09-5194-4692-a4fe-dbdab292e9d9.dat' 'paired_reads1/ecoli_1K2.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py  -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-1 1 'paired_reads1/ecoli_1K1.fastq.gz' --pe-2 1 'paired_reads1/ecoli_1K2.fastq.gz' --pe-or 1 fr        --cov-cutoff off    --sc --careful]
galaxy.jobs.runners DEBUG 2025-11-05 01:18:22,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/d/b/d/dataset_dbdcc72e-8de7-4d7e-8a9f-e64ba80c2bd1.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/d/b/d/dataset_dbdcc72e-8de7-4d7e-8a9f-e64ba80c2bd1.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/3/5/d/dataset_35dc758d-12d0-466b-9de0-968c5cffca06.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/3/5/d/dataset_35dc758d-12d0-466b-9de0-968c5cffca06.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/71/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/7/d/1/dataset_7d1a855c-4d16-4852-99c2-b42dd7960f5c.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/71/working/output/contigs.fasta" "/galaxy/server/database/objects/7/d/1/dataset_7d1a855c-4d16-4852-99c2-b42dd7960f5c.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/0/4/5/dataset_0457a4c5-f22e-4e88-aa1f-2d1027bb414a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.fasta" "/galaxy/server/database/objects/0/4/5/dataset_0457a4c5-f22e-4e88-aa1f-2d1027bb414a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:22,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:22,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:18:22,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:22,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:22,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:22,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p76tr failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-p76tr.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-11-05 01:18:37,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-11-05-00-42-1/jobs/gxy-p76tr

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-p76tr": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71/gxy-p76tr) tool_stdout: Command line: /usr/local/bin/spades.py	-o	/galaxy/server/database/jobs_directory/000/71/working/output	-t	8	-m	14	--tmp-dir	/galaxy/server/database/jobs_directory/000/71/tmp	--pe-1	1	/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz	--pe-2	1	/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz	--pe-or	1	fr	--cov-cutoff	off	--sc	--careful	

System information:
  SPAdes version: 4.2.0
  Python version: 3.13.7
  OS: Linux-6.6.97+-x86_64-with-glibc2.36

Output dir: /galaxy/server/database/jobs_directory/000/71/working/output
Mode: read error correction and assembling
Debug mode is turned OFF

Dataset parameters:
  Single-cell mode
  Reads:
    Library number: 1, library type: paired-end
      orientation: fr
      left reads: ['/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz']
      right reads: ['/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz']
      interlaced reads: not specified
      single reads: not specified
      merged reads: not specified
Read error correction parameters:
  Iterations: 1
  PHRED offset will be auto-detected
  Corrected reads will be compressed
Assembly parameters:
  k: [21, 33, 55]
  Repeat resolution is enabled
  Mismatch careful mode is turned ON
  MismatchCorrector will be used
  Coverage cutoff is turned OFF
  Assembly graph output will use GFA v1.2 format
Other parameters:
  Dir for temp files: /galaxy/server/database/jobs_directory/000/71/tmp
  Threads: 8
  Memory limit (in Gb): 14


======= SPAdes pipeline started. Log can be found here: /galaxy/server/database/jobs_directory/000/71/working/output/spades.log

/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz: max reads length: 100
/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz: max reads length: 100

Reads length: 100


===== Before start started. 


===== Read error correction started. 


===== Read error correction started. 


== Running: /usr/local/bin/spades-hammer /galaxy/server/database/jobs_directory/000/71/working/output/corrected/configs/config.info

  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  :  76)   Starting BayesHammer, built from N/A, git revision N/A
  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  :  77)   Loading config from "/galaxy/server/database/jobs_directory/000/71/working/output/corrected/configs/config.info"
  0:00:00.004     1M / 23M   INFO    General                 (main.cpp                  :  79)   Maximum # of threads to use (adjusted due to OMP capabilities): 8
  0:00:00.004     1M / 23M   INFO    General                 (memory_limit.cpp          :  55)   Memory limit set to 14 Gb
  0:00:00.005     1M / 23M   INFO    General                 (main.cpp                  :  87)   Trying to determine PHRED offset
  0:00:00.005     1M / 23M   INFO    General                 (main.cpp                  :  93)   Determined value is 33
  0:00:00.005     1M / 23M   INFO    General                 (hammer_tools.cpp          :  40)   Hamming graph threshold tau=1, k=21, subkmer positions = [ 0 10 ]
  0:00:00.005     1M / 23M   INFO    General                 (main.cpp                  : 114)   Size of aux. kmer data 24 bytes
     === ITERATION 0 begins ===
  0:00:00.008     1M / 23M   INFO    General                 (kmer_index_builder.hpp    : 308)   Splitting kmer instances into 16 files using 8 threads. This might take a while.
  0:00:00.028     1M / 23M   INFO    General                 (file_limit.hpp            :  43)   Open file limit set to 1048576
  0:00:00.029     1M / 23M   INFO    General                 (kmer_splitter.hpp         :  96)   Memory available for splitting buffers: 0.583329 Gb
  0:00:00.029     1M / 23M   INFO    General                 (kmer_splitter.hpp         : 104)   Using cell size of 4194304
  0:00:00.031  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             :  98)   Processing "/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz"
  0:00:00.166  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             :  98)   Processing "/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz"
  0:00:00.305  4609M / 4609M INFO   K-mer Splitting          (kmer_data.cpp             : 113)   Total 4108 reads processed
  0:00:00.309     1M / 23M   INFO    General                 (kmer_index_builder.hpp    : 314)   Starting k-mer counting.
  0:00:00.354     1M / 23M   INFO    General                 (kmer_index_builder.hpp    : 325)   K-mer counting done. There are 1974 kmers in total.
  0:00:00.355     1M / 23M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 460)   Building perfect hash indices
  0:00:00.378     1M / 23M   INFO   K-mer Index Building     (kmer_index_builder.hpp    : 496)   Index built. Total 1974 kmers, 13944 bytes occupied (56.5106 bits per kmer).
  0:00:00.378     1M / 23M   INFO   K-mer Counting           (kmer_data.cpp             : 355)   Arranging kmers in hash map order
  0:00:00.393     1M / 23M   INFO    General                 (main.cpp                  : 149)   Clustering Hamming graph.
  0:00:00.394     1M / 23M   INFO    General                 (main.cpp                  : 156)   Extracting clusters:
  0:00:00.394     1M / 23M   INFO    General                 (concurrent_dsu.cpp        :  19)   Connecting to root
  0:00:00.394     1M / 23M   INFO    General                 (concurrent_dsu.cpp        :  35)   Calculating counts
  0:00:00.394     1M / 23M   INFO    General                 (concurrent_dsu.cpp        :  64)   Writing down entries
  0:00:00.407     1M / 23M   INFO    General                 (main.cpp                  : 168)   Clustering done. Total clusters: 1960
  0:00:00.407     1M / 23M   INFO   K-mer Counting           (kmer_data.cpp             : 372)   Collecting K-mer information, this takes a while.
  0:00:00.407     1M / 23M   INFO   K-mer Counting           (kmer_data.cpp             : 378)   Processing "/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz"
  0:00:00.422     1M / 23M   INFO   K-mer Counting           (kmer_data.cpp             : 378)   Processing "/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz"
  0:00:00.434     1M / 23M   INFO   K-mer Counting           (kmer_data.cpp             : 385)   Collection done, postprocessing.
  0:00:00.434     1M / 23M   INFO   K-mer Counting           (kmer_data.cpp             : 398)   There are 1974 kmers in total. Among them 0 (0%) are singletons.
  0:00:00.434     1M / 23M   INFO    General                 (main.cpp                  : 174)   Subclustering Hamming graph
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 637)   Subclustering done. Total 0 non-read kmers were generated.
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 638)   Subclustering statistics:
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 639)     Total singleton hamming clusters: 1946. Among them 1946 (100%) are good
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 640)     Total singleton subclusters: 8. Among them 8 (100%) are good
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 641)     Total non-singleton subcluster centers: 10. Among them 10 (100%) are good
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 642)     Average size of non-trivial subcluster: 2.8 kmers
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 643)     Average number of sub-clusters per non-singleton cluster: 1.28571
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 644)     Total solid k-mers: 1964
  0:00:00.446     1M / 23M   INFO   Hamming Subclustering    (kmer_cluster.cpp          : 645)     Substitution probabilities: (        0.875        0.125            0            0 )
(            0            1            0            0 )
(            0            0            1            0 )
(            0            0        0.125        0.875 )
  0:00:00.448     1M / 23M   INFO    General                 (main.cpp                  : 179)   Finished clustering.
  0:00:00.448     1M / 23M   INFO    General                 (main.cpp                  : 198)   Starting solid k-mers expansion in 8 threads.
  0:00:00.461     1M / 23M   INFO    General                 (main.cpp                  : 219)   Solid k-mers iteration 0 produced 0 new k-mers.
  0:00:00.462     1M / 23M   INFO    General                 (main.cpp                  : 223)   Solid k-mers finalized
  0:00:00.462     1M / 23M   INFO    General                 (hammer_tools.cpp          : 226)   Starting read correction in 8 threads.
  0:00:00.462     1M / 23M   INFO    General                 (hammer_tools.cpp          : 239)   Correcting pair of reads: "/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz" and "/galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz"
  0:00:00.551   178M / 179M  INFO    General                 (hammer_tools.cpp          : 173)   Prepared batch 0 of 2054 reads.
  0:00:00.554   178M / 179M  INFO    General                 (hammer_tools.cpp          : 180)   Processed batch 0
  0:00:00.556   178M / 179M  INFO    General                 (hammer_tools.cpp          : 190)   Written batch 0
  0:00:00.606     1M / 179M  INFO    General                 (hammer_tools.cpp          : 280)   Correction done. Changed 4 bases in 4 reads.
  0:00:00.606     1M / 179M  INFO    General                 (hammer_tools.cpp          : 281)   Failed to correct 0 bases out of 353915.
  0:00:00.606     1M / 179M  INFO    General                 (main.cpp                  : 256)   Saving corrected dataset description to /galaxy/server/database/jobs_directory/000/71/working/output/corrected/corrected.yaml
  0:00:00.611     1M / 179M  INFO    General                 (main.cpp                  : 263)   All done. Exiting.

===== Read error correction finished. 


===== corrected reads compression started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/compress_all.py --input_file /galaxy/server/database/jobs_directory/000/71/working/output/corrected/corrected.yaml --ext_python_modules_home /usr/local/share/spades --max_threads 8 --output_dir /galaxy/server/database/jobs_directory/000/71/working/output/corrected --gzip_output

== Compressing corrected reads (with gzip)
== Files to compress: ['/galaxy/server/database/jobs_directory/000/71/working/output/corrected/ecoli_1K1.fastq00.0_0.cor.fastq', '/galaxy/server/database/jobs_directory/000/71/working/output/corrected/ecoli_1K2.fastq00.0_0.cor.fastq', '/galaxy/server/database/jobs_directory/000/71/working/output/corrected/ecoli_1K_unpaired00.0_0.cor.fastq']
== Files compression is finished
== Dataset yaml file is updated

===== corrected reads compression finished. 


===== Read error correction finished. 


===== Assembling started. 


===== K21 started. 


== Running: /usr/local/bin/spades-core /galaxy/server/database/jobs_directory/000/71/working/output/K21/configs/config.info /galaxy/server/database/jobs_directory/000/71/working/output/K21/configs/mda_mode.info /galaxy/server/database/jobs_directory/000/71/working/output/K21/configs/careful_mda_mode.info

  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  :  94)   Loaded config from "/galaxy/server/database/jobs_directory/000/71/working/output/K21/configs/config.info"
  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  :  94)   Loaded config from "/galaxy/server/database/jobs_directory/000/71/working/output/K21/configs/mda_mode.info"
  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  :  94)   Loaded config from "/galaxy/server/database/jobs_directory/000/71/working/output/K21/configs/careful_mda_mode.info"
  0:00:00.000     1M / 23M   INFO    General                 (memory_limit.cpp          :  55)   Memory limit set to 14 Gb
  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  : 102)   Starting SPAdes, built from N/A, git revision N/A
  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  : 103)   Maximum k-mer length: 128
  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  : 104)   Assembling dataset ("/galaxy/server/database/jobs_directory/000/71/working/output/dataset.info") with K=21
  0:00:00.000     1M / 23M   INFO    General                 (main.cpp                  : 105)   Maximum # of threads to use (adjusted due to OMP capabilities): 8
  0:00:00.000     1M / 23M   INFO    General                 (pipeline.cpp              : 221)   SPAdes started
  0:00:00.000     1M / 23M   INFO    General                 (pipeline.cpp              : 234)   Starting from stage: read_conversion
  0:00:00.000     1M / 23M   INFO    General                 (pipeline.cpp              : 245)   Two-step repeat resolution disabled
  0:00:00.000     1M / 23M   INFO   GraphCore                (graph_core.hpp            : 689)   Graph created, vertex min_id: 3, edge min_id: 3
  0:00:00.000     1M / 23M   INFO   GraphCore                (graph_core.hpp            : 690)   Vertex size: 48, edge size: 40
  0:00:00.000     1M / 23M   INFO    General                 (edge_index.hpp            : 132)   Size of edge index entries: 12/8
  0:00:00.001     1M / 23M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == Binary Read Conversion (id: read_conversion)
  0:00:00.005     1M / 23M   INFO    General                 (read_converter.cpp        :  78)   Converting reads to binary format for library #0 (takes a while)
  0:00:00.005     1M / 23M   INFO    General                 (read_converter.cpp        :  99)   Converting paired reads
  0:00:00.033    81M / 81M   INFO    General                 (binary_converter.cpp      : 143)   2054 reads written
  0:00:00.036    52M / 52M   INFO    General                 (read_converter.cpp        : 113)   Converting single reads
  0:00:00.047    92M / 92M   INFO    General                 (binary_converter.cpp      : 143)   0 reads written
  0:00:00.048    79M / 79M   INFO    General                 (read_converter.cpp        : 119)   Converting merged reads
  0:00:00.051    92M / 92M   INFO    General                 (binary_converter.cpp      : 143)   0 reads written
  0:00:00.127     1M / 36M   INFO   StageManager             (stage.cpp                 : 212)   STAGE == de Bruijn graph construction (id: construction)
  0:00:00.146     1M / 36M   INFO    General                 (construction.cpp          : 150)   Max read length 100
  0:00:00.146     1M / 36M   INFO    General                 (construction.cpp          : 156)   Average read length 86.1526
  0:00:00.146     1M / 36M   INFO    General                 (stage.cpp                 : 121)   PROCEDURE == k+1-mer counting (id: construction:kpomer_counting)
  0:00:00.146     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 308)   Splitting kmer instances into 80 files using 8 threads. This might take a while.
  0:00:00.219     1M / 36M   INFO    General                 (file_limit.hpp            :  43)   Open file limit set to 1048576
  0:00:00.219     1M / 36M   INFO    General                 (kmer_splitter.hpp         :  96)   Memory available for splitting buffers: 0.583324 Gb
  0:00:00.219     1M / 36M   INFO    General                 (kmer_splitter.hpp         : 104)   Using cell size of 838860
  0:00:00.898     1M / 36M   INFO    General                 (kmer_splitters.hpp        : 134)   Used 8216 reads
  0:00:00.898     1M / 36M   INFO    General                 (kmer_index_builder.hpp    : 314)   Starting k-mer counting.
  0:00:01.113     1M / 36M   INFO    General               
..
/ 155M  INFO    General                 (launcher.cpp              : 262)   Deduplicating paths
  0:00:02.956     1M / 155M  INFO    General                 (launcher.cpp              : 266)   Paths deduplicated
  0:00:02.956     1M / 155M  INFO   PEResolver               (pe_resolver.cpp           :  70)   Removing overlaps
  0:00:02.956     1M / 155M  INFO   PEResolver               (pe_resolver.cpp           :  73)   Sorting paths
  0:00:02.956     1M / 155M  INFO   PEResolver               (pe_resolver.cpp           :  80)   Marking overlaps
  0:00:02.956     1M / 155M  INFO   OverlapRemover           (overlap_remover.hpp       : 120)   Marking start/end overlaps
  0:00:02.956     1M / 155M  INFO   OverlapRemover           (overlap_remover.hpp       : 123)   Marking remaining overlaps
  0:00:02.956     1M / 155M  INFO   PEResolver               (pe_resolver.cpp           :  83)   Splitting paths
  0:00:02.956     1M / 155M  INFO   PEResolver               (pe_resolver.cpp           :  88)   Deduplicating paths
  0:00:02.956     1M / 155M  INFO   PEResolver               (pe_resolver.cpp           :  90)   Overlaps removed
  0:00:02.956     1M / 155M  INFO    General                 (launcher.cpp              : 283)   Paths finalized
  0:00:02.956     1M / 155M  INFO    General                 (launcher.cpp              : 697)   ExSPAnder repeat resolving tool finished
  0:00:02.956     1M / 155M  INFO   StageManager             (stage.cpp                 : 212)   STAGE == Contig Output (id: contig_output)
  0:00:02.956     1M / 155M  INFO    General                 (contig_output.hpp         :  20)   Outputting contigs to "/galaxy/server/database/jobs_directory/000/71/working/output/K55/before_rr.fasta"
  0:00:02.964     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 159)   Writing GFA graph to "/galaxy/server/database/jobs_directory/000/71/working/output/K55/assembly_graph_with_scaffolds.gfa"
  0:00:02.964     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 175)   Outputting FastG graph to "/galaxy/server/database/jobs_directory/000/71/working/output/K55/assembly_graph.fastg"
  0:00:02.971     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 210)   Breaking scaffolds
  0:00:02.971     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 101)   Outputting contigs to /galaxy/server/database/jobs_directory/000/71/working/output/K55/final_contigs.fasta
  0:00:02.980     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 107)   Outputting FastG paths to /galaxy/server/database/jobs_directory/000/71/working/output/K55/final_contigs.paths
  0:00:02.989     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 101)   Outputting contigs to /galaxy/server/database/jobs_directory/000/71/working/output/K55/scaffolds.fasta
  0:00:02.996     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 107)   Outputting FastG paths to /galaxy/server/database/jobs_directory/000/71/working/output/K55/scaffolds.paths
  0:00:03.002     1M / 155M  INFO    General                 (contig_output_stage.cpp   : 114)   Populating GFA with scaffold paths
  0:00:03.012     1M / 155M  INFO    General                 (pipeline.cpp              : 303)   SPAdes finished
  0:00:03.012     1M / 155M  INFO    General                 (main.cpp                  : 131)   Assembling time: 0 hours 0 minutes 3 seconds

===== K55 finished. 


===== Copy files started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/copy_files.py /galaxy/server/database/jobs_directory/000/71/working/output/K55/before_rr.fasta /galaxy/server/database/jobs_directory/000/71/working/output/before_rr.fasta /galaxy/server/database/jobs_directory/000/71/working/output/K55/assembly_graph_after_simplification.gfa /galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph_after_simplification.gfa /galaxy/server/database/jobs_directory/000/71/working/output/K55/final_contigs.fasta /galaxy/server/database/jobs_directory/000/71/working/output/contigs.fasta /galaxy/server/database/jobs_directory/000/71/working/output/K55/first_pe_contigs.fasta /galaxy/server/database/jobs_directory/000/71/working/output/first_pe_contigs.fasta /galaxy/server/database/jobs_directory/000/71/working/output/K55/strain_graph.gfa /galaxy/server/database/jobs_directory/000/71/working/output/strain_graph.gfa /galaxy/server/database/jobs_directory/000/71/working/output/K55/scaffolds.fasta /galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.fasta /galaxy/server/database/jobs_directory/000/71/working/output/K55/scaffolds.paths /galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.paths /galaxy/server/database/jobs_directory/000/71/working/output/K55/assembly_graph_with_scaffolds.gfa /galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph_with_scaffolds.gfa /galaxy/server/database/jobs_directory/000/71/working/output/K55/assembly_graph.fastg /galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph.fastg /galaxy/server/database/jobs_directory/000/71/working/output/K55/final_contigs.paths /galaxy/server/database/jobs_directory/000/71/working/output/contigs.paths


===== Copy files finished. 


===== Assembling finished. 


===== Mismatch correction started. 


===== Mismatch correction contigs started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/correction_iteration_script.py --corrected /galaxy/server/database/jobs_directory/000/71/working/output/contigs.fasta --assembled /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta --assembly_type contigs --output_dir /galaxy/server/database/jobs_directory/000/71/working/output --bin_home /usr/local/bin

== Processing of contigs
== Running contig polishing tool: /usr/local/bin/spades-corrector-core /galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/contigs/configs/corrector.info /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta
== Dataset description file was created: /galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/contigs/configs/corrector.info
Run: /usr/local/bin/spades-corrector-core /galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/contigs/configs/corrector.info /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta
"/galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/contigs/configs/log.properties"  0:00:00.000     1M / 14M   INFO    General                 (main.cpp                  :  57)   Starting mismatch corrector, built from N/A, git revision N/A
  0:00:00.001     1M / 14M   INFO    General                 (main.cpp                  :  58)   Maximum # of threads to use (adjusted due to OMP capabilities): 8
  0:00:00.001     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 173)   Splitting assembly...
  0:00:00.001     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 174)   Assembly file: "/galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta"
  0:00:00.007     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 126)   Running bwa index ...: /usr/local/bin/spades-bwa index /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta
[bwa_index] Pack FASTA... 0.00 sec
[bwa_index] Construct BWT for the packed sequence...
[bwa_index] 0.00 seconds elapse.
[bwa_index] Update BWT... 0.00 sec
[bwa_index] Pack forward-only FASTA... 0.00 sec
[bwa_index] Construct SA from BWT and Occ... 0.00 sec
[main] Version: 0.7.16a-r1185-dirty
[main] CMD: /usr/local/bin/spades-bwa index /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta
[main] Real time: 0.043 sec; CPU: 0.007 sec
  0:00:00.054     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 188)   Processing paired sublib of number 0
  0:00:00.054     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 189)   /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz
  0:00:00.055     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 151)   Running bwa mem ...:/usr/local/bin/spades-bwa mem  -v 1 -t 8  /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz   > /galaxy/server/database/jobs_directory/000/71/tmp/corrector_mf67n22y/lib0_Cf9xNw/tmp.sam
[main] Version: 0.7.16a-r1185-dirty
[main] CMD: /usr/local/bin/spades-bwa mem -v 1 -t 8 /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_contigs.fasta /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz
[main] Real time: 0.049 sec; CPU: 0.095 sec
  0:00:00.113     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 197)   Adding samfile "/galaxy/server/database/jobs_directory/000/71/tmp/corrector_mf67n22y/lib0_Cf9xNw/tmp.sam"
  0:00:00.142     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 225)   Processing contigs
  0:00:00.206     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 245)   Gluing processed contigs
  0:00:00.212     1M / 14M   INFO    General                 (main.cpp                  :  71)   Correcting time: 0 hours 0 minutes 0 seconds

===== Mismatch correction contigs finished. 


===== Mismatch correction scaffolds started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/correction_iteration_script.py --corrected /galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.fasta --assembled /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta --assembly_type scaffolds --output_dir /galaxy/server/database/jobs_directory/000/71/working/output --bin_home /usr/local/bin

== Processing of scaffolds
== Running contig polishing tool: /usr/local/bin/spades-corrector-core /galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/scaffolds/configs/corrector.info /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta
== Dataset description file was created: /galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/scaffolds/configs/corrector.info
Run: /usr/local/bin/spades-corrector-core /galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/scaffolds/configs/corrector.info /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta
"/galaxy/server/database/jobs_directory/000/71/working/output/mismatch_corrector/scaffolds/configs/log.properties"  0:00:00.000     1M / 14M   INFO    General                 (main.cpp                  :  57)   Starting mismatch corrector, built from N/A, git revision N/A
  0:00:00.001     1M / 14M   INFO    General                 (main.cpp                  :  58)   Maximum # of threads to use (adjusted due to OMP capabilities): 8
  0:00:00.001     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 173)   Splitting assembly...
  0:00:00.001     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 174)   Assembly file: "/galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta"
  0:00:00.007     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 126)   Running bwa index ...: /usr/local/bin/spades-bwa index /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta
[bwa_index] Pack FASTA... 0.00 sec
[bwa_index] Construct BWT for the packed sequence...
[bwa_index] 0.00 seconds elapse.
[bwa_index] Update BWT... 0.00 sec
[bwa_index] Pack forward-only FASTA... 0.00 sec
[bwa_index] Construct SA from BWT and Occ... 0.00 sec
[main] Version: 0.7.16a-r1185-dirty
[main] CMD: /usr/local/bin/spades-bwa index /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta
[main] Real time: 0.038 sec; CPU: 0.006 sec
  0:00:00.049     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 188)   Processing paired sublib of number 0
  0:00:00.049     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 189)   /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz
  0:00:00.050     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 151)   Running bwa mem ...:/usr/local/bin/spades-bwa mem  -v 1 -t 8  /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz   > /galaxy/server/database/jobs_directory/000/71/tmp/corrector_yeeiwzfo/lib0_7FHatH/tmp.sam
[main] Version: 0.7.16a-r1185-dirty
[main] CMD: /usr/local/bin/spades-bwa mem -v 1 -t 8 /galaxy/server/database/jobs_directory/000/71/working/output/misc/assembled_scaffolds.fasta /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K1.fastq.gz /galaxy/server/database/jobs_directory/000/71/working/paired_reads1/ecoli_1K2.fastq.gz
[main] Real time: 0.046 sec; CPU: 0.088 sec
  0:00:00.102     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 197)   Adding samfile "/galaxy/server/database/jobs_directory/000/71/tmp/corrector_yeeiwzfo/lib0_7FHatH/tmp.sam"
  0:00:00.139     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 225)   Processing contigs
  0:00:00.201     1M / 14M   INFO   DatasetProcessor         (dataset_processor.cpp     : 245)   Gluing processed contigs
  0:00:00.206     1M / 14M   INFO    General                 (main.cpp                  :  71)   Correcting time: 0 hours 0 minutes 0 seconds

===== Mismatch correction scaffolds finished. 


===== Mismatch correction finished. 


===== Breaking scaffolds started. 


== Running: /usr/local/bin/python3 /usr/local/share/spades/spades_pipeline/scripts/breaking_scaffolds_script.py --result_scaffolds_filename /galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.fasta --misc_dir /galaxy/server/database/jobs_directory/000/71/working/output/misc --threshold_for_breaking_scaffolds 3


===== Breaking scaffolds finished. 


===== Terminate started. 


===== Terminate finished. 

 * Corrected reads are in /galaxy/server/database/jobs_directory/000/71/working/output/corrected/
 * Assembled contigs are in /galaxy/server/database/jobs_directory/000/71/working/output/contigs.fasta
 * Assembled scaffolds are in /galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.fasta
 * Paths in the assembly graph corresponding to the contigs are in /galaxy/server/database/jobs_directory/000/71/working/output/contigs.paths
 * Paths in the assembly graph corresponding to the scaffolds are in /galaxy/server/database/jobs_directory/000/71/working/output/scaffolds.paths
 * Assembly graph is in /galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph.fastg
 * Assembly graph in GFA format is in /galaxy/server/database/jobs_directory/000/71/working/output/assembly_graph_with_scaffolds.gfa

======= SPAdes pipeline finished.

SPAdes log can be found here: /galaxy/server/database/jobs_directory/000/71/working/output/spades.log

Thank you for using SPAdes! If you use it in your research, please cite:

  Prjibelski, A., Antipov, D., Meleshko, D., Lapidus, A. and Korobeynikov, A., 2020. Using SPAdes de novo assembler. Current protocols in bioinformatics, 70(1), p.e102.
  doi.org/10.1002/cpbi.102


galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71/gxy-p76tr) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71/gxy-p76tr) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71/gxy-p76tr) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-p76tr.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 71 (gxy-p76tr)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-p76tr to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 71 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:37,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71/gxy-p76tr) Terminated at user's request
galaxy.util WARNING 2025-11-05 01:18:37,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/b/d/dataset_dbdcc72e-8de7-4d7e-8a9f-e64ba80c2bd1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/b/d/dataset_dbdcc72e-8de7-4d7e-8a9f-e64ba80c2bd1.dat'
galaxy.util WARNING 2025-11-05 01:18:37,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/5/d/dataset_35dc758d-12d0-466b-9de0-968c5cffca06.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/5/d/dataset_35dc758d-12d0-466b-9de0-968c5cffca06.dat'
galaxy.util WARNING 2025-11-05 01:18:37,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/d/1/dataset_7d1a855c-4d16-4852-99c2-b42dd7960f5c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/d/1/dataset_7d1a855c-4d16-4852-99c2-b42dd7960f5c.dat'
galaxy.util WARNING 2025-11-05 01:18:37,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/4/5/dataset_0457a4c5-f22e-4e88-aa1f-2d1027bb414a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/4/5/dataset_0457a4c5-f22e-4e88-aa1f-2d1027bb414a.dat'
galaxy.jobs.handler DEBUG 2025-11-05 01:18:39,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72, 73
tpv.core.entities DEBUG 2025-11-05 01:18:39,583 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:18:39,583 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:18:39,584 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:18:39,587 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:18:39,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:18:39,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-11-05 01:18:39,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (38.896 ms)
galaxy.jobs.handler INFO 2025-11-05 01:18:39,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:39,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 72
tpv.core.entities DEBUG 2025-11-05 01:18:39,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:18:39,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:18:39,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:18:39,647 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:18:39,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:18:39,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-11-05 01:18:39,695 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (47.970 ms)
galaxy.jobs.handler INFO 2025-11-05 01:18:39,697 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:39,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-11-05 01:18:39,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [72] prepared (79.274 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:18:39,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/72/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/72/configs/tmplx45xo76']
galaxy.jobs.runners DEBUG 2025-11-05 01:18:39,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-11-05 01:18:39,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [73] prepared (71.538 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:39,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:18:39,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/73/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/73/configs/tmpfmsl0b34']
galaxy.jobs.runners DEBUG 2025-11-05 01:18:39,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:39,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:39,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:39,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:40,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:40,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:49,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sc47q with k8s id: gxy-sc47q succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:49,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7xmbg with k8s id: gxy-7xmbg succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:18:49,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:18:49,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:18:57,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 73 finished
galaxy.jobs.runners DEBUG 2025-11-05 01:18:57,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 72 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:18:57,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/73/working/gxupload_0', 'object_id': 113}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:18:57,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/72/working/gxupload_0', 'object_id': 112}]}]}]
galaxy.jobs INFO 2025-11-05 01:18:57,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs INFO 2025-11-05 01:18:57,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs DEBUG 2025-11-05 01:18:57,804 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 73 executed (162.107 ms)
galaxy.jobs DEBUG 2025-11-05 01:18:57,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 72 executed (160.872 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:57,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 72 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:57,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 73 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:18:59,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-11-05 01:18:59,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:18:59,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:18:59,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:18:59,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:18:59,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:18:59,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-11-05 01:18:59,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (48.068 ms)
galaxy.jobs.handler INFO 2025-11-05 01:18:59,151 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:59,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-11-05 01:18:59,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [74] prepared (85.283 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:59,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:18:59,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:59,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:18:59,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/74/outputs/COMMAND_VERSION 2>&1;
mkdir -p paired_reads1 && ln -s '/galaxy/server/database/objects/1/b/f/dataset_1bfa8d32-53db-4933-a145-8ca12eae1662.dat' 'paired_reads1/ecoli_1K1.fastq.gz' &&  ln -s '/galaxy/server/database/objects/3/2/7/dataset_327ad125-42b3-490b-ae56-39a37baf0a40.dat' 'paired_reads1/ecoli_1K2.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py --only-assembler -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-1 1 'paired_reads1/ecoli_1K1.fastq.gz' --pe-2 1 'paired_reads1/ecoli_1K2.fastq.gz' --pe-or 1 fr        --cov-cutoff off    --sc --careful]
galaxy.jobs.runners DEBUG 2025-11-05 01:18:59,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/74/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/6/2/3/dataset_623be669-aecd-4dc8-a03c-60d78ea7f162.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/74/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/6/2/3/dataset_623be669-aecd-4dc8-a03c-60d78ea7f162.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/74/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/6/b/f/dataset_6bf7f85a-6757-4527-b2f7-8aa4273bd8e5.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/74/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/6/b/f/dataset_6bf7f85a-6757-4527-b2f7-8aa4273bd8e5.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/74/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/c/1/b/dataset_c1b488ea-335e-4c95-bc5c-348418c9e57d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/74/working/output/contigs.fasta" "/galaxy/server/database/objects/c/1/b/dataset_c1b488ea-335e-4c95-bc5c-348418c9e57d.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/74/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/6/6/4/dataset_66444a28-aa77-4c2c-936d-3116f459903e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/74/working/output/scaffolds.fasta" "/galaxy/server/database/objects/6/6/4/dataset_66444a28-aa77-4c2c-936d-3116f459903e.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:59,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:59,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:18:59,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:18:59,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:59,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:18:59,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:14,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w9p7c with k8s id: gxy-w9p7c succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:19:14,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 74: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:19:21,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 74 finished
galaxy.model.metadata DEBUG 2025-11-05 01:19:21,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 114
galaxy.model.metadata DEBUG 2025-11-05 01:19:21,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 115
galaxy.model.metadata DEBUG 2025-11-05 01:19:21,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 116
galaxy.model.metadata DEBUG 2025-11-05 01:19:21,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 117
galaxy.util WARNING 2025-11-05 01:19:21,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/2/3/dataset_623be669-aecd-4dc8-a03c-60d78ea7f162.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/2/3/dataset_623be669-aecd-4dc8-a03c-60d78ea7f162.dat'
galaxy.util WARNING 2025-11-05 01:19:21,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/b/f/dataset_6bf7f85a-6757-4527-b2f7-8aa4273bd8e5.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/b/f/dataset_6bf7f85a-6757-4527-b2f7-8aa4273bd8e5.dat'
galaxy.util WARNING 2025-11-05 01:19:21,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/1/b/dataset_c1b488ea-335e-4c95-bc5c-348418c9e57d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/1/b/dataset_c1b488ea-335e-4c95-bc5c-348418c9e57d.dat'
galaxy.util WARNING 2025-11-05 01:19:21,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/6/4/dataset_66444a28-aa77-4c2c-936d-3116f459903e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/6/4/dataset_66444a28-aa77-4c2c-936d-3116f459903e.dat'
galaxy.jobs INFO 2025-11-05 01:19:21,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 74 in /galaxy/server/database/jobs_directory/000/74
galaxy.jobs DEBUG 2025-11-05 01:19:21,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 74 executed (159.742 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:21,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 74 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:19:23,587 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76, 75
tpv.core.entities DEBUG 2025-11-05 01:19:23,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:19:23,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:19:23,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:19:23,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:19:23,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:19:23,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-11-05 01:19:23,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (32.661 ms)
galaxy.jobs.handler INFO 2025-11-05 01:19:23,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:23,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 75
tpv.core.entities DEBUG 2025-11-05 01:19:23,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:19:23,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:19:23,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:19:23,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:19:23,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:19:23,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-11-05 01:19:23,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (44.023 ms)
galaxy.jobs.handler INFO 2025-11-05 01:19:23,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:23,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-11-05 01:19:23,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [75] prepared (76.176 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:19:23,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/75/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/75/configs/tmp0jntkwds']
galaxy.jobs.runners DEBUG 2025-11-05 01:19:23,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:23,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:19:23,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [76] prepared (78.520 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:23,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:19:23,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/76/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/76/configs/tmpxzd18p3z']
galaxy.jobs.runners DEBUG 2025-11-05 01:19:23,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:23,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:23,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:24,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:24,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:34,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sf7wd with k8s id: gxy-sf7wd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:34,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvd5x with k8s id: gxy-dvd5x succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:19:34,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:19:34,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:19:42,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 76 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:19:42,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fasta.gz', 'dbkey': '?', 'ext': 'fasta.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/76/working/gxupload_0', 'object_id': 119}]}]}]
galaxy.jobs.runners DEBUG 2025-11-05 01:19:42,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 75 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:19:42,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/75/working/gxupload_0', 'object_id': 118}]}]}]
galaxy.jobs INFO 2025-11-05 01:19:42,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.jobs INFO 2025-11-05 01:19:42,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-11-05 01:19:42,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 76 executed (155.028 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:42,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 76 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:19:42,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 75 executed (152.320 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:42,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 75 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:19:43,074 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-11-05 01:19:43,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:19:43,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:19:43,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:19:43,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:19:43,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:19:43,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-11-05 01:19:43,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (41.776 ms)
galaxy.jobs.handler INFO 2025-11-05 01:19:43,151 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:43,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-11-05 01:19:43,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [77] prepared (89.447 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:19:43,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:19:43,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:19:43,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:19:43,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/77/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/4/c/6/dataset_4c605f71-2fa6-46a4-934e-6fa4fb9cf568.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&    mkdir -p nanopore_reads && ln -s '/galaxy/server/database/objects/4/c/6/dataset_4c605f71-2fa6-46a4-934e-6fa4fb9cf568.dat' 'nanopore_reads/file1.fastq.gz' &&  mkdir -p pacbio_reads && ln -s '/galaxy/server/database/objects/4/c/6/dataset_4c605f71-2fa6-46a4-934e-6fa4fb9cf568.dat' 'pacbio_reads/file1.fastq.gz' &&    mkdir -p trusted_contigs && ln -s '/galaxy/server/database/objects/a/3/3/dataset_a337b096-0b75-4538-baec-eb0cc2b41f82.dat' 'trusted_contigs/file1.fasta.gz' &&    mkdir -p sanger_reads && ln -s '/galaxy/server/database/objects/4/c/6/dataset_4c605f71-2fa6-46a4-934e-6fa4fb9cf568.dat' 'sanger_reads/file1.fastq.gz' &&     export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py  -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --pe-12 1 'reads1/ecoli_1K.fastq.gz.fastq.gz' --pe-or 1 fr   --nanopore nanopore_reads/file1.fastq.gz --pacbio pacbio_reads/file1.fastq.gz   --sanger sanger_reads/file1.fastq.gz   --trusted-contigs trusted_contigs/file1.fasta.gz  --cov-cutoff off]
galaxy.jobs.runners DEBUG 2025-11-05 01:19:43,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/77/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/f/5/8/dataset_f58bf150-b569-4153-b1b2-60827f1087c3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/77/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/f/5/8/dataset_f58bf150-b569-4153-b1b2-60827f1087c3.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/77/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/3/d/b/dataset_3db50c4b-adab-4a1a-a05b-57749a7844c5.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/77/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/3/d/b/dataset_3db50c4b-adab-4a1a-a05b-57749a7844c5.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/77/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/3/8/3/dataset_3838ef85-2399-407c-b90b-9b2381670f2d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/77/working/output/contigs.fasta" "/galaxy/server/database/objects/3/8/3/dataset_3838ef85-2399-407c-b90b-9b2381670f2d.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/77/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/4/5/d/dataset_45d9644b-d8bf-464a-a904-63b16ce307fd.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/77/working/output/scaffolds.fasta" "/galaxy/server/database/objects/4/5/d/dataset_45d9644b-d8bf-464a-a904-63b16ce307fd.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:43,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:19:43,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:19:43,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:19:43,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:43,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:19:44,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:01,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7sc2p with k8s id: gxy-7sc2p succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:20:02,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:20:09,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 77 finished
galaxy.model.metadata DEBUG 2025-11-05 01:20:09,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 120
galaxy.model.metadata DEBUG 2025-11-05 01:20:09,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 121
galaxy.model.metadata DEBUG 2025-11-05 01:20:09,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 122
galaxy.model.metadata DEBUG 2025-11-05 01:20:09,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 123
galaxy.util WARNING 2025-11-05 01:20:09,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/5/8/dataset_f58bf150-b569-4153-b1b2-60827f1087c3.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/5/8/dataset_f58bf150-b569-4153-b1b2-60827f1087c3.dat'
galaxy.util WARNING 2025-11-05 01:20:09,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/d/b/dataset_3db50c4b-adab-4a1a-a05b-57749a7844c5.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/d/b/dataset_3db50c4b-adab-4a1a-a05b-57749a7844c5.dat'
galaxy.util WARNING 2025-11-05 01:20:09,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/8/3/dataset_3838ef85-2399-407c-b90b-9b2381670f2d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/8/3/dataset_3838ef85-2399-407c-b90b-9b2381670f2d.dat'
galaxy.util WARNING 2025-11-05 01:20:09,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/5/d/dataset_45d9644b-d8bf-464a-a904-63b16ce307fd.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/5/d/dataset_45d9644b-d8bf-464a-a904-63b16ce307fd.dat'
galaxy.jobs INFO 2025-11-05 01:20:09,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-11-05 01:20:09,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 77 executed (128.482 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:09,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 77 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:20:11,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79, 78
tpv.core.entities DEBUG 2025-11-05 01:20:11,711 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:20:11,711 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:20:11,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:20:11,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:20:11,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:20:11,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-11-05 01:20:11,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (41.422 ms)
galaxy.jobs.handler INFO 2025-11-05 01:20:11,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:11,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 78
tpv.core.entities DEBUG 2025-11-05 01:20:11,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:20:11,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:20:11,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:20:11,785 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:20:11,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:20:11,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-11-05 01:20:11,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (57.339 ms)
galaxy.jobs.handler INFO 2025-11-05 01:20:11,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:11,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-11-05 01:20:11,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [78] prepared (85.177 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:20:11,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/78/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/78/configs/tmpvodth2nb']
galaxy.jobs.runners DEBUG 2025-11-05 01:20:11,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:11,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:20:11,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [79] prepared (75.390 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:11,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:20:11,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/79/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/79/configs/tmpwm7enzfs']
galaxy.jobs.runners DEBUG 2025-11-05 01:20:11,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:11,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:11,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:12,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:13,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:21,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-59tjs with k8s id: gxy-59tjs succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:22,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-98gsg with k8s id: gxy-98gsg succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:20:22,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:20:22,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:20:30,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 78 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:20:30,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/78/working/gxupload_0', 'object_id': 124}]}]}]
galaxy.jobs.runners DEBUG 2025-11-05 01:20:30,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 79 finished
galaxy.jobs INFO 2025-11-05 01:20:30,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:20:30,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/79/working/gxupload_0', 'object_id': 125}]}]}]
galaxy.jobs DEBUG 2025-11-05 01:20:30,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 78 executed (157.767 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:30,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 78 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-11-05 01:20:30,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-11-05 01:20:30,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 79 executed (153.992 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:30,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 79 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:20:31,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 80
tpv.core.entities DEBUG 2025-11-05 01:20:31,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:20:31,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:20:31,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:20:31,332 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:20:31,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:20:31,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-11-05 01:20:31,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (40.580 ms)
galaxy.jobs.handler INFO 2025-11-05 01:20:31,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:31,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 80
galaxy.jobs DEBUG 2025-11-05 01:20:31,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [80] prepared (67.835 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:20:31,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:20:31,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:20:31,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:20:31,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/80/outputs/COMMAND_VERSION 2>&1;
mkdir -p paired_reads1 && ln -s '/galaxy/server/database/objects/3/4/2/dataset_342ab174-e1a0-40e1-bfc3-438c5dd48879.dat' 'paired_reads1/ecoli_1K1.fastq.gz' &&  ln -s '/galaxy/server/database/objects/c/5/a/dataset_c5a3907c-f02f-490f-bf22-fc73bbed8601.dat' 'paired_reads1/ecoli_1K2.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py  -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --hqmp-1 1 'paired_reads1/ecoli_1K1.fastq.gz' --hqmp-2 1 'paired_reads1/ecoli_1K2.fastq.gz' --hqmp-or 1 rf        --cov-cutoff off]
galaxy.jobs.runners DEBUG 2025-11-05 01:20:31,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/80/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/3/b/d/dataset_3bd7cfb5-964d-456e-8b1b-7638fe404ed7.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/80/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/3/b/d/dataset_3bd7cfb5-964d-456e-8b1b-7638fe404ed7.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/80/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/6/7/2/dataset_67209991-ba9a-4e87-899c-d273b9805286.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/80/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/6/7/2/dataset_67209991-ba9a-4e87-899c-d273b9805286.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/80/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/5/0/9/dataset_5094671b-6871-43d8-9316-db50d1c11d86.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/80/working/output/contigs.fasta" "/galaxy/server/database/objects/5/0/9/dataset_5094671b-6871-43d8-9316-db50d1c11d86.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/80/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/6/1/4/dataset_6144c77b-7c96-4aab-8054-a271aab7dcfc.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/80/working/output/scaffolds.fasta" "/galaxy/server/database/objects/6/1/4/dataset_6144c77b-7c96-4aab-8054-a271aab7dcfc.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:31,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:20:31,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:20:31,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:20:31,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:31,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:32,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:49,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n7jgr with k8s id: gxy-n7jgr succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:20:50,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:20:57,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 80 finished
galaxy.model.metadata DEBUG 2025-11-05 01:20:57,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 126
galaxy.model.metadata DEBUG 2025-11-05 01:20:57,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 127
galaxy.model.metadata DEBUG 2025-11-05 01:20:57,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 128
galaxy.model.metadata DEBUG 2025-11-05 01:20:57,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 129
galaxy.util WARNING 2025-11-05 01:20:57,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/b/d/dataset_3bd7cfb5-964d-456e-8b1b-7638fe404ed7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/b/d/dataset_3bd7cfb5-964d-456e-8b1b-7638fe404ed7.dat'
galaxy.util WARNING 2025-11-05 01:20:57,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/7/2/dataset_67209991-ba9a-4e87-899c-d273b9805286.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/7/2/dataset_67209991-ba9a-4e87-899c-d273b9805286.dat'
galaxy.util WARNING 2025-11-05 01:20:57,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/0/9/dataset_5094671b-6871-43d8-9316-db50d1c11d86.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/0/9/dataset_5094671b-6871-43d8-9316-db50d1c11d86.dat'
galaxy.jobs INFO 2025-11-05 01:20:57,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-11-05 01:20:57,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 80 executed (197.724 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:57,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 80 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:20:59,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82, 81
tpv.core.entities DEBUG 2025-11-05 01:20:59,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:20:59,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:20:59,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:20:59,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:20:59,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:20:59,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-11-05 01:20:59,978 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (34.258 ms)
galaxy.jobs.handler INFO 2025-11-05 01:20:59,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:20:59,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 81
tpv.core.entities DEBUG 2025-11-05 01:20:59,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:20:59,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:20:59,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:20:59,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:21:00,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:21:00,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-11-05 01:21:00,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (49.466 ms)
galaxy.jobs.handler INFO 2025-11-05 01:21:00,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:00,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-11-05 01:21:00,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [81] prepared (81.017 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:21:00,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/81/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/81/configs/tmpxl00im6u']
galaxy.jobs.runners DEBUG 2025-11-05 01:21:00,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:00,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:21:00,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [82] prepared (73.862 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:00,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:21:00,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/82/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/82/configs/tmpc7gy6814']
galaxy.jobs.runners DEBUG 2025-11-05 01:21:00,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:00,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:00,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:00,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:01,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:10,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bvwrw with k8s id: gxy-bvwrw succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:21:10,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 82: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:11,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mbmg5 with k8s id: gxy-mbmg5 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:21:11,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:21:18,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 82 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:21:18,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_2.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/82/working/gxupload_0', 'object_id': 131}]}]}]
galaxy.jobs INFO 2025-11-05 01:21:18,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 82 in /galaxy/server/database/jobs_directory/000/82
galaxy.jobs DEBUG 2025-11-05 01:21:18,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 82 executed (145.601 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:18,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 82 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-11-05 01:21:19,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 81 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:21:19,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K_1.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/81/working/gxupload_0', 'object_id': 130}]}]}]
galaxy.jobs INFO 2025-11-05 01:21:19,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-11-05 01:21:19,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 81 executed (124.755 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:19,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 81 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:21:20,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83
tpv.core.entities DEBUG 2025-11-05 01:21:20,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:21:20,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:21:20,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:21:20,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:21:20,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:21:20,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-11-05 01:21:20,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (48.268 ms)
galaxy.jobs.handler INFO 2025-11-05 01:21:20,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:20,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 83
galaxy.jobs DEBUG 2025-11-05 01:21:20,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [83] prepared (84.069 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:21:20,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:21:20,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:21:20,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:21:20,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/83/outputs/COMMAND_VERSION 2>&1;
mkdir -p paired_reads1 && ln -s '/galaxy/server/database/objects/2/6/a/dataset_26a95cbe-6a72-4d0c-9ffd-c3327e29a029.dat' 'paired_reads1/ecoli.fastq1.fastq.gz' &&  ln -s '/galaxy/server/database/objects/3/f/2/dataset_3f2ded67-57a8-4fb5-b3a3-bc4f6eb4cafa.dat' 'paired_reads1/ecoli.fastq2.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py  -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --hqmp-1 1 'paired_reads1/ecoli.fastq1.fastq.gz' --hqmp-2 1 'paired_reads1/ecoli.fastq2.fastq.gz' --hqmp-or 1 fr        --cov-cutoff off]
galaxy.jobs.runners DEBUG 2025-11-05 01:21:20,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/83/working/output/assembly_graph.fastg" -a -f "/galaxy/server/database/objects/1/d/1/dataset_1d108900-3373-4e0f-aa41-c75dbbe17d19.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/83/working/output/assembly_graph.fastg" "/galaxy/server/database/objects/1/d/1/dataset_1d108900-3373-4e0f-aa41-c75dbbe17d19.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/83/working/output/assembly_graph_with_scaffolds.gfa" -a -f "/galaxy/server/database/objects/6/9/8/dataset_698dd250-3059-4c53-b594-a5977c0d6c88.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/83/working/output/assembly_graph_with_scaffolds.gfa" "/galaxy/server/database/objects/6/9/8/dataset_698dd250-3059-4c53-b594-a5977c0d6c88.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/83/working/output/contigs.fasta" -a -f "/galaxy/server/database/objects/c/3/0/dataset_c30540e2-0d8f-4407-953b-ca6fd370886d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/83/working/output/contigs.fasta" "/galaxy/server/database/objects/c/3/0/dataset_c30540e2-0d8f-4407-953b-ca6fd370886d.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/83/working/output/scaffolds.fasta" -a -f "/galaxy/server/database/objects/b/f/6/dataset_bf6c1e83-3562-433e-a2a2-0e7328ebcc64.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/83/working/output/scaffolds.fasta" "/galaxy/server/database/objects/b/f/6/dataset_bf6c1e83-3562-433e-a2a2-0e7328ebcc64.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:20,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:21:20,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:21:20,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:21:20,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:20,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:22,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:39,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-49m5s with k8s id: gxy-49m5s succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:21:39,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:21:47,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 83 finished
galaxy.model.metadata DEBUG 2025-11-05 01:21:47,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 132
galaxy.model.metadata DEBUG 2025-11-05 01:21:47,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 133
galaxy.model.metadata DEBUG 2025-11-05 01:21:47,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.model.metadata DEBUG 2025-11-05 01:21:47,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 135
galaxy.util WARNING 2025-11-05 01:21:47,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/d/1/dataset_1d108900-3373-4e0f-aa41-c75dbbe17d19.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/d/1/dataset_1d108900-3373-4e0f-aa41-c75dbbe17d19.dat'
galaxy.util WARNING 2025-11-05 01:21:47,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/9/8/dataset_698dd250-3059-4c53-b594-a5977c0d6c88.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/9/8/dataset_698dd250-3059-4c53-b594-a5977c0d6c88.dat'
galaxy.util WARNING 2025-11-05 01:21:47,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/3/0/dataset_c30540e2-0d8f-4407-953b-ca6fd370886d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/3/0/dataset_c30540e2-0d8f-4407-953b-ca6fd370886d.dat'
galaxy.util WARNING 2025-11-05 01:21:47,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/f/6/dataset_bf6c1e83-3562-433e-a2a2-0e7328ebcc64.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/f/6/dataset_bf6c1e83-3562-433e-a2a2-0e7328ebcc64.dat'
galaxy.jobs INFO 2025-11-05 01:21:47,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.jobs DEBUG 2025-11-05 01:21:47,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 83 executed (174.300 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:47,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 83 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:21:50,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 84
tpv.core.entities DEBUG 2025-11-05 01:21:50,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:21:50,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:21:50,108 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:21:50,111 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:21:50,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:21:50,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-11-05 01:21:50,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (31.734 ms)
galaxy.jobs.handler INFO 2025-11-05 01:21:50,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:50,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-11-05 01:21:50,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [84] prepared (63.752 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:21:50,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/84/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/84/configs/tmpk6pdn8qq']
galaxy.jobs.runners DEBUG 2025-11-05 01:21:50,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:50,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:50,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:21:50,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:00,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rxw5n with k8s id: gxy-rxw5n succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:22:01,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:22:08,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 84 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:22:08,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'ecoli_1K.fastq.gz', 'dbkey': '?', 'ext': 'fastqsanger.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fastqsanger.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/84/working/gxupload_0', 'object_id': 136}]}]}]
galaxy.jobs INFO 2025-11-05 01:22:08,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-11-05 01:22:08,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 84 executed (127.038 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:08,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 84 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:22:09,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-11-05 01:22:09,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:22:09,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/nml/spades/spades/.*, abstract=False, cores=20, mem=min(max(int(input_size * 32), 14), 240), gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:22:09,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:22:09,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:22:09,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:22:09,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-11-05 01:22:09,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (26.498 ms)
galaxy.jobs.handler INFO 2025-11-05 01:22:09,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:09,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-11-05 01:22:09,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [85] prepared (46.254 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:09,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:22:09,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:09,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:22:09,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [spades.py --version 2>&1 | awk -F 'v' '{print $2}' > /galaxy/server/database/jobs_directory/000/85/outputs/COMMAND_VERSION 2>&1;
mkdir -p reads1 && ln -s '/galaxy/server/database/objects/9/1/7/dataset_917647b1-2b4d-4196-b1bd-4e98412a9077.dat' 'reads1/ecoli_1K.fastq.gz.fastq.gz' &&           export OMP_THREAD_LIMIT=${GALAXY_SLOTS:-4} &&  spades.py  -o 'output'  -t ${GALAXY_SLOTS:-4} -m $((${GALAXY_MEMORY_MB:-8192}/1024)) --tmp-dir ${TMPDIR}   --s 1 'reads1/ecoli_1K.fastq.gz.fastq.gz'        --cov-cutoff off         && (test -f 'output/scaffolds.fasta' && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/nml/spades/822954de3f59/spades/write_tsv_script.py' < 'output/scaffolds.fasta' > '/galaxy/server/database/objects/b/f/a/dataset_bfad1280-ac08-4035-9e2e-3997655da4a1.dat' || echo 'No scaffolds.fasta.')]
galaxy.jobs.runners DEBUG 2025-11-05 01:22:09,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:09,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:09,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:22:09,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/nml/spades/spades/4.2.0+galaxy0: spades:4.2.0
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:09,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/spades:4.2.0--h8d6e82b_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:09,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:10,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:27,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9k4p7 with k8s id: gxy-9k4p7 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:22:27,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:22:35,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 85 finished
galaxy.model.metadata DEBUG 2025-11-05 01:22:35,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 137
galaxy.jobs INFO 2025-11-05 01:22:35,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-11-05 01:22:35,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 85 executed (105.893 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:35,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 85 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:22:38,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 86
tpv.core.entities DEBUG 2025-11-05 01:22:38,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:22:38,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:22:38,094 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:22:38,097 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:22:38,108 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:22:38,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-11-05 01:22:38,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (34.216 ms)
galaxy.jobs.handler INFO 2025-11-05 01:22:38,134 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:38,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 86
galaxy.jobs DEBUG 2025-11-05 01:22:38,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [86] prepared (58.103 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:22:38,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/86/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/86/configs/tmp6rtif23n']
galaxy.jobs.runners DEBUG 2025-11-05 01:22:38,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:38,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:38,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:38,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:48,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b648k with k8s id: gxy-b648k succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:22:48,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:22:56,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 86 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:22:56,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'input.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/86/working/data_fetch_upload_tlg5j4gr', 'object_id': 138}]}]}]
galaxy.jobs INFO 2025-11-05 01:22:56,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.jobs DEBUG 2025-11-05 01:22:56,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 86 executed (125.795 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:56,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 86 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:22:57,488 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87
tpv.core.entities DEBUG 2025-11-05 01:22:57,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:22:57,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:22:57,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:22:57,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:22:57,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:22:57,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-11-05 01:22:57,560 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (40.338 ms)
galaxy.jobs.handler INFO 2025-11-05 01:22:57,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:57,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-11-05 01:22:57,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [87] prepared (53.643 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:57,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:22:57,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19: snpeff:4.5covid19
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:57,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/snpeff:4.5covid19--hdfd78af_2,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:22:57,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [snpEff -version > /galaxy/server/database/jobs_directory/000/87/outputs/COMMAND_VERSION 2>&1;
snpEff -Xmx${GALAXY_MEMORY_MB:-8192}m eff -i vcf -o vcf -upDownStreamLen 0 -noLog  NC_045512.2  '/galaxy/server/database/objects/e/a/7/dataset_ea7702d3-d89b-42fb-8d3b-624ec8ab4e04.dat' > '/galaxy/server/database/objects/2/b/a/dataset_2ba530a6-3658-41eb-a336-a07edc1172de.dat']
galaxy.jobs.runners DEBUG 2025-11-05 01:22:57,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:57,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:57,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:22:57,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19: snpeff:4.5covid19
galaxy.tool_util.deps.containers INFO 2025-11-05 01:22:57,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/snpeff:4.5covid19--hdfd78af_2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:57,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:22:58,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:23:36,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jt6n8 with k8s id: gxy-jt6n8 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:23:36,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:23:44,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 87 finished
galaxy.model.metadata DEBUG 2025-11-05 01:23:44,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 139
galaxy.jobs INFO 2025-11-05 01:23:44,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs DEBUG 2025-11-05 01:23:44,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 87 executed (136.270 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:23:44,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 87 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:23:47,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-11-05 01:23:47,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:23:47,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:23:47,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:23:47,545 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:23:47,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:23:47,570 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-11-05 01:23:47,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (32.449 ms)
galaxy.jobs.handler INFO 2025-11-05 01:23:47,580 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:23:47,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-11-05 01:23:47,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [88] prepared (67.058 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:23:47,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/88/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/88/configs/tmpyjqui3kf']
galaxy.jobs.runners DEBUG 2025-11-05 01:23:47,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:23:47,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:23:47,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:23:48,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:23:58,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9j5bg with k8s id: gxy-9j5bg succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:23:58,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:24:05,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 88 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:24:05,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/88/working/gxupload_0', 'object_id': 140}]}]}]
galaxy.jobs INFO 2025-11-05 01:24:05,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-11-05 01:24:05,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 88 executed (129.591 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:05,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 88 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:24:06,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 89
tpv.core.entities DEBUG 2025-11-05 01:24:06,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:24:06,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:24:06,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:24:06,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:24:06,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:24:06,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-11-05 01:24:06,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (25.781 ms)
galaxy.jobs.handler INFO 2025-11-05 01:24:06,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:06,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 89
cheetah_DynamicallyCompiledCheetahTemplate_1762305847_0432897_97209.py:272: SyntaxWarning: invalid escape sequence '\w'
galaxy.jobs DEBUG 2025-11-05 01:24:07,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [89] prepared (60.677 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:07,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:24:07,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:07,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:24:07,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/89/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&    stringtie '/galaxy/server/database/objects/6/5/d/dataset_65d5ccba-e271-4465-8905-5b54f8db8e41.dat'  -o '/galaxy/server/database/objects/e/0/4/dataset_e045a3ef-87fe-4d53-b64e-b1e67a5726eb.dat' -p "${GALAXY_SLOTS:-1}"     -f '0.01' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:24:07,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:07,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:07,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:24:07,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:07,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:07,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:08,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:18,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-86qdd with k8s id: gxy-86qdd succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:24:18,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:24:26,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 89 finished
galaxy.model.metadata DEBUG 2025-11-05 01:24:26,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.jobs INFO 2025-11-05 01:24:26,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-11-05 01:24:26,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 89 executed (98.232 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:26,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 89 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:24:27,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90
tpv.core.entities DEBUG 2025-11-05 01:24:27,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:24:27,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:24:27,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:24:27,387 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:24:27,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:24:27,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-11-05 01:24:27,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (30.000 ms)
galaxy.jobs.handler INFO 2025-11-05 01:24:27,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:27,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-11-05 01:24:27,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [90] prepared (72.611 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:24:27,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/90/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/90/configs/tmpkze0lmnk']
galaxy.jobs.runners DEBUG 2025-11-05 01:24:27,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:27,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:27,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:28,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:37,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4hj5x with k8s id: gxy-4hj5x succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:24:37,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:24:45,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 90 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:24:45,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/90/working/gxupload_0', 'object_id': 142}]}]}]
galaxy.jobs INFO 2025-11-05 01:24:45,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-11-05 01:24:45,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 90 executed (127.357 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:45,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 90 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:24:45,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-11-05 01:24:45,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:24:45,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:24:45,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:24:45,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:24:45,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:24:45,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-11-05 01:24:45,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (41.671 ms)
galaxy.jobs.handler INFO 2025-11-05 01:24:45,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:45,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-11-05 01:24:45,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [91] prepared (44.021 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:45,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:24:45,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:46,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:24:46,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/91/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&    stringtie '/galaxy/server/database/objects/8/6/8/dataset_868c1da7-e42b-4898-8677-9e543d22c074.dat'  -o '/galaxy/server/database/objects/9/0/6/dataset_906f4cd4-8145-4f08-b687-a15df283cd9e.dat' -p "${GALAXY_SLOTS:-1}"     -f '0.17' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:24:46,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:46,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:46,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:24:46,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:24:46,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:46,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:46,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:49,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tthxb with k8s id: gxy-tthxb succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:24:50,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:24:57,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 91 finished
galaxy.model.metadata DEBUG 2025-11-05 01:24:57,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 143
galaxy.jobs INFO 2025-11-05 01:24:57,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-11-05 01:24:57,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 91 executed (98.272 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:57,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 91 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:24:59,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92, 93
tpv.core.entities DEBUG 2025-11-05 01:24:59,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:24:59,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:24:59,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:24:59,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:24:59,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:24:59,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-11-05 01:24:59,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (33.551 ms)
galaxy.jobs.handler INFO 2025-11-05 01:24:59,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:59,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 92
tpv.core.entities DEBUG 2025-11-05 01:24:59,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:24:59,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:24:59,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:24:59,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:24:59,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:24:59,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-11-05 01:24:59,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (44.131 ms)
galaxy.jobs.handler INFO 2025-11-05 01:24:59,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:59,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 93
galaxy.jobs DEBUG 2025-11-05 01:24:59,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [92] prepared (87.523 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:24:59,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/92/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/92/configs/tmpakk3tspu']
galaxy.jobs.runners DEBUG 2025-11-05 01:24:59,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:59,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:24:59,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [93] prepared (75.698 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:59,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:24:59,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/93/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/93/configs/tmp3d237fd5']
galaxy.jobs.runners DEBUG 2025-11-05 01:24:59,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:59,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:24:59,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:00,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:00,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:10,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f2fqz with k8s id: gxy-f2fqz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:10,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7qtr7 with k8s id: gxy-7qtr7 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:25:10,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:25:10,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:25:18,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 93 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:25:18,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in.gtf', 'dbkey': '?', 'ext': 'gtf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gtf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/93/working/data_fetch_upload_i6zfg4ug', 'object_id': 145}]}]}]
galaxy.jobs INFO 2025-11-05 01:25:18,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs DEBUG 2025-11-05 01:25:18,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 93 executed (142.694 ms)
galaxy.jobs.runners DEBUG 2025-11-05 01:25:18,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 92 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:18,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 93 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:25:18,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/92/working/gxupload_0', 'object_id': 144}]}]}]
galaxy.jobs INFO 2025-11-05 01:25:18,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs DEBUG 2025-11-05 01:25:18,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 92 executed (123.520 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:18,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 92 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:25:19,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94
tpv.core.entities DEBUG 2025-11-05 01:25:19,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:25:19,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:25:19,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:25:19,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:25:19,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:25:19,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-11-05 01:25:19,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (31.356 ms)
galaxy.jobs.handler INFO 2025-11-05 01:25:19,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:19,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-11-05 01:25:19,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [94] prepared (54.933 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:19,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:25:19,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:19,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:25:19,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/94/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&   ln -s '/galaxy/server/database/objects/c/0/9/dataset_c091646b-69e5-4b99-a6e4-c15ceb6f5a09.dat' guide.gff &&  stringtie '/galaxy/server/database/objects/5/8/f/dataset_58f856c7-e333-4638-947d-78dfe7540295.dat'  -o '/galaxy/server/database/objects/3/9/5/dataset_395005ef-c77f-4456-a185-b55663782962.dat' -p "${GALAXY_SLOTS:-1}"    -G guide.gff  -b ./special_de_output/sample1/  -f '0.01' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:25:19,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:19,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:19,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:25:19,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:19,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:19,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:20,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:24,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hl48v with k8s id: gxy-hl48v succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:25:24,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:25:32,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 94 finished
galaxy.model.metadata DEBUG 2025-11-05 01:25:32,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 146
galaxy.jobs INFO 2025-11-05 01:25:32,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.jobs DEBUG 2025-11-05 01:25:32,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 94 executed (96.277 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:32,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 94 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:25:34,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95, 96
tpv.core.entities DEBUG 2025-11-05 01:25:34,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:25:34,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:25:34,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:25:34,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:25:34,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:25:34,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-11-05 01:25:34,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (36.069 ms)
galaxy.jobs.handler INFO 2025-11-05 01:25:34,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 95
tpv.core.entities DEBUG 2025-11-05 01:25:34,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:25:34,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:25:34,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:25:34,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:25:34,156 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:25:34,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-11-05 01:25:34,189 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (47.174 ms)
galaxy.jobs.handler INFO 2025-11-05 01:25:34,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 96
galaxy.jobs DEBUG 2025-11-05 01:25:34,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [95] prepared (84.714 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:25:34,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/95/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/95/configs/tmpvvcga2lh']
galaxy.jobs.runners DEBUG 2025-11-05 01:25:34,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:25:34,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [96] prepared (69.378 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:25:34,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/96/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/96/configs/tmp8k6zaosp']
galaxy.jobs.runners DEBUG 2025-11-05 01:25:34,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:34,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:44,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4n6qz with k8s id: gxy-4n6qz succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:25:44,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:45,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cs97b with k8s id: gxy-cs97b succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:25:46,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:25:52,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 96 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:25:52,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in.gtf', 'dbkey': '?', 'ext': 'gtf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gtf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/96/working/data_fetch_upload_gkd6y5kz', 'object_id': 148}]}]}]
galaxy.jobs INFO 2025-11-05 01:25:53,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs DEBUG 2025-11-05 01:25:53,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 96 executed (136.589 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:53,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 96 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-11-05 01:25:53,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 95 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:25:53,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/95/working/gxupload_0', 'object_id': 147}]}]}]
galaxy.jobs INFO 2025-11-05 01:25:54,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-11-05 01:25:54,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 95 executed (152.038 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:54,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 95 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:25:54,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97
tpv.core.entities DEBUG 2025-11-05 01:25:54,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:25:54,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:25:54,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:25:54,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:25:54,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:25:54,671 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-11-05 01:25:54,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (31.984 ms)
galaxy.jobs.handler INFO 2025-11-05 01:25:54,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:54,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-11-05 01:25:54,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [97] prepared (48.003 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:54,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:25:54,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:54,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:25:54,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/97/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&   ln -s '/galaxy/server/database/objects/5/0/9/dataset_5095929f-4157-4b37-a95f-8d0a455dbc98.dat' guide.gff &&  stringtie '/galaxy/server/database/objects/e/f/9/dataset_ef9bcf0b-18c7-47e3-af14-993377ad7090.dat'  -o '/galaxy/server/database/objects/d/1/f/dataset_d1f69c84-34a5-4cea-bb89-2177a4cadeea.dat' -p "${GALAXY_SLOTS:-1}"    -G guide.gff  -b ./special_de_output/sample1/  -f '0.17' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:25:54,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:54,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:54,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:25:54,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:25:54,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:54,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:55,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:25:59,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zm52d with k8s id: gxy-zm52d succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:25:59,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:26:06,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 97 finished
galaxy.model.metadata DEBUG 2025-11-05 01:26:06,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 149
galaxy.jobs INFO 2025-11-05 01:26:06,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs DEBUG 2025-11-05 01:26:06,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 97 executed (87.062 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:06,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 97 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:26:07,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98, 99
tpv.core.entities DEBUG 2025-11-05 01:26:07,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:26:07,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:26:07,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:26:07,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:26:07,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:26:07,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-11-05 01:26:07,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (30.770 ms)
galaxy.jobs.handler INFO 2025-11-05 01:26:07,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:07,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 98
tpv.core.entities DEBUG 2025-11-05 01:26:07,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:26:07,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:26:07,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:26:07,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:26:08,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:26:08,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-11-05 01:26:08,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (41.297 ms)
galaxy.jobs.handler INFO 2025-11-05 01:26:08,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:08,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 99
galaxy.jobs DEBUG 2025-11-05 01:26:08,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [98] prepared (71.670 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:26:08,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/98/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/98/configs/tmp_inkire1']
galaxy.jobs.runners DEBUG 2025-11-05 01:26:08,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:08,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:26:08,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [99] prepared (65.852 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:08,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:26:08,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/99/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/99/configs/tmppq8316dd']
galaxy.jobs.runners DEBUG 2025-11-05 01:26:08,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:08,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:08,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:09,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:09,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:17,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-khvfp with k8s id: gxy-khvfp succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:26:17,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:18,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vtr8t with k8s id: gxy-vtr8t succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:26:18,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:26:24,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 99 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:26:24,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in.gtf', 'dbkey': '?', 'ext': 'gtf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gtf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/99/working/data_fetch_upload_ceync5k5', 'object_id': 151}]}]}]
galaxy.jobs INFO 2025-11-05 01:26:24,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs DEBUG 2025-11-05 01:26:25,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 99 executed (120.508 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:25,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 99 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-11-05 01:26:25,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 98 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:26:25,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/98/working/gxupload_0', 'object_id': 150}]}]}]
galaxy.jobs INFO 2025-11-05 01:26:26,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs DEBUG 2025-11-05 01:26:26,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 98 executed (131.057 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:26,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 98 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:26:26,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100
tpv.core.entities DEBUG 2025-11-05 01:26:26,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:26:26,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:26:26,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:26:26,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:26:26,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:26:26,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-11-05 01:26:26,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (53.029 ms)
galaxy.jobs.handler INFO 2025-11-05 01:26:26,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:26,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-11-05 01:26:26,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [100] prepared (58.625 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:26:26,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:26:26,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:26:26,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:26:26,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/100/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&   ln -s '/galaxy/server/database/objects/d/5/4/dataset_d541e3d1-fe5b-427d-97d7-d77b5629c8b3.dat' guide.gff &&  stringtie '/galaxy/server/database/objects/0/6/f/dataset_06f533c7-4169-4419-b4c3-6739e2d2af08.dat'  -o '/galaxy/server/database/objects/2/6/2/dataset_262d593c-dfa7-452b-931a-0948b9ad2d10.dat' -p "${GALAXY_SLOTS:-1}"    -G guide.gff -C '/galaxy/server/database/objects/a/4/6/dataset_a46a838d-a29b-4dd7-8115-3125430e7b65.dat'  -b ./special_de_output/sample1/  -f '0.01' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:26:26,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/e_data.ctab" -a -f "/galaxy/server/database/objects/7/1/6/dataset_716cd7e0-d070-4f47-9c4b-c21af493ba0b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/e_data.ctab" "/galaxy/server/database/objects/7/1/6/dataset_716cd7e0-d070-4f47-9c4b-c21af493ba0b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/i_data.ctab" -a -f "/galaxy/server/database/objects/c/1/6/dataset_c166ccf0-8047-4a84-88c8-9d8434a88db3.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/i_data.ctab" "/galaxy/server/database/objects/c/1/6/dataset_c166ccf0-8047-4a84-88c8-9d8434a88db3.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/t_data.ctab" -a -f "/galaxy/server/database/objects/b/4/6/dataset_b46e4cf0-fd81-4211-8cc8-cec00c571cdc.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/t_data.ctab" "/galaxy/server/database/objects/b/4/6/dataset_b46e4cf0-fd81-4211-8cc8-cec00c571cdc.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/e2t.ctab" -a -f "/galaxy/server/database/objects/8/1/e/dataset_81e1835c-7004-41a8-9476-43525453c361.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/e2t.ctab" "/galaxy/server/database/objects/8/1/e/dataset_81e1835c-7004-41a8-9476-43525453c361.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/i2t.ctab" -a -f "/galaxy/server/database/objects/d/a/e/dataset_dae3f550-76fb-4e60-97b8-0cc57a9ed948.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/100/working/special_de_output/sample1/i2t.ctab" "/galaxy/server/database/objects/d/a/e/dataset_dae3f550-76fb-4e60-97b8-0cc57a9ed948.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:26,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:26:26,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:26:26,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:26:26,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:26,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:27,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:31,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-l2fpc with k8s id: gxy-l2fpc succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:26:31,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:26:39,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 100 finished
galaxy.model.metadata DEBUG 2025-11-05 01:26:39,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 152
galaxy.model.metadata DEBUG 2025-11-05 01:26:39,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 153
galaxy.model.metadata DEBUG 2025-11-05 01:26:39,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 154
galaxy.model.metadata DEBUG 2025-11-05 01:26:39,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 155
galaxy.model.metadata DEBUG 2025-11-05 01:26:39,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 156
galaxy.model.metadata DEBUG 2025-11-05 01:26:39,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 157
galaxy.model.metadata DEBUG 2025-11-05 01:26:39,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 158
galaxy.util WARNING 2025-11-05 01:26:39,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/1/6/dataset_716cd7e0-d070-4f47-9c4b-c21af493ba0b.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/1/6/dataset_716cd7e0-d070-4f47-9c4b-c21af493ba0b.dat'
galaxy.util WARNING 2025-11-05 01:26:39,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/1/6/dataset_c166ccf0-8047-4a84-88c8-9d8434a88db3.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/1/6/dataset_c166ccf0-8047-4a84-88c8-9d8434a88db3.dat'
galaxy.util WARNING 2025-11-05 01:26:39,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/4/6/dataset_b46e4cf0-fd81-4211-8cc8-cec00c571cdc.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/4/6/dataset_b46e4cf0-fd81-4211-8cc8-cec00c571cdc.dat'
galaxy.util WARNING 2025-11-05 01:26:39,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/1/e/dataset_81e1835c-7004-41a8-9476-43525453c361.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/1/e/dataset_81e1835c-7004-41a8-9476-43525453c361.dat'
galaxy.util WARNING 2025-11-05 01:26:39,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/a/e/dataset_dae3f550-76fb-4e60-97b8-0cc57a9ed948.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/a/e/dataset_dae3f550-76fb-4e60-97b8-0cc57a9ed948.dat'
galaxy.jobs INFO 2025-11-05 01:26:39,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-11-05 01:26:39,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 100 executed (216.692 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:39,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 100 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:26:43,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 102, 101
tpv.core.entities DEBUG 2025-11-05 01:26:43,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:26:43,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:26:43,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:26:43,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:26:43,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:26:43,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-11-05 01:26:43,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (33.208 ms)
galaxy.jobs.handler INFO 2025-11-05 01:26:43,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:43,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 101
tpv.core.entities DEBUG 2025-11-05 01:26:43,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:26:43,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:26:43,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:26:43,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:26:43,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:26:43,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-11-05 01:26:43,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (47.439 ms)
galaxy.jobs.handler INFO 2025-11-05 01:26:43,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:43,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 102
galaxy.jobs DEBUG 2025-11-05 01:26:43,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [101] prepared (79.876 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:26:44,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/101/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/101/configs/tmpsy43pcnf']
galaxy.jobs.runners DEBUG 2025-11-05 01:26:44,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:44,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:26:44,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [102] prepared (68.759 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:44,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:26:44,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/102/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/102/configs/tmphrdthtkl']
galaxy.jobs.runners DEBUG 2025-11-05 01:26:44,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:44,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:44,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:44,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:44,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:54,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bv8dc with k8s id: gxy-bv8dc succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:26:54,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z57zs with k8s id: gxy-z57zs succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:26:54,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:26:54,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 102: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:27:02,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 102 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:27:02,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in.gtf', 'dbkey': '?', 'ext': 'gtf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gtf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/102/working/data_fetch_upload_7zc8_htw', 'object_id': 160}]}]}]
galaxy.jobs INFO 2025-11-05 01:27:02,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 102 in /galaxy/server/database/jobs_directory/000/102
galaxy.jobs DEBUG 2025-11-05 01:27:02,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 102 executed (153.526 ms)
galaxy.jobs.runners DEBUG 2025-11-05 01:27:02,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 101 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:02,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 102 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:27:03,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/101/working/gxupload_0', 'object_id': 159}]}]}]
galaxy.jobs INFO 2025-11-05 01:27:03,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-11-05 01:27:03,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 101 executed (148.119 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:03,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 101 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:27:04,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 103
tpv.core.entities DEBUG 2025-11-05 01:27:04,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:27:04,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:27:04,365 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:27:04,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:27:04,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:27:04,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-11-05 01:27:04,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (52.027 ms)
galaxy.jobs.handler INFO 2025-11-05 01:27:04,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:04,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 103
galaxy.jobs DEBUG 2025-11-05 01:27:04,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [103] prepared (73.879 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:27:04,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:27:04,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:27:04,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:27:04,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/103/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&   ln -s '/galaxy/server/database/objects/8/6/f/dataset_86faa2c3-295a-43c0-88cd-db893a739046.dat' guide.gff &&  stringtie '/galaxy/server/database/objects/3/9/b/dataset_39bffa4c-d4fd-47dc-820f-5055d47ef5dc.dat'  -o '/galaxy/server/database/objects/1/5/9/dataset_1599ef1e-a76c-4392-a853-47883baa5337.dat' -p "${GALAXY_SLOTS:-1}"    -G guide.gff -C '/galaxy/server/database/objects/6/1/6/dataset_61659190-5993-402c-aa7c-cd081e002189.dat' -e -b ./special_de_output/sample1/  -f '0.01' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0'    && ln -s '/galaxy/server/database/objects/1/5/9/dataset_1599ef1e-a76c-4392-a853-47883baa5337.dat' ./special_de_output/sample1/output.gtf && TAB=$(printf '\t') && CR=$(printf '\r') && prepDE.py -i ./special_de_output/ -g gene_counts.csv -t transcript_counts.csv -l 75 -c --legend '/galaxy/server/database/objects/e/2/a/dataset_e2aac8df-9d33-49a5-bced-f82e5fbe82dc.dat' > /dev/null && sed -i.bak -e "s/,/${TAB}/g" -e "s/${CR}//g" '/galaxy/server/database/objects/e/2/a/dataset_e2aac8df-9d33-49a5-bced-f82e5fbe82dc.dat'  && sed -i.bak -e "s/,/${TAB}/g" -e "s/${CR}//g" gene_counts.csv transcript_counts.csv && head -n 1 gene_counts.csv | sed -e 's/sample1/stringtie_in1_bam/' > '/galaxy/server/database/objects/6/e/0/dataset_6e0da354-252d-4c6b-994b-3b6b50dbb0f8.dat' && head -n 1 transcript_counts.csv | sed -e 's/sample1/stringtie_in1_bam/' > '/galaxy/server/database/objects/2/c/4/dataset_2c4ede9f-3a6f-41bd-84a3-866ef39adf45.dat' && tail -n +2 gene_counts.csv | sort -t"${TAB}" -k1,1 >> '/galaxy/server/database/objects/6/e/0/dataset_6e0da354-252d-4c6b-994b-3b6b50dbb0f8.dat' && tail -n +2 transcript_counts.csv | sort -t"${TAB}" -k1,1 >> '/galaxy/server/database/objects/2/c/4/dataset_2c4ede9f-3a6f-41bd-84a3-866ef39adf45.dat']
galaxy.jobs.runners DEBUG 2025-11-05 01:27:04,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/103/working/special_de_output/sample1/gene_counts.tsv" -a -f "/galaxy/server/database/objects/6/e/0/dataset_6e0da354-252d-4c6b-994b-3b6b50dbb0f8.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/103/working/special_de_output/sample1/gene_counts.tsv" "/galaxy/server/database/objects/6/e/0/dataset_6e0da354-252d-4c6b-994b-3b6b50dbb0f8.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/103/working/special_de_output/sample1/transcript_counts.tsv" -a -f "/galaxy/server/database/objects/2/c/4/dataset_2c4ede9f-3a6f-41bd-84a3-866ef39adf45.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/103/working/special_de_output/sample1/transcript_counts.tsv" "/galaxy/server/database/objects/2/c/4/dataset_2c4ede9f-3a6f-41bd-84a3-866ef39adf45.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/103/working/special_de_output/sample1/legend.tsv" -a -f "/galaxy/server/database/objects/e/2/a/dataset_e2aac8df-9d33-49a5-bced-f82e5fbe82dc.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/103/working/special_de_output/sample1/legend.tsv" "/galaxy/server/database/objects/e/2/a/dataset_e2aac8df-9d33-49a5-bced-f82e5fbe82dc.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:04,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:27:04,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:27:04,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:27:04,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:04,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:04,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:10,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5qh7h with k8s id: gxy-5qh7h succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:27:10,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:27:18,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 103 finished
galaxy.model.metadata DEBUG 2025-11-05 01:27:18,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 161
galaxy.model.metadata DEBUG 2025-11-05 01:27:18,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 162
galaxy.model.metadata DEBUG 2025-11-05 01:27:18,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 163
galaxy.model.metadata DEBUG 2025-11-05 01:27:18,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 164
galaxy.model.metadata DEBUG 2025-11-05 01:27:18,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 165
galaxy.jobs INFO 2025-11-05 01:27:18,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs DEBUG 2025-11-05 01:27:18,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 103 executed (222.866 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:18,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 103 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:27:21,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 104, 105
tpv.core.entities DEBUG 2025-11-05 01:27:21,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:27:21,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:27:21,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:27:21,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:27:21,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:27:21,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Working directory for job is: /galaxy/server/database/jobs_directory/000/104
galaxy.jobs.runners DEBUG 2025-11-05 01:27:21,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [104] queued (44.002 ms)
galaxy.jobs.handler INFO 2025-11-05 01:27:21,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:21,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 104
tpv.core.entities DEBUG 2025-11-05 01:27:21,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:27:21,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:27:21,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:27:21,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:27:21,882 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:27:21,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Working directory for job is: /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-11-05 01:27:21,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [105] queued (47.505 ms)
galaxy.jobs.handler INFO 2025-11-05 01:27:21,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:21,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 105
galaxy.jobs DEBUG 2025-11-05 01:27:21,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [104] prepared (82.134 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:27:21,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/104/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/104/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/104/configs/tmpg6y8w0yw']
galaxy.jobs.runners DEBUG 2025-11-05 01:27:21,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (104) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/104/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/104/galaxy_104.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-11-05 01:27:21,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [105] prepared (69.536 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:21,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:22,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:27:22,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/105/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/105/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/105/configs/tmppiiju5lk']
galaxy.jobs.runners DEBUG 2025-11-05 01:27:22,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (105) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/105/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/105/galaxy_105.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:22,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:22,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:23,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:23,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b6v9r failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-b6v9r.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-11-05 01:27:32,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-11-05-00-42-1/jobs/gxy-b6v9r

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-b6v9r": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (105/gxy-b6v9r) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (105/gxy-b6v9r) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (105/gxy-b6v9r) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (105/gxy-b6v9r) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-b6v9r.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 105 (gxy-b6v9r)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-b6v9r to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 105 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:32,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (105/gxy-b6v9r) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:33,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wgs9v with k8s id: gxy-wgs9v succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:27:33,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 104: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:27:40,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 104 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:27:40,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/104/working/gxupload_0', 'object_id': 166}]}]}]
galaxy.jobs INFO 2025-11-05 01:27:40,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 104 in /galaxy/server/database/jobs_directory/000/104
galaxy.jobs DEBUG 2025-11-05 01:27:40,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 104 executed (146.255 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:41,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 104 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:27:42,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 107, 106
tpv.core.entities DEBUG 2025-11-05 01:27:42,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:27:42,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:27:42,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:27:42,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:27:42,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:27:42,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Working directory for job is: /galaxy/server/database/jobs_directory/000/106
galaxy.jobs.runners DEBUG 2025-11-05 01:27:42,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [106] queued (38.174 ms)
galaxy.jobs.handler INFO 2025-11-05 01:27:42,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:42,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 106
tpv.core.entities DEBUG 2025-11-05 01:27:42,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:27:42,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:27:42,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:27:42,483 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:27:42,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:27:42,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Working directory for job is: /galaxy/server/database/jobs_directory/000/107
galaxy.jobs.runners DEBUG 2025-11-05 01:27:42,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [107] queued (47.117 ms)
galaxy.jobs.handler INFO 2025-11-05 01:27:42,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:42,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 107
galaxy.jobs DEBUG 2025-11-05 01:27:42,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [106] prepared (83.886 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:27:42,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/106/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/106/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/106/configs/tmpa2btx0py']
galaxy.jobs.runners DEBUG 2025-11-05 01:27:42,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (106) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/106/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/106/galaxy_106.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:42,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:27:42,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [107] prepared (69.541 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:42,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:27:42,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/107/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/107/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/107/configs/tmpe08bymx6']
galaxy.jobs.runners DEBUG 2025-11-05 01:27:42,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (107) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/107/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/107/galaxy_107.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:42,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:42,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:43,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:43,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:53,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-955wt with k8s id: gxy-955wt succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:27:53,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 107: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:27:54,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c5xmb with k8s id: gxy-c5xmb succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:27:54,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 106: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:28:01,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 107 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:28:01,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in.gtf', 'dbkey': '?', 'ext': 'gtf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gtf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/107/working/data_fetch_upload_s7an340h', 'object_id': 169}]}]}]
galaxy.jobs INFO 2025-11-05 01:28:01,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 107 in /galaxy/server/database/jobs_directory/000/107
galaxy.jobs DEBUG 2025-11-05 01:28:01,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 107 executed (133.933 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:01,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 107 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-11-05 01:28:02,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 106 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:28:02,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/106/working/gxupload_0', 'object_id': 168}]}]}]
galaxy.jobs INFO 2025-11-05 01:28:02,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 106 in /galaxy/server/database/jobs_directory/000/106
galaxy.jobs DEBUG 2025-11-05 01:28:02,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 106 executed (134.729 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:02,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 106 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:28:03,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 108
tpv.core.entities DEBUG 2025-11-05 01:28:03,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:28:03,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:28:03,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:28:03,975 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:28:03,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:28:03,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Working directory for job is: /galaxy/server/database/jobs_directory/000/108
galaxy.jobs.runners DEBUG 2025-11-05 01:28:04,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [108] queued (32.080 ms)
galaxy.jobs.handler INFO 2025-11-05 01:28:04,010 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:04,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 108
galaxy.jobs DEBUG 2025-11-05 01:28:04,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [108] prepared (56.801 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:28:04,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:28:04,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:28:04,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:28:04,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/108/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/108/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&   ln -s '/galaxy/server/database/objects/4/7/2/dataset_47258b2a-dd29-488d-ad52-bfb0d1e265b1.dat' guide.gff &&  stringtie '/galaxy/server/database/objects/b/b/b/dataset_bbbdc3af-8525-440e-89fb-550dc772c890.dat'  -o '/galaxy/server/database/objects/a/d/5/dataset_ad513249-e007-4762-bfa0-06bd49a8bbb9.dat' -p "${GALAXY_SLOTS:-1}"    -G guide.gff  -b ./special_de_output/sample1/  -f '0.15' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:28:04,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (108) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/108/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/108/galaxy_108.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:04,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:28:04,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:28:04,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:28:04,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:04,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:04,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qksbg failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-qksbg.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-11-05 01:28:08,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/prod-25-11-05-00-42-1/jobs/gxy-qksbg

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-qksbg": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (108/gxy-qksbg) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (108/gxy-qksbg) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (108/gxy-qksbg) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (108/gxy-qksbg) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-qksbg.
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 108 (gxy-qksbg)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-qksbg to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 108 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:08,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (108/gxy-qksbg) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-11-05 01:28:10,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 109
tpv.core.entities DEBUG 2025-11-05 01:28:10,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:28:10,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:28:10,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:28:10,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:28:10,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:28:10,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Working directory for job is: /galaxy/server/database/jobs_directory/000/109
galaxy.jobs.runners DEBUG 2025-11-05 01:28:10,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [109] queued (34.383 ms)
galaxy.jobs.handler INFO 2025-11-05 01:28:10,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:10,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 109
galaxy.jobs DEBUG 2025-11-05 01:28:10,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [109] prepared (53.938 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:28:10,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/109/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/109/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/109/configs/tmpyxtu5nsj']
galaxy.jobs.runners DEBUG 2025-11-05 01:28:10,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (109) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/109/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/109/galaxy_109.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:10,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:10,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:10,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:21,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vvz2b with k8s id: gxy-vvz2b succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:28:21,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 109: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:28:28,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 109 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:28:28,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/109/working/gxupload_0', 'object_id': 171}]}]}]
galaxy.jobs INFO 2025-11-05 01:28:28,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 109 in /galaxy/server/database/jobs_directory/000/109
galaxy.jobs DEBUG 2025-11-05 01:28:28,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 109 executed (153.664 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:28,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 109 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:28:29,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 110
tpv.core.entities DEBUG 2025-11-05 01:28:29,554 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:28:29,554 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:28:29,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:28:29,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:28:29,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:28:29,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Working directory for job is: /galaxy/server/database/jobs_directory/000/110
galaxy.jobs.runners DEBUG 2025-11-05 01:28:29,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [110] queued (39.212 ms)
galaxy.jobs.handler INFO 2025-11-05 01:28:29,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:29,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 110
galaxy.jobs DEBUG 2025-11-05 01:28:29,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [110] prepared (60.889 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:28:29,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/110/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/110/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/110/configs/tmps0nxyhhe']
galaxy.jobs.runners DEBUG 2025-11-05 01:28:29,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (110) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/110/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/110/galaxy_110.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:29,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:29,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:30,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:40,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jjrm4 with k8s id: gxy-jjrm4 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:28:40,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 110: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:28:47,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 110 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:28:47,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in1.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/110/working/gxupload_0', 'object_id': 172}]}]}]
galaxy.jobs INFO 2025-11-05 01:28:47,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 110 in /galaxy/server/database/jobs_directory/000/110
galaxy.jobs DEBUG 2025-11-05 01:28:47,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 110 executed (149.375 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:48,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 110 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:28:48,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 111
tpv.core.entities DEBUG 2025-11-05 01:28:48,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:28:48,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:28:48,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:28:48,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:28:48,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:28:49,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Working directory for job is: /galaxy/server/database/jobs_directory/000/111
galaxy.jobs.runners DEBUG 2025-11-05 01:28:49,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [111] queued (42.089 ms)
galaxy.jobs.handler INFO 2025-11-05 01:28:49,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:49,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 111
galaxy.jobs DEBUG 2025-11-05 01:28:49,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [111] prepared (61.578 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:28:49,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/111/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/111/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/111/configs/tmpdzrsfw84']
galaxy.jobs.runners DEBUG 2025-11-05 01:28:49,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (111) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/111/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/111/galaxy_111.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:49,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:49,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:49,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:28:59,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nm9d8 with k8s id: gxy-nm9d8 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:28:59,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 111: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:29:07,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 111 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:29:07,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'long_reads.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/111/working/gxupload_0', 'object_id': 173}]}]}]
galaxy.jobs INFO 2025-11-05 01:29:07,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.jobs DEBUG 2025-11-05 01:29:07,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 111 executed (149.978 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:07,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:29:08,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 112, 113
tpv.core.entities DEBUG 2025-11-05 01:29:08,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:29:08,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:29:08,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:29:08,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:29:08,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:29:08,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Working directory for job is: /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-11-05 01:29:08,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [112] queued (47.377 ms)
galaxy.jobs.handler INFO 2025-11-05 01:29:08,458 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:08,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 112
tpv.core.entities DEBUG 2025-11-05 01:29:08,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:29:08,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:29:08,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:29:08,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:29:08,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:29:08,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Working directory for job is: /galaxy/server/database/jobs_directory/000/113
galaxy.jobs.runners DEBUG 2025-11-05 01:29:08,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [113] queued (46.385 ms)
galaxy.jobs.handler INFO 2025-11-05 01:29:08,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:08,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 113
galaxy.jobs DEBUG 2025-11-05 01:29:08,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [112] prepared (78.615 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:29:08,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/112/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/112/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/112/configs/tmp01c6slf9']
galaxy.jobs.runners DEBUG 2025-11-05 01:29:08,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (112) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/112/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/112/galaxy_112.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:08,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-11-05 01:29:08,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [113] prepared (78.313 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:08,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-11-05 01:29:08,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/113/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/113/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/113/configs/tmpnyoog3mj']
galaxy.jobs.runners DEBUG 2025-11-05 01:29:08,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (113) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/113/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/113/galaxy_113.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:08,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:08,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:09,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:09,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:19,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v2f6f with k8s id: gxy-v2f6f succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:19,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sdr24 with k8s id: gxy-sdr24 succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:29:19,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 112: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:29:19,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 113: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:29:27,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 112 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:29:27,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'short_reads.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/112/working/gxupload_0', 'object_id': 174}]}]}]
galaxy.jobs INFO 2025-11-05 01:29:27,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.jobs DEBUG 2025-11-05 01:29:27,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 112 executed (149.816 ms)
galaxy.jobs.runners DEBUG 2025-11-05 01:29:27,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 113 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:27,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:29:27,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'long_reads.bam', 'dbkey': '?', 'ext': 'bam', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bam file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/113/working/gxupload_0', 'object_id': 175}]}]}]
galaxy.jobs INFO 2025-11-05 01:29:27,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 113 in /galaxy/server/database/jobs_directory/000/113
galaxy.jobs DEBUG 2025-11-05 01:29:27,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 113 executed (171.868 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:27,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 113 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:29:27,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 114
tpv.core.entities DEBUG 2025-11-05 01:29:28,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:29:28,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:29:28,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:29:28,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:29:28,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:29:28,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Working directory for job is: /galaxy/server/database/jobs_directory/000/114
galaxy.jobs.runners DEBUG 2025-11-05 01:29:28,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [114] queued (32.791 ms)
galaxy.jobs.handler INFO 2025-11-05 01:29:28,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:28,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 114
galaxy.jobs DEBUG 2025-11-05 01:29:28,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [114] prepared (46.454 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:28,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:29:28,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:28,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:29:28,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/114/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/114/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&    stringtie '/galaxy/server/database/objects/e/b/9/dataset_eb9b3985-141e-4ff2-bbb2-9ea2c12a6b36.dat' '/galaxy/server/database/objects/5/7/5/dataset_575a4da0-7b28-4caf-bff4-28d2624da097.dat' -E 25  -o '/galaxy/server/database/objects/6/4/2/dataset_6426e032-b5b9-40b6-9d88-b4eb272aa52a.dat' -p "${GALAXY_SLOTS:-1}"     -f '0.01' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:29:28,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (114) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/114/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/114/galaxy_114.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:28,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:28,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:29:28,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:28,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:28,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:29,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:32,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-769hv with k8s id: gxy-769hv succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:29:32,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 114: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:29:39,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 114 finished
galaxy.model.metadata DEBUG 2025-11-05 01:29:39,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 176
galaxy.jobs INFO 2025-11-05 01:29:39,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 114 in /galaxy/server/database/jobs_directory/000/114
galaxy.jobs DEBUG 2025-11-05 01:29:39,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 114 executed (90.844 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:39,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:29:41,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 115
tpv.core.entities DEBUG 2025-11-05 01:29:41,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:29:41,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:29:41,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:29:41,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:29:41,323 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:29:41,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Working directory for job is: /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-11-05 01:29:41,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [115] queued (36.089 ms)
galaxy.jobs.handler INFO 2025-11-05 01:29:41,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:41,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 115
galaxy.jobs DEBUG 2025-11-05 01:29:41,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [115] prepared (72.078 ms)
galaxy.jobs.command_factory INFO 2025-11-05 01:29:41,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/115/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/115/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/115/configs/tmpcos29gsk']
galaxy.jobs.runners DEBUG 2025-11-05 01:29:41,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (115) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/115/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/115/galaxy_115.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:41,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:41,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:42,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:51,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kx59c with k8s id: gxy-kx59c succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:29:51,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 115: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:29:59,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 115 finished
galaxy.tool_util.provided_metadata DEBUG 2025-11-05 01:29:59,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'stringtie_in.cram', 'dbkey': '?', 'ext': 'cram', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded cram file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/115/working/gxupload_0', 'object_id': 177}]}]}]
galaxy.jobs INFO 2025-11-05 01:29:59,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs DEBUG 2025-11-05 01:29:59,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 115 executed (137.047 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:59,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-11-05 01:29:59,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 116
tpv.core.entities DEBUG 2025-11-05 01:29:59,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} using default ranker
tpv.core.entities DEBUG 2025-11-05 01:29:59,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Destination: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/.*, abstract=False, cores=1, mem=25, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=, inherits=None, context={}, rules={} scored: 0
galaxy.jobs.mapper DEBUG 2025-11-05 01:29:59,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-11-05 01:29:59,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-11-05 01:29:59,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-11-05 01:29:59,754 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Working directory for job is: /galaxy/server/database/jobs_directory/000/116
galaxy.jobs.runners DEBUG 2025-11-05 01:29:59,766 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [116] queued (48.017 ms)
galaxy.jobs.handler INFO 2025-11-05 01:29:59,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:59,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 116
galaxy.jobs DEBUG 2025-11-05 01:29:59,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [116] prepared (53.334 ms)
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:59,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:29:59,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:59,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-11-05 01:29:59,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/116/tool_script.sh] for tool command [stringtie --version > /galaxy/server/database/jobs_directory/000/116/outputs/COMMAND_VERSION 2>&1;
mkdir -p ./special_de_output/sample1/ &&    stringtie '/galaxy/server/database/objects/f/d/6/dataset_fd66c75e-39fe-44cb-94c3-0402e322de25.dat' '/galaxy/server/database/objects/f/d/6/dataset_fd66c75e-39fe-44cb-94c3-0402e322de25.dat' -E 25  -o '/galaxy/server/database/objects/2/f/e/dataset_2feae2e2-fde9-4ce9-9a76-281d74dc9512.dat' -p "${GALAXY_SLOTS:-1}"     -f '0.01' -m '200' -a '10' -j '1' -c '1' -g '50' -M '1.0']
galaxy.jobs.runners DEBUG 2025-11-05 01:29:59,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (116) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/116/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/116/galaxy_116.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:59,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:59,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-11-05 01:29:59,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0: mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8
galaxy.tool_util.deps.containers INFO 2025-11-05 01:29:59,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-704aee0c18e3278557ac3bedcbb7986e3913f703:b3c012f26a7d4eba804b5c3f70ec20f899f97bb8-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:29:59,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:30:00,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:30:04,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zjbgs with k8s id: gxy-zjbgs succeeded
galaxy.jobs.runners DEBUG 2025-11-05 01:30:05,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 116: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-11-05 01:30:12,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 116 finished
galaxy.model.metadata DEBUG 2025-11-05 01:30:12,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 178
galaxy.jobs INFO 2025-11-05 01:30:12,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 116 in /galaxy/server/database/jobs_directory/000/116
galaxy.jobs DEBUG 2025-11-05 01:30:12,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 116 executed (93.810 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-11-05 01:30:12,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
