galaxy.jobs INFO 2025-03-02 06:42:00,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.jobs DEBUG 2025-03-02 06:42:00,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 111 executed (228.135 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:00,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 111 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:00,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 112 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:00,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 122
galaxy.jobs INFO 2025-03-02 06:42:00,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-03-02 06:42:00,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 116: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs DEBUG 2025-03-02 06:42:00,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 112 executed (198.040 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:00,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 112 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:01,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 117: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:01,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pl58z with k8s id: gxy-pl58z succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:02,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lpz4k with k8s id: gxy-lpz4k succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:02,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m977c with k8s id: gxy-m977c succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:02,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bhpd6 with k8s id: gxy-bhpd6 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:42:09,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 114 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:09,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 124
galaxy.jobs INFO 2025-03-02 06:42:09,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 114 in /galaxy/server/database/jobs_directory/000/114
galaxy.jobs DEBUG 2025-03-02 06:42:09,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 114 executed (195.585 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:09,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 114 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:09,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 118: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:16,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 115 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:16,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 125
galaxy.jobs INFO 2025-03-02 06:42:16,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs DEBUG 2025-03-02 06:42:16,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 115 executed (382.176 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:16,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 115 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:16,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:17,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 116 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:17,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 126
galaxy.jobs INFO 2025-03-02 06:42:17,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 116 in /galaxy/server/database/jobs_directory/000/116
galaxy.jobs DEBUG 2025-03-02 06:42:17,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 116 executed (202.371 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:17,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 116 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:17,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 120: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:17,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 117 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:17,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 127
galaxy.jobs INFO 2025-03-02 06:42:17,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 117 in /galaxy/server/database/jobs_directory/000/117
galaxy.jobs DEBUG 2025-03-02 06:42:17,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 117 executed (216.660 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:17,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 117 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:18,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 121: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:25,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 118 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:25,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 128
galaxy.jobs INFO 2025-03-02 06:42:25,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 118 in /galaxy/server/database/jobs_directory/000/118
galaxy.jobs DEBUG 2025-03-02 06:42:26,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 118 executed (287.788 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:26,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 118 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:26,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 122: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:33,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 119 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:33,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 129
galaxy.jobs INFO 2025-03-02 06:42:33,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs DEBUG 2025-03-02 06:42:33,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 119 executed (197.223 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:33,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 119 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:33,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 120 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:33,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 130
galaxy.jobs INFO 2025-03-02 06:42:33,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 120 in /galaxy/server/database/jobs_directory/000/120
galaxy.jobs DEBUG 2025-03-02 06:42:33,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 120 executed (199.836 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:33,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 120 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:33,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 123: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:33,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 124: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:34,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 121 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:34,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 131
galaxy.jobs INFO 2025-03-02 06:42:34,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 121 in /galaxy/server/database/jobs_directory/000/121
galaxy.jobs DEBUG 2025-03-02 06:42:34,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 121 executed (194.028 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:34,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 121 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:34,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 126: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:42,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 122 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:42,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 132
galaxy.jobs INFO 2025-03-02 06:42:42,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 122 in /galaxy/server/database/jobs_directory/000/122
galaxy.jobs DEBUG 2025-03-02 06:42:42,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 122 executed (194.770 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:42,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 122 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:42,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 138: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:49,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 124 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:49,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.jobs INFO 2025-03-02 06:42:49,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 124 in /galaxy/server/database/jobs_directory/000/124
galaxy.jobs.runners DEBUG 2025-03-02 06:42:49,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 123 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:49,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 133
galaxy.jobs DEBUG 2025-03-02 06:42:49,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 124 executed (202.464 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:49,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 124 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-02 06:42:49,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 123 in /galaxy/server/database/jobs_directory/000/123
galaxy.jobs DEBUG 2025-03-02 06:42:49,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 123 executed (197.779 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:49,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 123 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:49,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 127: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:50,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 128: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:50,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 126 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:50,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 136
galaxy.jobs INFO 2025-03-02 06:42:50,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 126 in /galaxy/server/database/jobs_directory/000/126
galaxy.jobs DEBUG 2025-03-02 06:42:50,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 126 executed (203.367 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:50,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 126 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:51,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 129: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:42:58,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 138 finished
galaxy.model.metadata DEBUG 2025-03-02 06:42:58,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 148
galaxy.jobs INFO 2025-03-02 06:42:58,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 138 in /galaxy/server/database/jobs_directory/000/138
galaxy.jobs DEBUG 2025-03-02 06:42:58,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 138 executed (192.728 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:42:58,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:42:59,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 131: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:05,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 127 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:05,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 137
galaxy.jobs.runners DEBUG 2025-03-02 06:43:05,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 128 finished
galaxy.jobs INFO 2025-03-02 06:43:05,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 127 in /galaxy/server/database/jobs_directory/000/127
galaxy.model.metadata DEBUG 2025-03-02 06:43:05,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 138
galaxy.jobs INFO 2025-03-02 06:43:06,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 128 in /galaxy/server/database/jobs_directory/000/128
galaxy.jobs DEBUG 2025-03-02 06:43:06,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 127 executed (375.547 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:06,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 127 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:43:06,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 128 executed (308.876 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:06,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 128 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:43:06,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 132: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:06,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 133: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:07,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 129 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:07,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 139
galaxy.jobs INFO 2025-03-02 06:43:07,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 129 in /galaxy/server/database/jobs_directory/000/129
galaxy.jobs DEBUG 2025-03-02 06:43:07,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 129 executed (196.662 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:07,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 129 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:43:07,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 125: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:14,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 131 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:14,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.jobs INFO 2025-03-02 06:43:14,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 131 in /galaxy/server/database/jobs_directory/000/131
galaxy.jobs DEBUG 2025-03-02 06:43:14,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 131 executed (215.683 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:14,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 131 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:43:15,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 139: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:22,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 133 finished
galaxy.jobs.runners DEBUG 2025-03-02 06:43:22,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 132 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:22,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 143
galaxy.model.metadata DEBUG 2025-03-02 06:43:22,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 142
galaxy.jobs INFO 2025-03-02 06:43:22,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 133 in /galaxy/server/database/jobs_directory/000/133
galaxy.jobs INFO 2025-03-02 06:43:22,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 132 in /galaxy/server/database/jobs_directory/000/132
galaxy.jobs DEBUG 2025-03-02 06:43:22,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 133 executed (258.952 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:22,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 133 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:43:22,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 132 executed (195.286 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:22,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 132 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:43:23,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 134: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:23,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 135: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:23,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 125 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:23,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 135
galaxy.jobs INFO 2025-03-02 06:43:23,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 125 in /galaxy/server/database/jobs_directory/000/125
galaxy.jobs DEBUG 2025-03-02 06:43:23,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 125 executed (188.715 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:23,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 125 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:43:24,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 136: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:31,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 139 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:31,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 149
galaxy.jobs INFO 2025-03-02 06:43:31,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 139 in /galaxy/server/database/jobs_directory/000/139
galaxy.jobs DEBUG 2025-03-02 06:43:31,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 139 executed (188.908 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:43:31,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] No k8s job found which matches job id 'gxy-8698l'. Ignoring...
galaxy.jobs.runners DEBUG 2025-03-02 06:43:31,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 137: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:43:39,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 135 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:39,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 145
galaxy.jobs INFO 2025-03-02 06:43:39,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 135 in /galaxy/server/database/jobs_directory/000/135
galaxy.jobs.runners DEBUG 2025-03-02 06:43:39,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 134 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:39,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 144
galaxy.jobs DEBUG 2025-03-02 06:43:39,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 135 executed (271.246 ms)
galaxy.jobs INFO 2025-03-02 06:43:39,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 134 in /galaxy/server/database/jobs_directory/000/134
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:43:39,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] No k8s job found which matches job id 'gxy-lpz4k'. Ignoring...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (130/gxy-bncg7) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (130/gxy-bncg7) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (130/gxy-bncg7) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (130/gxy-bncg7) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-bncg7.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 130 (gxy-bncg7)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Could not find job with id gxy-bncg7 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:39,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (130/gxy-bncg7) Terminated at user's request
galaxy.jobs DEBUG 2025-03-02 06:43:39,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 134 executed (191.014 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:43:39,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] No k8s job found which matches job id 'gxy-pl58z'. Ignoring...
galaxy.jobs.runners DEBUG 2025-03-02 06:43:40,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 136 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:40,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 146
galaxy.jobs INFO 2025-03-02 06:43:40,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 136 in /galaxy/server/database/jobs_directory/000/136
galaxy.jobs DEBUG 2025-03-02 06:43:40,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 136 executed (107.567 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:43:40,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] No k8s job found which matches job id 'gxy-m977c'. Ignoring...
galaxy.jobs.handler DEBUG 2025-03-02 06:43:40,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 141, 140
tpv.core.entities DEBUG 2025-03-02 06:43:40,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:43:40,776 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:43:40,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:43:40,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:43:40,802 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Working directory for job is: /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-03-02 06:43:40,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [140] queued (30.354 ms)
galaxy.jobs.handler INFO 2025-03-02 06:43:40,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:40,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 140
tpv.core.entities DEBUG 2025-03-02 06:43:40,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:43:40,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:43:40,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:43:40,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:43:40,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Working directory for job is: /galaxy/server/database/jobs_directory/000/141
galaxy.jobs.runners DEBUG 2025-03-02 06:43:40,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [141] queued (57.139 ms)
galaxy.jobs.handler INFO 2025-03-02 06:43:40,894 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:40,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 141
galaxy.jobs DEBUG 2025-03-02 06:43:40,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [140] prepared (109.568 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:43:40,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/140/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/140/registry.xml' '/galaxy/server/database/jobs_directory/000/140/upload_params.json' '150:/galaxy/server/database/objects/1/1/2/dataset_112b6c7f-ac5a-42cd-9b4f-2ef3522f0c62_files:/galaxy/server/database/objects/1/1/2/dataset_112b6c7f-ac5a-42cd-9b4f-2ef3522f0c62.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:43:40,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (140) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/140/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/140/galaxy_140.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:43:40,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [141] prepared (77.664 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:40,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:41,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:43:41,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/141/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/141/registry.xml' '/galaxy/server/database/jobs_directory/000/141/upload_params.json' '151:/galaxy/server/database/objects/0/f/a/dataset_0fab72cb-9959-40dd-9757-3dd8fb4d4f63_files:/galaxy/server/database/objects/0/f/a/dataset_0fab72cb-9959-40dd-9757-3dd8fb4d4f63.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:43:41,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (141) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/141/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/141/galaxy_141.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:41,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:41,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:42,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:42,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:43:43,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 137 finished
galaxy.model.metadata DEBUG 2025-03-02 06:43:43,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 147
galaxy.jobs INFO 2025-03-02 06:43:43,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 137 in /galaxy/server/database/jobs_directory/000/137
galaxy.jobs DEBUG 2025-03-02 06:43:43,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 137 executed (100.900 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:43:43,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] No k8s job found which matches job id 'gxy-bhpd6'. Ignoring...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:51,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zxgkx with k8s id: gxy-zxgkx succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:43:51,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 140: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:43:52,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fqb5n with k8s id: gxy-fqb5n succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:43:52,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 141: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:44:00,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 140 finished
galaxy.model.metadata DEBUG 2025-03-02 06:44:00,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 150
galaxy.jobs INFO 2025-03-02 06:44:00,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 140 in /galaxy/server/database/jobs_directory/000/140
galaxy.jobs DEBUG 2025-03-02 06:44:00,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 140 executed (103.821 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:00,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:44:01,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 141 finished
galaxy.model.metadata DEBUG 2025-03-02 06:44:01,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 151
galaxy.jobs INFO 2025-03-02 06:44:01,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 141 in /galaxy/server/database/jobs_directory/000/141
galaxy.jobs DEBUG 2025-03-02 06:44:01,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 141 executed (97.804 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:01,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:44:02,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 142
tpv.core.entities DEBUG 2025-03-02 06:44:02,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:44:02,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:44:02,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:44:02,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:44:02,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Working directory for job is: /galaxy/server/database/jobs_directory/000/142
galaxy.jobs.runners DEBUG 2025-03-02 06:44:02,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [142] queued (29.446 ms)
galaxy.jobs.handler INFO 2025-03-02 06:44:02,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:02,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 142
galaxy.jobs DEBUG 2025-03-02 06:44:02,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [142] prepared (58.881 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:02,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:44:02,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:02,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:44:02,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/142/tool_script.sh] for tool command [multiqc --version > /galaxy/server/database/jobs_directory/000/142/outputs/COMMAND_VERSION 2>&1;
die() { echo "$@" 1>&2 ; exit 1; } &&  mkdir multiqc_WDir &&   mkdir multiqc_WDir/custom_content_0 &&  ln -s '/galaxy/server/database/objects/1/1/2/dataset_112b6c7f-ac5a-42cd-9b4f-2ef3522f0c62.dat' 'multiqc_WDir/custom_content_0/file_0_0' && more /galaxy/server/database/objects/1/1/2/dataset_112b6c7f-ac5a-42cd-9b4f-2ef3522f0c62.dat && ln -s '/galaxy/server/database/objects/0/f/a/dataset_0fab72cb-9959-40dd-9757-3dd8fb4d4f63.dat' 'multiqc_WDir/custom_content_0/file_0_1' && more /galaxy/server/database/objects/0/f/a/dataset_0fab72cb-9959-40dd-9757-3dd8fb4d4f63.dat &&  multiqc multiqc_WDir --filename "report"      --config '/galaxy/server/database/jobs_directory/000/142/configs/tmpc4fvji6t']
galaxy.jobs.runners DEBUG 2025-03-02 06:44:02,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (142) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/142/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/142/galaxy_142.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/142/working/report.html" -a -f "/galaxy/server/database/objects/0/8/1/dataset_0814ff0e-2747-43d0-8b66-100bbea468cf.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/142/working/report.html" "/galaxy/server/database/objects/0/8/1/dataset_0814ff0e-2747-43d0-8b66-100bbea468cf.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:02,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:02,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:44:02,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:02,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:02,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:02,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:11,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t7bvb with k8s id: gxy-t7bvb succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:44:11,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 142: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:44:18,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 142 finished
galaxy.model.store.discover DEBUG 2025-03-02 06:44:18,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (142) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/142/working/report_data/multiqc_sources.txt] with element identifier [sources] for output [stats] (3.711 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:44:18,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (142) Add dynamic collection datasets to history for output [stats] (16.433 ms)
galaxy.model.metadata DEBUG 2025-03-02 06:44:18,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 152
galaxy.jobs INFO 2025-03-02 06:44:18,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 142 in /galaxy/server/database/jobs_directory/000/142
galaxy.jobs DEBUG 2025-03-02 06:44:18,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 142 executed (122.417 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:18,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:44:20,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 144, 143
tpv.core.entities DEBUG 2025-03-02 06:44:20,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:44:20,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:44:20,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:44:20,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:44:20,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Working directory for job is: /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-03-02 06:44:20,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [143] queued (26.223 ms)
galaxy.jobs.handler INFO 2025-03-02 06:44:20,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:20,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 143
tpv.core.entities DEBUG 2025-03-02 06:44:20,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:44:20,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:44:20,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:44:20,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:44:20,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Working directory for job is: /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-03-02 06:44:20,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [144] queued (62.694 ms)
galaxy.jobs.handler INFO 2025-03-02 06:44:20,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:20,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 144
galaxy.jobs DEBUG 2025-03-02 06:44:20,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [143] prepared (110.918 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:44:20,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/143/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/143/registry.xml' '/galaxy/server/database/jobs_directory/000/143/upload_params.json' '154:/galaxy/server/database/objects/7/3/0/dataset_730b439f-2e70-466a-be4b-3ccf43cb55a0_files:/galaxy/server/database/objects/7/3/0/dataset_730b439f-2e70-466a-be4b-3ccf43cb55a0.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:44:20,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (143) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/143/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/143/galaxy_143.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:20,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:44:20,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [144] prepared (75.741 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:20,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:44:20,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/144/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/144/registry.xml' '/galaxy/server/database/jobs_directory/000/144/upload_params.json' '155:/galaxy/server/database/objects/c/8/8/dataset_c88a3afb-df1b-4c99-bb4b-2c1e0ead1867_files:/galaxy/server/database/objects/c/8/8/dataset_c88a3afb-df1b-4c99-bb4b-2c1e0ead1867.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:44:20,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (144) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/144/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/144/galaxy_144.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:20,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:20,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:21,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:22,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:30,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nbnhm with k8s id: gxy-nbnhm succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:30,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fxrhf with k8s id: gxy-fxrhf succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:44:30,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 143: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:44:30,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 144: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:44:38,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 143 finished
galaxy.model.metadata DEBUG 2025-03-02 06:44:38,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 154
galaxy.jobs INFO 2025-03-02 06:44:38,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 143 in /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-03-02 06:44:38,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 144 finished
galaxy.jobs DEBUG 2025-03-02 06:44:38,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 143 executed (115.577 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:38,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.model.metadata DEBUG 2025-03-02 06:44:38,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 155
galaxy.jobs INFO 2025-03-02 06:44:38,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 144 in /galaxy/server/database/jobs_directory/000/144
galaxy.jobs DEBUG 2025-03-02 06:44:38,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 144 executed (107.979 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:38,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:44:39,192 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 145
tpv.core.entities DEBUG 2025-03-02 06:44:39,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:44:39,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:44:39,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:44:39,248 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:44:39,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Working directory for job is: /galaxy/server/database/jobs_directory/000/145
galaxy.jobs.runners DEBUG 2025-03-02 06:44:39,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [145] queued (48.673 ms)
galaxy.jobs.handler INFO 2025-03-02 06:44:39,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:39,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 145
galaxy.jobs DEBUG 2025-03-02 06:44:39,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [145] prepared (77.025 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:39,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:44:39,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:39,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:44:39,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/145/tool_script.sh] for tool command [multiqc --version > /galaxy/server/database/jobs_directory/000/145/outputs/COMMAND_VERSION 2>&1;
die() { echo "$@" 1>&2 ; exit 1; } &&  mkdir multiqc_WDir &&   mkdir multiqc_WDir/fastqc_0 &&    mkdir 'multiqc_WDir/fastqc_0/data_0' &&  mkdir 'multiqc_WDir/fastqc_0/data_0/file_0' && ln -s '/galaxy/server/database/objects/7/3/0/dataset_730b439f-2e70-466a-be4b-3ccf43cb55a0.dat' 'multiqc_WDir/fastqc_0/data_0/file_0/fastqc_data.txt' && mkdir 'multiqc_WDir/fastqc_0/data_0/file_1' && ln -s '/galaxy/server/database/objects/c/8/8/dataset_c88a3afb-df1b-4c99-bb4b-2c1e0ead1867.dat' 'multiqc_WDir/fastqc_0/data_0/file_1/fastqc_data.txt' &&  multiqc multiqc_WDir --filename "report"  --title "Title of the report" --comment "Commment for the report"  --flat --export]
galaxy.jobs.runners DEBUG 2025-03-02 06:44:39,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/145/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/145/galaxy_145.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/145/working/report.html" -a -f "/galaxy/server/database/objects/8/e/a/dataset_8ea9c5c3-0f37-4604-8237-5af5070d7ba0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/145/working/report.html" "/galaxy/server/database/objects/8/e/a/dataset_8ea9c5c3-0f37-4604-8237-5af5070d7ba0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:39,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:39,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:44:39,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:44:39,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:39,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:40,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:44:54,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fxc9b with k8s id: gxy-fxc9b succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:44:54,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 145: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:45:01,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 145 finished
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/mqc_fastqc_per_base_n_content_plot_1.txt] with element identifier [fastqc_per_base_n_content_plot_1] for output [plots] (3.737 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/mqc_fastqc_per_base_sequence_quality_plot_1.txt] with element identifier [fastqc_per_base_sequence_quality_plot_1] for output [plots] (0.596 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/mqc_fastqc_per_sequence_gc_content_plot_Counts.txt] with element identifier [fastqc_per_sequence_gc_content_plot_Counts] for output [plots] (0.487 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/mqc_fastqc_per_sequence_gc_content_plot_Percentages.txt] with element identifier [fastqc_per_sequence_gc_content_plot_Percentages] for output [plots] (0.461 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/mqc_fastqc_per_sequence_quality_scores_plot_1.txt] with element identifier [fastqc_per_sequence_quality_scores_plot_1] for output [plots] (0.453 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/mqc_fastqc_sequence_counts_plot_1.txt] with element identifier [fastqc_sequence_counts_plot_1] for output [plots] (0.397 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/mqc_fastqc_sequence_duplication_levels_plot_1.txt] with element identifier [fastqc_sequence_duplication_levels_plot_1] for output [plots] (0.589 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Add dynamic collection datasets to history for output [plots] (67.613 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/multiqc_fastqc.txt] with element identifier [fastqc] for output [stats] (0.732 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/multiqc_general_stats.txt] with element identifier [general_stats] for output [stats] (0.447 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/145/working/report_data/multiqc_sources.txt] with element identifier [sources] for output [stats] (0.401 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:02,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (145) Add dynamic collection datasets to history for output [stats] (28.677 ms)
galaxy.model.metadata DEBUG 2025-03-02 06:45:02,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 156
galaxy.jobs INFO 2025-03-02 06:45:02,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 145 in /galaxy/server/database/jobs_directory/000/145
galaxy.jobs DEBUG 2025-03-02 06:45:02,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 145 executed (320.477 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:02,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:45:04,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 146
tpv.core.entities DEBUG 2025-03-02 06:45:04,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:04,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:04,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:04,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:04,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Working directory for job is: /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners DEBUG 2025-03-02 06:45:04,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [146] queued (36.427 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:04,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:04,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 146
galaxy.jobs DEBUG 2025-03-02 06:45:04,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [146] prepared (73.533 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:45:04,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/146/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/146/registry.xml' '/galaxy/server/database/jobs_directory/000/146/upload_params.json' '167:/galaxy/server/database/objects/9/3/3/dataset_933905ee-f629-4cbe-bee4-00b6ab11afe2_files:/galaxy/server/database/objects/9/3/3/dataset_933905ee-f629-4cbe-bee4-00b6ab11afe2.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:04,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (146) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/146/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/146/galaxy_146.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:04,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:05,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:05,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:14,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w6pph with k8s id: gxy-w6pph succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:45:14,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 146: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:45:22,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 146 finished
galaxy.model.metadata DEBUG 2025-03-02 06:45:22,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 167
galaxy.jobs INFO 2025-03-02 06:45:22,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 146 in /galaxy/server/database/jobs_directory/000/146
galaxy.jobs DEBUG 2025-03-02 06:45:22,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 146 executed (91.792 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:22,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:45:24,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 147
tpv.core.entities DEBUG 2025-03-02 06:45:24,210 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:24,210 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:24,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:24,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:24,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Working directory for job is: /galaxy/server/database/jobs_directory/000/147
galaxy.jobs.runners DEBUG 2025-03-02 06:45:24,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [147] queued (36.313 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:24,254 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:24,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 147
galaxy.jobs DEBUG 2025-03-02 06:45:24,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [147] prepared (58.617 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:45:24,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:45:24,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:45:24,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:45:24,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/147/tool_script.sh] for tool command [multiqc --version > /galaxy/server/database/jobs_directory/000/147/outputs/COMMAND_VERSION 2>&1;
die() { echo "$@" 1>&2 ; exit 1; } &&  mkdir multiqc_WDir &&   mkdir multiqc_WDir/pycoqc_0 &&         grep -q '"pycoqc":' /galaxy/server/database/objects/9/3/3/dataset_933905ee-f629-4cbe-bee4-00b6ab11afe2.dat || die "Module 'pycoqc: '"pycoqc":' not found in the file 'pycoqc_json'" && ln -s '/galaxy/server/database/objects/9/3/3/dataset_933905ee-f629-4cbe-bee4-00b6ab11afe2.dat' 'multiqc_WDir/pycoqc_0/pycoqc_json'  &&    multiqc multiqc_WDir --filename "report"  --title "Title of the report" --comment "Commment for the report"]
galaxy.jobs.runners DEBUG 2025-03-02 06:45:24,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (147) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/147/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/147/galaxy_147.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/147/working/report.html" -a -f "/galaxy/server/database/objects/4/b/d/dataset_4bd8fd49-3870-4d77-9f91-634b6643016b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/147/working/report.html" "/galaxy/server/database/objects/4/b/d/dataset_4bd8fd49-3870-4d77-9f91-634b6643016b.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:24,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:45:24,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:45:24,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:45:24,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:24,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:24,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:32,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-b5qb4 with k8s id: gxy-b5qb4 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:45:32,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 147: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:45:39,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 147 finished
galaxy.model.store.discover DEBUG 2025-03-02 06:45:39,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (147) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/147/working/report_data/multiqc_general_stats.txt] with element identifier [general_stats] for output [stats] (2.548 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:39,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (147) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/147/working/report_data/multiqc_sources.txt] with element identifier [sources] for output [stats] (0.650 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:45:39,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (147) Add dynamic collection datasets to history for output [stats] (21.551 ms)
galaxy.model.metadata DEBUG 2025-03-02 06:45:39,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 168
galaxy.jobs INFO 2025-03-02 06:45:39,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 147 in /galaxy/server/database/jobs_directory/000/147
galaxy.jobs DEBUG 2025-03-02 06:45:39,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 147 executed (136.892 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:39,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:45:42,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 148, 149, 151, 150
tpv.core.entities DEBUG 2025-03-02 06:45:42,599 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:42,599 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:42,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:42,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:42,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Working directory for job is: /galaxy/server/database/jobs_directory/000/148
galaxy.jobs.runners DEBUG 2025-03-02 06:45:42,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [148] queued (33.610 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:42,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 148
tpv.core.entities DEBUG 2025-03-02 06:45:42,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:42,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:42,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:42,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:42,701 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Working directory for job is: /galaxy/server/database/jobs_directory/000/149
galaxy.jobs.runners DEBUG 2025-03-02 06:45:42,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [149] queued (49.372 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:42,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 149
tpv.core.entities DEBUG 2025-03-02 06:45:42,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:42,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:42,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:42,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:42,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [148] prepared (133.315 ms)
galaxy.jobs DEBUG 2025-03-02 06:45:42,794 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Working directory for job is: /galaxy/server/database/jobs_directory/000/150
galaxy.jobs.runners DEBUG 2025-03-02 06:45:42,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [150] queued (69.099 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:42,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 150
galaxy.jobs.command_factory INFO 2025-03-02 06:45:42,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/148/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/148/registry.xml' '/galaxy/server/database/jobs_directory/000/148/upload_params.json' '171:/galaxy/server/database/objects/2/5/a/dataset_25a485ca-a120-4af5-9904-bddad3fc93f9_files:/galaxy/server/database/objects/2/5/a/dataset_25a485ca-a120-4af5-9904-bddad3fc93f9.dat']
tpv.core.entities DEBUG 2025-03-02 06:45:42,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:42,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:42,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Dispatching to k8s runner
galaxy.jobs.runners DEBUG 2025-03-02 06:45:42,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (148) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/148/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/148/galaxy_148.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:45:42,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Persisting job destination (destination id: k8s)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:45:42,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [149] prepared (157.356 ms)
galaxy.jobs DEBUG 2025-03-02 06:45:42,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Working directory for job is: /galaxy/server/database/jobs_directory/000/151
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:45:42,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [151] queued (77.762 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:42,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 151
galaxy.jobs.command_factory INFO 2025-03-02 06:45:42,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/149/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/149/registry.xml' '/galaxy/server/database/jobs_directory/000/149/upload_params.json' '172:/galaxy/server/database/objects/c/c/e/dataset_cce7646d-6078-498a-8cdc-07de944ea30f_files:/galaxy/server/database/objects/c/c/e/dataset_cce7646d-6078-498a-8cdc-07de944ea30f.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:42,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (149) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/149/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/149/galaxy_149.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:45:42,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [150] prepared (141.054 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:42,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:45:43,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/150/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/150/registry.xml' '/galaxy/server/database/jobs_directory/000/150/upload_params.json' '173:/galaxy/server/database/objects/b/0/0/dataset_b001d3b5-f20f-4ea5-b5a8-911b56cc444a_files:/galaxy/server/database/objects/b/0/0/dataset_b001d3b5-f20f-4ea5-b5a8-911b56cc444a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:43,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (150) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/150/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/150/galaxy_150.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:45:43,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [151] prepared (94.228 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:43,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:43,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:45:43,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/151/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/151/registry.xml' '/galaxy/server/database/jobs_directory/000/151/upload_params.json' '174:/galaxy/server/database/objects/2/1/9/dataset_21982ffc-c35e-4c9b-b1c0-78c42eb039fc_files:/galaxy/server/database/objects/2/1/9/dataset_21982ffc-c35e-4c9b-b1c0-78c42eb039fc.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:43,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (151) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/151/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/151/galaxy_151.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:43,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:43,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:43,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 06:45:43,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 153, 157, 156, 154, 155, 152
tpv.core.entities DEBUG 2025-03-02 06:45:43,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:43,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:43,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:43,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:43,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Working directory for job is: /galaxy/server/database/jobs_directory/000/152
galaxy.jobs.runners DEBUG 2025-03-02 06:45:43,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [152] queued (28.867 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:43,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:43,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 152
tpv.core.entities DEBUG 2025-03-02 06:45:44,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:44,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:44,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:44,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:44,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Working directory for job is: /galaxy/server/database/jobs_directory/000/153
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [153] queued (51.852 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:44,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 153
tpv.core.entities DEBUG 2025-03-02 06:45:44,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:44,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:44,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:44,108 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:44,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [152] prepared (126.413 ms)
galaxy.jobs DEBUG 2025-03-02 06:45:44,151 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Working directory for job is: /galaxy/server/database/jobs_directory/000/154
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,159 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [154] queued (67.392 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:44,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 154
galaxy.jobs.command_factory INFO 2025-03-02 06:45:44,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/152/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/152/registry.xml' '/galaxy/server/database/jobs_directory/000/152/upload_params.json' '175:/galaxy/server/database/objects/c/2/9/dataset_c29453b4-b0f7-407a-8758-bb6a28864498_files:/galaxy/server/database/objects/c/2/9/dataset_c29453b4-b0f7-407a-8758-bb6a28864498.dat']
tpv.core.entities DEBUG 2025-03-02 06:45:44,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:44,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:44,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Dispatching to k8s runner
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (152) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/152/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/152/galaxy_152.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:45:44,260 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:44,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [153] prepared (176.596 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:45:44,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/153/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/153/registry.xml' '/galaxy/server/database/jobs_directory/000/153/upload_params.json' '176:/galaxy/server/database/objects/c/8/2/dataset_c82340c9-3971-4a00-9aac-267ad34aa8fb_files:/galaxy/server/database/objects/c/8/2/dataset_c82340c9-3971-4a00-9aac-267ad34aa8fb.dat']
galaxy.jobs DEBUG 2025-03-02 06:45:44,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Working directory for job is: /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [155] queued (126.365 ms)
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/153/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/153/galaxy_153.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler INFO 2025-03-02 06:45:44,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 155
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
tpv.core.entities DEBUG 2025-03-02 06:45:44,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:44,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Mapped job to destination id: k8s
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:45:44,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Dispatching to k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:45:44,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:44,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [154] prepared (289.599 ms)
galaxy.jobs DEBUG 2025-03-02 06:45:44,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Working directory for job is: /galaxy/server/database/jobs_directory/000/156
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [156] queued (101.074 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:44,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Job dispatched
galaxy.jobs.command_factory INFO 2025-03-02 06:45:44,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/154/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/154/registry.xml' '/galaxy/server/database/jobs_directory/000/154/upload_params.json' '177:/galaxy/server/database/objects/d/6/0/dataset_d60cf646-a9cf-4183-bb2a-8b6c34250cca_files:/galaxy/server/database/objects/d/6/0/dataset_d60cf646-a9cf-4183-bb2a-8b6c34250cca.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 156
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
tpv.core.entities DEBUG 2025-03-02 06:45:44,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:44,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:45:44,592 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Dispatching to k8s runner
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (154) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/154/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/154/galaxy_154.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:45:44,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Persisting job destination (destination id: k8s)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:45:44,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [155] prepared (255.542 ms)
galaxy.jobs DEBUG 2025-03-02 06:45:44,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Working directory for job is: /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [157] queued (115.484 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler INFO 2025-03-02 06:45:44,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 157
galaxy.jobs.command_factory INFO 2025-03-02 06:45:44,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/155/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/155/registry.xml' '/galaxy/server/database/jobs_directory/000/155/upload_params.json' '178:/galaxy/server/database/objects/c/e/4/dataset_ce4774a1-7eda-4323-ac2e-d8187808c750_files:/galaxy/server/database/objects/c/e/4/dataset_ce4774a1-7eda-4323-ac2e-d8187808c750.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (155) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/155/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/155/galaxy_155.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:45:44,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [156] prepared (197.298 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:45:44,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/156/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/156/registry.xml' '/galaxy/server/database/jobs_directory/000/156/upload_params.json' '179:/galaxy/server/database/objects/9/a/6/dataset_9a62b1e4-12b9-4ed3-a059-055a38d6a532_files:/galaxy/server/database/objects/9/a/6/dataset_9a62b1e4-12b9-4ed3-a059-055a38d6a532.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (156) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/156/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/156/galaxy_156.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:45:44,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [157] prepared (129.215 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:45:44,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/157/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/157/registry.xml' '/galaxy/server/database/jobs_directory/000/157/upload_params.json' '180:/galaxy/server/database/objects/6/d/a/dataset_6dabc76e-a7f7-4052-8064-3f4912a519c4_files:/galaxy/server/database/objects/6/d/a/dataset_6dabc76e-a7f7-4052-8064-3f4912a519c4.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:44,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (157) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/157/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/157/galaxy_157.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:44,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:45:45,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 158
tpv.core.entities DEBUG 2025-03-02 06:45:45,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:45:45,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Mapped job to destination id: k8s
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:45,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 06:45:45,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:45:45,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:45:45,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Working directory for job is: /galaxy/server/database/jobs_directory/000/158
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:45,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:45:45,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [158] queued (62.847 ms)
galaxy.jobs.handler INFO 2025-03-02 06:45:45,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:45,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 158
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:45,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:46,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs DEBUG 2025-03-02 06:45:46,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [158] prepared (181.338 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:45:46,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/158/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/158/registry.xml' '/galaxy/server/database/jobs_directory/000/158/upload_params.json' '181:/galaxy/server/database/objects/f/0/a/dataset_f0aa1cc8-74c7-4fcb-9c13-a8458f047b2b_files:/galaxy/server/database/objects/f/0/a/dataset_f0aa1cc8-74c7-4fcb-9c13-a8458f047b2b.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:45:46,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (158) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/158/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/158/galaxy_158.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:46,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:46,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:46,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:46,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:47,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:54,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j76bv with k8s id: gxy-j76bv succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:54,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w62nt with k8s id: gxy-w62nt succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:54,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-q8kwv with k8s id: gxy-q8kwv succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:45:54,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 148: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:45:54,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 149: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:45:54,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 150: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:55,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7f2cb with k8s id: gxy-7f2cb succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:55,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9bmgd failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:55,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:55,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-9bmgd.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:55,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:45:55,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-9bmgd

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-9bmgd": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:56,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gn6bj with k8s id: gxy-gn6bj succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:45:56,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 151: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:56,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mhgm6 with k8s id: gxy-mhgm6 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:56,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f2kng with k8s id: gxy-f2kng succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:57,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5bjqm with k8s id: gxy-5bjqm succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:57,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fxpgw with k8s id: gxy-fxpgw succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:45:57,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7br62 with k8s id: gxy-7br62 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:46:10,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 148 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:10,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 171
galaxy.jobs INFO 2025-03-02 06:46:10,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 148 in /galaxy/server/database/jobs_directory/000/148
galaxy.jobs DEBUG 2025-03-02 06:46:10,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 148 executed (266.536 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:10,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:46:10,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 149 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:11,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 172
galaxy.jobs INFO 2025-03-02 06:46:11,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 149 in /galaxy/server/database/jobs_directory/000/149
galaxy.jobs DEBUG 2025-03-02 06:46:11,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 149 executed (206.542 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:11,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:46:11,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 150 finished
galaxy.jobs.runners DEBUG 2025-03-02 06:46:11,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 154: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.model.metadata DEBUG 2025-03-02 06:46:11,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 173
galaxy.jobs INFO 2025-03-02 06:46:11,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 150 in /galaxy/server/database/jobs_directory/000/150
galaxy.jobs DEBUG 2025-03-02 06:46:11,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 150 executed (160.529 ms)
galaxy.jobs.runners DEBUG 2025-03-02 06:46:11,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 155: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:11,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:46:12,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 151 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:12,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 174
galaxy.jobs INFO 2025-03-02 06:46:13,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 151 in /galaxy/server/database/jobs_directory/000/151
galaxy.jobs.runners DEBUG 2025-03-02 06:46:13,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 156: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs DEBUG 2025-03-02 06:46:13,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 151 executed (1287.362 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:13,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:46:14,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 152: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:46:26,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 154 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:27,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 177
galaxy.jobs INFO 2025-03-02 06:46:27,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 154 in /galaxy/server/database/jobs_directory/000/154
galaxy.jobs DEBUG 2025-03-02 06:46:27,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 154 executed (223.339 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:27,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:46:27,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 155 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:27,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 178
galaxy.jobs INFO 2025-03-02 06:46:27,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 155 in /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-03-02 06:46:27,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 157: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs DEBUG 2025-03-02 06:46:27,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 155 executed (229.856 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:27,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:46:28,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 158: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:46:29,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 156 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:29,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 179
galaxy.jobs INFO 2025-03-02 06:46:29,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 156 in /galaxy/server/database/jobs_directory/000/156
galaxy.jobs DEBUG 2025-03-02 06:46:29,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 156 executed (206.708 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153/gxy-9bmgd) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153/gxy-9bmgd) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153/gxy-9bmgd) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153/gxy-9bmgd) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-9bmgd.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 153 (gxy-9bmgd)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-9bmgd to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:29,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:30,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153/gxy-9bmgd) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-03-02 06:46:30,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 152 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:30,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 175
galaxy.jobs INFO 2025-03-02 06:46:30,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 152 in /galaxy/server/database/jobs_directory/000/152
galaxy.jobs DEBUG 2025-03-02 06:46:30,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 152 executed (172.016 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:30,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:46:31,170 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 160, 159
tpv.core.entities DEBUG 2025-03-02 06:46:31,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:31,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:31,221 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:31,234 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:31,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Working directory for job is: /galaxy/server/database/jobs_directory/000/159
galaxy.jobs.runners DEBUG 2025-03-02 06:46:31,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [159] queued (34.853 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:31,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:31,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 159
tpv.core.entities DEBUG 2025-03-02 06:46:31,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:31,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:31,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:31,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:31,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Working directory for job is: /galaxy/server/database/jobs_directory/000/160
galaxy.jobs.runners DEBUG 2025-03-02 06:46:31,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [160] queued (80.494 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:31,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:31,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 160
galaxy.jobs DEBUG 2025-03-02 06:46:31,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [159] prepared (157.608 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:46:31,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/159/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/159/registry.xml' '/galaxy/server/database/jobs_directory/000/159/upload_params.json' '182:/galaxy/server/database/objects/7/c/5/dataset_7c5e0ab0-a553-4c41-8f75-dcdf197c8e8a_files:/galaxy/server/database/objects/7/c/5/dataset_7c5e0ab0-a553-4c41-8f75-dcdf197c8e8a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:31,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (159) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/159/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/159/galaxy_159.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:46:31,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [160] prepared (150.550 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:31,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:31,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:46:31,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/160/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/160/registry.xml' '/galaxy/server/database/jobs_directory/000/160/upload_params.json' '183:/galaxy/server/database/objects/0/e/5/dataset_0e5055d5-c444-415b-b3f2-ae47995afe39_files:/galaxy/server/database/objects/0/e/5/dataset_0e5055d5-c444-415b-b3f2-ae47995afe39.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:31,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (160) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/160/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/160/galaxy_160.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:31,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:31,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 06:46:32,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 163, 162, 164, 165, 161
tpv.core.entities DEBUG 2025-03-02 06:46:32,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:32,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:32,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:32,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:32,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Working directory for job is: /galaxy/server/database/jobs_directory/000/161
galaxy.jobs.runners DEBUG 2025-03-02 06:46:32,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [161] queued (42.364 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:32,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 161
tpv.core.entities DEBUG 2025-03-02 06:46:32,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:32,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:32,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:32,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:32,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Working directory for job is: /galaxy/server/database/jobs_directory/000/162
galaxy.jobs.runners DEBUG 2025-03-02 06:46:32,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [162] queued (71.536 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:32,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 162
tpv.core.entities DEBUG 2025-03-02 06:46:32,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:32,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:32,611 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:32,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:32,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [161] prepared (182.689 ms)
galaxy.jobs DEBUG 2025-03-02 06:46:32,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Working directory for job is: /galaxy/server/database/jobs_directory/000/163
galaxy.jobs.runners DEBUG 2025-03-02 06:46:32,705 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [163] queued (93.220 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:32,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Job dispatched
galaxy.jobs.command_factory INFO 2025-03-02 06:46:32,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/161/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/161/registry.xml' '/galaxy/server/database/jobs_directory/000/161/upload_params.json' '184:/galaxy/server/database/objects/b/d/3/dataset_bd380d1d-b8b4-43cb-a1d7-4d7d8b737c51_files:/galaxy/server/database/objects/b/d/3/dataset_bd380d1d-b8b4-43cb-a1d7-4d7d8b737c51.dat']
tpv.core.entities DEBUG 2025-03-02 06:46:32,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:32,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:32,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Dispatching to k8s runner
galaxy.jobs.runners DEBUG 2025-03-02 06:46:32,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (161) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/161/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/161/galaxy_161.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:46:32,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:32,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [162] prepared (151.545 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:32,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Working directory for job is: /galaxy/server/database/jobs_directory/000/164
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:46:32,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/162/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/162/registry.xml' '/galaxy/server/database/jobs_directory/000/162/upload_params.json' '185:/galaxy/server/database/objects/9/4/0/dataset_9406df60-6928-4d10-bd52-9292d1a86571_files:/galaxy/server/database/objects/9/4/0/dataset_9406df60-6928-4d10-bd52-9292d1a86571.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:32,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [164] queued (99.934 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:32,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Job dispatched
galaxy.jobs.runners DEBUG 2025-03-02 06:46:32,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (162) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/162/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/162/galaxy_162.ec; sh -c "exit $return_code"
tpv.core.entities DEBUG 2025-03-02 06:46:32,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:32,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:32,855 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Dispatching to k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:32,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Persisting job destination (destination id: k8s)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:32,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 163
galaxy.jobs DEBUG 2025-03-02 06:46:32,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Working directory for job is: /galaxy/server/database/jobs_directory/000/165
galaxy.jobs.runners DEBUG 2025-03-02 06:46:32,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [165] queued (93.251 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:32,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 164
galaxy.jobs DEBUG 2025-03-02 06:46:33,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [163] prepared (111.404 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:46:33,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/163/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/163/registry.xml' '/galaxy/server/database/jobs_directory/000/163/upload_params.json' '186:/galaxy/server/database/objects/7/f/1/dataset_7f10e282-3469-4edb-9f30-9cf8aedfae7a_files:/galaxy/server/database/objects/7/f/1/dataset_7f10e282-3469-4edb-9f30-9cf8aedfae7a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:33,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (163) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/163/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/163/galaxy_163.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:33,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [164] prepared (115.990 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:46:33,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/164/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/164/registry.xml' '/galaxy/server/database/jobs_directory/000/164/upload_params.json' '187:/galaxy/server/database/objects/9/3/6/dataset_9365fa71-be2e-4405-8f98-d15c4460b81b_files:/galaxy/server/database/objects/9/3/6/dataset_9365fa71-be2e-4405-8f98-d15c4460b81b.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:33,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (164) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/164/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/164/galaxy_164.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 165
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:33,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [165] prepared (107.681 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:46:33,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/165/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/165/registry.xml' '/galaxy/server/database/jobs_directory/000/165/upload_params.json' '188:/galaxy/server/database/objects/c/4/9/dataset_c49784cc-b79a-437e-854c-74c66fe09c8d_files:/galaxy/server/database/objects/c/4/9/dataset_c49784cc-b79a-437e-854c-74c66fe09c8d.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:33,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (165) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/165/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/165/galaxy_165.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:33,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 06:46:33,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 167, 172, 170, 169, 166, 168, 171
tpv.core.entities DEBUG 2025-03-02 06:46:33,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:33,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:34,010 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:34,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:34,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Working directory for job is: /galaxy/server/database/jobs_directory/000/166
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [166] queued (40.900 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:34,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 166
tpv.core.entities DEBUG 2025-03-02 06:46:34,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:34,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:34,074 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:34,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:34,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Working directory for job is: /galaxy/server/database/jobs_directory/000/167
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,153 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [167] queued (79.318 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:34,156 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 167
tpv.core.entities DEBUG 2025-03-02 06:46:34,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:34,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:34,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:34,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:34,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [166] prepared (181.845 ms)
galaxy.jobs DEBUG 2025-03-02 06:46:34,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Working directory for job is: /galaxy/server/database/jobs_directory/000/168
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,316 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [168] queued (105.216 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:34,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Job dispatched
galaxy.jobs.command_factory INFO 2025-03-02 06:46:34,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/166/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/166/registry.xml' '/galaxy/server/database/jobs_directory/000/166/upload_params.json' '189:/galaxy/server/database/objects/e/2/5/dataset_e25a8b50-e373-4e0f-accb-4f7db52a72c2_files:/galaxy/server/database/objects/e/2/5/dataset_e25a8b50-e373-4e0f-accb-4f7db52a72c2.dat']
tpv.core.entities DEBUG 2025-03-02 06:46:34,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:34,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:34,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Dispatching to k8s runner
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (166) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/166/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/166/galaxy_166.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:46:34,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [167] prepared (152.289 ms)
galaxy.jobs DEBUG 2025-03-02 06:46:34,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Persisting job destination (destination id: k8s)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:34,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Working directory for job is: /galaxy/server/database/jobs_directory/000/169
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:46:34,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/167/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/167/registry.xml' '/galaxy/server/database/jobs_directory/000/167/upload_params.json' '190:/galaxy/server/database/objects/e/2/4/dataset_e24a5067-0a55-4e82-be2c-0afb685e3b49_files:/galaxy/server/database/objects/e/2/4/dataset_e24a5067-0a55-4e82-be2c-0afb685e3b49.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [169] queued (103.127 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:34,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Job dispatched
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (167) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/167/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/167/galaxy_167.ec; sh -c "exit $return_code"
tpv.core.entities DEBUG 2025-03-02 06:46:34,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:34,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:34,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Dispatching to k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 168
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:34,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Persisting job destination (destination id: k8s)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:34,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Working directory for job is: /galaxy/server/database/jobs_directory/000/170
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [170] queued (61.294 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:34,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:46:34,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:34,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:34,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:46:34,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:46:34,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Working directory for job is: /galaxy/server/database/jobs_directory/000/171
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [171] queued (78.787 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:34,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Job dispatched
galaxy.jobs DEBUG 2025-03-02 06:46:34,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [168] prepared (191.627 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 169
tpv.core.entities DEBUG 2025-03-02 06:46:34,742 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:46:34,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:46:34,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Dispatching to k8s runner
galaxy.jobs.command_factory INFO 2025-03-02 06:46:34,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/168/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/168/registry.xml' '/galaxy/server/database/jobs_directory/000/168/upload_params.json' '191:/galaxy/server/database/objects/6/2/2/dataset_6228e96a-c4b7-43ac-ac66-00b3b3de43bc_files:/galaxy/server/database/objects/6/2/2/dataset_6228e96a-c4b7-43ac-ac66-00b3b3de43bc.dat']
galaxy.jobs DEBUG 2025-03-02 06:46:34,766 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Persisting job destination (destination id: k8s)
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (168) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/168/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/168/galaxy_168.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:46:34,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Working directory for job is: /galaxy/server/database/jobs_directory/000/172
galaxy.jobs.runners DEBUG 2025-03-02 06:46:34,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [172] queued (105.677 ms)
galaxy.jobs.handler INFO 2025-03-02 06:46:34,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:46:34,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [169] prepared (186.404 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:34,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:46:34,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/169/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/169/registry.xml' '/galaxy/server/database/jobs_directory/000/169/upload_params.json' '192:/galaxy/server/database/objects/7/a/b/dataset_7ab2af7b-1008-4577-b24b-d3a20bff4ff8_files:/galaxy/server/database/objects/7/a/b/dataset_7ab2af7b-1008-4577-b24b-d3a20bff4ff8.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:35,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (169) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/169/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/169/galaxy_169.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 170
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 171
galaxy.jobs DEBUG 2025-03-02 06:46:35,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [170] prepared (179.898 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:46:35,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/170/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/170/registry.xml' '/galaxy/server/database/jobs_directory/000/170/upload_params.json' '193:/galaxy/server/database/objects/5/f/d/dataset_5fd841ee-309d-4c22-b8a3-e7c474620a71_files:/galaxy/server/database/objects/5/f/d/dataset_5fd841ee-309d-4c22-b8a3-e7c474620a71.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:35,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (170) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/170/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/170/galaxy_170.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:46:35,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [171] prepared (126.591 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:46:35,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/171/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/171/registry.xml' '/galaxy/server/database/jobs_directory/000/171/upload_params.json' '194:/galaxy/server/database/objects/7/9/9/dataset_79971e3c-5e0d-4163-a018-0d2fcbaa4f95_files:/galaxy/server/database/objects/7/9/9/dataset_79971e3c-5e0d-4163-a018-0d2fcbaa4f95.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:35,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (171) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/171/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/171/galaxy_171.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 172
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs DEBUG 2025-03-02 06:46:35,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [172] prepared (138.034 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:46:35,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/172/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/172/registry.xml' '/galaxy/server/database/jobs_directory/000/172/upload_params.json' '195:/galaxy/server/database/objects/c/b/0/dataset_cb07e7d8-5bcf-42df-924e-4bba290a3246_files:/galaxy/server/database/objects/c/b/0/dataset_cb07e7d8-5bcf-42df-924e-4bba290a3246.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:46:35,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (172) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/172/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/172/galaxy_172.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:35,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:37,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:37,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:37,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:37,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:37,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:46:39,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 157 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:39,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 180
galaxy.jobs INFO 2025-03-02 06:46:39,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 157 in /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners DEBUG 2025-03-02 06:46:39,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 158 finished
galaxy.jobs DEBUG 2025-03-02 06:46:39,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 157 executed (120.114 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:39,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.model.metadata DEBUG 2025-03-02 06:46:39,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 181
galaxy.jobs INFO 2025-03-02 06:46:39,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 158 in /galaxy/server/database/jobs_directory/000/158
galaxy.jobs DEBUG 2025-03-02 06:46:39,804 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 158 executed (134.027 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:39,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:43,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7dr9p with k8s id: gxy-7dr9p succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:43,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bhvjp with k8s id: gxy-bhvjp succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:46:43,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 160: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:46:43,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 159: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:44,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2x9rm with k8s id: gxy-2x9rm succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:46:45,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 162: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d2qhh failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-d2qhh.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:46:45,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-d2qhh

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-d2qhh": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-52c2g with k8s id: gxy-52c2g succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (164/gxy-d2qhh) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (164/gxy-d2qhh) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (164/gxy-d2qhh) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (164/gxy-d2qhh) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-d2qhh.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 164 (gxy-d2qhh)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-d2qhh to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:45,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (164/gxy-d2qhh) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-03-02 06:46:46,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 165: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:46,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7lbz7 with k8s id: gxy-7lbz7 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:46,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gdpr7 with k8s id: gxy-gdpr7 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:46,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mwgvn with k8s id: gxy-mwgvn succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:47,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-57l9p failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:47,011 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:47,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-57l9p.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:47,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:46:47,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-57l9p

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-57l9p": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:47,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-smp2m with k8s id: gxy-smp2m succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:47,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ngdlr with k8s id: gxy-ngdlr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:47,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-49dmr with k8s id: gxy-49dmr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:48,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9znxf with k8s id: gxy-9znxf succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:48,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6mkhv with k8s id: gxy-6mkhv succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:46:58,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 160 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:58,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 183
galaxy.jobs INFO 2025-03-02 06:46:59,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 160 in /galaxy/server/database/jobs_directory/000/160
galaxy.jobs.runners DEBUG 2025-03-02 06:46:59,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 159 finished
galaxy.model.metadata DEBUG 2025-03-02 06:46:59,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 182
galaxy.jobs DEBUG 2025-03-02 06:46:59,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 160 executed (232.903 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:59,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-02 06:46:59,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 159 in /galaxy/server/database/jobs_directory/000/159
galaxy.jobs DEBUG 2025-03-02 06:46:59,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 159 executed (146.098 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:46:59,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:46:59,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 161: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:46:59,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 163: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:47:01,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 162 finished
galaxy.model.metadata DEBUG 2025-03-02 06:47:01,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 185
galaxy.jobs INFO 2025-03-02 06:47:01,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 162 in /galaxy/server/database/jobs_directory/000/162
galaxy.jobs DEBUG 2025-03-02 06:47:01,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 162 executed (193.233 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:01,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:01,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 166: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:47:02,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 165 finished
galaxy.model.metadata DEBUG 2025-03-02 06:47:02,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 188
galaxy.jobs INFO 2025-03-02 06:47:02,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 165 in /galaxy/server/database/jobs_directory/000/165
galaxy.jobs DEBUG 2025-03-02 06:47:02,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 165 executed (200.534 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:02,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:03,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 168: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:47:15,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 161 finished
galaxy.model.metadata DEBUG 2025-03-02 06:47:15,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 184
galaxy.jobs INFO 2025-03-02 06:47:15,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 161 in /galaxy/server/database/jobs_directory/000/161
galaxy.jobs DEBUG 2025-03-02 06:47:15,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 161 executed (208.066 ms)
galaxy.jobs.runners DEBUG 2025-03-02 06:47:15,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 163 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:15,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.model.metadata DEBUG 2025-03-02 06:47:15,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 186
galaxy.jobs INFO 2025-03-02 06:47:16,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 163 in /galaxy/server/database/jobs_directory/000/163
galaxy.jobs DEBUG 2025-03-02 06:47:16,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 163 executed (320.262 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:16,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:16,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 169: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:47:16,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 170: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-02 06:47:17,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 175, 174, 177, 173, 176
tpv.core.entities DEBUG 2025-03-02 06:47:17,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:17,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:17,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:17,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:17,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Working directory for job is: /galaxy/server/database/jobs_directory/000/173
galaxy.jobs.runners DEBUG 2025-03-02 06:47:17,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [173] queued (91.851 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:17,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:17,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:17,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:17,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:17,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:17,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Working directory for job is: /galaxy/server/database/jobs_directory/000/174
galaxy.jobs.runners DEBUG 2025-03-02 06:47:17,824 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [174] queued (88.706 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:17,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:17,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:17,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:17,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:17,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:17,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Working directory for job is: /galaxy/server/database/jobs_directory/000/175
galaxy.jobs.runners DEBUG 2025-03-02 06:47:17,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [175] queued (93.951 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:17,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:18,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:18,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:18,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:18,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Persisting job destination (destination id: k8s)
galaxy.jobs.runners DEBUG 2025-03-02 06:47:18,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 166 finished
galaxy.jobs DEBUG 2025-03-02 06:47:18,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Working directory for job is: /galaxy/server/database/jobs_directory/000/176
galaxy.jobs.runners DEBUG 2025-03-02 06:47:18,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [176] queued (94.446 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:18,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:18,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:18,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:18,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:18,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Persisting job destination (destination id: k8s)
galaxy.model.metadata DEBUG 2025-03-02 06:47:18,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 189
galaxy.jobs DEBUG 2025-03-02 06:47:18,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Working directory for job is: /galaxy/server/database/jobs_directory/000/177
galaxy.jobs.runners DEBUG 2025-03-02 06:47:18,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [177] queued (93.721 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:18,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Job dispatched
galaxy.jobs INFO 2025-03-02 06:47:18,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 166 in /galaxy/server/database/jobs_directory/000/166
galaxy.jobs DEBUG 2025-03-02 06:47:18,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 166 executed (229.253 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:18,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:18,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 171: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:47:19,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 168 finished
galaxy.model.metadata DEBUG 2025-03-02 06:47:19,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 191
galaxy.jobs.handler DEBUG 2025-03-02 06:47:19,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 182, 181, 178, 180, 179
tpv.core.entities DEBUG 2025-03-02 06:47:19,324 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:19,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:19,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Dispatching to k8s runner
galaxy.jobs INFO 2025-03-02 06:47:19,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 168 in /galaxy/server/database/jobs_directory/000/168
galaxy.jobs DEBUG 2025-03-02 06:47:19,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:19,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Working directory for job is: /galaxy/server/database/jobs_directory/000/178
galaxy.jobs.runners DEBUG 2025-03-02 06:47:19,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [178] queued (97.236 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:19,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:19,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:19,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:19,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:19,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:19,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Working directory for job is: /galaxy/server/database/jobs_directory/000/179
galaxy.jobs DEBUG 2025-03-02 06:47:19,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 168 executed (311.097 ms)
galaxy.jobs.runners DEBUG 2025-03-02 06:47:19,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [179] queued (82.760 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:19,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:19,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
tpv.core.entities DEBUG 2025-03-02 06:47:19,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:19,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:19,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:19,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:19,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Working directory for job is: /galaxy/server/database/jobs_directory/000/180
galaxy.jobs.runners DEBUG 2025-03-02 06:47:19,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [180] queued (262.600 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:19,821 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:19,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:19,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:19,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:19,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:19,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Working directory for job is: /galaxy/server/database/jobs_directory/000/181
galaxy.jobs.runners DEBUG 2025-03-02 06:47:19,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [181] queued (95.808 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:19,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:19,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:19,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:19,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:20,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:20,042 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Working directory for job is: /galaxy/server/database/jobs_directory/000/182
galaxy.jobs.runners DEBUG 2025-03-02 06:47:20,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [182] queued (97.756 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:20,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Job dispatched
galaxy.jobs.runners DEBUG 2025-03-02 06:47:20,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 172: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,108 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 191, 184, 189, 183, 186, 185, 188, 187, 190
tpv.core.entities DEBUG 2025-03-02 06:47:21,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:21,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:21,219 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:21,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Working directory for job is: /galaxy/server/database/jobs_directory/000/183
galaxy.jobs.runners DEBUG 2025-03-02 06:47:21,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [183] queued (38.604 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:21,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:21,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:21,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:21,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:21,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Working directory for job is: /galaxy/server/database/jobs_directory/000/184
galaxy.jobs.runners DEBUG 2025-03-02 06:47:21,417 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [184] queued (95.089 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:21,419 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:21,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:21,429 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:21,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:21,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Working directory for job is: /galaxy/server/database/jobs_directory/000/185
galaxy.jobs.runners DEBUG 2025-03-02 06:47:21,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [185] queued (95.363 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:21,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:21,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:21,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:21,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:21,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Working directory for job is: /galaxy/server/database/jobs_directory/000/186
galaxy.jobs.runners DEBUG 2025-03-02 06:47:21,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [186] queued (92.931 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:21,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:21,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:21,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:21,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:21,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Working directory for job is: /galaxy/server/database/jobs_directory/000/187
galaxy.jobs.runners DEBUG 2025-03-02 06:47:21,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [187] queued (31.623 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:21,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:21,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:21,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,821 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:21,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:21,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Working directory for job is: /galaxy/server/database/jobs_directory/000/188
galaxy.jobs.runners DEBUG 2025-03-02 06:47:21,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [188] queued (95.510 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:21,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:21,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:21,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:21,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:21,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:22,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Working directory for job is: /galaxy/server/database/jobs_directory/000/189
galaxy.jobs.runners DEBUG 2025-03-02 06:47:22,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [189] queued (93.860 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:22,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:22,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:22,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:22,042 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:22,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:22,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Working directory for job is: /galaxy/server/database/jobs_directory/000/190
galaxy.jobs.runners DEBUG 2025-03-02 06:47:22,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [190] queued (93.463 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:22,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:22,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:22,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:22,209 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:22,220 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:22,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Working directory for job is: /galaxy/server/database/jobs_directory/000/191
galaxy.jobs.runners DEBUG 2025-03-02 06:47:22,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [191] queued (34.622 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:22,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Job dispatched
galaxy.jobs.handler DEBUG 2025-03-02 06:47:23,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 197, 196, 195, 199, 193, 198, 194, 200, 192
tpv.core.entities DEBUG 2025-03-02 06:47:23,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:23,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:23,332 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:23,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:23,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Working directory for job is: /galaxy/server/database/jobs_directory/000/192
galaxy.jobs.runners DEBUG 2025-03-02 06:47:23,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [192] queued (88.951 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:23,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:23,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:23,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:23,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:23,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:23,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Working directory for job is: /galaxy/server/database/jobs_directory/000/193
galaxy.jobs.runners DEBUG 2025-03-02 06:47:23,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [193] queued (86.323 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:23,528 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:23,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:23,541 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:23,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:23,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:23,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Working directory for job is: /galaxy/server/database/jobs_directory/000/194
galaxy.jobs.runners DEBUG 2025-03-02 06:47:23,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [194] queued (31.403 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:23,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:23,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:23,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:23,711 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:23,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:23,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Working directory for job is: /galaxy/server/database/jobs_directory/000/195
galaxy.jobs.runners DEBUG 2025-03-02 06:47:23,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [195] queued (33.961 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:23,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:23,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:23,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:23,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:23,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:23,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Working directory for job is: /galaxy/server/database/jobs_directory/000/196
galaxy.jobs.runners DEBUG 2025-03-02 06:47:23,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [196] queued (29.457 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:23,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:23,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:23,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:23,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:23,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:23,938 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Working directory for job is: /galaxy/server/database/jobs_directory/000/197
galaxy.jobs.runners DEBUG 2025-03-02 06:47:23,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [197] queued (25.748 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:24,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:24,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:24,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:24,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:24,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:24,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Working directory for job is: /galaxy/server/database/jobs_directory/000/198
galaxy.jobs.runners DEBUG 2025-03-02 06:47:24,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [198] queued (125.879 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:24,234 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:24,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:24,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:24,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:24,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:24,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Working directory for job is: /galaxy/server/database/jobs_directory/000/199
galaxy.jobs.runners DEBUG 2025-03-02 06:47:24,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [199] queued (95.920 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:24,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:24,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:24,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:24,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:24,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:24,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Working directory for job is: /galaxy/server/database/jobs_directory/000/200
galaxy.jobs.runners DEBUG 2025-03-02 06:47:24,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [200] queued (86.466 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:24,514 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Job dispatched
galaxy.jobs.handler DEBUG 2025-03-02 06:47:25,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 202, 201
tpv.core.entities DEBUG 2025-03-02 06:47:25,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:25,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:25,608 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:25,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:25,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Working directory for job is: /galaxy/server/database/jobs_directory/000/201
galaxy.jobs.runners DEBUG 2025-03-02 06:47:25,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [201] queued (34.580 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:25,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:47:25,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:47:25,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:47:25,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:47:25,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:47:25,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Working directory for job is: /galaxy/server/database/jobs_directory/000/202
galaxy.jobs.runners DEBUG 2025-03-02 06:47:25,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [202] queued (95.473 ms)
galaxy.jobs.handler INFO 2025-03-02 06:47:25,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Job dispatched
galaxy.jobs.runners DEBUG 2025-03-02 06:47:32,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 169 finished
galaxy.model.metadata DEBUG 2025-03-02 06:47:32,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 192
galaxy.jobs INFO 2025-03-02 06:47:32,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 169 in /galaxy/server/database/jobs_directory/000/169
galaxy.jobs DEBUG 2025-03-02 06:47:32,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 169 executed (132.427 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (167/gxy-57l9p) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (167/gxy-57l9p) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (167/gxy-57l9p) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (167/gxy-57l9p) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-57l9p.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 167 (gxy-57l9p)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-57l9p to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:33,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 170 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (167/gxy-57l9p) Terminated at user's request
galaxy.model.metadata DEBUG 2025-03-02 06:47:33,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 193
galaxy.jobs INFO 2025-03-02 06:47:33,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 170 in /galaxy/server/database/jobs_directory/000/170
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 173
galaxy.jobs DEBUG 2025-03-02 06:47:33,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 170 executed (219.514 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:33,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [173] prepared (119.238 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 174
galaxy.jobs.command_factory INFO 2025-03-02 06:47:33,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/173/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/173/registry.xml' '/galaxy/server/database/jobs_directory/000/173/upload_params.json' '196:/galaxy/server/database/objects/5/d/6/dataset_5d62876b-66f5-481a-9002-eace1a6fb9ed_files:/galaxy/server/database/objects/5/d/6/dataset_5d62876b-66f5-481a-9002-eace1a6fb9ed.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:33,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (173) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/173/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/173/galaxy_173.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 175
galaxy.jobs DEBUG 2025-03-02 06:47:33,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [174] prepared (137.819 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:33,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/174/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/174/registry.xml' '/galaxy/server/database/jobs_directory/000/174/upload_params.json' '197:/galaxy/server/database/objects/8/6/3/dataset_863836c7-02ba-4310-991e-67ad0ca13fdc_files:/galaxy/server/database/objects/8/6/3/dataset_863836c7-02ba-4310-991e-67ad0ca13fdc.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:33,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (174) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/174/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/174/galaxy_174.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:33,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [175] prepared (115.844 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:33,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/175/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/175/registry.xml' '/galaxy/server/database/jobs_directory/000/175/upload_params.json' '198:/galaxy/server/database/objects/e/5/f/dataset_e5f5c750-571d-4b8a-bdb7-1dfaef70c3ab_files:/galaxy/server/database/objects/e/5/f/dataset_e5f5c750-571d-4b8a-bdb7-1dfaef70c3ab.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:33,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (175) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/175/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/175/galaxy_175.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 176
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:33,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 177
galaxy.jobs DEBUG 2025-03-02 06:47:34,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [176] prepared (107.970 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:34,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/176/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/176/registry.xml' '/galaxy/server/database/jobs_directory/000/176/upload_params.json' '199:/galaxy/server/database/objects/1/2/c/dataset_12c86a86-1ae9-4ad0-aad8-f8f7a1b41c46_files:/galaxy/server/database/objects/1/2/c/dataset_12c86a86-1ae9-4ad0-aad8-f8f7a1b41c46.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:34,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (176) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/176/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/176/galaxy_176.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:34,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [177] prepared (142.124 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:34,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/177/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/177/registry.xml' '/galaxy/server/database/jobs_directory/000/177/upload_params.json' '200:/galaxy/server/database/objects/6/d/0/dataset_6d08fbaf-e8be-419d-af03-b952151384b7_files:/galaxy/server/database/objects/6/d/0/dataset_6d08fbaf-e8be-419d-af03-b952151384b7.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 178
galaxy.jobs.runners DEBUG 2025-03-02 06:47:34,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (177) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/177/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/177/galaxy_177.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:34,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [178] prepared (129.756 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 179
galaxy.jobs.command_factory INFO 2025-03-02 06:47:34,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/178/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/178/registry.xml' '/galaxy/server/database/jobs_directory/000/178/upload_params.json' '201:/galaxy/server/database/objects/4/7/3/dataset_473fd6ac-9aca-4610-91bc-dbc1497f3c73_files:/galaxy/server/database/objects/4/7/3/dataset_473fd6ac-9aca-4610-91bc-dbc1497f3c73.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:34,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (178) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/178/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/178/galaxy_178.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:34,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [179] prepared (125.948 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:34,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/179/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/179/registry.xml' '/galaxy/server/database/jobs_directory/000/179/upload_params.json' '202:/galaxy/server/database/objects/9/1/c/dataset_91ca69be-1ffc-438d-a292-901e61236d6e_files:/galaxy/server/database/objects/9/1/c/dataset_91ca69be-1ffc-438d-a292-901e61236d6e.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 180
galaxy.jobs.runners DEBUG 2025-03-02 06:47:34,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (179) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/179/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/179/galaxy_179.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 181
galaxy.jobs DEBUG 2025-03-02 06:47:34,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [180] prepared (128.957 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:34,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/180/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/180/registry.xml' '/galaxy/server/database/jobs_directory/000/180/upload_params.json' '203:/galaxy/server/database/objects/4/9/e/dataset_49eeaf63-34b5-4036-9ab5-a688513d11da_files:/galaxy/server/database/objects/4/9/e/dataset_49eeaf63-34b5-4036-9ab5-a688513d11da.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:47:34,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (180) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/180/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/180/galaxy_180.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:34,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs DEBUG 2025-03-02 06:47:34,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [181] prepared (192.192 ms)
galaxy.jobs.runners DEBUG 2025-03-02 06:47:34,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 171 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 182
galaxy.jobs.command_factory INFO 2025-03-02 06:47:35,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/181/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/181/registry.xml' '/galaxy/server/database/jobs_directory/000/181/upload_params.json' '204:/galaxy/server/database/objects/7/f/e/dataset_7fe414f9-095a-4b33-8636-7c2717d1422c_files:/galaxy/server/database/objects/7/f/e/dataset_7fe414f9-095a-4b33-8636-7c2717d1422c.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:47:35,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (181) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/181/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/181/galaxy_181.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.model.metadata DEBUG 2025-03-02 06:47:35,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 194
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 183
galaxy.jobs INFO 2025-03-02 06:47:35,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 171 in /galaxy/server/database/jobs_directory/000/171
galaxy.jobs DEBUG 2025-03-02 06:47:35,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [182] prepared (188.755 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.command_factory INFO 2025-03-02 06:47:35,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/182/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/182/registry.xml' '/galaxy/server/database/jobs_directory/000/182/upload_params.json' '205:/galaxy/server/database/objects/a/4/9/dataset_a491aeee-56bc-40b7-9744-40e1a9bcb6f1_files:/galaxy/server/database/objects/a/4/9/dataset_a491aeee-56bc-40b7-9744-40e1a9bcb6f1.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:35,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (182) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/182/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/182/galaxy_182.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs DEBUG 2025-03-02 06:47:35,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [183] prepared (156.367 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:35,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 171 executed (377.187 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:35,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/183/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/183/registry.xml' '/galaxy/server/database/jobs_directory/000/183/upload_params.json' '206:/galaxy/server/database/objects/2/1/b/dataset_21b7df12-9e3b-43cd-99a0-869edf686dac_files:/galaxy/server/database/objects/2/1/b/dataset_21b7df12-9e3b-43cd-99a0-869edf686dac.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:35,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (183) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/183/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/183/galaxy_183.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:35,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 172 finished
galaxy.model.metadata DEBUG 2025-03-02 06:47:35,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 195
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-02 06:47:35,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 172 in /galaxy/server/database/jobs_directory/000/172
galaxy.jobs DEBUG 2025-03-02 06:47:35,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 172 executed (133.225 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 184
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 185
galaxy.jobs DEBUG 2025-03-02 06:47:35,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [184] prepared (114.344 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:35,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/184/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/184/registry.xml' '/galaxy/server/database/jobs_directory/000/184/upload_params.json' '207:/galaxy/server/database/objects/6/b/3/dataset_6b3b2fe9-b3ea-4b8d-91d6-bf9233779add_files:/galaxy/server/database/objects/6/b/3/dataset_6b3b2fe9-b3ea-4b8d-91d6-bf9233779add.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:35,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (184) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/184/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/184/galaxy_184.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 184 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:35,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [185] prepared (106.708 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 186
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 184 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:47:35,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/185/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/185/registry.xml' '/galaxy/server/database/jobs_directory/000/185/upload_params.json' '208:/galaxy/server/database/objects/a/0/a/dataset_a0a485fb-ac82-42ff-bad6-4678ce9e261f_files:/galaxy/server/database/objects/a/0/a/dataset_a0a485fb-ac82-42ff-bad6-4678ce9e261f.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:35,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (185) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/185/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/185/galaxy_185.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 185 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:35,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 185 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:36,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [186] prepared (139.877 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:36,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/186/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/186/registry.xml' '/galaxy/server/database/jobs_directory/000/186/upload_params.json' '209:/galaxy/server/database/objects/d/f/7/dataset_df765e9d-0e47-4a0c-b171-9e1b14aeee26_files:/galaxy/server/database/objects/d/f/7/dataset_df765e9d-0e47-4a0c-b171-9e1b14aeee26.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:36,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (186) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/186/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/186/galaxy_186.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 186 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 186 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 187
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 188
galaxy.jobs DEBUG 2025-03-02 06:47:36,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [187] prepared (130.541 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 189
galaxy.jobs.command_factory INFO 2025-03-02 06:47:36,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/187/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/187/registry.xml' '/galaxy/server/database/jobs_directory/000/187/upload_params.json' '210:/galaxy/server/database/objects/4/c/3/dataset_4c36c346-bff6-467d-91a9-05b59385e4da_files:/galaxy/server/database/objects/4/c/3/dataset_4c36c346-bff6-467d-91a9-05b59385e4da.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:36,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (187) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/187/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/187/galaxy_187.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 190
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 187 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 187 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:47:36,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [188] prepared (164.401 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:36,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/188/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/188/registry.xml' '/galaxy/server/database/jobs_directory/000/188/upload_params.json' '211:/galaxy/server/database/objects/c/8/0/dataset_c8035823-58a4-4f10-9af6-373fb5721221_files:/galaxy/server/database/objects/c/8/0/dataset_c8035823-58a4-4f10-9af6-373fb5721221.dat']
galaxy.jobs DEBUG 2025-03-02 06:47:36,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [189] prepared (181.616 ms)
galaxy.jobs.runners DEBUG 2025-03-02 06:47:36,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (188) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/188/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/188/galaxy_188.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:47:36,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [190] prepared (138.451 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:36,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/189/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/189/registry.xml' '/galaxy/server/database/jobs_directory/000/189/upload_params.json' '212:/galaxy/server/database/objects/a/d/7/dataset_ad77347a-ed3f-41bd-bbc8-d1ca2d0efc9c_files:/galaxy/server/database/objects/a/d/7/dataset_ad77347a-ed3f-41bd-bbc8-d1ca2d0efc9c.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 188 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:36,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (189) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/189/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/189/galaxy_189.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 188 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:47:36,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/190/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/190/registry.xml' '/galaxy/server/database/jobs_directory/000/190/upload_params.json' '213:/galaxy/server/database/objects/9/4/0/dataset_940737f2-b869-4027-8f03-d83f7de1245a_files:/galaxy/server/database/objects/9/4/0/dataset_940737f2-b869-4027-8f03-d83f7de1245a.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 189 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:47:36,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (190) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/190/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/190/galaxy_190.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 189 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 190 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 190 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 191
galaxy.jobs DEBUG 2025-03-02 06:47:36,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [191] prepared (81.986 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:36,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/191/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/191/registry.xml' '/galaxy/server/database/jobs_directory/000/191/upload_params.json' '214:/galaxy/server/database/objects/6/c/4/dataset_6c47ca39-eea0-4d9b-b5e0-00661fed94e7_files:/galaxy/server/database/objects/6/c/4/dataset_6c47ca39-eea0-4d9b-b5e0-00661fed94e7.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:36,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (191) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/191/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/191/galaxy_191.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 191 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:36,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 191 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 192
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 193
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 194
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 195
galaxy.jobs DEBUG 2025-03-02 06:47:37,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [192] prepared (169.898 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:37,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/192/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/192/registry.xml' '/galaxy/server/database/jobs_directory/000/192/upload_params.json' '215:/galaxy/server/database/objects/1/3/7/dataset_13797b84-c105-443d-bb8e-340e83976320_files:/galaxy/server/database/objects/1/3/7/dataset_13797b84-c105-443d-bb8e-340e83976320.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:37,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (192) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/192/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/192/galaxy_192.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:47:37,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [193] prepared (229.988 ms)
galaxy.jobs DEBUG 2025-03-02 06:47:37,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [194] prepared (212.776 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 192 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.command_factory INFO 2025-03-02 06:47:37,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/193/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/193/registry.xml' '/galaxy/server/database/jobs_directory/000/193/upload_params.json' '216:/galaxy/server/database/objects/5/4/6/dataset_54690d4e-764d-4fd2-9ef3-140017bf6d53_files:/galaxy/server/database/objects/5/4/6/dataset_54690d4e-764d-4fd2-9ef3-140017bf6d53.dat']
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 192 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:47:37,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/194/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/194/registry.xml' '/galaxy/server/database/jobs_directory/000/194/upload_params.json' '217:/galaxy/server/database/objects/2/4/d/dataset_24d17de5-befd-4933-9568-962823bb5215_files:/galaxy/server/database/objects/2/4/d/dataset_24d17de5-befd-4933-9568-962823bb5215.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:37,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (193) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/193/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/193/galaxy_193.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-03-02 06:47:37,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (194) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/194/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/194/galaxy_194.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:47:37,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [195] prepared (213.798 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 194 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 193 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 193 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 194 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:47:37,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/195/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/195/registry.xml' '/galaxy/server/database/jobs_directory/000/195/upload_params.json' '218:/galaxy/server/database/objects/3/6/d/dataset_36dcdb4b-5716-4af3-b1f5-e39c5f368bd2_files:/galaxy/server/database/objects/3/6/d/dataset_36dcdb4b-5716-4af3-b1f5-e39c5f368bd2.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:37,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (195) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/195/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/195/galaxy_195.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 195 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 195 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 196
galaxy.jobs DEBUG 2025-03-02 06:47:37,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [196] prepared (75.731 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:37,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/196/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/196/registry.xml' '/galaxy/server/database/jobs_directory/000/196/upload_params.json' '219:/galaxy/server/database/objects/c/1/e/dataset_c1e2d8ba-b492-4f71-952f-1271f6c4bc9b_files:/galaxy/server/database/objects/c/1/e/dataset_c1e2d8ba-b492-4f71-952f-1271f6c4bc9b.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:37,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (196) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/196/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/196/galaxy_196.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 196 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 198
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 199
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 197
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:37,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 196 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs DEBUG 2025-03-02 06:47:38,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [198] prepared (195.976 ms)
galaxy.jobs DEBUG 2025-03-02 06:47:38,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [199] prepared (197.961 ms)
galaxy.jobs DEBUG 2025-03-02 06:47:38,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [197] prepared (190.343 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.command_factory INFO 2025-03-02 06:47:38,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/197/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/197/registry.xml' '/galaxy/server/database/jobs_directory/000/197/upload_params.json' '220:/galaxy/server/database/objects/6/c/6/dataset_6c603c62-3d5b-434e-a211-f89303a7b0e5_files:/galaxy/server/database/objects/6/c/6/dataset_6c603c62-3d5b-434e-a211-f89303a7b0e5.dat']
galaxy.jobs.command_factory INFO 2025-03-02 06:47:38,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/198/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/198/registry.xml' '/galaxy/server/database/jobs_directory/000/198/upload_params.json' '221:/galaxy/server/database/objects/d/3/e/dataset_d3e58cc8-4cf4-4a44-a1bb-e13ba03b248b_files:/galaxy/server/database/objects/d/3/e/dataset_d3e58cc8-4cf4-4a44-a1bb-e13ba03b248b.dat']
galaxy.jobs.command_factory INFO 2025-03-02 06:47:38,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/199/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/199/registry.xml' '/galaxy/server/database/jobs_directory/000/199/upload_params.json' '222:/galaxy/server/database/objects/7/1/3/dataset_713a5ee4-e52a-4f5a-8b0f-3ae195fe745a_files:/galaxy/server/database/objects/7/1/3/dataset_713a5ee4-e52a-4f5a-8b0f-3ae195fe745a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:38,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (197) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/197/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/197/galaxy_197.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-03-02 06:47:38,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (198) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/198/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/198/galaxy_198.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-03-02 06:47:38,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (199) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/199/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/199/galaxy_199.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 197 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 199 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 198 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 199 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 197 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 198 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 200
galaxy.jobs DEBUG 2025-03-02 06:47:38,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [200] prepared (85.017 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:38,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/200/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/200/registry.xml' '/galaxy/server/database/jobs_directory/000/200/upload_params.json' '223:/galaxy/server/database/objects/7/8/f/dataset_78f4eea3-99bf-4077-a0b3-5a14411efe57_files:/galaxy/server/database/objects/7/8/f/dataset_78f4eea3-99bf-4077-a0b3-5a14411efe57.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:38,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (200) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/200/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/200/galaxy_200.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 200 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 200 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 201
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 202
galaxy.jobs DEBUG 2025-03-02 06:47:38,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [201] prepared (121.406 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:47:38,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/201/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/201/registry.xml' '/galaxy/server/database/jobs_directory/000/201/upload_params.json' '224:/galaxy/server/database/objects/c/1/0/dataset_c10d7257-cebf-4aac-9fbc-e7c82b0ccf4e_files:/galaxy/server/database/objects/c/1/0/dataset_c10d7257-cebf-4aac-9fbc-e7c82b0ccf4e.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:38,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (201) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/201/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/201/galaxy_201.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:47:38,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [202] prepared (131.848 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 201 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 201 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:47:38,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/202/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/202/registry.xml' '/galaxy/server/database/jobs_directory/000/202/upload_params.json' '225:/galaxy/server/database/objects/4/c/f/dataset_4cff4118-be4c-4eda-b8a0-a0a06322d153_files:/galaxy/server/database/objects/4/c/f/dataset_4cff4118-be4c-4eda-b8a0-a0a06322d153.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:47:38,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (202) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/202/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/202/galaxy_202.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 202 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:38,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 202 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:40,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:40,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sdmsz with k8s id: gxy-sdmsz  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:40,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v9n7p with k8s id: gxy-v9n7p  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:40,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kcq99 with k8s id: gxy-kcq99  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:40,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pkklg with k8s id: gxy-pkklg  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:40,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dhslb with k8s id: gxy-dhslb  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d2plr with k8s id: gxy-d2plr  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kgvch with k8s id: gxy-kgvch  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pfqr9 with k8s id: gxy-pfqr9  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zjspw with k8s id: gxy-zjspw  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ql6n7 with k8s id: gxy-ql6n7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xngw9 with k8s id: gxy-xngw9  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvbsv with k8s id: gxy-dvbsv  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7wnth with k8s id: gxy-7wnth  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:41,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9g6c7 with k8s id: gxy-9g6c7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:42,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rh79x with k8s id: gxy-rh79x  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sdmsz with k8s id: gxy-sdmsz  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v9n7p with k8s id: gxy-v9n7p  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kcq99 with k8s id: gxy-kcq99  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pkklg with k8s id: gxy-pkklg  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dhslb with k8s id: gxy-dhslb  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d2plr with k8s id: gxy-d2plr  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kgvch with k8s id: gxy-kgvch  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pfqr9 with k8s id: gxy-pfqr9  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zjspw with k8s id: gxy-zjspw  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:43,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ql6n7 with k8s id: gxy-ql6n7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:44,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xngw9 with k8s id: gxy-xngw9  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:44,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvbsv with k8s id: gxy-dvbsv  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:44,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7wnth with k8s id: gxy-7wnth  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:44,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9g6c7 with k8s id: gxy-9g6c7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:44,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rh79x with k8s id: gxy-rh79x  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-psvdl with k8s id: gxy-psvdl succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gxbsz failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners DEBUG 2025-03-02 06:47:45,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 173: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gxbsz.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:47:45,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-gxbsz

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-gxbsz": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7pz5x with k8s id: gxy-7pz5x succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (174/gxy-gxbsz) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (174/gxy-gxbsz) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (174/gxy-gxbsz) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (174/gxy-gxbsz) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-gxbsz.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 174 (gxy-gxbsz)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-gxbsz to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hr5l5 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners DEBUG 2025-03-02 06:47:45,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 175: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-hr5l5.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:47:45,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-hr5l5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-hr5l5": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (176/gxy-hr5l5) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (176/gxy-hr5l5) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (176/gxy-hr5l5) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (176/gxy-hr5l5) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-hr5l5.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 176 (gxy-hr5l5)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-hr5l5 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (174/gxy-gxbsz) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:45,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (176/gxy-hr5l5) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v9n7p with k8s id: gxy-v9n7p  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kcq99 with k8s id: gxy-kcq99  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pkklg with k8s id: gxy-pkklg  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dhslb with k8s id: gxy-dhslb  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d2plr with k8s id: gxy-d2plr  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kgvch with k8s id: gxy-kgvch  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pfqr9 with k8s id: gxy-pfqr9  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zjspw with k8s id: gxy-zjspw  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ql6n7 with k8s id: gxy-ql6n7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:46,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xngw9 with k8s id: gxy-xngw9  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:47,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvbsv with k8s id: gxy-dvbsv  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:47,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7wnth with k8s id: gxy-7wnth  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:47,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9g6c7 with k8s id: gxy-9g6c7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:47,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rh79x with k8s id: gxy-rh79x  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:48,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hzvz5 with k8s id: gxy-hzvz5 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:48,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dsjvk with k8s id: gxy-dsjvk succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:48,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vm4c8 with k8s id: gxy-vm4c8 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:48,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-glp87 with k8s id: gxy-glp87 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:48,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-596dd with k8s id: gxy-596dd succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:47:48,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 177: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:48,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-msx5f with k8s id: gxy-msx5f succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:47:48,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 178: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:48,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c9p27 with k8s id: gxy-c9p27 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:49,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:49,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:49,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:49,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:49,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:49,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvbsv with k8s id: gxy-dvbsv  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7wnth with k8s id: gxy-7wnth  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9g6c7 with k8s id: gxy-9g6c7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:50,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rh79x with k8s id: gxy-rh79x  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:51,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9srlq with k8s id: gxy-9srlq succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:51,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-r99sd with k8s id: gxy-r99sd succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:51,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-274mg with k8s id: gxy-274mg succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:51,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rxf62 with k8s id: gxy-rxf62 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:52,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvbsv with k8s id: gxy-dvbsv  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:52,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7wnth with k8s id: gxy-7wnth  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:52,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9g6c7 with k8s id: gxy-9g6c7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:52,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rh79x with k8s id: gxy-rh79x  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:54,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvbsv with k8s id: gxy-dvbsv  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:54,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7wnth with k8s id: gxy-7wnth  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:54,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9g6c7 with k8s id: gxy-9g6c7  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:54,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rh79x with k8s id: gxy-rh79x  pending...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:55,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sdmsz with k8s id: gxy-sdmsz succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:56,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:56,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:56,907 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:57,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:47:59,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kcq99 with k8s id: gxy-kcq99 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:00,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pfqr9 with k8s id: gxy-pfqr9 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:00,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zjspw with k8s id: gxy-zjspw succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:01,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v9n7p with k8s id: gxy-v9n7p succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:01,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pkklg with k8s id: gxy-pkklg succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:01,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dhslb with k8s id: gxy-dhslb succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:01,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d2plr with k8s id: gxy-d2plr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:01,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kgvch with k8s id: gxy-kgvch succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:01,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ql6n7 with k8s id: gxy-ql6n7 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:01,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xngw9 with k8s id: gxy-xngw9 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:48:03,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 175 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:03,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 198
galaxy.jobs INFO 2025-03-02 06:48:04,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 175 in /galaxy/server/database/jobs_directory/000/175
galaxy.jobs DEBUG 2025-03-02 06:48:04,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 175 executed (211.708 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:04,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:04,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 173 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:04,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 196
galaxy.jobs INFO 2025-03-02 06:48:04,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 173 in /galaxy/server/database/jobs_directory/000/173
galaxy.jobs.runners DEBUG 2025-03-02 06:48:04,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 179: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs DEBUG 2025-03-02 06:48:04,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 173 executed (285.450 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:04,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:04,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 180: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-02 06:48:05,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 203, 204
tpv.core.entities DEBUG 2025-03-02 06:48:05,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:48:05,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:48:05,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:48:05,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:48:05,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Working directory for job is: /galaxy/server/database/jobs_directory/000/203
galaxy.jobs.runners DEBUG 2025-03-02 06:48:05,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [203] queued (98.232 ms)
galaxy.jobs.handler INFO 2025-03-02 06:48:05,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Job dispatched
tpv.core.entities DEBUG 2025-03-02 06:48:05,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:48:05,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:48:05,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:48:06,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:48:06,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Working directory for job is: /galaxy/server/database/jobs_directory/000/204
galaxy.jobs.runners DEBUG 2025-03-02 06:48:06,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [204] queued (193.213 ms)
galaxy.jobs.handler INFO 2025-03-02 06:48:06,113 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:06,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dvbsv failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:06,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:06,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-dvbsv.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:06,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 199 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:48:06,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-dvbsv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-dvbsv": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:07,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rh79x with k8s id: gxy-rh79x succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:08,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7wnth with k8s id: gxy-7wnth succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:08,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9g6c7 with k8s id: gxy-9g6c7 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:48:08,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 177 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:08,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 200
galaxy.jobs INFO 2025-03-02 06:48:08,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 177 in /galaxy/server/database/jobs_directory/000/177
galaxy.jobs DEBUG 2025-03-02 06:48:08,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 177 executed (206.229 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:08,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:09,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 181: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:09,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 178 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:09,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 201
galaxy.jobs INFO 2025-03-02 06:48:10,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 178 in /galaxy/server/database/jobs_directory/000/178
galaxy.jobs DEBUG 2025-03-02 06:48:10,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 178 executed (250.352 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:10,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:10,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 182: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:21,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 179 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:21,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 202
galaxy.jobs INFO 2025-03-02 06:48:21,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 179 in /galaxy/server/database/jobs_directory/000/179
galaxy.jobs.runners DEBUG 2025-03-02 06:48:21,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 180 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:22,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 203
galaxy.jobs DEBUG 2025-03-02 06:48:22,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 179 executed (279.656 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:22,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-02 06:48:22,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 180 in /galaxy/server/database/jobs_directory/000/180
galaxy.jobs DEBUG 2025-03-02 06:48:22,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 180 executed (206.052 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:22,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:22,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 183: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:22,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 184: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:25,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 181 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:25,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 204
galaxy.jobs INFO 2025-03-02 06:48:25,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 181 in /galaxy/server/database/jobs_directory/000/181
galaxy.jobs DEBUG 2025-03-02 06:48:25,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 181 executed (198.736 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:25,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:25,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 185: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:26,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 182 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:26,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 205
galaxy.jobs INFO 2025-03-02 06:48:26,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 182 in /galaxy/server/database/jobs_directory/000/182
galaxy.jobs DEBUG 2025-03-02 06:48:26,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 182 executed (227.489 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:27,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:27,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 186: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:38,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 183 finished
galaxy.jobs.runners DEBUG 2025-03-02 06:48:38,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 184 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:38,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 206
galaxy.model.metadata DEBUG 2025-03-02 06:48:38,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 207
galaxy.jobs INFO 2025-03-02 06:48:38,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 183 in /galaxy/server/database/jobs_directory/000/183
galaxy.jobs INFO 2025-03-02 06:48:38,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 184 in /galaxy/server/database/jobs_directory/000/184
galaxy.jobs DEBUG 2025-03-02 06:48:38,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 183 executed (209.087 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:38,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:48:38,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 184 executed (199.285 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:38,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 184 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:39,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 187: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:39,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 188: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:41,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 185 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:41,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 208
galaxy.jobs INFO 2025-03-02 06:48:41,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 185 in /galaxy/server/database/jobs_directory/000/185
galaxy.jobs DEBUG 2025-03-02 06:48:41,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 185 executed (207.409 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:42,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 185 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:42,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 189: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:42,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 186 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:42,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 209
galaxy.jobs INFO 2025-03-02 06:48:42,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 186 in /galaxy/server/database/jobs_directory/000/186
galaxy.jobs DEBUG 2025-03-02 06:48:43,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 186 executed (200.539 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:43,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 186 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:43,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 194: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:54,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 188 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:54,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 211
galaxy.jobs.runners DEBUG 2025-03-02 06:48:54,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 187 finished
galaxy.jobs INFO 2025-03-02 06:48:54,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 188 in /galaxy/server/database/jobs_directory/000/188
galaxy.model.metadata DEBUG 2025-03-02 06:48:54,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 210
galaxy.jobs INFO 2025-03-02 06:48:55,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 187 in /galaxy/server/database/jobs_directory/000/187
galaxy.jobs DEBUG 2025-03-02 06:48:55,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 188 executed (220.937 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:55,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 188 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:48:55,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 187 executed (183.201 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:55,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 187 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:55,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 196: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:55,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 190: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:48:58,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 189 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:58,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 212
galaxy.jobs INFO 2025-03-02 06:48:58,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 189 in /galaxy/server/database/jobs_directory/000/189
galaxy.jobs DEBUG 2025-03-02 06:48:58,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 189 executed (147.839 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:58,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 189 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:59,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 194 finished
galaxy.model.metadata DEBUG 2025-03-02 06:48:59,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 217
galaxy.jobs INFO 2025-03-02 06:48:59,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 194 in /galaxy/server/database/jobs_directory/000/194
galaxy.jobs.runners DEBUG 2025-03-02 06:48:59,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 191: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs DEBUG 2025-03-02 06:48:59,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 194 executed (303.402 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:48:59,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 194 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:48:59,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 192: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:11,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 196 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:11,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 219
galaxy.jobs INFO 2025-03-02 06:49:11,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 196 in /galaxy/server/database/jobs_directory/000/196
galaxy.jobs DEBUG 2025-03-02 06:49:11,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 196 executed (200.117 ms)
galaxy.jobs.runners DEBUG 2025-03-02 06:49:11,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 190 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:11,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 213
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:11,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 196 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-02 06:49:11,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 190 in /galaxy/server/database/jobs_directory/000/190
galaxy.jobs DEBUG 2025-03-02 06:49:11,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 190 executed (123.485 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:11,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 190 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:49:12,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 193: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:12,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 195: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:14,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 191 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:14,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 214
galaxy.jobs INFO 2025-03-02 06:49:14,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 191 in /galaxy/server/database/jobs_directory/000/191
galaxy.jobs DEBUG 2025-03-02 06:49:14,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 191 executed (198.818 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:14,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 191 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:49:15,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 198: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:15,241 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 192 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:15,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 215
galaxy.jobs INFO 2025-03-02 06:49:15,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 192 in /galaxy/server/database/jobs_directory/000/192
galaxy.jobs DEBUG 2025-03-02 06:49:15,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 192 executed (197.584 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:15,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 192 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:49:16,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 197: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:28,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 193 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:28,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 216
galaxy.jobs INFO 2025-03-02 06:49:28,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 193 in /galaxy/server/database/jobs_directory/000/193
galaxy.jobs DEBUG 2025-03-02 06:49:28,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 193 executed (250.517 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:28,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 193 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:28,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 203
galaxy.jobs DEBUG 2025-03-02 06:49:29,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [203] prepared (110.167 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:49:29,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/203/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/203/registry.xml' '/galaxy/server/database/jobs_directory/000/203/upload_params.json' '226:/galaxy/server/database/objects/e/e/6/dataset_ee6dde48-bf3e-4407-b89f-5d72da99e015_files:/galaxy/server/database/objects/e/e/6/dataset_ee6dde48-bf3e-4407-b89f-5d72da99e015.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:49:29,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (203) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/203/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/203/galaxy_203.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-03-02 06:49:29,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 195 finished
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 203 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 203 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 204
galaxy.model.metadata DEBUG 2025-03-02 06:49:29,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 218
galaxy.jobs INFO 2025-03-02 06:49:29,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 195 in /galaxy/server/database/jobs_directory/000/195
galaxy.jobs DEBUG 2025-03-02 06:49:29,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [204] prepared (147.511 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:49:29,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/204/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/204/registry.xml' '/galaxy/server/database/jobs_directory/000/204/upload_params.json' '227:/galaxy/server/database/objects/5/7/e/dataset_57e24615-29f0-4129-810f-9e1a63b84cec_files:/galaxy/server/database/objects/5/7/e/dataset_57e24615-29f0-4129-810f-9e1a63b84cec.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:49:29,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (204) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/204/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/204/galaxy_204.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:49:29,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 195 executed (217.797 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 204 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 195 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 204 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:49:29,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 201: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:29,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 200: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:29,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:49:31,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 198 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:31,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 221
galaxy.jobs INFO 2025-03-02 06:49:31,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 198 in /galaxy/server/database/jobs_directory/000/198
galaxy.jobs DEBUG 2025-03-02 06:49:31,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 198 executed (195.019 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:49:31,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] No k8s job found which matches job id 'gxy-ql6n7'. Ignoring...
galaxy.jobs.runners DEBUG 2025-03-02 06:49:31,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 197 finished
galaxy.jobs.runners DEBUG 2025-03-02 06:49:31,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 202: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.model.metadata DEBUG 2025-03-02 06:49:32,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 220
galaxy.jobs INFO 2025-03-02 06:49:32,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 197 in /galaxy/server/database/jobs_directory/000/197
galaxy.jobs DEBUG 2025-03-02 06:49:32,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 197 executed (226.366 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:49:32,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] No k8s job found which matches job id 'gxy-xngw9'. Ignoring...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (199/gxy-dvbsv) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (199/gxy-dvbsv) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (199/gxy-dvbsv) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (199/gxy-dvbsv) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-dvbsv.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 199 (gxy-dvbsv)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-dvbsv to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 199 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:32,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (199/gxy-dvbsv) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:39,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-62tpn with k8s id: gxy-62tpn succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:39,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8jxhv with k8s id: gxy-8jxhv succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:49:39,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 203: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:43,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 201 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:43,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 224
galaxy.jobs INFO 2025-03-02 06:49:43,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 201 in /galaxy/server/database/jobs_directory/000/201
galaxy.jobs.runners DEBUG 2025-03-02 06:49:44,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 200 finished
galaxy.jobs DEBUG 2025-03-02 06:49:44,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 201 executed (263.031 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:49:44,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] No k8s job found which matches job id 'gxy-rh79x'. Ignoring...
galaxy.model.metadata DEBUG 2025-03-02 06:49:44,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 223
galaxy.jobs INFO 2025-03-02 06:49:44,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 200 in /galaxy/server/database/jobs_directory/000/200
galaxy.jobs DEBUG 2025-03-02 06:49:44,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 200 executed (209.903 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:49:44,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] No k8s job found which matches job id 'gxy-7wnth'. Ignoring...
galaxy.jobs.runners DEBUG 2025-03-02 06:49:44,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 204: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:49:45,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 202 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:45,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 225
galaxy.jobs INFO 2025-03-02 06:49:45,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 202 in /galaxy/server/database/jobs_directory/000/202
galaxy.jobs DEBUG 2025-03-02 06:49:45,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 202 executed (174.506 ms)
galaxy.jobs.runners.kubernetes WARNING 2025-03-02 06:49:45,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] No k8s job found which matches job id 'gxy-9g6c7'. Ignoring...
galaxy.jobs.runners DEBUG 2025-03-02 06:49:50,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 203 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:50,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 226
galaxy.jobs INFO 2025-03-02 06:49:50,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 203 in /galaxy/server/database/jobs_directory/000/203
galaxy.jobs DEBUG 2025-03-02 06:49:50,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 203 executed (109.350 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:50,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 203 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:49:52,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 204 finished
galaxy.model.metadata DEBUG 2025-03-02 06:49:53,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 227
galaxy.jobs INFO 2025-03-02 06:49:53,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 204 in /galaxy/server/database/jobs_directory/000/204
galaxy.jobs DEBUG 2025-03-02 06:49:53,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 204 executed (105.263 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:53,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 204 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:49:53,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 205
tpv.core.entities DEBUG 2025-03-02 06:49:53,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:49:53,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:49:53,610 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:49:53,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:49:53,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Working directory for job is: /galaxy/server/database/jobs_directory/000/205
galaxy.jobs.runners DEBUG 2025-03-02 06:49:53,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [205] queued (33.904 ms)
galaxy.jobs.handler INFO 2025-03-02 06:49:53,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:53,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 205
galaxy.jobs DEBUG 2025-03-02 06:49:53,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [205] prepared (61.172 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:49:53,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:49:53,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:49:53,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:49:53,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/205/tool_script.sh] for tool command [multiqc --version > /galaxy/server/database/jobs_directory/000/205/outputs/COMMAND_VERSION 2>&1;
die() { echo "$@" 1>&2 ; exit 1; } &&  mkdir multiqc_WDir &&   mkdir multiqc_WDir/custom_content_0 &&  ln -s '/galaxy/server/database/objects/e/e/6/dataset_ee6dde48-bf3e-4407-b89f-5d72da99e015.dat' 'multiqc_WDir/custom_content_0/file_0_0' && more /galaxy/server/database/objects/e/e/6/dataset_ee6dde48-bf3e-4407-b89f-5d72da99e015.dat && ln -s '/galaxy/server/database/objects/5/7/e/dataset_57e24615-29f0-4129-810f-9e1a63b84cec.dat' 'multiqc_WDir/custom_content_0/file_0_1' && more /galaxy/server/database/objects/5/7/e/dataset_57e24615-29f0-4129-810f-9e1a63b84cec.dat &&  multiqc multiqc_WDir --filename "report"      --config '/galaxy/server/database/jobs_directory/000/205/configs/tmpzgs249mq']
galaxy.jobs.runners DEBUG 2025-03-02 06:49:53,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (205) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/205/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/205/galaxy_205.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/205/working/report.html" -a -f "/galaxy/server/database/objects/e/4/0/dataset_e400598e-576c-4d4a-8116-9e6b2c1fe9f5.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/205/working/report.html" "/galaxy/server/database/objects/e/4/0/dataset_e400598e-576c-4d4a-8116-9e6b2c1fe9f5.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:54,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 205 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:49:54,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:49:54,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:49:54,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:54,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 205 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:49:54,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-646g9 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-646g9.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 205 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:50:02,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-646g9

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-646g9": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (205/gxy-646g9) tool_stdout: ::::::::::::::
/galaxy/server/database/objects/e/e/6/dataset_ee6dde48-bf3e-4407-b89f-5d72da99e015.dat
::::::::::::::
2502.98928676082	43888
2504.55428676082	43960
2506.11928676082	43392
2507.68428676082	42632
2509.24928676082	42200
2510.81428676082	42288
2512.37928676082	42664
2513.94428676082	42480
2515.50928676082	42320
2517.07428676082	42528
2518.63928676082	43376
2520.20428676082	44240
2521.76928676082	44696
2523.33328676082	44560
2524.89828676082	44112
2526.46328676082	43936
2528.02828676082	43736
2529.59328676082	42912
2531.15828676082	42360
2532.72328676082	43128
2534.28828676082	43880
2535.85328676082	44104
2537.41828676082	44256
2538.98328676082	44664
2540.54828676082	44600
2542.11328676082	44224
2543.67828676082	44120
2545.24328676082	44328
2546.80828676082	44536
2548.37328676082	44872
2549.93828676082	44592
2551.50328676082	43600
2553.06828676082	42872
2554.63328676082	43528
2556.19728676082	44072
2557.76228676082	43360
2559.32728676082	41632
2560.89228676082	40752
2562.45728676082	40472
2564.02228676082	39792
2565.58728676082	39096
2567.15228676082	39696
2568.71728676082	41152
2570.28228676082	41896
2571.84728676082	41568
2573.41228676082	41448
2574.97728676082	42336
2576.54228676082	43432
2578.10728676082	43720
2579.67228676082	43352
2581.23728676082	43528
2582.80228676082	55720
2584.36728676082	74184
2585.93228676082	91992
2587.49728676082	105888
2589.06228676082	114512
2590.62628676082	120272
2592.19128676082	123640
2593.75628676082	122024
2595.32128676082	115504
2596.88628676082	106736
2598.45128676082	97864
2600.01628676082	87640
2601.58128676082	77312
2603.14628676082	67952
2604.71128676082	59464
2606.27628676082	50624
2607.84128676082	43672
2609.40628676082	43920
2610.97128676082	43880
2612.53628676082	44024
2614.10128676082	44192
2615.66628676082	44352
2617.23128676082	44048
2618.79628676082	43312
2620.4002803902	42128
2622.00450588418	41032
2623.60789985113	40696
2625.2124745374	41264
2626.81716666296	42000
2628.42193807791	42792
2630.02675063237	43880
2631.63156617644	45064
2633.23634656023	45872
2634.84105363384	45696
2636.44564924738	44992
2638.05009525096	43776
2639.65435349467	42608
2641.25838582864	41656
2642.86215410296	41328
2644.46562016774	41360
2646.06874587309	41520
2647.67149306911	49200
2649.2738236059	82296
2650.87569933359	127032
2652.47708210226	178368
2654.07793376204	231808
2655.67821616301	285120
2657.2778911553	334464
2658.87589905919	369856
2660.47424523347	387136
2662.07186957376	393344
2663.66930678668	395904
2665.26703198027	395456
2666.86491760675	391936
2668.46283611837	396608
2670.06065996733	419008
2671.65826160588	465280
2673.25551348623	526336
2674.85228806062	586624
2676.44845778127	625472
2678.04389510041	642304
2679.63847247027	651392
2681.23206234307	664256
2682.82633158321	672064
2684.41623929172	664448
2686.00135805823	642048
2687.58194841108	611776
2689.1582708786	577216
2690.73058598911	537408
2692.29815313176	489984
2693.86323725773	437248
2695.42509544522	385536
2696.98398822256	336256
2698.54017611806	287104
2700.09391966008	240512
2701.64547937692	201024
2703.19511579694	166656
2704.74308944844	137088
2706.28966085977	117720
2707.83509055926	98584
2709.37963907523	81576
2710.92356693602	66760
2712.46713466996	53440
2714.01060280537	41984
2715.55423187058	37808
2717.09813305014	37464
2718.64227188502	37832
2720.18669043818	38328
2721.73143077261	38720
2723.27653495129	38992
2724.82204503719	39120
2726.36701510869	38968
2727.91346287142	38728
2729.46044270343	38912
2731.00799666769	38984
2732.55616682718	38640
2734.10499524488	37584
2735.65452398377	36608
2737.20479510683	36312
2738.75585067703	36920
2740.30773275734	37592
2741.86048341076	52864
2743.41414470024	76280
2744.96875868878	101448
2746.52436743935	125712
2748.08101301492	147648
2749.63873747848	164544
2751.19758289299	174080
2752.75759132145	176512
2754.31880482682	173248
2755.88126547208	164800
2757.44501532022	152128
2759.00909594836	137152
2760.57554950021	121488
2762.14341841699	106160
2763.71274476166	91640
2765.28357059722	78056
2766.85593798664	66968
2768.42988899288	63072
2770.00546567894	72680
2771.58271010779	100080
2773.16166434241	150464
2774.74237044577	227328
2776.32487048085	325760
2777.90920651063	435520
2779.49542059808	542464
2781.08355480619	636544
2782.67365119793	708160
2784.26575183628	758912
2785.86223081059	790208
2787.4613789788	805248
2789.06120506784	797184
2790.65654763967	762112
2792.24788678458	708992
2793.83543262583	643776
2795.42243917229	574400
2797.00911516256	502464
2798.59668514771	435904
2800.18637367879	370048
2801.7784139276	304256
2803.37212118778	240064
2804.96734319314	187072
2806.5639276775	145344
2808.16172237469	112592
2809.76057501852	86312
2811.36033334282	66688
2812.9608450814	52600
2814.56195796808	42264
2816.16351973668	39056
2817.76537812102	38920
2819.36738085492	38912
2820.9693756722	38328
2822.57121030668	37568
2824.17273249218	37104
2825.77378996252	37384
2827.37320803523	38432
2828.97287981685	40584
2830.57163018206	41920
2832.16930686469	41336
2833.76575759855	39200
2835.36083011745	37400
2836.95437215523	36560
2838.54623144569	36560
2840.13625572267	36728
2841.72429271997	36808
2843.31019017143	37216
2844.89379581084	38176
2846.47495737205	38736
2848.05352258887	38416
2849.62933919511	37984
2851.20225492459	38320
2852.77211751114	38648
2854.33877468858	38264
2855.90207419072	37912
2857.46186375138	38344
2859.01799110439	39064
2860.57030398356	39320
2862.11766206308	38864
2863.66189187661	38520
2865.20185051505	38256
2866.73738571223	37088
2868.26834520196	35312
2869.79948751649	33456
2871.33550080546	32488
2872.87612722882	32200
2874.4211089465	32296
2875.97018811845	32960
2877.5231069046	34224
2879.07960746489	35032
2880.63943195926	34776
2882.20232254765	33912
2883.76802139	33504
2885.33627064625	33768
2886.90681247633	34072
2888.47938904019	34768
2890.05374249777	35248
2891.62961500899	35248
2893.20674873382	34608
2894.78387717236	34144
2896.36275941013	34216
2897.94212950607	34888
2899.5217296201	35360
2901.10130191218	35368
2902.68058854224	34928
2904.25933167022	34920
2905.83727345606	35632
2907.4141560597	36880
2908.98972164108	37160
2910.56371236013	35752
2912.1358703768	33936
2913.70593785103	33272
2915.27239902125	36680
2916.83419636666	47800
2918.39175940831	59184
2919.94551766722	70248
2921.49590066445	80216
2923.04333792104	88104
2924.58825895802	92568
2926.13109329645	94576
2927.67227045736	94384
2929.21123626907	91672
2930.75038801093	85328
2932.28917086394	77064
2933.82801434915	68696
2935.36734798761	60960
2936.90760130035	53184
2938.44920380841	45536
2939.99258503284	38808
2941.53817449469	33040
2943.08640171498	34376
2944.63769621478	34840
2946.19064610336	34136
2947.74354970432	33256
2949.29640186015	33176
2950.84919741336	33728
2952.40193120644	34656
2953.95459808188	35000
2955.5071928822	34600
2957.05971044989	33424
2958.61214562743	32472
2960.16449325734	32032
2961.71575635663	32816
2963.26791348294	33432
2964.8199675924	33072
2966.37191352751	31704
2967.92374613077	30808
2969.47546024467	31664
2971.02705071172	33248
2972.57851237441	34000
2974.12984007525	33552
2975.68102865672	32936
2977.23207296132	32624
2978.78296783157	32512
2980.33370810994	32944
2981.88428863895	33872
2983.43470426108	34408
2984.98494981884	33704
2986.53502015473	33592
2988.08491011124	35232
2989.63461453087	37120
2991.18412825612	36560
2992.73344612949	34832
2994.28256299347	33936
2995.83048403819	34376
2997.37918354758	34944
2998.92766657837	35640
3000.47592797306	35864
3002.02396257416	35288
3003.57176522416	34392
3005.11933076555	37728
3006.66665404083	40760
3008.21372989252	41048
3009.76045391304	38448
3011.30673336694	34768
3012.85258060488	35480
3014.39800797751	35688
3015.94302783547	35512
3017.48765252941	35416
3019.03189440998	35648
3020.57576582784	36464
3022.11927913362	37696
3023.66244667799	38768
3025.20528081158	38816
3026.74779388504	38280
3028.28901291088	37712
3029.83092110147	37864
3031.372545276	37632
3032.9138977851	36304
3034.45499097944	35544
3035.99583720966	35224
3037.53644882641	35584
3039.07683818033	35736
3040.61701762208	35712
3042.15699950231	35144
3043.69679617167	34992
3045.23641998079	34944
3046.77588328035	34424
3048.31519842097	33216
3049.85437775332	40400
3051.39343362804	47376
3052.93237839577	52136
3054.47122440718	54480
3056.0099840129	54440
3057.5486695636	52288
3059.08729340991	66408
3060.62586790248	78056
3062.16342231011	87352
3063.70193515897	93968
3065.24043569815	96216
3066.7789362783	93320
3068.31753117451	88592
3069.85631402629	84080
3071.39529623095	79696
3072.93448918581	74240
3074.47390428815	69120
3076.0135529353	65328
3077.55344652456	62992
3079.09359645324	60928
3080.63401411864	58144
3082.17471091808	54504
3083.71569824885	50440
3085.25698750826	46176
3086.79859009363	42256
3088.34051740225	38840
3089.88278083144	39592
3091.4253917785	39280
3092.96836164074	37320
3094.51071553496	35776
3096.05443717193	35392
3097.59855190873	35960
3099.14307114265	36608
3100.68800627101	37296
3102.2333686911	37688
3103.77916980023	37680
3105.32542099572	37272
3106.87213367487	36240
3108.41931923499	35056
3109.96698907338	34264
3111.51515458735	34984
3113.0638271742	35824
3114.61301823126	37192
3116.16273915581	38016
3117.71300134517	38232
3119.26381619664	37760
3120.81519510754	37376
3122.36714947517	36920
3123.91969069683	36304
3125.47283016984	36120
3127.02558628348	37024
3128.57995605062	37752
3130.13495825374	37440
3131.69060429015	36904
3133.24690555716	36944
3134.80387345206	37064
3136.36151937218	36872
3137.91985471481	36424
3139.47889087727	35744
3141.03863925685	35016
3142.59911125088	66240
3144.16031825664	128352
3145.72227167146	224384
3147.28498289264	350528
3148.84846331748	489792
3150.41272434329	620352
3151.97777736738	716608
3153.54363378706	764800
3155.11030499963	765440
3156.6778024024	738432
3158.24545876284	702080
3159.81262393884	663424
3161.37833400712	633472
3162.94462816766	659456
3164.51054221883	675200
3166.07611319354	693568
3167.64137812471	706240
3169.20637404524	696384
3170.77113798805	672576
3172.33570698606	652992
3173.90011807216	635392
3175.46440827929	604544
3177.02861464035	560128
3178.59277418825	516352
3180.1569239559	476864
3181.72110097623	434880
3183.28534228214	388800
3184.84968490654	344448
3186.41416588236	304768
3187.97882224249	268352
3189.54369101986	237504
3191.10880924737	214016
3192.67421395795	194880
3194.238941608	176448
3195.80503014123	161728
3197.37151623259	152448
3198.938436915	146688
3200.50582922137	142336
3202.07373018462	137792
3203.64217683765	133376
3205.21120621338	129152
3206.78085534473	126720
3208.3511612646	125816
3209.92216100591	125032
3211.49389160157	123208
3213.06639008449	120264
3214.6396934876	115040
3216.21383884379	108344
3217.78886318599	102752
3219.3648035471	99024
3220.94169696004	94816
3222.51958045772	89288
3224.09849107306	84000
3225.67846583896	98400
3227.25954178835	109664
3228.84074458337	115416
3230.42413323567	116864
3232.00873414653	114544
3233.59458434886	109096
3235.18172087557	99640
3236.77018075957	90224
3238.36000103379	82168
3239.95121873112	73104
3241.54387088449	98824
3243.1379945268	136384
3244.73362669097	174144
3246.33080440992	208000
3247.92956471655	233984
3249.52994464377	248128
3251.13198122451	252736
3252.73571149168	250432
3254.33971258947	244608
3255.94260727745	236096
3257.54450101158	225984
3259.14549924782	209408
3260.74570744212	191680
3262.34420918678	172352
3263.94315400148	151168
3265.54162507473	189760
3267.1397278625	266112
3268.73756782075	345088
3270.33525040543	412608
3271.9328810725	465856
3273.52960680781	507904
3275.12473374223	540032
3276.71860604923	554112
3278.31156790231	550144
3279.90396347494	535232
3281.49564400442	520512
3283.08625547511	503808
3284.67583418109	477440
3286.26441641646	435264
3287.85203847529	385280
3289.43873665169	334080
3291.02454723973	367872
3292.6095065335	507712
3294.19365082709	651840
3295.77701641459	784192
3297.35862855858	918272
3298.94054605575	1047552
3300.52179370591	1158656
3302.10240780312	1249792
3303.68242464148	1320448
3305.26188051509	1366016
3306.84081171801	1373184
3308.41925454435	1353728
3309.99724528819	1328640
3311.57482024361	1306624
3313.1520157047	1280512
3314.72886796556	1263104
3316.30541332026	1240064
3317.8816880629	1215488
3319.45772848755	1199104
3321.03357088832	1170944
3322.60925155928	1122816
3324.18480679452	1055232
3325.76027288814	999680
3327.335343408	940224
3328.90967134892	867264
3330.4822269254	772544
3332.0549965256	663168
3333.626949866	550400
3335.19806238107	487232
3336.76830950528	591872
3338.33766667311	682496
3339.90610931904	754624
3341.47361287755	804928
3343.04015278311	836608
3344.60570447021	866304
3346.17024337331	882304
3347.7337449269	889280
3349.29618456545	888256
3350.85695421667	866816
3352.41554007478	827392
3353.97205897406	777728
3355.52662774881	727040
3357.07936323329	678464
3358.63038226179	629120
3360.1798016686	578048
3361.727738288	524288
3363.27430895426	471040
3364.81864344972	416320
3366.36283339876	360064
3367.90600782285	302400
3369.44828355629	249152
3370.98977743335	295232
3372.53060628831	438016
3374.07088695545	592256
3375.61073626906	737408
3377.15027106342	858624
3378.68960817281	938368
3380.22886443151	982976
3381.76815667381	1015232
3383.30760173397	1057280
3384.8473164463	1095680
3386.38741764506	1122304
3387.92802216455	1128960
3389.46924683903	1111040
3391.0112085028	1058816
3392.55402399013	996608
3394.09781013532	942272
3395.64268377263	888640
3397.18777341772	820160
3398.73517166064	744448
3400.28400782388	674688
3401.83439874172	609600
3403.38646124844	543552
3404.94031217833	570304
3406.49606836566	592512
3408.05384664472	598912
3409.6137638498	595008
3411.17593681516	588416
3412.7404823751	575936
3414.30751736389	551360
3415.87715861582	520064
3417.44952296517	491264
3419.02421978847	461440
3420.60072228854	428224
3422.17894303914	393664
3423.75879461401	360000
3425.34018958692	323072
3426.9230405316	287616
3428.5072600218	258048
3430.09276063128	234112
3431.67844070908	210240
3433.26624059939	186688
3434.85505938608	166080
3436.44480964291	148544
3438.03540394362	134016
3439.62675486196	130328
3441.21877497169	139968
3442.81137684655	160000
3444.40447306028	199040
3445.99797618665	235008
3447.5917987994	270080
3449.18585347227	304320
3450.78005277903	335040
3452.3743092934	356352
3453.96853558916	371392
3455.56264424004	379456
3457.15654781979	380736
3458.75015890217	375360
3460.34339006092	368256
3461.93615386979	356544
3463.52836290254	337536
3465.11864061023	310784
3466.70874766651	280704
3468.29771029223	250560
3469.88557189862	222720
3471.47237589688	198272
3473.05816569824	176832
3474.6429847139	158144
3476.22687635508	141120
3477.809884033	139648
3479.39205115886	140672
3480.97342114389	139200
3482.55403739929	135232
3484.13394333629	128312
3485.71318236608	119408
3487.2917978999	109664
3488.86983334896	100032
3490.44733212446	90264
3492.02433763762	93024
3493.60089329966	98072
3495.17704252179	102528
3496.75308645138	104712
3498.32889314628	103496
3499.90373818767	166720
3501.48020549821	293952
3503.05711300005	465920
3504.6342853155	663424
3506.21154706686	849280
3507.78872287645	998080
3509.36563736658	1111040
3510.94211515955	1198080
3512.51798087768	1254400
3514.09305914328	1280000
3515.66717457865	1294848
3517.24015180611	1298944
3518.81181544796	1288192
3520.38334302036	1266176
3521.95600694465	1251840
3523.52977420091	1232896
3525.10461176924	1197568
3526.68048662971	1152512
3528.25736576243	1121792
3529.83521614748	1090048
3531.41400476494	1044096
3532.99268892389	986112
3534.57325439968	938688
3536.15465906925	898048
3537.7368699127	855616
3539.3198539101	804416
3540.90357804154	753280
3542.48800928712	703872
3544.07311462692	655168
3545.65886104103	604800
3547.24521550954	556608
3548.83214501254	516928
3550.41961653011	484608
3552.00759704235	454208
3553.59605352934	425920
3555.18495297117	403264
3556.77426234792	388224
3558.36394863969	376128
3559.95397882657	362624
3561.54431988864	346176
3563.134938806	333440
3564.72580255872	322624
3566.3168781269	311360
3567.90711566462	298816
3569.49851572137	289728
3571.09002855495	281600
3572.68162114544	269696
3574.27326047293	251904
3575.8649135175	230912
3577.45654725925	211968
3579.04812867827	214080
3580.63962475463	217024
3582.23100246844	215872
3583.82222879977	210304
3585.41327072872	203328
3587.00409523538	224000
3588.59466929983	239232
3590.1849599
..
336
3383.32875490952	1155072
3384.92104541292	1177600
3386.51298404383	1181184
3388.10455326274	1186304
3389.69573553011	1186304
3391.28651330641	1169920
3392.87686905211	1131520
3394.46678522768	1079808
3396.0562442936	1018944
3397.64522871033	946624
3399.23270608822	868032
3400.82068891929	789568
3402.4081444938	720576
3403.9950552722	653376
3405.58140371498	586432
3407.1671722826	518848
3408.75234343553	541760
3410.33689963425	570432
3411.92082333921	587264
3413.5040970109	582528
3415.08670310979	562688
3416.66862409634	540288
3418.24984243102	515840
3419.8303405743	489088
3421.41010098666	461248
3422.98910612857	435008
3424.56733846049	405312
3426.14478044289	372096
3427.72141453625	338560
3429.29722320104	307968
3430.87218889772	278016
3432.44528854724	246784
3434.01851625578	216128
3435.59084838883	190848
3437.16778794794	172672
3438.75373296649	157568
3440.34700845275	142080
3441.94593941496	126936
3443.54885086137	129848
3445.15406780025	134080
3446.75991523983	161216
3448.36471818838	195968
3449.96680165415	226304
3451.56449064538	250688
3453.15611017034	270272
3454.73998523726	287424
3456.31444085442	305472
3457.87780203005	323648
3459.42839377241	337984
3460.97221817713	340672
3462.51637297288	328128
3464.06082661751	305408
3465.60554756889	284416
3467.14951702351	269312
3468.69467784151	254080
3470.24001135996	232640
3471.78548603673	207488
3473.33107032966	183360
3474.8767326966	159808
3476.42244159541	136448
3477.96816548395	146368
3479.51387282006	151104
3481.05953206161	150464
3482.60511166643	145984
3484.15058009239	140032
3485.69590579734	131584
3487.24105723914	123240
3488.78600287563	114296
3490.33071116467	104400
3491.87515056411	91784
3493.41928953181	91088
3494.96309652562	96464
3496.50654000339	99408
3498.04958842297	99352
3499.5907337547	123016
3501.13294578439	223232
3502.67787245347	363648
3504.22462616196	537408
3505.77314021334	725760
3507.32334791108	897920
3508.87518255866	1029952
3510.42857745954	1125888
3511.98346591722	1202688
3513.53978123516	1263616
3515.09745671683	1297920
3516.65642566571	1306624
3518.21662138527	1301504
3519.777977179	1301504
3521.34042635036	1306624
3522.90390220282	1306624
3524.46833803987	1286656
3526.03366716498	1249792
3527.59982288162	1198592
3529.16673849327	1139712
3530.7343473034	1082880
3532.30258261548	1048512
3533.87037514346	1031808
3535.43966307609	1011008
3537.00937746372	970560
3538.57945160982	915200
3540.14981881787	851584
3541.72041239134	788032
3543.29116563371	731200
3544.86201184844	681664
3546.43288433903	631744
3548.00371640893	583552
3549.57444136163	541248
3551.1449925006	502720
3552.71530312931	464832
3554.28530655125	431616
3555.85493606987	406656
3557.42412498867	385344
3558.9928066111	364416
3560.56091424066	345472
3562.1283811808	329600
3563.69514073502	316800
3565.26112620677	307136
3566.82527108973	300160
3568.38922661993	290816
3569.95168939513	273728
3571.51266164505	250496
3573.07214559939	226560
3574.63014348789	209152
3576.18665754026	197568
3577.74168998622	201216
3579.29524305549	207680
3580.84731897778	213568
3582.39791998282	213312
3583.94704830033	204032
3585.49470616001	187968
3587.0408957916	184320
3588.58561942482	190784
3590.12887928937	193856
3591.67067761497	195520
3593.21101663136	195648
3594.74989856824	190848
3596.28732565534	181376
3597.82330012237	169408
3599.35782419906	156224
3600.88992097579	140928
3602.42155188414	123240
3603.95173908987	105680
3605.48048482271	90128
3607.00779131237	77928
3608.53366078857	67144
3610.05809548103	57728
3611.58109761947	84952
3613.1026694336	127232
3614.62281315315	177408
3616.14153100784	233472
3617.65882522738	292160
3619.17469804149	344832
3620.68915167989	383104
3622.2021883723	403456
3623.71381034844	410176
3625.22401983803	409024
3626.73281907078	406144
3628.24021027642	391808
3629.74619568467	367744
3631.25754203364	344384
3632.77747282402	320256
3634.30408221023	293440
3635.83254947199	267712
3637.35902306758	238528
3638.8796514553	203456
3640.39929305088	165120
3641.92509656914	128968
3643.45664591697	129904
3644.9935250013	133440
3646.53531772905	134528
3648.08160800711	159360
3649.63197974242	230720
3651.18601684187	315392
3652.7433032124	410624
3654.3034227609	510016
3655.86595939429	599872
3657.43049701949	664320
3658.99661954341	706496
3660.56391087297	742720
3662.13195491507	779520
3663.70033557664	803456
3665.26863676458	811072
3666.83544083985	808000
3668.40233551667	795072
3669.96790270647	766720
3671.53172631619	729600
3673.09339025272	687936
3674.65247842298	639616
3676.20857473389	582976
3677.76126309236	529856
3679.32355368889	483072
3680.90199303028	443136
3682.48646949786	405888
3684.06687147297	371584
3685.63308733696	335424
3687.18932031058	300736
3688.74650410743	272192
3690.30462284817	250112
3691.86366065346	228480
3693.42360164396	206912
3694.98442994032	188928
3696.54612966319	173632
3698.10868493323	177664
3699.67108063199	192256
3701.23529883703	200064
3702.80032496136	201472
3704.36614312564	201152
3705.93273745051	199872
3707.50009205663	198016
3709.06819106466	191360
3710.63701859526	180224
3712.20655876908	165120
3713.77679570678	150208
3715.34771352901	134016
3716.91929635642	114816
3718.49152830969	90776
3720.06439350945	67176
3721.63787607637	67696
3723.2119601311	85640
3724.7866297943	100864
3726.36186918663	112496
3727.93766242873	118736
3729.51399364127	120896
3731.0908469449	121072
3732.66820646028	117840
3734.24504794411	110632
3735.82337194686	102376
3737.40215453346	93352
3738.98137982459	84720
3740.56103194088	76040
3742.14109500301	74440
3743.72155313162	78952
3745.30239044737	79272
3746.88359107091	76808
3748.46513912291	73256
3750.04701872402	69152
3751.62921399489	65288
3753.21170905618	62144
3754.79448802855	60416
3756.37753503264	59840
3757.96083418913	60304
3759.54436961866	61136
3761.12812544188	61568
3762.71208577947	60968
3764.29623475206	59320
3765.88055648032	57408
3767.4640225918	55024
3769.04864210834	51728
3770.63338675265	46560
3772.21824064541	40760
3773.80318790725	35808
3775.38821265884	38496
3776.97329902084	40856
3778.5584311139	45760
3780.14359305867	51184
3781.72876897582	52720
3783.31394298599	51688
3784.89909920985	50456
3786.48422176804	56976
3788.06929478123	64448
3789.65430237007	71480
3791.23922865522	77128
3792.82405775733	82272
3794.40877379706	85600
3795.99336089506	85488
3797.577803172	81624
3799.16208474852	75048
3800.74618974528	65952
3802.32909026307	54400
3803.91279460048	42984
3805.49627473024	35136
3807.07951477301	32144
3808.66249884945	35512
3810.24521108022	47448
3811.82763558596	68288
3813.40975648734	96352
3814.99155790501	126824
3816.57302395962	156096
3818.15413877184	179904
3819.73488646232	198528
3821.31525115171	211712
3822.89521696067	219904
3824.47476800986	222144
3826.05388841993	218432
3827.63256231154	209600
3829.21077380534	195264
3830.78850702199	177152
3832.36574608214	158336
3833.94247510646	141312
3835.51794589405	123848
3837.09444263788	106280
3838.67095770774	89112
3840.24749080167	74536
3841.8240416177	62968
3843.40060985387	55224
3844.97719520822	50416
3846.55379737878	50680
3848.1304160636	51504
3849.7070509607	50976
3851.28370176813	48552
3852.86036818393	50520
3854.43704990612	50944
3856.01374663275	49224
3857.59045806186	45160
3859.16718389147	39920
3860.74392381963	34456
3862.32067754438	30024
3863.89744476375	26568
3865.47422517578	24104
3867.0510184785	22088
3868.62782436996	20768
3870.20363499278	20200
3871.78046514825	20440
3873.35730698676	20840
3874.93416020634	21192
3876.51102450504	25696
3878.08789958088	32104
3879.6647851319	37400
3881.24168085615	41816
3882.81858645166	44184
3884.39550161646	43784
3885.9724260486	41264
3887.5493594461	37784
3889.12630150701	33944
3890.70325192937	30056
3892.28021041121	26344
3893.85717665056	22392
3895.43415034547	18224
3897.01113119397	14083
3898.5881188941	11714
3900.16511314389	11137
3901.74211364139	10216
3903.31811241095	9559
3904.89512449445	9523
3906.47214191997	10208
3908.04916438552	11674
3909.62619158916	12758
3911.20322322891	13421
3912.78025900282	13964
3914.35729860892	14748
3915.93434174525	15722
3917.51138810985	16496
3919.08843740074	17104
3920.66548931598	17504
3922.2425435536	17376
3923.81959981163	16464
3925.3966577881	14797
3926.97371718107	13137
3928.55077768856	12026
3930.12783900862	11821
3931.70490083927	11841
3933.28196287856	11490
3934.85902482452	10833
3936.43507866832	10352
3938.01213952227	9879
3939.58919937721	10236
3941.16625793116	10608
3942.74331488216	11292
3944.32036992825	11922
3945.89742276746	11820
3947.47447309783	11086
3949.05152061741	11641
3950.62856502422	12086
3952.2056060163	12050
3953.78264329169	11881
3955.35967654843	11786
3956.93670548455	11778
3958.51372979809	11511
3960.09074918709	10948
3961.66776334958	10120
3963.24477198361	9389
3964.82177478721	8463
3966.39877145841	7598
3967.97576169525	7235
3969.55274519577	7670
3971.12871400763	11043
3972.70568313441	14538
3974.28264461918	17016
3975.85959815996	18232
3977.4365434548	18832
3979.01348020174	18632
3980.59040809881	17240
3982.16732684405	15080
3983.74423613549	13394
3985.32113567117	12435
3986.89802514914	11661
3988.47490426742	11174
3990.05177272406	13882
3991.62863021708	16224
3993.20547644454	17928
3994.78231110446	19640
3996.35913389488	21056
3997.93594451384	21880
3999.51274265937	21960
4001.08952802952	21696
4002.66630032231	20824
4004.24205172645	19496
4005.8187969675	18344
4007.3955282255	17880
4008.97224519849	17480
4010.5489475845	16496
4012.12563508159	15235
4013.70230738777	14278
4015.27896420109	13548
4016.85560521959	12711
4018.4322301413	12421
4020.00883866426	15227
4021.58543048651	19056
4023.16200530608	23000
4024.73856282101	27272
4026.31510272934	32288
4027.8916247291	37584
4029.46812851834	42240
4031.04461379508	44904
4032.62108025737	45424
4034.19752760324	44016
4035.77395553073	41744
4037.35036373788	38792
4038.92574465238	35240
4040.50211252604	35552
4042.07845977366	40920
4043.65478609327	45784
4045.23109118292	48872
4046.80737474064	50240
4048.38363646447	49456
4049.95987605244	46152
4051.5360932026	51840
4053.11228761297	56072
4054.68845399697	58256
4056.26458848254	57784
4057.84069288953	53384
4059.4167690378	45920
4060.99281874721	37616
4062.56884383761	30448
4064.14484612887	25544
4065.72082744082	22552
4067.29678959334	20296
4068.87273440628	17936
4070.44866369949	16145
4072.02357232199	15139
4073.59947604232	14311
4075.17536970134	13165
4076.75125511888	12338
4078.32713411482	11996
4079.903008509	11945
4081.47888012129	11796
4083.05475077153	11775
4084.63062227959	11923
4086.20649646532	12158
4087.78237514858	12168
4089.35826014922	12190
4090.93415328709	12300
4092.51005638207	12679
4094.08597125399	14415
4095.66189972272	17576
4097.23784360811	21112
4098.81380473002	24568
4100.38978490831	26952
4101.96578596283	27208
4103.54180971343	26120
4105.11785797998	24888
4106.6929254971	23464
4108.26902823653	20840
4109.84516095031	16936
4111.42132545829	29880
4112.99752358033	62360
4114.57375713629	114688
4116.15002794601	186752
4117.72633782937	271872
4119.30268860621	364608
4120.87908209639	459840
4122.45552011977	543104
4124.0320044962	604352
4125.60853704554	636736
4127.18511958764	644864
4128.76175394237	628992
4130.33844192957	590144
4131.9151853691	527616
4133.49198608082	442624
4135.06884588458	342656
4136.64576660025	245824
4138.22275004767	168512
4139.79880766744	114128
4141.37597099195	77496
4142.9532295589	50944
4144.53058058354	32192
4146.10802128113	19864
4147.68554886694	12529
4149.26316055623	8237
4150.84085356425	5760
4152.41862510627	4094
4153.99647239755	2902
4155.57439265334	2188
4157.15238308892	1852
4158.73044091953	1684
4160.30856336044	1719
4161.88674762692	1822
4163.46499093421	1982
4165.04329049758	2136
4166.6216435323	2229
4168.20004725361	2312
4169.77849887679	2522
4171.35699561709	2725
4172.93553468978	2751
4174.51310462164	2554
4176.09171998227	2839
4177.67036932284	3068
4179.24904985862	2911
4180.82775880486	2469
4182.40649337683	1934
4183.98525078979	1620
4185.56402825899	1533
4187.14282299969	1272
4188.72163222717	1091
4190.30045315667	968
4191.87928300346	1080
4193.4581189828	1622
4195.03695830995	1959
4196.61579820016	2010
4198.19463586871	1928
4199.77346853085	1848
4201.35229340183	1800
4202.93110769693	1941
4204.5099086314	2830
4206.08869342051	3992
4207.66645049006	4889
4209.24519464898	5608
4210.82391431008	6101
4212.40260668864	6340
4213.98126899991	6427
4215.55989845916	6551
4217.13849228164	6489
4218.71704768262	6237
4220.29556187735	5705
4221.8740320811	6032
4223.45245550913	7069
4225.03082937669	7329
4226.60915089905	6847
4228.17415089905	5812
4229.73915089905	4742
4231.30415089905	3929
4232.86915089905	3246
4234.43415089905	2973
4235.99915089905	2620
4237.56415089905	2296
4239.12915089905	2214
4240.69415089905	2221
4242.25815089905	2112
4243.82315089905	1787
4245.38815089905	1594
4246.95315089905	1593
4248.51815089905	1604
4250.08315089905	1417
4251.64815089905	1264
4253.21315089905	1229
4254.77815089905	1307
4256.34315089905	1303
4257.90815089905	1247
4259.47315089905	1232
4261.03815089905	1225
4262.60315089905	1173
4264.16815089905	1066
4265.73315089905	989
4267.29815089905	884
4268.86315089905	826
4270.42815089905	765
4271.99315089905	819
4273.55815089905	908
4275.12215089905	959
4276.68715089905	933
4278.25215089905	903
4279.81715089905	862
4281.38215089905	785
4282.94715089905	692
4284.51215089905	567
4286.07715089905	405
4287.64215089905	300
4289.20715089905	293
4290.77215089905	344
4292.33715089905	323
4293.90215089905	334
4295.46715089905	343
4297.03215089905	295
4298.59715089905	264
4300.16215089905	251
4301.72715089905	230
4303.29215089905	216
4304.85715089905	170
4306.42215089905	145
4307.98715089905	145
4309.55115089905	145
4311.11615089905	162
4312.68115089905	190
4314.24615089905	173
4315.81115089905	159
4317.37615089905	156
4318.94115089905	160
4320.50615089905	166
4322.07115089905	190
4323.63615089905	216
4325.20115089905	268
4326.76615089905	298
4328.33115089905	366
4329.89615089905	442
4331.46115089905	440
4333.02615089905	310
4334.59115089905	284
4336.15615089905	260
4337.72115089905	280
4339.28615089905	295
4340.85115089905	230
4342.41515089905	230
4343.98015089905	205
4345.54515089905	203
4347.11015089905	229
4348.67515089905	224
4350.24015089905	213
4351.80515089905	242
4353.37015089905	243
4354.93515089905	230
4356.50015089905	205
4358.06515089905	188
4359.63015089905	150
4361.19515089905	180
4362.76015089905	209
4364.32515089905	218
4365.89015089905	228
4367.45515089905	275
4369.02015089905	326
4370.58515089905	336
4372.15015089905	327
4373.71515089905	270
4375.28015089905	237
4376.84415089905	317
4378.40915089905	297
4379.97415089905	292
4381.53915089905	282
4383.10415089905	265
4384.66915089905	244
4386.23415089905	254
4387.79915089905	242
4389.36415089905	245
4390.92915089905	282
4392.49415089905	272
4394.05915089905	261
4395.62415089905	267
4397.18915089905	296
4398.75415089905	311
4400.31915089905	359
4401.88415089905	497
4403.44915089905	574
4405.01415089905	551
4406.57915089905	473
4408.14415089905	405
4409.70815089905	419
4411.27315089905	388
4412.83815089905	383
4414.40315089905	394
4415.96815089905	438
4417.53315089905	583
4419.09815089905	705
4420.66315089905	736
4422.22815089905	736
4423.79315089905	799
4425.35815089905	882
4426.92315089905	865
4428.48815089905	820
4430.05315089905	815
4431.61815089905	910
4433.18315089905	959
4434.74815089905	946
4436.31315089905	955
4437.87815089905	950
4439.44315089905	982
4441.00815089905	1030
4442.57315089905	1127
4444.13715089905	1218
4445.70215089905	1305
4447.26715089905	1363
4448.83215089905	1532
4450.39715089905	1694
4451.96215089905	1884
4453.52715089905	2156
4455.09215089905	2656
4456.65715089905	3328
4458.22215089905	3827
4459.78715089905	3969
4461.35215089905	4055
4462.91715089905	4262
4464.48215089905	4494
4466.04715089905	4501
4467.61215089905	4404
4469.17715089905	4316
4470.74215089905	4290
4472.30715089905	4232
4473.87215089905	4215
4475.43715089905	4443
4477.00115089905	4838
4478.56615089905	4920
4480.13115089905	4294
4481.69615089905	3437
4483.26115089905	3096
4484.82615089905	3312
4486.39115089905	3426
4487.95615089905	3085
4489.52115089905	2750
4491.08615089905	2696
4492.65115089905	2920
4494.21615089905	3119
4495.78115089905	3322
4497.34615089905	3463
4498.91115089905	3313
4500.47615089905	3184
4502.04115089905	3217
4503.60615089905	3240
4505.17115089905	3288
4506.73615089905	3582
4508.30115089905	3963

galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (205/gxy-646g9) tool_stderr: [WARNING]         multiqc : MultiQC Version v1.27.1 now available!
[INFO   ]         multiqc : This is MultiQC v1.9
[INFO   ]         multiqc : Template    : default
[INFO   ]         multiqc : Searching   : /galaxy/server/database/jobs_directory/000/205/working/multiqc_WDir
[INFO   ]  custom_content : section_0: Found 2 samples (linegraph)
[INFO   ]         multiqc : Compressing plot data
[INFO   ]         multiqc : Report      : report.html
[INFO   ]         multiqc : Data        : report_data
[INFO   ]         multiqc : MultiQC complete

galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (205/gxy-646g9) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (205/gxy-646g9) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-646g9.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 205 (gxy-646g9)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-646g9 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 205 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:02,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (205/gxy-646g9) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-02 06:50:03,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 207, 206
tpv.core.entities DEBUG 2025-03-02 06:50:03,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:50:03,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:50:03,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:50:03,866 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:50:03,878 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Working directory for job is: /galaxy/server/database/jobs_directory/000/206
galaxy.jobs.runners DEBUG 2025-03-02 06:50:03,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [206] queued (30.855 ms)
galaxy.jobs.handler INFO 2025-03-02 06:50:03,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:03,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 206
tpv.core.entities DEBUG 2025-03-02 06:50:03,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:50:03,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:50:03,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:50:03,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:50:03,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Working directory for job is: /galaxy/server/database/jobs_directory/000/207
galaxy.jobs.runners DEBUG 2025-03-02 06:50:03,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [207] queued (42.715 ms)
galaxy.jobs.handler INFO 2025-03-02 06:50:03,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:03,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 207
galaxy.jobs DEBUG 2025-03-02 06:50:04,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [206] prepared (114.634 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:50:04,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/206/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/206/registry.xml' '/galaxy/server/database/jobs_directory/000/206/upload_params.json' '229:/galaxy/server/database/objects/6/c/d/dataset_6cd18600-3756-49fc-9a17-c6ac7d5d0709_files:/galaxy/server/database/objects/6/c/d/dataset_6cd18600-3756-49fc-9a17-c6ac7d5d0709.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:50:04,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (206) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/206/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/206/galaxy_206.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:50:04,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [207] prepared (99.060 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:04,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 206 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:04,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 206 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:50:04,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/207/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/207/registry.xml' '/galaxy/server/database/jobs_directory/000/207/upload_params.json' '230:/galaxy/server/database/objects/3/3/4/dataset_334422db-e7cf-453e-a88b-cd9ec02edc0a_files:/galaxy/server/database/objects/3/3/4/dataset_334422db-e7cf-453e-a88b-cd9ec02edc0a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:50:04,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (207) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/207/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/207/galaxy_207.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:04,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 207 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:04,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 207 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:05,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:05,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:14,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rl9mr with k8s id: gxy-rl9mr succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:14,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gpzr8 with k8s id: gxy-gpzr8 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:50:14,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 207: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:50:14,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 206: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:50:22,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 206 finished
galaxy.model.metadata DEBUG 2025-03-02 06:50:22,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 229
galaxy.jobs INFO 2025-03-02 06:50:22,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 206 in /galaxy/server/database/jobs_directory/000/206
galaxy.jobs DEBUG 2025-03-02 06:50:22,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 206 executed (88.837 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:22,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 206 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:50:22,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 207 finished
galaxy.model.metadata DEBUG 2025-03-02 06:50:22,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 230
galaxy.jobs INFO 2025-03-02 06:50:23,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 207 in /galaxy/server/database/jobs_directory/000/207
galaxy.jobs DEBUG 2025-03-02 06:50:23,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 207 executed (116.395 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:23,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 207 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:50:24,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 208
tpv.core.entities DEBUG 2025-03-02 06:50:24,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:50:24,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:50:24,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:50:24,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:50:24,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Working directory for job is: /galaxy/server/database/jobs_directory/000/208
galaxy.jobs.runners DEBUG 2025-03-02 06:50:24,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [208] queued (30.185 ms)
galaxy.jobs.handler INFO 2025-03-02 06:50:24,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:24,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 208
galaxy.jobs DEBUG 2025-03-02 06:50:24,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [208] prepared (61.593 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:50:24,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:50:24,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:50:24,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:50:24,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/208/tool_script.sh] for tool command [multiqc --version > /galaxy/server/database/jobs_directory/000/208/outputs/COMMAND_VERSION 2>&1;
die() { echo "$@" 1>&2 ; exit 1; } &&  mkdir multiqc_WDir &&   mkdir multiqc_WDir/fastqc_0 &&    mkdir 'multiqc_WDir/fastqc_0/data_0' &&  mkdir 'multiqc_WDir/fastqc_0/data_0/file_0' && ln -s '/galaxy/server/database/objects/6/c/d/dataset_6cd18600-3756-49fc-9a17-c6ac7d5d0709.dat' 'multiqc_WDir/fastqc_0/data_0/file_0/fastqc_data.txt' && mkdir 'multiqc_WDir/fastqc_0/data_0/file_1' && ln -s '/galaxy/server/database/objects/3/3/4/dataset_334422db-e7cf-453e-a88b-cd9ec02edc0a.dat' 'multiqc_WDir/fastqc_0/data_0/file_1/fastqc_data.txt' &&  multiqc multiqc_WDir --filename "report"  --title "Title of the report" --comment "Commment for the report"  --flat --export]
galaxy.jobs.runners DEBUG 2025-03-02 06:50:24,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (208) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/208/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/208/galaxy_208.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/208/working/report.html" -a -f "/galaxy/server/database/objects/9/e/e/dataset_9ee11509-2f7e-4e28-9023-322342988d1b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/208/working/report.html" "/galaxy/server/database/objects/9/e/e/dataset_9ee11509-2f7e-4e28-9023-322342988d1b.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:24,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 208 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:50:24,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:50:24,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:50:24,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:24,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 208 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:24,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:38,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nvg68 with k8s id: gxy-nvg68 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:50:39,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 208: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:50:46,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 208 finished
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/mqc_fastqc_per_base_n_content_plot_1.txt] with element identifier [fastqc_per_base_n_content_plot_1] for output [plots] (3.728 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/mqc_fastqc_per_base_sequence_quality_plot_1.txt] with element identifier [fastqc_per_base_sequence_quality_plot_1] for output [plots] (0.521 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/mqc_fastqc_per_sequence_gc_content_plot_Counts.txt] with element identifier [fastqc_per_sequence_gc_content_plot_Counts] for output [plots] (0.481 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/mqc_fastqc_per_sequence_gc_content_plot_Percentages.txt] with element identifier [fastqc_per_sequence_gc_content_plot_Percentages] for output [plots] (0.417 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/mqc_fastqc_per_sequence_quality_scores_plot_1.txt] with element identifier [fastqc_per_sequence_quality_scores_plot_1] for output [plots] (0.408 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/mqc_fastqc_sequence_counts_plot_1.txt] with element identifier [fastqc_sequence_counts_plot_1] for output [plots] (0.632 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/mqc_fastqc_sequence_duplication_levels_plot_1.txt] with element identifier [fastqc_sequence_duplication_levels_plot_1] for output [plots] (0.408 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Add dynamic collection datasets to history for output [plots] (85.583 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/multiqc_fastqc.txt] with element identifier [fastqc] for output [stats] (0.659 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/multiqc_general_stats.txt] with element identifier [general_stats] for output [stats] (0.530 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/208/working/report_data/multiqc_sources.txt] with element identifier [sources] for output [stats] (0.438 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:50:46,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) Add dynamic collection datasets to history for output [stats] (31.975 ms)
galaxy.model.metadata DEBUG 2025-03-02 06:50:46,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 231
galaxy.jobs INFO 2025-03-02 06:50:46,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 208 in /galaxy/server/database/jobs_directory/000/208
galaxy.jobs DEBUG 2025-03-02 06:50:46,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 208 executed (319.293 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:46,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 208 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:50:48,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 209
tpv.core.entities DEBUG 2025-03-02 06:50:48,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:50:48,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:50:48,878 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:50:48,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:50:48,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Working directory for job is: /galaxy/server/database/jobs_directory/000/209
galaxy.jobs.runners DEBUG 2025-03-02 06:50:48,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [209] queued (33.851 ms)
galaxy.jobs.handler INFO 2025-03-02 06:50:48,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:48,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 209
galaxy.jobs DEBUG 2025-03-02 06:50:48,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [209] prepared (72.500 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:50:49,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/209/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/209/registry.xml' '/galaxy/server/database/jobs_directory/000/209/upload_params.json' '242:/galaxy/server/database/objects/c/3/5/dataset_c357c214-1b0b-4cbe-b48f-8206d3cb8211_files:/galaxy/server/database/objects/c/3/5/dataset_c357c214-1b0b-4cbe-b48f-8206d3cb8211.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:50:49,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (209) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/209/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/209/galaxy_209.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:49,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 209 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:49,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 209 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:50,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:50:59,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dmtm6 with k8s id: gxy-dmtm6 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:50:59,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 209: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:51:06,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 209 finished
galaxy.model.metadata DEBUG 2025-03-02 06:51:07,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 242
galaxy.jobs INFO 2025-03-02 06:51:07,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 209 in /galaxy/server/database/jobs_directory/000/209
galaxy.jobs DEBUG 2025-03-02 06:51:07,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 209 executed (125.500 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:07,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 209 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:51:08,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 210
tpv.core.entities DEBUG 2025-03-02 06:51:08,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:51:08,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:51:08,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:51:08,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:51:08,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Working directory for job is: /galaxy/server/database/jobs_directory/000/210
galaxy.jobs.runners DEBUG 2025-03-02 06:51:08,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [210] queued (33.408 ms)
galaxy.jobs.handler INFO 2025-03-02 06:51:08,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:08,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 210
galaxy.jobs DEBUG 2025-03-02 06:51:08,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [210] prepared (52.006 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:08,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:51:08,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:08,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:51:08,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/210/tool_script.sh] for tool command [multiqc --version > /galaxy/server/database/jobs_directory/000/210/outputs/COMMAND_VERSION 2>&1;
die() { echo "$@" 1>&2 ; exit 1; } &&  mkdir multiqc_WDir &&   mkdir multiqc_WDir/pycoqc_0 &&         grep -q '"pycoqc":' /galaxy/server/database/objects/c/3/5/dataset_c357c214-1b0b-4cbe-b48f-8206d3cb8211.dat || die "Module 'pycoqc: '"pycoqc":' not found in the file 'pycoqc_json'" && ln -s '/galaxy/server/database/objects/c/3/5/dataset_c357c214-1b0b-4cbe-b48f-8206d3cb8211.dat' 'multiqc_WDir/pycoqc_0/pycoqc_json'  &&    multiqc multiqc_WDir --filename "report"  --title "Title of the report" --comment "Commment for the report"]
galaxy.jobs.runners DEBUG 2025-03-02 06:51:08,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (210) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/210/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/210/galaxy_210.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/210/working/report.html" -a -f "/galaxy/server/database/objects/b/4/8/dataset_b48dd67d-8f36-4650-ba8d-1f29982aaccf.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/210/working/report.html" "/galaxy/server/database/objects/b/4/8/dataset_b48dd67d-8f36-4650-ba8d-1f29982aaccf.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:08,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 210 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:08,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:51:08,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1: multiqc:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:08,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/multiqc:1.9--py_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:08,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 210 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:09,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:15,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hj8qj with k8s id: gxy-hj8qj succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:51:15,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 210: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:51:22,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 210 finished
galaxy.model.store.discover DEBUG 2025-03-02 06:51:22,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (210) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/210/working/report_data/multiqc_general_stats.txt] with element identifier [general_stats] for output [stats] (5.629 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:51:22,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (210) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/210/working/report_data/multiqc_sources.txt] with element identifier [sources] for output [stats] (0.881 ms)
galaxy.model.store.discover DEBUG 2025-03-02 06:51:23,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (210) Add dynamic collection datasets to history for output [stats] (24.970 ms)
galaxy.model.metadata DEBUG 2025-03-02 06:51:23,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 243
galaxy.jobs INFO 2025-03-02 06:51:23,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 210 in /galaxy/server/database/jobs_directory/000/210
galaxy.jobs DEBUG 2025-03-02 06:51:23,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 210 executed (153.366 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:23,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 210 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:51:26,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 211
tpv.core.entities DEBUG 2025-03-02 06:51:26,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:51:26,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:51:26,660 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:51:26,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:51:26,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Working directory for job is: /galaxy/server/database/jobs_directory/000/211
galaxy.jobs.runners DEBUG 2025-03-02 06:51:26,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [211] queued (27.528 ms)
galaxy.jobs.handler INFO 2025-03-02 06:51:26,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:26,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 211
galaxy.jobs DEBUG 2025-03-02 06:51:26,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [211] prepared (70.951 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:51:26,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/211/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/211/registry.xml' '/galaxy/server/database/jobs_directory/000/211/upload_params.json' '246:/galaxy/server/database/objects/8/f/6/dataset_8f6c6813-8c80-42fa-8bcb-2c4c1d02a12a_files:/galaxy/server/database/objects/8/f/6/dataset_8f6c6813-8c80-42fa-8bcb-2c4c1d02a12a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:51:26,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (211) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/211/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/211/galaxy_211.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:26,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 211 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:26,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 211 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:27,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:36,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-28xmw with k8s id: gxy-28xmw succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:51:36,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 211: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:51:43,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 211 finished
galaxy.model.metadata DEBUG 2025-03-02 06:51:43,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 246
galaxy.jobs INFO 2025-03-02 06:51:43,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 211 in /galaxy/server/database/jobs_directory/000/211
galaxy.jobs DEBUG 2025-03-02 06:51:44,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 211 executed (118.656 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:44,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 211 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:51:45,010 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 212
tpv.core.entities DEBUG 2025-03-02 06:51:45,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/.*, abstract=False, cores=2, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:51:45,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:51:45,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:51:45,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:51:45,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Working directory for job is: /galaxy/server/database/jobs_directory/000/212
galaxy.jobs.runners DEBUG 2025-03-02 06:51:45,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [212] queued (42.541 ms)
galaxy.jobs.handler INFO 2025-03-02 06:51:45,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:45,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 212
galaxy.jobs DEBUG 2025-03-02 06:51:45,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [212] prepared (58.982 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:45,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:51:45,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/1.43.0+galaxy0: nanoplot:1.43.0
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:45,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/nanoplot:1.43.0--pyhdfd78af_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:51:45,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/212/tool_script.sh] for tool command [NanoPlot --version > /galaxy/server/database/jobs_directory/000/212/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/8/f/6/dataset_8f6c6813-8c80-42fa-8bcb-2c4c1d02a12a.dat' './read_0.fastq.gz' &&   NanoPlot --threads ${GALAXY_SLOTS:-4} --tsv_stats --no_static --fastq_rich read_0.fastq.gz --downsample 800 --plots kde        -o '.' && >&2 cat *log]
galaxy.jobs.runners DEBUG 2025-03-02 06:51:45,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (212) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/212/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/212/galaxy_212.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/212/working/NanoPlot-report.html" -a -f "/galaxy/server/database/objects/1/3/1/dataset_131e1494-1270-4f4c-8e38-1c9335896630.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/212/working/NanoPlot-report.html" "/galaxy/server/database/objects/1/3/1/dataset_131e1494-1270-4f4c-8e38-1c9335896630.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/212/working/NanoStats.txt" -a -f "/galaxy/server/database/objects/6/2/9/dataset_62901691-d353-4d12-8ea8-2aee03e16db6.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/212/working/NanoStats.txt" "/galaxy/server/database/objects/6/2/9/dataset_62901691-d353-4d12-8ea8-2aee03e16db6.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/212/working/NanoStats_post_filtering.txt" -a -f "/galaxy/server/database/objects/9/0/4/dataset_90400d63-9cb6-403f-ae5c-4a89091d2a46.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/212/working/NanoStats_post_filtering.txt" "/galaxy/server/database/objects/9/0/4/dataset_90400d63-9cb6-403f-ae5c-4a89091d2a46.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:45,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 212 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:45,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:51:45,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/1.43.0+galaxy0: nanoplot:1.43.0
galaxy.tool_util.deps.containers INFO 2025-03-02 06:51:45,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/nanoplot:1.43.0--pyhdfd78af_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:45,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 212 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:51:45,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j9vps failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-j9vps.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 212 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:52:35,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-j9vps

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-j9vps": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (212/gxy-j9vps) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (212/gxy-j9vps) tool_stderr: 2025-03-02 06:52:23,065 NanoPlot 1.43.0 started with arguments Namespace(threads=2, verbose=False, store=False, raw=False, huge=False, outdir='.', no_static=True, prefix='', tsv_stats=True, only_report=False, info_in_report=False, maxlength=None, minlength=None, drop_outliers=False, downsample=800, loglength=False, percentqual=False, alength=False, minqual=None, runtime_until=None, readtype='1D', barcoded=False, no_supplementary=False, color='#4CB391', colormap='Greens', format=['png'], plots=['kde'], legacy=None, listcolors=False, listcolormaps=False, no_N50=False, N50=False, title=None, font_scale=1, dpi=100, hide_stats=False, fastq=None, fasta=None, fastq_rich=['read_0.fastq.gz'], fastq_minimal=None, summary=None, bam=None, ubam=None, cram=None, pickle=None, feather=None, path='./')
2025-03-02 06:52:23,065 Python version is: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:36:51) [GCC 12.4.0]
2025-03-02 06:52:23,100 Nanoget: Starting to collect statistics from rich fastq file.
2025-03-02 06:52:23,102 Nanoget: Decompressing gzipped fastq read_0.fastq.gz
2025-03-02 06:52:23,714 Reduced DataFrame memory usage from 0.042937278747558594Mb to 0.042937278747558594Mb
2025-03-02 06:52:23,738 Nanoget: Gathered all metrics of 371 reads
2025-03-02 06:52:23,756 Calculated statistics
2025-03-02 06:52:23,757 Using sequenced read lengths for plotting.
2025-03-02 06:52:23,758 Downsampling the dataset from 371 to 371 reads
2025-03-02 06:52:23,770 Calculated statistics
2025-03-02 06:52:23,771 NanoPlot:  Valid color #4CB391.
2025-03-02 06:52:23,771 NanoPlot:  Valid colormap Greens.
2025-03-02 06:52:23,772 NanoPlot:  Creating length plots for Read length.
2025-03-02 06:52:23,772 NanoPlot:  Using 371 reads maximum of 393431bp.
2025-03-02 06:52:24,419 Created length plots
2025-03-02 06:52:24,420 NanoPlot: Creating Read lengths vs Average read quality plots using 371 reads.
2025-03-02 06:52:24,443 Created LengthvsQual plot
2025-03-02 06:52:24,443 Nanoplotter: Creating heatmap of reads per channel using 371 reads.
2025-03-02 06:52:24,457 Created spatialheatmap for succesfull basecalls.
2025-03-02 06:52:24,457 Nanoplotter: Creating timeplots using 371 (full) or 371 (subsampled dataset) reads.
2025-03-02 06:52:24,626 Created timeplots.
2025-03-02 06:52:24,626 Writing html report.
2025-03-02 06:52:24,644 Finished!

galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (212/gxy-j9vps) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (212/gxy-j9vps) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-j9vps.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 212 (gxy-j9vps)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-j9vps to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 212 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:35,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (212/gxy-j9vps) Terminated at user's request
galaxy.util WARNING 2025-03-02 06:52:35,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/3/1/dataset_131e1494-1270-4f4c-8e38-1c9335896630.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/3/1/dataset_131e1494-1270-4f4c-8e38-1c9335896630.dat'
galaxy.util WARNING 2025-03-02 06:52:35,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/2/9/dataset_62901691-d353-4d12-8ea8-2aee03e16db6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/2/9/dataset_62901691-d353-4d12-8ea8-2aee03e16db6.dat'
galaxy.util WARNING 2025-03-02 06:52:35,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/0/4/dataset_90400d63-9cb6-403f-ae5c-4a89091d2a46.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/0/4/dataset_90400d63-9cb6-403f-ae5c-4a89091d2a46.dat'
galaxy.jobs.handler DEBUG 2025-03-02 06:52:38,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 213
tpv.core.entities DEBUG 2025-03-02 06:52:38,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:52:38,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (213) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:52:38,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (213) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:52:38,099 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (213) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:52:38,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (213) Working directory for job is: /galaxy/server/database/jobs_directory/000/213
galaxy.jobs.runners DEBUG 2025-03-02 06:52:38,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [213] queued (41.210 ms)
galaxy.jobs.handler INFO 2025-03-02 06:52:38,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (213) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:38,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 213
galaxy.jobs DEBUG 2025-03-02 06:52:38,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [213] prepared (79.478 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:52:38,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/213/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/213/registry.xml' '/galaxy/server/database/jobs_directory/000/213/upload_params.json' '250:/galaxy/server/database/objects/9/8/2/dataset_982544cc-d7fb-424d-9896-93096858dcef_files:/galaxy/server/database/objects/9/8/2/dataset_982544cc-d7fb-424d-9896-93096858dcef.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:52:38,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (213) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/213/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/213/galaxy_213.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:38,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 213 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:38,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 213 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:38,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:48,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fzts7 with k8s id: gxy-fzts7 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:52:49,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 213: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:52:56,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 213 finished
galaxy.model.metadata DEBUG 2025-03-02 06:52:56,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 250
galaxy.jobs INFO 2025-03-02 06:52:56,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 213 in /galaxy/server/database/jobs_directory/000/213
galaxy.jobs DEBUG 2025-03-02 06:52:57,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 213 executed (104.509 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:57,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 213 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:52:57,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 214
tpv.core.entities DEBUG 2025-03-02 06:52:57,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/.*, abstract=False, cores=2, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:52:57,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (214) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:52:57,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (214) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:52:57,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (214) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:52:57,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (214) Working directory for job is: /galaxy/server/database/jobs_directory/000/214
galaxy.jobs.runners DEBUG 2025-03-02 06:52:57,545 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [214] queued (37.505 ms)
galaxy.jobs.handler INFO 2025-03-02 06:52:57,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (214) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:57,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 214
galaxy.jobs DEBUG 2025-03-02 06:52:57,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [214] prepared (55.079 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:52:57,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:52:57,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/1.43.0+galaxy0: nanoplot:1.43.0
galaxy.tool_util.deps.containers INFO 2025-03-02 06:52:57,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/nanoplot:1.43.0--pyhdfd78af_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:52:57,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/214/tool_script.sh] for tool command [NanoPlot --version > /galaxy/server/database/jobs_directory/000/214/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/9/8/2/dataset_982544cc-d7fb-424d-9896-93096858dcef.dat' './read_0.bam' && ln -s '/galaxy/server/database/objects/_metadata_files/f/5/0/metadata_f502e605-4f80-4c16-88c9-05c3003be43f.dat' './read_0.bam.bai' &&   NanoPlot --threads ${GALAXY_SLOTS:-4} --tsv_stats --no_static --bam read_0.bam --maxlength 2000 --color yellow        -o '.' && >&2 cat *log]
galaxy.jobs.runners DEBUG 2025-03-02 06:52:57,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (214) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/214/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/214/galaxy_214.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/214/working/NanoPlot-report.html" -a -f "/galaxy/server/database/objects/0/1/6/dataset_016f2ff8-4ef4-4e35-ad6e-f361c8649e8b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/214/working/NanoPlot-report.html" "/galaxy/server/database/objects/0/1/6/dataset_016f2ff8-4ef4-4e35-ad6e-f361c8649e8b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/214/working/NanoStats.txt" -a -f "/galaxy/server/database/objects/9/9/5/dataset_99550b13-a545-447f-95f3-462de031b183.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/214/working/NanoStats.txt" "/galaxy/server/database/objects/9/9/5/dataset_99550b13-a545-447f-95f3-462de031b183.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/214/working/NanoStats_post_filtering.txt" -a -f "/galaxy/server/database/objects/0/3/9/dataset_039c53c9-2881-4cae-988b-1ea5ca8c3b85.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/214/working/NanoStats_post_filtering.txt" "/galaxy/server/database/objects/0/3/9/dataset_039c53c9-2881-4cae-988b-1ea5ca8c3b85.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:57,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 214 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:52:57,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:52:57,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/1.43.0+galaxy0: nanoplot:1.43.0
galaxy.tool_util.deps.containers INFO 2025-03-02 06:52:57,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/nanoplot:1.43.0--pyhdfd78af_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:57,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 214 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:52:57,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:09,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hfqm4 with k8s id: gxy-hfqm4 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:53:09,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 214: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:53:16,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 214 finished
galaxy.model.metadata DEBUG 2025-03-02 06:53:16,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 251
galaxy.model.metadata DEBUG 2025-03-02 06:53:16,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 252
galaxy.model.metadata DEBUG 2025-03-02 06:53:16,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 253
galaxy.util WARNING 2025-03-02 06:53:16,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/1/6/dataset_016f2ff8-4ef4-4e35-ad6e-f361c8649e8b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/1/6/dataset_016f2ff8-4ef4-4e35-ad6e-f361c8649e8b.dat'
galaxy.util WARNING 2025-03-02 06:53:16,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/9/5/dataset_99550b13-a545-447f-95f3-462de031b183.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/9/5/dataset_99550b13-a545-447f-95f3-462de031b183.dat'
galaxy.jobs INFO 2025-03-02 06:53:17,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 214 in /galaxy/server/database/jobs_directory/000/214
galaxy.jobs DEBUG 2025-03-02 06:53:17,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 214 executed (189.390 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:17,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 214 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:53:18,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 215, 216
tpv.core.entities DEBUG 2025-03-02 06:53:18,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:53:18,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (215) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:53:18,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (215) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:53:18,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (215) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:53:19,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (215) Working directory for job is: /galaxy/server/database/jobs_directory/000/215
galaxy.jobs.runners DEBUG 2025-03-02 06:53:19,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [215] queued (32.587 ms)
galaxy.jobs.handler INFO 2025-03-02 06:53:19,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (215) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:19,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 215
tpv.core.entities DEBUG 2025-03-02 06:53:19,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:53:19,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (216) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:53:19,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (216) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:53:19,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (216) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:53:19,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (216) Working directory for job is: /galaxy/server/database/jobs_directory/000/216
galaxy.jobs.runners DEBUG 2025-03-02 06:53:19,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [216] queued (45.943 ms)
galaxy.jobs.handler INFO 2025-03-02 06:53:19,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (216) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:19,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 216
galaxy.jobs DEBUG 2025-03-02 06:53:19,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [215] prepared (98.379 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:53:19,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/215/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/215/registry.xml' '/galaxy/server/database/jobs_directory/000/215/upload_params.json' '254:/galaxy/server/database/objects/4/0/7/dataset_4075b0f1-53d9-45a6-b13a-4fa0cb98d3a6_files:/galaxy/server/database/objects/4/0/7/dataset_4075b0f1-53d9-45a6-b13a-4fa0cb98d3a6.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:53:19,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (215) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/215/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/215/galaxy_215.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:53:19,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [216] prepared (86.495 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:19,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 215 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:19,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 215 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:53:19,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/216/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/216/registry.xml' '/galaxy/server/database/jobs_directory/000/216/upload_params.json' '255:/galaxy/server/database/objects/3/d/a/dataset_3dafe580-c9a8-4efa-b7af-f7f3f5ccc09e_files:/galaxy/server/database/objects/3/d/a/dataset_3dafe580-c9a8-4efa-b7af-f7f3f5ccc09e.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:53:19,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (216) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/216/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/216/galaxy_216.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:19,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 216 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:19,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 216 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:20,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:20,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:29,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kp2gg with k8s id: gxy-kp2gg succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:29,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-k2j96 with k8s id: gxy-k2j96 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:53:29,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 215: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:53:29,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 216: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:53:37,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 215 finished
galaxy.jobs.runners DEBUG 2025-03-02 06:53:37,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 216 finished
galaxy.model.metadata DEBUG 2025-03-02 06:53:37,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 254
galaxy.model.metadata DEBUG 2025-03-02 06:53:37,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 255
galaxy.jobs INFO 2025-03-02 06:53:37,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 215 in /galaxy/server/database/jobs_directory/000/215
galaxy.jobs INFO 2025-03-02 06:53:37,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 216 in /galaxy/server/database/jobs_directory/000/216
galaxy.jobs DEBUG 2025-03-02 06:53:37,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 215 executed (131.946 ms)
galaxy.jobs DEBUG 2025-03-02 06:53:37,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 216 executed (117.691 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:37,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 215 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:37,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 216 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:53:38,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 217
tpv.core.entities DEBUG 2025-03-02 06:53:38,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/.*, abstract=False, cores=2, mem=12, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:53:38,473 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (217) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:53:38,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (217) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:53:38,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (217) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:53:38,506 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (217) Working directory for job is: /galaxy/server/database/jobs_directory/000/217
galaxy.jobs.runners DEBUG 2025-03-02 06:53:38,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [217] queued (38.772 ms)
galaxy.jobs.handler INFO 2025-03-02 06:53:38,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (217) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:38,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 217
galaxy.jobs DEBUG 2025-03-02 06:53:38,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [217] prepared (50.115 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:53:38,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:53:38,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/1.43.0+galaxy0: nanoplot:1.43.0
galaxy.tool_util.deps.containers INFO 2025-03-02 06:53:38,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/nanoplot:1.43.0--pyhdfd78af_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:53:38,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/217/tool_script.sh] for tool command [NanoPlot --version > /galaxy/server/database/jobs_directory/000/217/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/4/0/7/dataset_4075b0f1-53d9-45a6-b13a-4fa0cb98d3a6.dat' './read_0.fasta' &&  ln -s '/galaxy/server/database/objects/3/d/a/dataset_3dafe580-c9a8-4efa-b7af-f7f3f5ccc09e.dat' './read_1.fasta' &&   NanoPlot --threads ${GALAXY_SLOTS:-4} --tsv_stats --no_static --fasta read_0.fasta read_1.fasta        -o '.' && >&2 cat *log]
galaxy.jobs.runners DEBUG 2025-03-02 06:53:38,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (217) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/217/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/217/galaxy_217.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/217/working/NanoPlot-report.html" -a -f "/galaxy/server/database/objects/8/e/5/dataset_8e5cf6be-3783-4290-a60b-666fa225266b.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/217/working/NanoPlot-report.html" "/galaxy/server/database/objects/8/e/5/dataset_8e5cf6be-3783-4290-a60b-666fa225266b.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/217/working/NanoStats.txt" -a -f "/galaxy/server/database/objects/6/a/f/dataset_6aff2b84-6fbf-4459-91d8-4f4ae11bec18.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/217/working/NanoStats.txt" "/galaxy/server/database/objects/6/a/f/dataset_6aff2b84-6fbf-4459-91d8-4f4ae11bec18.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/217/working/NanoStats_post_filtering.txt" -a -f "/galaxy/server/database/objects/a/c/0/dataset_ac07c3bc-c8f4-44e2-9a71-9e3a9a4576d0.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/217/working/NanoStats_post_filtering.txt" "/galaxy/server/database/objects/a/c/0/dataset_ac07c3bc-c8f4-44e2-9a71-9e3a9a4576d0.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:38,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 217 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:53:38,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:53:38,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/nanoplot/nanoplot/1.43.0+galaxy0: nanoplot:1.43.0
galaxy.tool_util.deps.containers INFO 2025-03-02 06:53:38,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/nanoplot:1.43.0--pyhdfd78af_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:38,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 217 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:39,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:47,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fltmw with k8s id: gxy-fltmw succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:53:48,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 217: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:53:55,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 217 finished
galaxy.model.metadata DEBUG 2025-03-02 06:53:55,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 256
galaxy.model.metadata DEBUG 2025-03-02 06:53:55,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 257
galaxy.model.metadata DEBUG 2025-03-02 06:53:55,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 258
galaxy.util WARNING 2025-03-02 06:53:55,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/e/5/dataset_8e5cf6be-3783-4290-a60b-666fa225266b.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/e/5/dataset_8e5cf6be-3783-4290-a60b-666fa225266b.dat'
galaxy.util WARNING 2025-03-02 06:53:55,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/a/f/dataset_6aff2b84-6fbf-4459-91d8-4f4ae11bec18.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/a/f/dataset_6aff2b84-6fbf-4459-91d8-4f4ae11bec18.dat'
galaxy.jobs INFO 2025-03-02 06:53:55,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 217 in /galaxy/server/database/jobs_directory/000/217
galaxy.jobs DEBUG 2025-03-02 06:53:55,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 217 executed (193.417 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:55,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 217 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:53:57,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 218
tpv.core.entities DEBUG 2025-03-02 06:53:57,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:53:57,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (218) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:53:57,984 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (218) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:53:57,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (218) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:53:58,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (218) Working directory for job is: /galaxy/server/database/jobs_directory/000/218
galaxy.jobs.runners DEBUG 2025-03-02 06:53:58,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [218] queued (31.479 ms)
galaxy.jobs.handler INFO 2025-03-02 06:53:58,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (218) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:58,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 218
galaxy.jobs DEBUG 2025-03-02 06:53:58,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [218] prepared (73.573 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:53:58,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/218/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/218/registry.xml' '/galaxy/server/database/jobs_directory/000/218/upload_params.json' '259:/galaxy/server/database/objects/b/a/d/dataset_baddf08e-4ca3-4f64-be5b-e8e0bba0e7b8_files:/galaxy/server/database/objects/b/a/d/dataset_baddf08e-4ca3-4f64-be5b-e8e0bba0e7b8.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:53:58,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (218) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/218/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/218/galaxy_218.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:58,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 218 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:58,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 218 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:53:58,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:08,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-kvvs4 with k8s id: gxy-kvvs4 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:54:08,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 218: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:54:15,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 218 finished
galaxy.model.metadata DEBUG 2025-03-02 06:54:15,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 259
galaxy.jobs INFO 2025-03-02 06:54:15,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 218 in /galaxy/server/database/jobs_directory/000/218
galaxy.jobs DEBUG 2025-03-02 06:54:15,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 218 executed (96.055 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:15,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 218 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:54:16,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 219
tpv.core.entities DEBUG 2025-03-02 06:54:16,371 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_extract/poretools_extract/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:54:16,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (219) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:54:16,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (219) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:54:16,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (219) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:54:16,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (219) Working directory for job is: /galaxy/server/database/jobs_directory/000/219
galaxy.jobs.runners DEBUG 2025-03-02 06:54:16,399 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [219] queued (24.680 ms)
galaxy.jobs.handler INFO 2025-03-02 06:54:16,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (219) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:16,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 219
galaxy.jobs DEBUG 2025-03-02 06:54:16,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [219] prepared (44.759 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:54:16,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:54:16,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_extract/poretools_extract/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-02 06:54:16,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:54:16,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/219/tool_script.sh] for tool command [poretools fastq --type all --min-length 0 --max-length 1000000000  '/galaxy/server/database/objects/b/a/d/dataset_baddf08e-4ca3-4f64-be5b-e8e0bba0e7b8.dat' > '/galaxy/server/database/objects/7/9/1/dataset_79119482-9325-42cd-9940-492c2a148005.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:54:16,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (219) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/219/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/219/galaxy_219.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:16,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 219 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:54:16,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:54:16,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_extract/poretools_extract/0.6.1a1.0: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-03-02 06:54:16,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:16,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 219 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:17,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:43,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vfxg9 with k8s id: gxy-vfxg9 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:54:43,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 219: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:54:51,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 219 finished
galaxy.model.metadata DEBUG 2025-03-02 06:54:51,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 260
galaxy.jobs INFO 2025-03-02 06:54:51,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 219 in /galaxy/server/database/jobs_directory/000/219
galaxy.jobs DEBUG 2025-03-02 06:54:51,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 219 executed (106.483 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:51,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 219 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:54:53,067 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 220
tpv.core.entities DEBUG 2025-03-02 06:54:53,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:54:53,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (220) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:54:53,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (220) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:54:53,105 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (220) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:54:53,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (220) Working directory for job is: /galaxy/server/database/jobs_directory/000/220
galaxy.jobs.runners DEBUG 2025-03-02 06:54:53,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [220] queued (28.673 ms)
galaxy.jobs.handler INFO 2025-03-02 06:54:53,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (220) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:53,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 220
galaxy.jobs DEBUG 2025-03-02 06:54:53,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [220] prepared (82.398 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:54:53,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/220/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/220/registry.xml' '/galaxy/server/database/jobs_directory/000/220/upload_params.json' '261:/galaxy/server/database/objects/6/1/0/dataset_610dbebb-e573-48ab-acdd-91c8b51a6c62_files:/galaxy/server/database/objects/6/1/0/dataset_610dbebb-e573-48ab-acdd-91c8b51a6c62.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:54:53,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (220) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/220/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/220/galaxy_220.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:53,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 220 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:53,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 220 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:54:53,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fmbkt failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fmbkt.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 220 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:55:03,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-fmbkt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-fmbkt": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (220/gxy-fmbkt) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (220/gxy-fmbkt) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (220/gxy-fmbkt) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (220/gxy-fmbkt) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-fmbkt.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 220 (gxy-fmbkt)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-fmbkt to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 220 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:03,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (220/gxy-fmbkt) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-02 06:55:05,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 221
tpv.core.entities DEBUG 2025-03-02 06:55:05,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:55:05,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (221) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:55:05,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (221) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:55:05,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (221) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:55:05,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (221) Working directory for job is: /galaxy/server/database/jobs_directory/000/221
galaxy.jobs.runners DEBUG 2025-03-02 06:55:05,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [221] queued (42.870 ms)
galaxy.jobs.handler INFO 2025-03-02 06:55:05,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (221) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:05,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 221
galaxy.jobs DEBUG 2025-03-02 06:55:05,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [221] prepared (96.566 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:55:05,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/221/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/221/registry.xml' '/galaxy/server/database/jobs_directory/000/221/upload_params.json' '262:/galaxy/server/database/objects/f/9/6/dataset_f9606814-e1a0-446c-ab16-937967f893c7_files:/galaxy/server/database/objects/f/9/6/dataset_f9606814-e1a0-446c-ab16-937967f893c7.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:55:05,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (221) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/221/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/221/galaxy_221.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:05,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 221 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:05,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 221 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:06,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:15,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rlpx6 with k8s id: gxy-rlpx6 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:55:15,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 221: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:55:23,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 221 finished
galaxy.model.metadata DEBUG 2025-03-02 06:55:23,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 262
galaxy.jobs INFO 2025-03-02 06:55:23,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 221 in /galaxy/server/database/jobs_directory/000/221
galaxy.jobs DEBUG 2025-03-02 06:55:23,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 221 executed (103.919 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:23,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 221 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:55:23,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 222
tpv.core.entities DEBUG 2025-03-02 06:55:23,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:55:23,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (222) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:55:23,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (222) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:55:23,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (222) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:55:23,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (222) Working directory for job is: /galaxy/server/database/jobs_directory/000/222
galaxy.jobs.runners DEBUG 2025-03-02 06:55:23,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [222] queued (38.484 ms)
galaxy.jobs.handler INFO 2025-03-02 06:55:23,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (222) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:23,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 222
galaxy.jobs DEBUG 2025-03-02 06:55:23,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [222] prepared (49.693 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:23,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:23,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [None]
galaxy.jobs.command_factory INFO 2025-03-02 06:55:23,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/222/tool_script.sh] for tool command [python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/devteam/pileup_interval/9c1c0b947e46/pileup_interval/pileup_interval.py' --input=/galaxy/server/database/objects/f/9/6/dataset_f9606814-e1a0-446c-ab16-937967f893c7.dat --output=/galaxy/server/database/objects/7/d/e/dataset_7de5f25f-e048-47f8-8423-7e86159efac6.dat --coverage=3 --format=six --base="None" --seq_column="None" --loc_column="None" --base_column="None" --cvrg_column="None"]
galaxy.jobs.runners DEBUG 2025-03-02 06:55:23,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (222) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/222/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/222/galaxy_222.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:23,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 222 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:23,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:23,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [None]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:23,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 222 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:24,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xgqwv failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xgqwv.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 222 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:55:27,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-xgqwv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-xgqwv": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (222/gxy-xgqwv) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (222/gxy-xgqwv) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (222/gxy-xgqwv) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (222/gxy-xgqwv) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xgqwv.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Attempting to stop job 222 (gxy-xgqwv)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Found job with id gxy-xgqwv to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 222 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:27,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (222/gxy-xgqwv) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-02 06:55:28,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 223
tpv.core.entities DEBUG 2025-03-02 06:55:28,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:55:28,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (223) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:55:28,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (223) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:55:28,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (223) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:55:28,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (223) Working directory for job is: /galaxy/server/database/jobs_directory/000/223
galaxy.jobs.runners DEBUG 2025-03-02 06:55:28,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [223] queued (27.420 ms)
galaxy.jobs.handler INFO 2025-03-02 06:55:28,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (223) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:28,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 223
galaxy.jobs DEBUG 2025-03-02 06:55:29,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [223] prepared (65.297 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:55:29,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/223/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/223/registry.xml' '/galaxy/server/database/jobs_directory/000/223/upload_params.json' '264:/galaxy/server/database/objects/e/f/d/dataset_efddd589-d5d3-4ddc-9fe7-0e185f4173db_files:/galaxy/server/database/objects/e/f/d/dataset_efddd589-d5d3-4ddc-9fe7-0e185f4173db.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:55:29,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (223) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/223/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/223/galaxy_223.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:29,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 223 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:29,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 223 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:29,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:39,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wd8wn with k8s id: gxy-wd8wn succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:55:39,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 223: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:55:47,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 223 finished
galaxy.model.metadata DEBUG 2025-03-02 06:55:47,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 264
galaxy.jobs INFO 2025-03-02 06:55:47,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 223 in /galaxy/server/database/jobs_directory/000/223
galaxy.jobs DEBUG 2025-03-02 06:55:47,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 223 executed (107.067 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:47,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 223 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:55:48,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 224
tpv.core.entities DEBUG 2025-03-02 06:55:48,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:55:48,298 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (224) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:55:48,301 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (224) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:55:48,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (224) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:55:48,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (224) Working directory for job is: /galaxy/server/database/jobs_directory/000/224
galaxy.jobs.runners DEBUG 2025-03-02 06:55:48,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [224] queued (30.268 ms)
galaxy.jobs.handler INFO 2025-03-02 06:55:48,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (224) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:48,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 224
galaxy.jobs DEBUG 2025-03-02 06:55:48,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [224] prepared (55.131 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:48,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:48,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [None]
galaxy.jobs.command_factory INFO 2025-03-02 06:55:48,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/224/tool_script.sh] for tool command [python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/devteam/pileup_interval/9c1c0b947e46/pileup_interval/pileup_interval.py' --input=/galaxy/server/database/objects/e/f/d/dataset_efddd589-d5d3-4ddc-9fe7-0e185f4173db.dat --output=/galaxy/server/database/objects/6/3/3/dataset_6336da59-a52a-42ad-a379-dabfe542b42f.dat --coverage=3 --format=ten --base=first --seq_column="None" --loc_column="None" --base_column="None" --cvrg_column="None"]
galaxy.jobs.runners DEBUG 2025-03-02 06:55:48,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (224) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/224/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/224/galaxy_224.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:48,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 224 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:48,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.containers INFO 2025-03-02 06:55:48,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [None]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:48,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 224 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:48,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:55:52,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fx46d with k8s id: gxy-fx46d succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:55:53,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 224: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:56:00,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 224 finished
galaxy.model.metadata DEBUG 2025-03-02 06:56:00,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 265
galaxy.jobs INFO 2025-03-02 06:56:00,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 224 in /galaxy/server/database/jobs_directory/000/224
galaxy.jobs DEBUG 2025-03-02 06:56:00,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 224 executed (98.819 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:00,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 224 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:56:01,560 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 225
tpv.core.entities DEBUG 2025-03-02 06:56:01,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:56:01,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (225) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:56:01,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (225) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:56:01,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (225) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:56:01,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (225) Working directory for job is: /galaxy/server/database/jobs_directory/000/225
galaxy.jobs.runners DEBUG 2025-03-02 06:56:01,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [225] queued (34.741 ms)
galaxy.jobs.handler INFO 2025-03-02 06:56:01,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (225) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:01,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 225
galaxy.jobs DEBUG 2025-03-02 06:56:01,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [225] prepared (63.957 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:56:01,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/225/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/225/registry.xml' '/galaxy/server/database/jobs_directory/000/225/upload_params.json' '266:/galaxy/server/database/objects/7/2/b/dataset_72ba31eb-57b8-496d-91b9-cc26c63d0962_files:/galaxy/server/database/objects/7/2/b/dataset_72ba31eb-57b8-496d-91b9-cc26c63d0962.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:56:01,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (225) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/225/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/225/galaxy_225.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:01,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 225 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:01,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 225 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:01,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:12,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m5954 with k8s id: gxy-m5954 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:56:12,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 225: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:56:19,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 225 finished
galaxy.model.metadata DEBUG 2025-03-02 06:56:19,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 266
galaxy.jobs INFO 2025-03-02 06:56:19,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 225 in /galaxy/server/database/jobs_directory/000/225
galaxy.jobs DEBUG 2025-03-02 06:56:19,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 225 executed (107.132 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:19,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 225 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:56:20,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 226
tpv.core.entities DEBUG 2025-03-02 06:56:21,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:56:21,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (226) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:56:21,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (226) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:56:21,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (226) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:56:21,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (226) Working directory for job is: /galaxy/server/database/jobs_directory/000/226
galaxy.jobs.runners DEBUG 2025-03-02 06:56:21,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [226] queued (39.130 ms)
galaxy.jobs.handler INFO 2025-03-02 06:56:21,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (226) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:21,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 226
galaxy.jobs DEBUG 2025-03-02 06:56:21,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [226] prepared (52.975 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:21,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:21,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [None]
galaxy.jobs.command_factory INFO 2025-03-02 06:56:21,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/226/tool_script.sh] for tool command [python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/devteam/pileup_interval/9c1c0b947e46/pileup_interval/pileup_interval.py' --input=/galaxy/server/database/objects/7/2/b/dataset_72ba31eb-57b8-496d-91b9-cc26c63d0962.dat --output=/galaxy/server/database/objects/1/2/e/dataset_12ef7f72-84b9-4e62-ad6f-241e4040ebbe.dat --coverage=3 --format=manual --base="None" --seq_column=1 --loc_column=2 --base_column=3 --cvrg_column=8]
galaxy.jobs.runners DEBUG 2025-03-02 06:56:21,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (226) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/226/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/226/galaxy_226.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:21,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 226 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:21,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:21,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [None]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:21,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 226 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:22,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:26,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5hxjq with k8s id: gxy-5hxjq succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:56:26,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 226: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:56:33,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 226 finished
galaxy.model.metadata DEBUG 2025-03-02 06:56:33,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 267
galaxy.jobs INFO 2025-03-02 06:56:33,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 226 in /galaxy/server/database/jobs_directory/000/226
galaxy.jobs DEBUG 2025-03-02 06:56:33,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 226 executed (94.362 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:33,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 226 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:56:36,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 227, 228
tpv.core.entities DEBUG 2025-03-02 06:56:36,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:56:36,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (227) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:56:36,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (227) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:56:36,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (227) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:56:36,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (227) Working directory for job is: /galaxy/server/database/jobs_directory/000/227
galaxy.jobs.runners DEBUG 2025-03-02 06:56:36,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [227] queued (28.189 ms)
galaxy.jobs.handler INFO 2025-03-02 06:56:36,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (227) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:36,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 227
tpv.core.entities DEBUG 2025-03-02 06:56:36,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:56:36,385 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (228) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:56:36,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (228) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:56:36,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (228) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:56:36,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (228) Working directory for job is: /galaxy/server/database/jobs_directory/000/228
galaxy.jobs.runners DEBUG 2025-03-02 06:56:36,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [228] queued (53.904 ms)
galaxy.jobs.handler INFO 2025-03-02 06:56:36,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (228) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:36,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 228
galaxy.jobs DEBUG 2025-03-02 06:56:36,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [227] prepared (98.753 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:56:36,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/227/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/227/registry.xml' '/galaxy/server/database/jobs_directory/000/227/upload_params.json' '268:/galaxy/server/database/objects/9/5/4/dataset_9543c922-9388-4ba3-921c-5d02ece7202d_files:/galaxy/server/database/objects/9/5/4/dataset_9543c922-9388-4ba3-921c-5d02ece7202d.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:56:36,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (227) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/227/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/227/galaxy_227.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:36,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 227 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:56:36,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [228] prepared (78.796 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:36,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 227 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:56:36,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/228/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/228/registry.xml' '/galaxy/server/database/jobs_directory/000/228/upload_params.json' '269:/galaxy/server/database/objects/c/a/b/dataset_cab83b44-4f8b-40f4-b460-225e57b85a28_files:/galaxy/server/database/objects/c/a/b/dataset_cab83b44-4f8b-40f4-b460-225e57b85a28.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:56:36,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (228) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/228/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/228/galaxy_228.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:36,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 228 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:36,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 228 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:37,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:37,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:46,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7jf9w with k8s id: gxy-7jf9w succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:46,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n947b with k8s id: gxy-n947b succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:56:47,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 227: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:56:47,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 228: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:56:54,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 227 finished
galaxy.model.metadata DEBUG 2025-03-02 06:56:54,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 268
galaxy.jobs INFO 2025-03-02 06:56:54,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 227 in /galaxy/server/database/jobs_directory/000/227
galaxy.jobs.runners DEBUG 2025-03-02 06:56:54,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 228 finished
galaxy.model.metadata DEBUG 2025-03-02 06:56:54,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 269
galaxy.jobs DEBUG 2025-03-02 06:56:54,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 227 executed (137.409 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:54,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 227 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-02 06:56:54,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 228 in /galaxy/server/database/jobs_directory/000/228
galaxy.jobs DEBUG 2025-03-02 06:56:54,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 228 executed (109.541 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:54,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 228 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:56:55,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 229
tpv.core.entities DEBUG 2025-03-02 06:56:55,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:56:55,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (229) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:56:55,966 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (229) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:56:55,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (229) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:56:55,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (229) Working directory for job is: /galaxy/server/database/jobs_directory/000/229
galaxy.jobs.runners DEBUG 2025-03-02 06:56:56,010 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [229] queued (44.730 ms)
galaxy.jobs.handler INFO 2025-03-02 06:56:56,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (229) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:56,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 229
galaxy.jobs DEBUG 2025-03-02 06:56:56,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [229] prepared (96.088 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:56,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:56:56,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:56,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:56:56,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/229/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/229/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/9/5/4/dataset_9543c922-9388-4ba3-921c-5d02ece7202d.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/6/5/7/metadata_65754ba0-b637-4461-a05d-289f7fb22f25.dat' '0.bai' &&   samtools depth  -b '/galaxy/server/database/objects/c/a/b/dataset_cab83b44-4f8b-40f4-b460-225e57b85a28.dat' 0 > '/galaxy/server/database/objects/e/8/6/dataset_e86850e9-d2b6-4013-beb8-a76f4e57f63f.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:56:56,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (229) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/229/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/229/galaxy_229.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:56,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 229 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:56,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:56:56,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:56:56,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:56,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 229 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:56:58,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:05,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rxwtw with k8s id: gxy-rxwtw succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:57:05,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 229: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:57:12,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 229 finished
galaxy.model.metadata DEBUG 2025-03-02 06:57:12,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 270
galaxy.jobs INFO 2025-03-02 06:57:12,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 229 in /galaxy/server/database/jobs_directory/000/229
galaxy.jobs DEBUG 2025-03-02 06:57:12,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 229 executed (100.936 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:12,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 229 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:57:14,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 230
tpv.core.entities DEBUG 2025-03-02 06:57:14,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:57:14,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (230) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:57:14,403 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (230) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:57:14,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (230) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:57:14,430 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (230) Working directory for job is: /galaxy/server/database/jobs_directory/000/230
galaxy.jobs.runners DEBUG 2025-03-02 06:57:14,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [230] queued (33.780 ms)
galaxy.jobs.handler INFO 2025-03-02 06:57:14,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (230) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:14,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 230
galaxy.jobs DEBUG 2025-03-02 06:57:14,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [230] prepared (85.960 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:57:14,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/230/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/230/registry.xml' '/galaxy/server/database/jobs_directory/000/230/upload_params.json' '271:/galaxy/server/database/objects/4/f/0/dataset_4f0195f3-d837-45d4-93e8-68772f14a453_files:/galaxy/server/database/objects/4/f/0/dataset_4f0195f3-d837-45d4-93e8-68772f14a453.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:57:14,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (230) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/230/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/230/galaxy_230.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:14,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 230 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:14,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 230 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:15,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:25,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ksrzx with k8s id: gxy-ksrzx succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:57:25,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 230: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:57:32,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 230 finished
galaxy.model.metadata DEBUG 2025-03-02 06:57:32,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 271
galaxy.jobs INFO 2025-03-02 06:57:32,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 230 in /galaxy/server/database/jobs_directory/000/230
galaxy.jobs DEBUG 2025-03-02 06:57:32,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 230 executed (109.272 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:32,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 230 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:57:33,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 231
tpv.core.entities DEBUG 2025-03-02 06:57:33,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:57:33,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (231) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:57:33,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (231) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:57:33,816 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (231) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:57:33,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (231) Working directory for job is: /galaxy/server/database/jobs_directory/000/231
galaxy.jobs.runners DEBUG 2025-03-02 06:57:33,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [231] queued (30.046 ms)
galaxy.jobs.handler INFO 2025-03-02 06:57:33,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (231) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:33,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 231
galaxy.jobs DEBUG 2025-03-02 06:57:33,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [231] prepared (46.134 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:57:33,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:57:33,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:57:33,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:57:33,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/231/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/231/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/4/f/0/dataset_4f0195f3-d837-45d4-93e8-68772f14a453.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/5/0/4/metadata_504e5890-05c3-4534-9f6e-501cc0fa5d7f.dat' '0.bai' &&   samtools depth  -r eboVir3:500-1500 0 > '/galaxy/server/database/objects/7/9/3/dataset_793e7233-08dd-4b21-9a58-0fe1428d8fa3.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:57:33,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (231) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/231/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/231/galaxy_231.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:33,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 231 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:57:33,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:57:33,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:57:34,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:34,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 231 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:34,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:38,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-l75tz with k8s id: gxy-l75tz succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:57:38,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 231: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:57:45,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 231 finished
galaxy.model.metadata DEBUG 2025-03-02 06:57:46,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 272
galaxy.jobs INFO 2025-03-02 06:57:46,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 231 in /galaxy/server/database/jobs_directory/000/231
galaxy.jobs DEBUG 2025-03-02 06:57:46,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 231 executed (176.859 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:46,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 231 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:57:47,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 232
tpv.core.entities DEBUG 2025-03-02 06:57:47,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:57:47,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (232) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:57:47,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (232) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:57:47,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (232) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:57:47,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (232) Working directory for job is: /galaxy/server/database/jobs_directory/000/232
galaxy.jobs.runners DEBUG 2025-03-02 06:57:47,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [232] queued (29.367 ms)
galaxy.jobs.handler INFO 2025-03-02 06:57:47,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (232) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:47,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 232
galaxy.jobs DEBUG 2025-03-02 06:57:47,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [232] prepared (71.048 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:57:47,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/232/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/232/registry.xml' '/galaxy/server/database/jobs_directory/000/232/upload_params.json' '273:/galaxy/server/database/objects/7/d/3/dataset_7d30359b-c717-4ec1-b15c-2963326b415f_files:/galaxy/server/database/objects/7/d/3/dataset_7d30359b-c717-4ec1-b15c-2963326b415f.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:57:47,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (232) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/232/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/232/galaxy_232.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:47,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 232 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:47,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 232 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:47,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 06:57:48,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 233
tpv.core.entities DEBUG 2025-03-02 06:57:48,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:57:48,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (233) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:57:48,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (233) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:57:48,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (233) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:57:48,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (233) Working directory for job is: /galaxy/server/database/jobs_directory/000/233
galaxy.jobs.runners DEBUG 2025-03-02 06:57:48,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [233] queued (29.545 ms)
galaxy.jobs.handler INFO 2025-03-02 06:57:48,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (233) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:48,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 233
galaxy.jobs DEBUG 2025-03-02 06:57:48,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [233] prepared (91.039 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:57:48,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/233/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/233/registry.xml' '/galaxy/server/database/jobs_directory/000/233/upload_params.json' '274:/galaxy/server/database/objects/6/3/f/dataset_63f5305e-10e5-4a30-b9ff-3795342a7535_files:/galaxy/server/database/objects/6/3/f/dataset_63f5305e-10e5-4a30-b9ff-3795342a7535.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:57:48,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (233) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/233/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/233/galaxy_233.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:48,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 233 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:48,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 233 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:48,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xq7wm failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xq7wm.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 232 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:57:57,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-xq7wm

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-xq7wm": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (232/gxy-xq7wm) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (232/gxy-xq7wm) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (232/gxy-xq7wm) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (232/gxy-xq7wm) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-xq7wm.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 232 (gxy-xq7wm)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-xq7wm to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 232 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:57,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (232/gxy-xq7wm) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:58,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fpb4l with k8s id: gxy-fpb4l succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:57:58,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 233: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-02 06:57:58,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 235, 234
tpv.core.entities DEBUG 2025-03-02 06:57:58,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:57:58,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (234) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:57:58,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (234) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:57:58,456 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (234) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:57:58,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (234) Working directory for job is: /galaxy/server/database/jobs_directory/000/234
galaxy.jobs.runners DEBUG 2025-03-02 06:57:58,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [234] queued (34.307 ms)
galaxy.jobs.handler INFO 2025-03-02 06:57:58,481 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (234) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:58,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 234
tpv.core.entities DEBUG 2025-03-02 06:57:58,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:57:58,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (235) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:57:58,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (235) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:57:58,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (235) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:57:58,549 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (235) Working directory for job is: /galaxy/server/database/jobs_directory/000/235
galaxy.jobs.runners DEBUG 2025-03-02 06:57:58,555 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [235] queued (53.216 ms)
galaxy.jobs.handler INFO 2025-03-02 06:57:58,557 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (235) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:58,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 235
galaxy.jobs DEBUG 2025-03-02 06:57:58,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [234] prepared (114.328 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:57:58,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/234/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/234/registry.xml' '/galaxy/server/database/jobs_directory/000/234/upload_params.json' '275:/galaxy/server/database/objects/6/a/7/dataset_6a7d11c3-319a-4be9-ab8c-7fae1aa4aab1_files:/galaxy/server/database/objects/6/a/7/dataset_6a7d11c3-319a-4be9-ab8c-7fae1aa4aab1.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:57:58,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (234) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/234/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/234/galaxy_234.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 06:57:58,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [235] prepared (90.323 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:58,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 234 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:58,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 234 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:57:58,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/235/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/235/registry.xml' '/galaxy/server/database/jobs_directory/000/235/upload_params.json' '276:/galaxy/server/database/objects/5/b/3/dataset_5b395ae0-d4d0-4d50-b4a2-1956c3737a67_files:/galaxy/server/database/objects/5/b/3/dataset_5b395ae0-d4d0-4d50-b4a2-1956c3737a67.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:57:58,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (235) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/235/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/235/galaxy_235.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:58,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 235 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:58,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 235 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:59,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:57:59,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 06:58:06,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 233 finished
galaxy.model.metadata DEBUG 2025-03-02 06:58:06,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 274
galaxy.jobs INFO 2025-03-02 06:58:06,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 233 in /galaxy/server/database/jobs_directory/000/233
galaxy.jobs DEBUG 2025-03-02 06:58:06,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 233 executed (150.018 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:06,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 233 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:09,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s485n with k8s id: gxy-s485n succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:09,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tbtvt with k8s id: gxy-tbtvt succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:58:09,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 234: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:58:09,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 235: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:58:17,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 234 finished
galaxy.model.metadata DEBUG 2025-03-02 06:58:17,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 275
galaxy.jobs INFO 2025-03-02 06:58:17,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 234 in /galaxy/server/database/jobs_directory/000/234
galaxy.jobs DEBUG 2025-03-02 06:58:17,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 234 executed (128.434 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:17,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 234 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 06:58:17,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 235 finished
galaxy.model.metadata DEBUG 2025-03-02 06:58:17,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 276
galaxy.jobs INFO 2025-03-02 06:58:17,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 235 in /galaxy/server/database/jobs_directory/000/235
galaxy.jobs DEBUG 2025-03-02 06:58:17,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 235 executed (106.710 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:17,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 235 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:58:18,968 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 236
tpv.core.entities DEBUG 2025-03-02 06:58:18,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:58:19,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (236) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:58:19,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (236) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:58:19,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (236) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:58:19,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (236) Working directory for job is: /galaxy/server/database/jobs_directory/000/236
galaxy.jobs.runners DEBUG 2025-03-02 06:58:19,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [236] queued (32.687 ms)
galaxy.jobs.handler INFO 2025-03-02 06:58:19,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (236) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:19,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 236
galaxy.jobs DEBUG 2025-03-02 06:58:19,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [236] prepared (58.462 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:19,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:58:19,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:19,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:58:19,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/236/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/236/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/6/a/7/dataset_6a7d11c3-319a-4be9-ab8c-7fae1aa4aab1.dat' '0' && ln -s '/galaxy/server/database/objects/_metadata_files/9/3/4/metadata_93487f03-55c4-4ff5-acf5-5ad357b027ab.dat' '0.bai' && ln -s '/galaxy/server/database/objects/5/b/3/dataset_5b395ae0-d4d0-4d50-b4a2-1956c3737a67.dat' '1' && ln -s '/galaxy/server/database/objects/_metadata_files/c/7/5/metadata_c75dd34f-b3e7-4cfc-9875-887d721407aa.dat' '1.bai' &&   samtools depth  -l 10 -m 4 -q 11 -Q 12 0 1 > '/galaxy/server/database/objects/e/6/5/dataset_e6541d1d-7477-45dd-919e-edaf057250be.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:58:19,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (236) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/236/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/236/galaxy_236.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:19,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 236 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:19,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:58:19,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_depth/samtools_depth/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:19,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:19,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 236 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:19,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:23,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6vqpb with k8s id: gxy-6vqpb succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:58:23,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 236: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:58:31,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 236 finished
galaxy.model.metadata DEBUG 2025-03-02 06:58:31,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 277
galaxy.jobs INFO 2025-03-02 06:58:31,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 236 in /galaxy/server/database/jobs_directory/000/236
galaxy.jobs DEBUG 2025-03-02 06:58:31,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 236 executed (79.479 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:31,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 236 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:58:33,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 237
tpv.core.entities DEBUG 2025-03-02 06:58:33,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:58:33,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (237) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:58:33,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (237) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:58:33,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (237) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:58:33,371 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (237) Working directory for job is: /galaxy/server/database/jobs_directory/000/237
galaxy.jobs.runners DEBUG 2025-03-02 06:58:33,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [237] queued (38.313 ms)
galaxy.jobs.handler INFO 2025-03-02 06:58:33,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (237) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:33,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 237
galaxy.jobs DEBUG 2025-03-02 06:58:33,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [237] prepared (67.420 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:58:33,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/237/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/237/registry.xml' '/galaxy/server/database/jobs_directory/000/237/upload_params.json' '278:/galaxy/server/database/objects/e/2/6/dataset_e2653f1d-ac21-4b51-b6b7-1152bea27f71_files:/galaxy/server/database/objects/e/2/6/dataset_e2653f1d-ac21-4b51-b6b7-1152bea27f71.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:58:33,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (237) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/237/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/237/galaxy_237.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:33,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 237 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:33,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 237 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:33,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:44,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nvkbv with k8s id: gxy-nvkbv succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:58:45,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 237: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:58:52,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 237 finished
galaxy.model.metadata DEBUG 2025-03-02 06:58:52,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 278
galaxy.jobs INFO 2025-03-02 06:58:52,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 237 in /galaxy/server/database/jobs_directory/000/237
galaxy.jobs DEBUG 2025-03-02 06:58:52,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 237 executed (114.786 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:52,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 237 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:58:53,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 238
tpv.core.entities DEBUG 2025-03-02 06:58:53,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/.*, abstract=False, cores=1, mem=7.6, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:58:53,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (238) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:58:53,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (238) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:58:53,781 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (238) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:58:53,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (238) Working directory for job is: /galaxy/server/database/jobs_directory/000/238
galaxy.jobs.runners DEBUG 2025-03-02 06:58:53,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [238] queued (41.403 ms)
galaxy.jobs.handler INFO 2025-03-02 06:58:53,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (238) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:53,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 238
galaxy.jobs DEBUG 2025-03-02 06:58:53,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [238] prepared (38.731 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:53,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:58:53,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/2.0.1: samtools:1.3.1
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:53,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.3.1--h60f3df9_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:58:53,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/238/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/238/outputs/COMMAND_VERSION 2>&1;
samtools rmdup  '/galaxy/server/database/objects/e/2/6/dataset_e2653f1d-ac21-4b51-b6b7-1152bea27f71.dat' '/galaxy/server/database/objects/3/6/2/dataset_362659ef-8630-4fc0-abdf-e3af0c0741c5.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:58:53,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (238) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/238/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/238/galaxy_238.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:53,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 238 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:53,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:58:53,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_rmdup/samtools_rmdup/2.0.1: samtools:1.3.1
galaxy.tool_util.deps.containers INFO 2025-03-02 06:58:53,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.3.1--h60f3df9_12,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:54,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 238 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:58:55,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lcms7 failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lcms7.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 238 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 06:59:00,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-lcms7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-lcms7": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (238/gxy-lcms7) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (238/gxy-lcms7) tool_stderr: [bam_rmdup_core] processing reference chrM...

galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (238/gxy-lcms7) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (238/gxy-lcms7) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-lcms7.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 238 (gxy-lcms7)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-lcms7 to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 238 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:00,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (238/gxy-lcms7) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-02 06:59:02,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 240, 239
tpv.core.entities DEBUG 2025-03-02 06:59:02,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:59:02,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (239) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:59:02,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (239) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:59:03,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (239) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:59:03,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (239) Working directory for job is: /galaxy/server/database/jobs_directory/000/239
galaxy.jobs.runners DEBUG 2025-03-02 06:59:03,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [239] queued (30.808 ms)
galaxy.jobs.handler INFO 2025-03-02 06:59:03,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (239) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:03,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 239
tpv.core.entities DEBUG 2025-03-02 06:59:03,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:59:03,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (240) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:59:03,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (240) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:59:03,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (240) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:59:03,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (240) Working directory for job is: /galaxy/server/database/jobs_directory/000/240
galaxy.jobs.runners DEBUG 2025-03-02 06:59:03,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [240] queued (59.806 ms)
galaxy.jobs.handler INFO 2025-03-02 06:59:03,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (240) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:03,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 240
galaxy.jobs DEBUG 2025-03-02 06:59:03,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [239] prepared (118.829 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:59:03,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/239/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/239/registry.xml' '/galaxy/server/database/jobs_directory/000/239/upload_params.json' '280:/galaxy/server/database/objects/1/e/6/dataset_1e6f68b8-a3fd-4c9b-b2a2-dc4d9fe43b91_files:/galaxy/server/database/objects/1/e/6/dataset_1e6f68b8-a3fd-4c9b-b2a2-dc4d9fe43b91.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:59:03,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (239) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/239/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/239/galaxy_239.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:03,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 239 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 06:59:03,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [240] prepared (101.371 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:03,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 239 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 06:59:03,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/240/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/240/registry.xml' '/galaxy/server/database/jobs_directory/000/240/upload_params.json' '281:/galaxy/server/database/objects/f/b/d/dataset_fbdd7f7f-7c6d-4558-a64c-ee45c54debcc_files:/galaxy/server/database/objects/f/b/d/dataset_fbdd7f7f-7c6d-4558-a64c-ee45c54debcc.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:59:03,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (240) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/240/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/240/galaxy_240.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:03,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 240 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:03,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 240 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:04,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:04,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:13,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rjnsq with k8s id: gxy-rjnsq succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:13,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mzxfr with k8s id: gxy-mzxfr succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:59:13,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 239: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:59:14,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 240: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:59:21,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 240 finished
galaxy.jobs.runners DEBUG 2025-03-02 06:59:21,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 239 finished
galaxy.model.metadata DEBUG 2025-03-02 06:59:21,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 281
galaxy.model.metadata DEBUG 2025-03-02 06:59:21,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 280
galaxy.jobs INFO 2025-03-02 06:59:21,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 240 in /galaxy/server/database/jobs_directory/000/240
galaxy.jobs INFO 2025-03-02 06:59:21,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 239 in /galaxy/server/database/jobs_directory/000/239
galaxy.jobs DEBUG 2025-03-02 06:59:22,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 240 executed (123.038 ms)
galaxy.jobs DEBUG 2025-03-02 06:59:22,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 239 executed (103.137 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:22,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 240 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:22,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 239 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:59:22,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 241
tpv.core.entities DEBUG 2025-03-02 06:59:22,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:59:22,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (241) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:59:22,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (241) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:59:22,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (241) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:59:22,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (241) Working directory for job is: /galaxy/server/database/jobs_directory/000/241
galaxy.jobs.runners DEBUG 2025-03-02 06:59:22,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [241] queued (42.779 ms)
galaxy.jobs.handler INFO 2025-03-02 06:59:22,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (241) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:22,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 241
galaxy.jobs DEBUG 2025-03-02 06:59:22,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [241] prepared (46.401 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 06:59:22,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:59:22,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/modify_loom/0.7.5+galaxy1: mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d
galaxy.tool_util.deps.containers INFO 2025-03-02 06:59:22,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 06:59:22,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/241/tool_script.sh] for tool command [python -c "import anndata as ad;print('anndata version: %s' % ad.__version__); import loompy;print('\nloompy version: %s' % loompy.__version__)" > /galaxy/server/database/jobs_directory/000/241/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/1/e/6/dataset_1e6f68b8-a3fd-4c9b-b2a2-dc4d9fe43b91.dat' loom_add_out.loom && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/05631436cdf1/modify_loom/modify_loom.py' -f 'loom_add_out.loom' -a cols -c '/galaxy/server/database/objects/f/b/d/dataset_fbdd7f7f-7c6d-4558-a64c-ee45c54debcc.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:59:22,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (241) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/241/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/241/galaxy_241.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/241/working/loom_add_out.loom" -a -f "/galaxy/server/database/objects/e/a/3/dataset_ea3480b2-371f-4d0e-b42d-9e07020c530d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/241/working/loom_add_out.loom" "/galaxy/server/database/objects/e/a/3/dataset_ea3480b2-371f-4d0e-b42d-9e07020c530d.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:22,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 241 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 06:59:22,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 06:59:22,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/modify_loom/0.7.5+galaxy1: mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d
galaxy.tool_util.deps.containers INFO 2025-03-02 06:59:22,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:22,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 241 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:23,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:36,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dghk2 with k8s id: gxy-dghk2 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:59:36,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 241: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 06:59:43,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 241 finished
galaxy.model.metadata DEBUG 2025-03-02 06:59:43,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 282
galaxy.util WARNING 2025-03-02 06:59:43,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/a/3/dataset_ea3480b2-371f-4d0e-b42d-9e07020c530d.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/a/3/dataset_ea3480b2-371f-4d0e-b42d-9e07020c530d.dat'
galaxy.jobs INFO 2025-03-02 06:59:43,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 241 in /galaxy/server/database/jobs_directory/000/241
galaxy.jobs DEBUG 2025-03-02 06:59:43,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 241 executed (107.983 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:43,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 241 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:59:44,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 242
tpv.core.entities DEBUG 2025-03-02 06:59:44,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:59:44,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (242) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:59:44,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (242) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:59:44,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (242) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:59:44,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (242) Working directory for job is: /galaxy/server/database/jobs_directory/000/242
galaxy.jobs.runners DEBUG 2025-03-02 06:59:44,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [242] queued (27.311 ms)
galaxy.jobs.handler INFO 2025-03-02 06:59:44,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (242) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:44,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 242
galaxy.jobs DEBUG 2025-03-02 06:59:45,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [242] prepared (81.906 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:59:45,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/242/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/242/registry.xml' '/galaxy/server/database/jobs_directory/000/242/upload_params.json' '283:/galaxy/server/database/objects/2/5/e/dataset_25e0ec4b-e6a8-4bb1-834d-f7610dac38dd_files:/galaxy/server/database/objects/2/5/e/dataset_25e0ec4b-e6a8-4bb1-834d-f7610dac38dd.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:59:45,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (242) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/242/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/242/galaxy_242.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:45,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 242 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:45,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 242 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 06:59:45,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 243
tpv.core.entities DEBUG 2025-03-02 06:59:46,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 06:59:46,019 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (243) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 06:59:46,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (243) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 06:59:46,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (243) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 06:59:46,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (243) Working directory for job is: /galaxy/server/database/jobs_directory/000/243
galaxy.jobs.runners DEBUG 2025-03-02 06:59:46,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [243] queued (33.072 ms)
galaxy.jobs.handler INFO 2025-03-02 06:59:46,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (243) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:46,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 243
galaxy.jobs DEBUG 2025-03-02 06:59:46,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [243] prepared (80.160 ms)
galaxy.jobs.command_factory INFO 2025-03-02 06:59:46,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/243/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/243/registry.xml' '/galaxy/server/database/jobs_directory/000/243/upload_params.json' '284:/galaxy/server/database/objects/b/9/0/dataset_b90e4dad-e324-4c9e-a237-35126fdf177c_files:/galaxy/server/database/objects/b/9/0/dataset_b90e4dad-e324-4c9e-a237-35126fdf177c.dat']
galaxy.jobs.runners DEBUG 2025-03-02 06:59:46,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (243) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/243/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/243/galaxy_243.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:46,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 243 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:46,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 243 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:46,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:47,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:54,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dsxds with k8s id: gxy-dsxds succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:59:54,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 242: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 06:59:56,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bghb4 with k8s id: gxy-bghb4 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 06:59:56,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 243: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:00:02,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 242 finished
galaxy.model.metadata DEBUG 2025-03-02 07:00:02,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 283
galaxy.jobs INFO 2025-03-02 07:00:02,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 242 in /galaxy/server/database/jobs_directory/000/242
galaxy.jobs DEBUG 2025-03-02 07:00:02,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 242 executed (127.038 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:03,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 242 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 07:00:04,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 243 finished
galaxy.model.metadata DEBUG 2025-03-02 07:00:04,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 284
galaxy.jobs INFO 2025-03-02 07:00:04,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 243 in /galaxy/server/database/jobs_directory/000/243
galaxy.jobs DEBUG 2025-03-02 07:00:04,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 243 executed (113.492 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:04,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 243 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:00:05,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 244
tpv.core.entities DEBUG 2025-03-02 07:00:05,464 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:00:05,465 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (244) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:00:05,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (244) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:00:05,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (244) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:00:05,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (244) Working directory for job is: /galaxy/server/database/jobs_directory/000/244
galaxy.jobs.runners DEBUG 2025-03-02 07:00:05,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [244] queued (34.607 ms)
galaxy.jobs.handler INFO 2025-03-02 07:00:05,506 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (244) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:05,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 244
galaxy.jobs DEBUG 2025-03-02 07:00:05,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [244] prepared (41.717 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:00:05,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:00:05,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/modify_loom/0.7.5+galaxy1: mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d
galaxy.tool_util.deps.containers INFO 2025-03-02 07:00:05,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:00:05,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/244/tool_script.sh] for tool command [python -c "import anndata as ad;print('anndata version: %s' % ad.__version__); import loompy;print('\nloompy version: %s' % loompy.__version__)" > /galaxy/server/database/jobs_directory/000/244/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/2/5/e/dataset_25e0ec4b-e6a8-4bb1-834d-f7610dac38dd.dat' loom_add_out.loom && python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/05631436cdf1/modify_loom/modify_loom.py' -f 'loom_add_out.loom']
galaxy.jobs.runners DEBUG 2025-03-02 07:00:05,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (244) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/244/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/244/galaxy_244.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/244/working/loom_add_out.loom" -a -f "/galaxy/server/database/objects/9/7/9/dataset_9798bafa-4eda-43f1-9773-3509d8385e09.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/244/working/loom_add_out.loom" "/galaxy/server/database/objects/9/7/9/dataset_9798bafa-4eda-43f1-9773-3509d8385e09.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:05,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 244 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:00:05,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:00:05,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/modify_loom/modify_loom/0.7.5+galaxy1: mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d
galaxy.tool_util.deps.containers INFO 2025-03-02 07:00:05,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-61fb942689aeef789decdfc6589cc2be67326474:0102e9b7b1e5f4c343a80f24e8371bc683d6111d-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:05,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 244 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:06,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:10,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nchs7 with k8s id: gxy-nchs7 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:00:10,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 244: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:00:18,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 244 finished
galaxy.model.metadata DEBUG 2025-03-02 07:00:18,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 285
galaxy.util WARNING 2025-03-02 07:00:18,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/7/9/dataset_9798bafa-4eda-43f1-9773-3509d8385e09.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/7/9/dataset_9798bafa-4eda-43f1-9773-3509d8385e09.dat'
galaxy.jobs INFO 2025-03-02 07:00:18,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 244 in /galaxy/server/database/jobs_directory/000/244
galaxy.jobs DEBUG 2025-03-02 07:00:18,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 244 executed (92.496 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:18,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 244 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:00:20,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 246, 245
tpv.core.entities DEBUG 2025-03-02 07:00:20,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:00:20,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (245) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:00:20,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (245) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:00:20,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (245) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:00:20,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (245) Working directory for job is: /galaxy/server/database/jobs_directory/000/245
galaxy.jobs.runners DEBUG 2025-03-02 07:00:20,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [245] queued (32.782 ms)
galaxy.jobs.handler INFO 2025-03-02 07:00:20,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (245) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:20,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 245
tpv.core.entities DEBUG 2025-03-02 07:00:20,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:00:20,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (246) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:00:20,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (246) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:00:20,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (246) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:00:20,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (246) Working directory for job is: /galaxy/server/database/jobs_directory/000/246
galaxy.jobs.runners DEBUG 2025-03-02 07:00:20,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [246] queued (51.643 ms)
galaxy.jobs.handler INFO 2025-03-02 07:00:20,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (246) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:20,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 246
galaxy.jobs DEBUG 2025-03-02 07:00:20,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [245] prepared (110.536 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:00:21,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/245/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/245/registry.xml' '/galaxy/server/database/jobs_directory/000/245/upload_params.json' '286:/galaxy/server/database/objects/d/e/0/dataset_de088ded-9e5b-4da9-89e7-0a03f27666bd_files:/galaxy/server/database/objects/d/e/0/dataset_de088ded-9e5b-4da9-89e7-0a03f27666bd.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:00:21,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (245) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/245/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/245/galaxy_245.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 07:00:21,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [246] prepared (94.660 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:21,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 245 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:21,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 245 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:00:21,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/246/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/246/registry.xml' '/galaxy/server/database/jobs_directory/000/246/upload_params.json' '287:/galaxy/server/database/objects/3/b/2/dataset_3b233fff-a829-4908-a0d2-c2a069b83c95_files:/galaxy/server/database/objects/3/b/2/dataset_3b233fff-a829-4908-a0d2-c2a069b83c95.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:00:21,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (246) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/246/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/246/galaxy_246.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:21,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 246 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:21,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 246 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:21,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:21,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ftbvj failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-ftbvj.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 246 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 07:00:30,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-ftbvj

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-ftbvj": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (246/gxy-ftbvj) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (246/gxy-ftbvj) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (246/gxy-ftbvj) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (246/gxy-ftbvj) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-ftbvj.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Attempting to stop job 246 (gxy-ftbvj)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Found job with id gxy-ftbvj to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 246 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:30,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (246/gxy-ftbvj) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:31,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lzwj2 with k8s id: gxy-lzwj2 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:00:31,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 245: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:00:38,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 245 finished
galaxy.model.metadata DEBUG 2025-03-02 07:00:38,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 286
galaxy.jobs INFO 2025-03-02 07:00:38,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 245 in /galaxy/server/database/jobs_directory/000/245
galaxy.jobs DEBUG 2025-03-02 07:00:38,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 245 executed (84.825 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:39,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 245 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:00:41,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 247
tpv.core.entities DEBUG 2025-03-02 07:00:41,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:00:41,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (247) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:00:41,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (247) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:00:41,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (247) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:00:41,400 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (247) Working directory for job is: /galaxy/server/database/jobs_directory/000/247
galaxy.jobs.runners DEBUG 2025-03-02 07:00:41,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [247] queued (33.313 ms)
galaxy.jobs.handler INFO 2025-03-02 07:00:41,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (247) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:41,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 247
galaxy.jobs DEBUG 2025-03-02 07:00:41,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [247] prepared (73.423 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:00:41,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/247/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/247/registry.xml' '/galaxy/server/database/jobs_directory/000/247/upload_params.json' '288:/galaxy/server/database/objects/1/a/3/dataset_1a33a6bc-9ef7-483b-b14d-52e4666c6a45_files:/galaxy/server/database/objects/1/a/3/dataset_1a33a6bc-9ef7-483b-b14d-52e4666c6a45.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:00:41,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (247) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/247/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/247/galaxy_247.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:41,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 247 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:41,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 247 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:42,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:51,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6c4m9 with k8s id: gxy-6c4m9 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:00:51,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 247: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:00:58,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 247 finished
galaxy.model.metadata DEBUG 2025-03-02 07:00:58,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 288
galaxy.jobs INFO 2025-03-02 07:00:58,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 247 in /galaxy/server/database/jobs_directory/000/247
galaxy.jobs DEBUG 2025-03-02 07:00:58,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 247 executed (88.729 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:58,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 247 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:00:59,764 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 248
tpv.core.entities DEBUG 2025-03-02 07:00:59,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:00:59,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (248) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:00:59,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (248) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:00:59,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (248) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:00:59,825 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (248) Working directory for job is: /galaxy/server/database/jobs_directory/000/248
galaxy.jobs.runners DEBUG 2025-03-02 07:00:59,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [248] queued (37.945 ms)
galaxy.jobs.handler INFO 2025-03-02 07:00:59,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (248) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:00:59,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 248
galaxy.jobs DEBUG 2025-03-02 07:00:59,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [248] prepared (46.127 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:00:59,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:00:59,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scater_plot_tsne/scater_plot_tsne/1.22.0: mulled-v2-e13023c8593d24824dd3fac34c8bd64338108543:66a209fb455058282232dc7688fd6ceb6b331057
galaxy.tool_util.deps.containers INFO 2025-03-02 07:01:00,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e13023c8593d24824dd3fac34c8bd64338108543:66a209fb455058282232dc7688fd6ceb6b331057-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:01:00,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/248/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/scater_plot_tsne/99f912d5af9f/scater_plot_tsne/scater-plot-tsne.R' -i '/galaxy/server/database/objects/1/a/3/dataset_1a33a6bc-9ef7-483b-b14d-52e4666c6a45.dat' --colour-by 'Treatment' --shape-by 'Mutation_Status' -o '/galaxy/server/database/objects/a/3/2/dataset_a320805f-7abf-48a1-972e-ee4d79ecda05.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:01:00,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (248) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/248/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/248/galaxy_248.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:00,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 248 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:01:00,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:01:00,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scater_plot_tsne/scater_plot_tsne/1.22.0: mulled-v2-e13023c8593d24824dd3fac34c8bd64338108543:66a209fb455058282232dc7688fd6ceb6b331057
galaxy.tool_util.deps.containers INFO 2025-03-02 07:01:00,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-e13023c8593d24824dd3fac34c8bd64338108543:66a209fb455058282232dc7688fd6ceb6b331057-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:00,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 248 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:00,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:43,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cjzj9 with k8s id: gxy-cjzj9 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:01:43,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 248: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:01:51,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 248 finished
galaxy.model.metadata DEBUG 2025-03-02 07:01:51,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 289
galaxy.jobs INFO 2025-03-02 07:01:51,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 248 in /galaxy/server/database/jobs_directory/000/248
galaxy.jobs DEBUG 2025-03-02 07:01:51,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 248 executed (72.300 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:51,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 248 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:01:57,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 249
tpv.core.entities DEBUG 2025-03-02 07:01:57,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:01:57,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (249) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:01:57,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (249) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:01:57,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (249) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:01:57,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (249) Working directory for job is: /galaxy/server/database/jobs_directory/000/249
galaxy.jobs.runners DEBUG 2025-03-02 07:01:57,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [249] queued (35.252 ms)
galaxy.jobs.handler INFO 2025-03-02 07:01:57,968 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (249) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:57,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 249
galaxy.jobs DEBUG 2025-03-02 07:01:58,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [249] prepared (75.626 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:01:58,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/249/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/249/registry.xml' '/galaxy/server/database/jobs_directory/000/249/upload_params.json' '290:/galaxy/server/database/objects/4/7/7/dataset_477bc097-48a7-4434-9d69-989cd1a0e36d_files:/galaxy/server/database/objects/4/7/7/dataset_477bc097-48a7-4434-9d69-989cd1a0e36d.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:01:58,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (249) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/249/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/249/galaxy_249.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:58,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 249 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:58,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 249 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:58,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 07:01:58,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 250
tpv.core.entities DEBUG 2025-03-02 07:01:58,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:01:58,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (250) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:01:58,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (250) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:01:59,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (250) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:01:59,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (250) Working directory for job is: /galaxy/server/database/jobs_directory/000/250
galaxy.jobs.runners DEBUG 2025-03-02 07:01:59,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [250] queued (38.650 ms)
galaxy.jobs.handler INFO 2025-03-02 07:01:59,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (250) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:59,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 250
galaxy.jobs DEBUG 2025-03-02 07:01:59,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [250] prepared (101.331 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:01:59,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/250/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/250/registry.xml' '/galaxy/server/database/jobs_directory/000/250/upload_params.json' '291:/galaxy/server/database/objects/3/c/3/dataset_3c3c1c99-a19d-4847-82ab-d1ff884781b5_files:/galaxy/server/database/objects/3/c/3/dataset_3c3c1c99-a19d-4847-82ab-d1ff884781b5.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:01:59,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (250) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/250/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/250/galaxy_250.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:59,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 250 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:59,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 250 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:01:59,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:07,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rgtsn with k8s id: gxy-rgtsn succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:02:07,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 249: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:08,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j4zjj with k8s id: gxy-j4zjj succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:02:08,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 250: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:02:15,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 249 finished
galaxy.model.metadata DEBUG 2025-03-02 07:02:15,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 290
galaxy.jobs INFO 2025-03-02 07:02:15,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 249 in /galaxy/server/database/jobs_directory/000/249
galaxy.jobs DEBUG 2025-03-02 07:02:15,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 249 executed (115.357 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:15,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 249 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 07:02:16,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 250 finished
galaxy.model.metadata DEBUG 2025-03-02 07:02:16,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 291
galaxy.jobs INFO 2025-03-02 07:02:16,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 250 in /galaxy/server/database/jobs_directory/000/250
galaxy.jobs DEBUG 2025-03-02 07:02:16,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 250 executed (94.567 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:16,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 250 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:02:17,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 251
tpv.core.entities DEBUG 2025-03-02 07:02:17,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:02:17,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (251) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:02:17,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (251) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:02:17,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (251) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:02:17,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (251) Working directory for job is: /galaxy/server/database/jobs_directory/000/251
galaxy.jobs.runners DEBUG 2025-03-02 07:02:17,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [251] queued (33.834 ms)
galaxy.jobs.handler INFO 2025-03-02 07:02:17,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (251) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:17,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 251
galaxy.jobs DEBUG 2025-03-02 07:02:17,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [251] prepared (74.181 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:17,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:02:17,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:17,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:02:17,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/251/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/251/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/3/c/3/dataset_3c3c1c99-a19d-4847-82ab-d1ff884781b5.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/4/7/7/dataset_477bc097-48a7-4434-9d69-989cd1a0e36d.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/d/c/f/metadata_dcf4636a-3718-4d9e-bddb-2c85e42f7448.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/9/9/3/dataset_99324180-dcc3-4093-a7c6-5581f0365a3d.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:02:17,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (251) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/251/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/251/galaxy_251.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:17,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 251 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:17,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:02:17,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:17,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:17,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 251 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:17,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:30,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ws9ct with k8s id: gxy-ws9ct succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:02:30,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 251: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:02:37,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 251 finished
galaxy.model.metadata DEBUG 2025-03-02 07:02:37,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 292
galaxy.jobs INFO 2025-03-02 07:02:37,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 251 in /galaxy/server/database/jobs_directory/000/251
galaxy.jobs DEBUG 2025-03-02 07:02:37,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 251 executed (107.891 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:37,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 251 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:02:38,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 252
tpv.core.entities DEBUG 2025-03-02 07:02:38,858 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:02:38,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (252) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:02:38,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (252) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:02:38,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (252) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:02:38,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (252) Working directory for job is: /galaxy/server/database/jobs_directory/000/252
galaxy.jobs.runners DEBUG 2025-03-02 07:02:38,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [252] queued (26.642 ms)
galaxy.jobs.handler INFO 2025-03-02 07:02:38,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (252) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:38,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 252
galaxy.jobs DEBUG 2025-03-02 07:02:38,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [252] prepared (63.282 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:02:38,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/252/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/252/registry.xml' '/galaxy/server/database/jobs_directory/000/252/upload_params.json' '293:/galaxy/server/database/objects/e/8/3/dataset_e839ebb4-f750-4320-b5d0-8e13182e8054_files:/galaxy/server/database/objects/e/8/3/dataset_e839ebb4-f750-4320-b5d0-8e13182e8054.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:02:38,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (252) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/252/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/252/galaxy_252.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:39,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 252 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:39,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 252 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:39,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 07:02:39,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 253
tpv.core.entities DEBUG 2025-03-02 07:02:39,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:02:39,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (253) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:02:39,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (253) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:02:39,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (253) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:02:39,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (253) Working directory for job is: /galaxy/server/database/jobs_directory/000/253
galaxy.jobs.runners DEBUG 2025-03-02 07:02:39,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [253] queued (26.416 ms)
galaxy.jobs.handler INFO 2025-03-02 07:02:39,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (253) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:39,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 253
galaxy.jobs DEBUG 2025-03-02 07:02:40,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [253] prepared (75.115 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:02:40,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/253/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/253/registry.xml' '/galaxy/server/database/jobs_directory/000/253/upload_params.json' '294:/galaxy/server/database/objects/d/2/6/dataset_d26390b3-53b5-4541-9e01-c4c8bb861008_files:/galaxy/server/database/objects/d/2/6/dataset_d26390b3-53b5-4541-9e01-c4c8bb861008.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:02:40,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (253) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/253/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/253/galaxy_253.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:40,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 253 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:40,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 253 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:40,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:48,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qg6tl with k8s id: gxy-qg6tl succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:02:48,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 252: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:49,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-sc77h with k8s id: gxy-sc77h succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:02:49,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 253: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:02:56,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 252 finished
galaxy.model.metadata DEBUG 2025-03-02 07:02:56,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 293
galaxy.jobs INFO 2025-03-02 07:02:56,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 252 in /galaxy/server/database/jobs_directory/000/252
galaxy.jobs DEBUG 2025-03-02 07:02:56,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 252 executed (96.461 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:56,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 252 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 07:02:57,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 253 finished
galaxy.model.metadata DEBUG 2025-03-02 07:02:57,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 294
galaxy.jobs INFO 2025-03-02 07:02:57,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 253 in /galaxy/server/database/jobs_directory/000/253
galaxy.jobs DEBUG 2025-03-02 07:02:57,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 253 executed (101.099 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:57,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 253 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:02:58,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 254
tpv.core.entities DEBUG 2025-03-02 07:02:58,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:02:58,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (254) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:02:58,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (254) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:02:58,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (254) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:02:58,387 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (254) Working directory for job is: /galaxy/server/database/jobs_directory/000/254
galaxy.jobs.runners DEBUG 2025-03-02 07:02:58,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [254] queued (26.726 ms)
galaxy.jobs.handler INFO 2025-03-02 07:02:58,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (254) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:58,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 254
galaxy.jobs DEBUG 2025-03-02 07:02:58,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [254] prepared (46.406 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:58,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:02:58,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:58,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:02:58,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/254/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/254/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/d/2/6/dataset_d26390b3-53b5-4541-9e01-c4c8bb861008.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/e/8/3/dataset_e839ebb4-f750-4320-b5d0-8e13182e8054.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/6/5/e/metadata_65ec94bf-516c-4758-807d-936034b69b5f.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 14 --skip-coverage 0 --limit-coverage 0   --haplotype-length 0 --min-alternate-count 1 --min-alternate-fraction 0.05 --pooled-continuous --report-monomorphic --standard-filters  "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/1/c/8/dataset_1c807182-d1c6-418f-b97e-54c888c74b28.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:02:58,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (254) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/254/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/254/galaxy_254.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:58,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 254 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:58,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:02:58,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:02:58,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:58,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 254 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:02:58,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:03,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rwp89 with k8s id: gxy-rwp89 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:03:03,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 254: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:03:10,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 254 finished
galaxy.model.metadata DEBUG 2025-03-02 07:03:10,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 295
galaxy.jobs INFO 2025-03-02 07:03:10,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 254 in /galaxy/server/database/jobs_directory/000/254
galaxy.jobs DEBUG 2025-03-02 07:03:10,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 254 executed (101.495 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:10,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 254 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:03:12,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 256, 255
tpv.core.entities DEBUG 2025-03-02 07:03:12,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:03:12,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (255) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:03:12,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (255) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:03:12,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (255) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:03:12,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (255) Working directory for job is: /galaxy/server/database/jobs_directory/000/255
galaxy.jobs.runners DEBUG 2025-03-02 07:03:12,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [255] queued (27.412 ms)
galaxy.jobs.handler INFO 2025-03-02 07:03:12,705 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (255) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:12,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 255
tpv.core.entities DEBUG 2025-03-02 07:03:12,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:03:12,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (256) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:03:12,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (256) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:03:12,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (256) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:03:12,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (256) Working directory for job is: /galaxy/server/database/jobs_directory/000/256
galaxy.jobs.runners DEBUG 2025-03-02 07:03:12,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [256] queued (49.129 ms)
galaxy.jobs.handler INFO 2025-03-02 07:03:12,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (256) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:12,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 256
galaxy.jobs DEBUG 2025-03-02 07:03:12,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [255] prepared (99.781 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:03:12,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/255/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/255/registry.xml' '/galaxy/server/database/jobs_directory/000/255/upload_params.json' '296:/galaxy/server/database/objects/e/1/8/dataset_e1826539-e6af-4f64-ba6b-5427535eaa94_files:/galaxy/server/database/objects/e/1/8/dataset_e1826539-e6af-4f64-ba6b-5427535eaa94.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:03:12,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (255) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/255/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/255/galaxy_255.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 07:03:12,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [256] prepared (80.529 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:12,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 255 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:12,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 255 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:03:12,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/256/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/256/registry.xml' '/galaxy/server/database/jobs_directory/000/256/upload_params.json' '297:/galaxy/server/database/objects/d/2/a/dataset_d2aa261f-7630-47a6-be12-0e0a4a284d88_files:/galaxy/server/database/objects/d/2/a/dataset_d2aa261f-7630-47a6-be12-0e0a4a284d88.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:03:12,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (256) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/256/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/256/galaxy_256.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:12,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 256 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:12,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 256 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:13,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:13,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:22,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gz5cq with k8s id: gxy-gz5cq succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:03:22,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 256: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:23,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5xrnt with k8s id: gxy-5xrnt succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:03:23,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 255: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:03:30,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 256 finished
galaxy.model.metadata DEBUG 2025-03-02 07:03:30,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 297
galaxy.jobs INFO 2025-03-02 07:03:30,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 256 in /galaxy/server/database/jobs_directory/000/256
galaxy.jobs DEBUG 2025-03-02 07:03:30,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 256 executed (104.214 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:30,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 256 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 07:03:31,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 255 finished
galaxy.model.metadata DEBUG 2025-03-02 07:03:31,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 296
galaxy.jobs INFO 2025-03-02 07:03:31,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 255 in /galaxy/server/database/jobs_directory/000/255
galaxy.jobs DEBUG 2025-03-02 07:03:31,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 255 executed (102.773 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:31,611 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 255 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:03:32,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 257
tpv.core.entities DEBUG 2025-03-02 07:03:32,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:03:32,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (257) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:03:32,146 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (257) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:03:32,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (257) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:03:32,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (257) Working directory for job is: /galaxy/server/database/jobs_directory/000/257
galaxy.jobs.runners DEBUG 2025-03-02 07:03:32,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [257] queued (26.760 ms)
galaxy.jobs.handler INFO 2025-03-02 07:03:32,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (257) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:32,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 257
galaxy.jobs DEBUG 2025-03-02 07:03:32,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [257] prepared (50.031 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:03:32,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:03:32,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:03:32,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:03:32,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/257/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/257/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/d/2/a/dataset_d2aa261f-7630-47a6-be12-0e0a4a284d88.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/e/1/8/dataset_e1826539-e6af-4f64-ba6b-5427535eaa94.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/3/1/4/metadata_3144c4b2-0ecd-47be-b784-1327b5eae842.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 14 --skip-coverage 0 --limit-coverage 0   --haplotype-length 0 --min-alternate-count 1 --min-alternate-fraction 0.05 --pooled-continuous --report-monomorphic --standard-filters  "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/2/b/4/dataset_2b488e72-6f4f-48b9-802f-ac461f9348bb.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:03:32,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (257) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/257/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/257/galaxy_257.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:32,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 257 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:03:32,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:03:32,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:03:32,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:32,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 257 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:32,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:36,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dr9w4 with k8s id: gxy-dr9w4 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:03:36,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 257: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:03:43,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 257 finished
galaxy.model.metadata DEBUG 2025-03-02 07:03:44,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 298
galaxy.jobs INFO 2025-03-02 07:03:44,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 257 in /galaxy/server/database/jobs_directory/000/257
galaxy.jobs DEBUG 2025-03-02 07:03:44,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 257 executed (101.766 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:44,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 257 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:03:45,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 258, 259
tpv.core.entities DEBUG 2025-03-02 07:03:45,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:03:45,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (258) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:03:45,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (258) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:03:45,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (258) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:03:45,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (258) Working directory for job is: /galaxy/server/database/jobs_directory/000/258
galaxy.jobs.runners DEBUG 2025-03-02 07:03:45,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [258] queued (36.057 ms)
galaxy.jobs.handler INFO 2025-03-02 07:03:45,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (258) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:45,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 258
tpv.core.entities DEBUG 2025-03-02 07:03:45,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:03:45,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (259) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:03:45,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (259) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:03:45,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (259) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:03:45,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (259) Working directory for job is: /galaxy/server/database/jobs_directory/000/259
galaxy.jobs.runners DEBUG 2025-03-02 07:03:45,549 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [259] queued (47.905 ms)
galaxy.jobs.handler INFO 2025-03-02 07:03:45,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (259) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:45,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 259
galaxy.jobs DEBUG 2025-03-02 07:03:45,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [258] prepared (95.471 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:03:45,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/258/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/258/registry.xml' '/galaxy/server/database/jobs_directory/000/258/upload_params.json' '299:/galaxy/server/database/objects/6/a/8/dataset_6a83152c-a803-4f25-873d-4503e6d4c7cb_files:/galaxy/server/database/objects/6/a/8/dataset_6a83152c-a803-4f25-873d-4503e6d4c7cb.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:03:45,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (258) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/258/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/258/galaxy_258.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 07:03:45,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [259] prepared (72.460 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:45,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 258 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:45,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 258 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:03:45,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/259/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/259/registry.xml' '/galaxy/server/database/jobs_directory/000/259/upload_params.json' '300:/galaxy/server/database/objects/f/3/4/dataset_f3407b22-57bb-462b-9e4b-5cf95bfbe35b_files:/galaxy/server/database/objects/f/3/4/dataset_f3407b22-57bb-462b-9e4b-5cf95bfbe35b.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:03:45,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (259) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/259/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/259/galaxy_259.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:45,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 259 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:45,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 259 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:46,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:46,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:55,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tdr4t with k8s id: gxy-tdr4t succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:03:55,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 259: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:03:56,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dfqsm with k8s id: gxy-dfqsm succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:03:56,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 258: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:04:03,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 259 finished
galaxy.model.metadata DEBUG 2025-03-02 07:04:03,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 300
galaxy.jobs INFO 2025-03-02 07:04:03,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 259 in /galaxy/server/database/jobs_directory/000/259
galaxy.jobs DEBUG 2025-03-02 07:04:03,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 259 executed (132.123 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:03,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 259 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 07:04:03,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 258 finished
galaxy.model.metadata DEBUG 2025-03-02 07:04:03,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 299
galaxy.jobs INFO 2025-03-02 07:04:03,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 258 in /galaxy/server/database/jobs_directory/000/258
galaxy.jobs DEBUG 2025-03-02 07:04:03,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 258 executed (120.364 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:03,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 258 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:04:04,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 260
tpv.core.entities DEBUG 2025-03-02 07:04:04,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:04:04,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (260) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:04:04,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (260) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:04:04,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (260) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:04:04,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (260) Working directory for job is: /galaxy/server/database/jobs_directory/000/260
galaxy.jobs.runners DEBUG 2025-03-02 07:04:04,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [260] queued (31.090 ms)
galaxy.jobs.handler INFO 2025-03-02 07:04:04,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (260) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:04,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 260
galaxy.jobs DEBUG 2025-03-02 07:04:05,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [260] prepared (57.999 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:05,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:04:05,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:05,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:04:05,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/260/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/260/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/f/3/4/dataset_f3407b22-57bb-462b-9e4b-5cf95bfbe35b.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/6/a/8/dataset_6a83152c-a803-4f25-873d-4503e6d4c7cb.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/3/a/9/metadata_3a953566-92c6-40be-a7e5-abb5ee3131b8.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'    --theta 0.001 --ploidy 1            "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/3/d/b/dataset_3db8a30c-bede-402c-a36b-c0164f83f52b.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:04:05,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (260) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/260/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/260/galaxy_260.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:05,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 260 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:05,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:04:05,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:05,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:05,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 260 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:05,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:09,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xzfr7 with k8s id: gxy-xzfr7 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:04:09,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 260: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:04:16,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 260 finished
galaxy.model.metadata DEBUG 2025-03-02 07:04:16,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 301
galaxy.jobs INFO 2025-03-02 07:04:16,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 260 in /galaxy/server/database/jobs_directory/000/260
galaxy.jobs DEBUG 2025-03-02 07:04:16,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 260 executed (102.469 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:16,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 260 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:04:18,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 262, 261
tpv.core.entities DEBUG 2025-03-02 07:04:18,259 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:04:18,259 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (261) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:04:18,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (261) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:04:18,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (261) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:04:18,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (261) Working directory for job is: /galaxy/server/database/jobs_directory/000/261
galaxy.jobs.runners DEBUG 2025-03-02 07:04:18,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [261] queued (33.672 ms)
galaxy.jobs.handler INFO 2025-03-02 07:04:18,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (261) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:18,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 261
tpv.core.entities DEBUG 2025-03-02 07:04:18,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:04:18,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (262) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:04:18,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (262) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:04:18,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (262) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:04:18,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (262) Working directory for job is: /galaxy/server/database/jobs_directory/000/262
galaxy.jobs.runners DEBUG 2025-03-02 07:04:18,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [262] queued (43.692 ms)
galaxy.jobs.handler INFO 2025-03-02 07:04:18,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (262) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:18,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 262
galaxy.jobs DEBUG 2025-03-02 07:04:18,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [261] prepared (84.223 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:04:18,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/261/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/261/registry.xml' '/galaxy/server/database/jobs_directory/000/261/upload_params.json' '302:/galaxy/server/database/objects/5/7/f/dataset_57f15a90-a6e1-468b-a432-20a463c789c1_files:/galaxy/server/database/objects/5/7/f/dataset_57f15a90-a6e1-468b-a432-20a463c789c1.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:04:18,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (261) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/261/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/261/galaxy_261.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 07:04:18,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [262] prepared (65.492 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:18,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 261 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:18,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 261 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:04:18,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/262/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/262/registry.xml' '/galaxy/server/database/jobs_directory/000/262/upload_params.json' '303:/galaxy/server/database/objects/d/8/f/dataset_d8f58686-44ad-474f-9daa-f913bd73485f_files:/galaxy/server/database/objects/d/8/f/dataset_d8f58686-44ad-474f-9daa-f913bd73485f.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:04:18,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (262) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/262/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/262/galaxy_262.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:18,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 262 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:18,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 262 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:19,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:19,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:28,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-59fz6 with k8s id: gxy-59fz6 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:28,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ts2x8 with k8s id: gxy-ts2x8 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:04:29,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 261: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:04:29,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 262: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:04:36,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 262 finished
galaxy.model.metadata DEBUG 2025-03-02 07:04:36,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 303
galaxy.jobs INFO 2025-03-02 07:04:36,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 262 in /galaxy/server/database/jobs_directory/000/262
galaxy.jobs DEBUG 2025-03-02 07:04:36,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 262 executed (115.551 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:36,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 262 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 07:04:36,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 261 finished
galaxy.model.metadata DEBUG 2025-03-02 07:04:36,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 302
galaxy.jobs INFO 2025-03-02 07:04:37,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 261 in /galaxy/server/database/jobs_directory/000/261
galaxy.jobs DEBUG 2025-03-02 07:04:37,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 261 executed (113.818 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:37,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 261 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:04:37,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 263
tpv.core.entities DEBUG 2025-03-02 07:04:37,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:04:37,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (263) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:04:37,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (263) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:04:37,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (263) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:04:37,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (263) Working directory for job is: /galaxy/server/database/jobs_directory/000/263
galaxy.jobs.runners DEBUG 2025-03-02 07:04:37,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [263] queued (43.046 ms)
galaxy.jobs.handler INFO 2025-03-02 07:04:37,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (263) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:37,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 263
galaxy.jobs DEBUG 2025-03-02 07:04:37,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [263] prepared (52.292 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:37,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:04:37,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:37,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:04:37,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/263/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/263/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/d/8/f/dataset_d8f58686-44ad-474f-9daa-f913bd73485f.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/5/7/f/dataset_57f15a90-a6e1-468b-a432-20a463c789c1.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/4/7/e/metadata_47e25d27-cf47-4eeb-bf61-f01c676f274f.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 250 --skip-coverage 0 --limit-coverage 0    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/0/d/2/dataset_0d20541d-ebed-4354-bce7-53f1fed49ca4.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:04:37,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (263) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/263/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/263/galaxy_263.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:37,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 263 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:37,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:04:37,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:04:37,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:37,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 263 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:39,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:43,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jt8dl with k8s id: gxy-jt8dl succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:04:43,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 263: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:04:50,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 263 finished
galaxy.model.metadata DEBUG 2025-03-02 07:04:50,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 304
galaxy.jobs INFO 2025-03-02 07:04:50,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 263 in /galaxy/server/database/jobs_directory/000/263
galaxy.jobs DEBUG 2025-03-02 07:04:50,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 263 executed (118.357 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:50,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 263 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:04:52,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 264, 265
tpv.core.entities DEBUG 2025-03-02 07:04:52,170 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:04:52,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (264) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:04:52,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (264) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:04:52,186 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (264) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:04:52,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (264) Working directory for job is: /galaxy/server/database/jobs_directory/000/264
galaxy.jobs.runners DEBUG 2025-03-02 07:04:52,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [264] queued (40.180 ms)
galaxy.jobs.handler INFO 2025-03-02 07:04:52,217 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (264) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:52,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 264
tpv.core.entities DEBUG 2025-03-02 07:04:52,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:04:52,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (265) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:04:52,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (265) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:04:52,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (265) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:04:52,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (265) Working directory for job is: /galaxy/server/database/jobs_directory/000/265
galaxy.jobs.runners DEBUG 2025-03-02 07:04:52,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [265] queued (44.969 ms)
galaxy.jobs.handler INFO 2025-03-02 07:04:52,288 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (265) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:52,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 265
galaxy.jobs DEBUG 2025-03-02 07:04:52,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [264] prepared (106.851 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:04:52,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/264/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/264/registry.xml' '/galaxy/server/database/jobs_directory/000/264/upload_params.json' '305:/galaxy/server/database/objects/3/8/b/dataset_38b3ceb8-a869-4e9a-883f-f381d5ba543c_files:/galaxy/server/database/objects/3/8/b/dataset_38b3ceb8-a869-4e9a-883f-f381d5ba543c.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:04:52,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (264) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/264/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/264/galaxy_264.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 07:04:52,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [265] prepared (84.699 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:52,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 264 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:52,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 264 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:04:52,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/265/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/265/registry.xml' '/galaxy/server/database/jobs_directory/000/265/upload_params.json' '306:/galaxy/server/database/objects/a/e/f/dataset_aef561f4-d8d4-47ad-a433-89c4e717a4c9_files:/galaxy/server/database/objects/a/e/f/dataset_aef561f4-d8d4-47ad-a433-89c4e717a4c9.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:04:52,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (265) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/265/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/265/galaxy_265.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:52,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 265 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:52,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 265 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:53,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:04:53,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:03,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t24gh with k8s id: gxy-t24gh succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:03,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lr9hc with k8s id: gxy-lr9hc succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:05:03,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 264: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:05:03,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 265: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:05:11,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 265 finished
galaxy.model.metadata DEBUG 2025-03-02 07:05:11,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 306
galaxy.jobs.runners DEBUG 2025-03-02 07:05:11,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 264 finished
galaxy.jobs INFO 2025-03-02 07:05:11,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 265 in /galaxy/server/database/jobs_directory/000/265
galaxy.model.metadata DEBUG 2025-03-02 07:05:11,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 305
galaxy.jobs DEBUG 2025-03-02 07:05:11,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 265 executed (120.407 ms)
galaxy.jobs INFO 2025-03-02 07:05:11,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 264 in /galaxy/server/database/jobs_directory/000/264
galaxy.jobs DEBUG 2025-03-02 07:05:11,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 264 executed (113.948 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:11,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 265 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:11,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 264 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:05:12,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 266
tpv.core.entities DEBUG 2025-03-02 07:05:12,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:05:12,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (266) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:05:12,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (266) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:05:12,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (266) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:05:12,765 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (266) Working directory for job is: /galaxy/server/database/jobs_directory/000/266
galaxy.jobs.runners DEBUG 2025-03-02 07:05:12,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [266] queued (52.642 ms)
galaxy.jobs.handler INFO 2025-03-02 07:05:12,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (266) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:12,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 266
galaxy.jobs DEBUG 2025-03-02 07:05:12,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [266] prepared (42.104 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:12,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:05:12,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:12,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:05:12,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/266/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/266/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/a/e/f/dataset_aef561f4-d8d4-47ad-a433-89c4e717a4c9.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/3/8/b/dataset_38b3ceb8-a869-4e9a-883f-f381d5ba543c.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/8/5/e/metadata_85e40fa5-8e62-43b2-8aaa-51010372c51b.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 0 --skip-coverage 0 --limit-coverage 400    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/0/0/6/dataset_00689544-48ec-4858-b3e4-370ab0e8e9fa.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:05:12,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (266) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/266/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/266/galaxy_266.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:12,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 266 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:12,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:05:12,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:12,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:12,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 266 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:13,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,354 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wx2hv failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-wx2hv.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 266 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 07:05:17,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-wx2hv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-wx2hv": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (266/gxy-wx2hv) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (266/gxy-wx2hv) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (266/gxy-wx2hv) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (266/gxy-wx2hv) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-wx2hv.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 266 (gxy-wx2hv)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-wx2hv to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 266 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:17,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (266/gxy-wx2hv) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-02 07:05:18,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 268, 267
tpv.core.entities DEBUG 2025-03-02 07:05:18,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:05:18,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (267) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:05:18,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (267) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:05:18,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (267) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:05:18,929 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (267) Working directory for job is: /galaxy/server/database/jobs_directory/000/267
galaxy.jobs.runners DEBUG 2025-03-02 07:05:18,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [267] queued (29.442 ms)
galaxy.jobs.handler INFO 2025-03-02 07:05:18,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (267) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:18,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 267
tpv.core.entities DEBUG 2025-03-02 07:05:18,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:05:18,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (268) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:05:18,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (268) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:05:18,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (268) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:05:18,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (268) Working directory for job is: /galaxy/server/database/jobs_directory/000/268
galaxy.jobs.runners DEBUG 2025-03-02 07:05:18,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [268] queued (41.958 ms)
galaxy.jobs.handler INFO 2025-03-02 07:05:18,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (268) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:18,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 268
galaxy.jobs DEBUG 2025-03-02 07:05:19,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [267] prepared (102.214 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:05:19,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/267/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/267/registry.xml' '/galaxy/server/database/jobs_directory/000/267/upload_params.json' '308:/galaxy/server/database/objects/c/9/7/dataset_c970e520-95a5-4947-aa14-019d95ff7642_files:/galaxy/server/database/objects/c/9/7/dataset_c970e520-95a5-4947-aa14-019d95ff7642.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:05:19,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (267) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/267/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/267/galaxy_267.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 07:05:19,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [268] prepared (93.496 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:19,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 267 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:19,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 267 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:05:19,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/268/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/268/registry.xml' '/galaxy/server/database/jobs_directory/000/268/upload_params.json' '309:/galaxy/server/database/objects/0/3/2/dataset_0323e431-b81a-416a-9b3e-64fc35a6529e_files:/galaxy/server/database/objects/0/3/2/dataset_0323e431-b81a-416a-9b3e-64fc35a6529e.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:05:19,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (268) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/268/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/268/galaxy_268.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:19,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 268 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:19,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 268 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:19,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:19,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:28,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w8v86 with k8s id: gxy-w8v86 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:05:28,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 268: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:29,752 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v5gvs with k8s id: gxy-v5gvs succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:05:29,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 267: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:05:36,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 268 finished
galaxy.model.metadata DEBUG 2025-03-02 07:05:36,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 309
galaxy.jobs INFO 2025-03-02 07:05:36,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 268 in /galaxy/server/database/jobs_directory/000/268
galaxy.jobs DEBUG 2025-03-02 07:05:36,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 268 executed (110.308 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:36,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 268 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners DEBUG 2025-03-02 07:05:37,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 267 finished
galaxy.model.metadata DEBUG 2025-03-02 07:05:37,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 308
galaxy.jobs INFO 2025-03-02 07:05:37,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 267 in /galaxy/server/database/jobs_directory/000/267
galaxy.jobs DEBUG 2025-03-02 07:05:37,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 267 executed (128.215 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:37,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 267 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:05:38,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 269
tpv.core.entities DEBUG 2025-03-02 07:05:38,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/.*, abstract=False, cores=10, mem=9 + input_size * 1, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:05:38,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (269) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:05:38,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (269) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:05:38,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (269) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:05:38,405 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (269) Working directory for job is: /galaxy/server/database/jobs_directory/000/269
galaxy.jobs.runners DEBUG 2025-03-02 07:05:38,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [269] queued (30.748 ms)
galaxy.jobs.handler INFO 2025-03-02 07:05:38,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (269) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:38,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 269
galaxy.jobs DEBUG 2025-03-02 07:05:38,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [269] prepared (42.451 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:38,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:05:38,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:38,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:05:38,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/269/tool_script.sh] for tool command [freebayes --version | cut -d v -f 3 > /galaxy/server/database/jobs_directory/000/269/outputs/COMMAND_VERSION 2>&1;
ln -s -f '/galaxy/server/database/objects/0/3/2/dataset_0323e431-b81a-416a-9b3e-64fc35a6529e.dat' 'localref.fa' && samtools faidx 'localref.fa' 2>&1 || echo "Error running samtools faidx for FreeBayes" >&2 &&   ln -s -f '/galaxy/server/database/objects/c/9/7/dataset_c970e520-95a5-4947-aa14-019d95ff7642.dat' 'b_0.bam' && ln -s -f '/galaxy/server/database/objects/_metadata_files/2/5/5/metadata_255a7492-f331-4862-a939-3f01821bd50b.dat' 'b_0.bam.bai' &&   samtools view -H b_0.bam| grep '^@SQ' | cut -f 2- | awk '{ gsub("^SN:","",$1); gsub("^LN:","",$2); print $1"\t0\t"$2; }' >> regions_all.bed &&  sort -u regions_all.bed > regions_uniq.bed &&  mkdir vcf_output failed_alleles trace &&   for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do echo "   freebayes  --region '$i'  --bam 'b_0.bam' --fasta-reference 'localref.fa'  --vcf './vcf_output/part_$i.vcf'   --min-coverage 0 --skip-coverage 100 --limit-coverage 0    "; done > freebayes_commands.sh &&  cat freebayes_commands.sh | parallel --will-cite -j ${GALAXY_SLOTS:-1} &&  grep "^#" "./vcf_output/part_$i.vcf" > header.txt &&  for i in `cat regions_uniq.bed | awk '{print $1":"$2".."$3}'`; do cat "./vcf_output/part_$i.vcf" | grep -v "^#" || true ; done | sort -k1,1 -k2,2n -k5,5 -u | cat header.txt - > '/galaxy/server/database/objects/c/f/e/dataset_cfe43310-cea0-4032-bdb2-e608c4195a1a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:05:38,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (269) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/269/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/269/galaxy_269.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:38,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 269 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:38,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:05:38,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/freebayes/freebayes/1.3.8+galaxy0: mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad
galaxy.tool_util.deps.containers INFO 2025-03-02 07:05:38,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-7bde9f0045905cc44cb726ad016ff10c70a194d7:b4de2951c942c93b5ed20929f2c533ceb2c2c1ad-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:38,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 269 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:38,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:42,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-z8gk8 with k8s id: gxy-z8gk8 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:05:43,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 269: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:05:50,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 269 finished
galaxy.model.metadata DEBUG 2025-03-02 07:05:50,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 310
galaxy.jobs INFO 2025-03-02 07:05:50,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 269 in /galaxy/server/database/jobs_directory/000/269
galaxy.jobs DEBUG 2025-03-02 07:05:50,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 269 executed (112.662 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:50,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 269 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:05:51,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 270
tpv.core.entities DEBUG 2025-03-02 07:05:51,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:05:51,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (270) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:05:51,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (270) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:05:51,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (270) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:05:51,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (270) Working directory for job is: /galaxy/server/database/jobs_directory/000/270
galaxy.jobs.runners DEBUG 2025-03-02 07:05:51,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [270] queued (28.481 ms)
galaxy.jobs.handler INFO 2025-03-02 07:05:51,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (270) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:51,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 270
galaxy.jobs DEBUG 2025-03-02 07:05:51,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [270] prepared (63.746 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:05:51,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/270/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/270/registry.xml' '/galaxy/server/database/jobs_directory/000/270/upload_params.json' '311:/galaxy/server/database/objects/f/6/3/dataset_f631f900-ac2d-45ca-a773-90bcab613991_files:/galaxy/server/database/objects/f/6/3/dataset_f631f900-ac2d-45ca-a773-90bcab613991.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:05:51,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (270) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/270/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/270/galaxy_270.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:51,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 270 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:51,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 270 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:52,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.handler DEBUG 2025-03-02 07:05:52,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 271
tpv.core.entities DEBUG 2025-03-02 07:05:52,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:05:52,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (271) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:05:52,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (271) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:05:52,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (271) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:05:52,803 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (271) Working directory for job is: /galaxy/server/database/jobs_directory/000/271
galaxy.jobs.runners DEBUG 2025-03-02 07:05:52,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [271] queued (35.795 ms)
galaxy.jobs.handler INFO 2025-03-02 07:05:52,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (271) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:52,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 271
galaxy.jobs DEBUG 2025-03-02 07:05:52,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [271] prepared (78.482 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:05:52,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/271/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/271/registry.xml' '/galaxy/server/database/jobs_directory/000/271/upload_params.json' '312:/galaxy/server/database/objects/2/0/7/dataset_207e5d27-c83c-4d1d-8260-e284bd9385c7_files:/galaxy/server/database/objects/2/0/7/dataset_207e5d27-c83c-4d1d-8260-e284bd9385c7.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:05:52,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (271) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/271/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/271/galaxy_271.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:52,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 271 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:52,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 271 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:05:53,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-875dh failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-875dh.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 270 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 07:06:01,615 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-875dh

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-875dh": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (270/gxy-875dh) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (270/gxy-875dh) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (270/gxy-875dh) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (270/gxy-875dh) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-875dh.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Attempting to stop job 270 (gxy-875dh)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Found job with id gxy-875dh to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 270 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:01,695 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (270/gxy-875dh) Terminated at user's request
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:02,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4f7tz with k8s id: gxy-4f7tz succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:06:02,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 271: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-03-02 07:06:04,005 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 272
tpv.core.entities DEBUG 2025-03-02 07:06:04,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:06:04,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (272) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:06:04,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (272) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:06:04,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (272) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:06:04,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (272) Working directory for job is: /galaxy/server/database/jobs_directory/000/272
galaxy.jobs.runners DEBUG 2025-03-02 07:06:04,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [272] queued (34.487 ms)
galaxy.jobs.handler INFO 2025-03-02 07:06:04,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (272) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:04,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 272
galaxy.jobs DEBUG 2025-03-02 07:06:04,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [272] prepared (75.219 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:06:04,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/272/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/272/registry.xml' '/galaxy/server/database/jobs_directory/000/272/upload_params.json' '313:/galaxy/server/database/objects/3/2/2/dataset_3229dbfa-a8ce-4cda-8b00-42df0916231c_files:/galaxy/server/database/objects/3/2/2/dataset_3229dbfa-a8ce-4cda-8b00-42df0916231c.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:06:04,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (272) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/272/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/272/galaxy_272.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:04,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 272 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:04,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 272 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:04,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners DEBUG 2025-03-02 07:06:10,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 271 finished
galaxy.model.metadata DEBUG 2025-03-02 07:06:10,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 312
galaxy.jobs INFO 2025-03-02 07:06:10,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 271 in /galaxy/server/database/jobs_directory/000/271
galaxy.jobs DEBUG 2025-03-02 07:06:10,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 271 executed (112.512 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:10,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 271 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:12,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-w98lk with k8s id: gxy-w98lk succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:06:13,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 272: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:06:20,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 272 finished
galaxy.model.metadata DEBUG 2025-03-02 07:06:20,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 313
galaxy.jobs INFO 2025-03-02 07:06:20,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 272 in /galaxy/server/database/jobs_directory/000/272
galaxy.jobs DEBUG 2025-03-02 07:06:20,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 272 executed (99.536 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:20,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 272 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:06:21,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 273
tpv.core.entities DEBUG 2025-03-02 07:06:21,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:06:21,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (273) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:06:21,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (273) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:06:21,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (273) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:06:21,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (273) Working directory for job is: /galaxy/server/database/jobs_directory/000/273
galaxy.jobs.runners DEBUG 2025-03-02 07:06:21,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [273] queued (42.148 ms)
galaxy.jobs.handler INFO 2025-03-02 07:06:21,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (273) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:21,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 273
galaxy.jobs DEBUG 2025-03-02 07:06:21,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [273] prepared (77.838 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:06:21,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:06:21,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:06:21,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:06:21,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/273/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/273/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/2/2/dataset_3229dbfa-a8ce-4cda-8b00-42df0916231c.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp    -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/2/a/c/dataset_2ac708d2-03a2-42fe-858e-87f114d590c9.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/4/a/f/dataset_4af03080-eca7-4f2b-ad27-a48f7b903d81.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/0/c/0/dataset_0c078031-6159-49d3-a252-c090fccbcafe.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/0/c/0/dataset_0c078031-6159-49d3-a252-c090fccbcafe.dat; python -m base64 $plot >> /galaxy/server/database/objects/0/c/0/dataset_0c078031-6159-49d3-a252-c090fccbcafe.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/0/c/0/dataset_0c078031-6159-49d3-a252-c090fccbcafe.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/0/c/0/dataset_0c078031-6159-49d3-a252-c090fccbcafe.dat;)]
galaxy.jobs.runners DEBUG 2025-03-02 07:06:21,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (273) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/273/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/273/galaxy_273.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:21,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 273 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:06:21,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:06:21,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:06:21,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:21,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 273 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:22,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:06:57,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-j5h75 with k8s id: gxy-j5h75 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:06:57,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 273: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:07:05,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 273 finished
galaxy.model.metadata DEBUG 2025-03-02 07:07:05,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 314
galaxy.model.metadata DEBUG 2025-03-02 07:07:05,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 315
galaxy.model.metadata DEBUG 2025-03-02 07:07:05,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 316
galaxy.util WARNING 2025-03-02 07:07:05,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/a/c/dataset_2ac708d2-03a2-42fe-858e-87f114d590c9.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/a/c/dataset_2ac708d2-03a2-42fe-858e-87f114d590c9.dat'
galaxy.util WARNING 2025-03-02 07:07:05,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/4/a/f/dataset_4af03080-eca7-4f2b-ad27-a48f7b903d81.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/4/a/f/dataset_4af03080-eca7-4f2b-ad27-a48f7b903d81.dat'
galaxy.jobs INFO 2025-03-02 07:07:05,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 273 in /galaxy/server/database/jobs_directory/000/273
galaxy.jobs DEBUG 2025-03-02 07:07:05,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 273 executed (137.991 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:05,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 273 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:07:07,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 274
tpv.core.entities DEBUG 2025-03-02 07:07:07,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:07:07,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (274) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:07:07,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (274) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:07:07,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (274) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:07:07,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (274) Working directory for job is: /galaxy/server/database/jobs_directory/000/274
galaxy.jobs.runners DEBUG 2025-03-02 07:07:07,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [274] queued (28.929 ms)
galaxy.jobs.handler INFO 2025-03-02 07:07:07,393 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (274) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:07,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 274
galaxy.jobs DEBUG 2025-03-02 07:07:07,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [274] prepared (74.616 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:07:07,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/274/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/274/registry.xml' '/galaxy/server/database/jobs_directory/000/274/upload_params.json' '317:/galaxy/server/database/objects/2/3/c/dataset_23c5483c-1fd2-4f50-a65d-3b6d82fa4d3b_files:/galaxy/server/database/objects/2/3/c/dataset_23c5483c-1fd2-4f50-a65d-3b6d82fa4d3b.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:07:07,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (274) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/274/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/274/galaxy_274.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:07,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 274 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:07,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 274 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:07,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:17,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lznbq with k8s id: gxy-lznbq succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:07:17,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 274: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:07:24,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 274 finished
galaxy.model.metadata DEBUG 2025-03-02 07:07:24,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 317
galaxy.jobs INFO 2025-03-02 07:07:24,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 274 in /galaxy/server/database/jobs_directory/000/274
galaxy.jobs DEBUG 2025-03-02 07:07:24,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 274 executed (97.540 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:24,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 274 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:07:25,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 275
tpv.core.entities DEBUG 2025-03-02 07:07:25,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:07:25,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (275) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:07:25,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (275) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:07:25,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (275) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:07:25,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (275) Working directory for job is: /galaxy/server/database/jobs_directory/000/275
galaxy.jobs.runners DEBUG 2025-03-02 07:07:25,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [275] queued (37.395 ms)
galaxy.jobs.handler INFO 2025-03-02 07:07:25,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (275) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:25,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 275
galaxy.jobs DEBUG 2025-03-02 07:07:25,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [275] prepared (51.388 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:25,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:07:25,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:26,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:07:26,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/275/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/275/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/2/3/c/dataset_23c5483c-1fd2-4f50-a65d-3b6d82fa4d3b.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp     --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/1/2/9/dataset_12952db4-40cc-46db-9674-4ac79c9e6b66.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/9/f/6/dataset_9f6bd190-1330-4466-a27d-04af2a5667f1.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:07:26,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (275) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/275/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/275/galaxy_275.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:26,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 275 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:26,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:07:26,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:26,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:26,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 275 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:27,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:31,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p2rqk with k8s id: gxy-p2rqk succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:07:31,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 275: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:07:39,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 275 finished
galaxy.model.metadata DEBUG 2025-03-02 07:07:39,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 318
galaxy.model.metadata DEBUG 2025-03-02 07:07:39,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 319
galaxy.util WARNING 2025-03-02 07:07:39,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/2/9/dataset_12952db4-40cc-46db-9674-4ac79c9e6b66.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/2/9/dataset_12952db4-40cc-46db-9674-4ac79c9e6b66.dat'
galaxy.util WARNING 2025-03-02 07:07:39,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/f/6/dataset_9f6bd190-1330-4466-a27d-04af2a5667f1.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/f/6/dataset_9f6bd190-1330-4466-a27d-04af2a5667f1.dat'
galaxy.jobs INFO 2025-03-02 07:07:39,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 275 in /galaxy/server/database/jobs_directory/000/275
galaxy.jobs DEBUG 2025-03-02 07:07:39,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 275 executed (102.861 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:39,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 275 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:07:41,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 276
tpv.core.entities DEBUG 2025-03-02 07:07:41,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:07:41,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (276) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:07:41,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (276) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:07:41,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (276) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:07:41,159 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (276) Working directory for job is: /galaxy/server/database/jobs_directory/000/276
galaxy.jobs.runners DEBUG 2025-03-02 07:07:41,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [276] queued (33.329 ms)
galaxy.jobs.handler INFO 2025-03-02 07:07:41,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (276) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:41,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 276
galaxy.jobs DEBUG 2025-03-02 07:07:41,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [276] prepared (73.399 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:07:41,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/276/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/276/registry.xml' '/galaxy/server/database/jobs_directory/000/276/upload_params.json' '320:/galaxy/server/database/objects/d/a/1/dataset_da1d9981-2e3b-424e-ae71-cab41dd8e353_files:/galaxy/server/database/objects/d/a/1/dataset_da1d9981-2e3b-424e-ae71-cab41dd8e353.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:07:41,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (276) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/276/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/276/galaxy_276.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:41,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 276 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:41,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 276 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:41,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:51,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ql7p7 with k8s id: gxy-ql7p7 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:07:51,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 276: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:07:58,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 276 finished
galaxy.model.metadata DEBUG 2025-03-02 07:07:58,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 320
galaxy.jobs INFO 2025-03-02 07:07:58,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 276 in /galaxy/server/database/jobs_directory/000/276
galaxy.jobs DEBUG 2025-03-02 07:07:58,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 276 executed (96.426 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:58,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 276 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:07:59,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 277
tpv.core.entities DEBUG 2025-03-02 07:07:59,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:07:59,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (277) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:07:59,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (277) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:07:59,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (277) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:07:59,564 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (277) Working directory for job is: /galaxy/server/database/jobs_directory/000/277
galaxy.jobs.runners DEBUG 2025-03-02 07:07:59,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [277] queued (43.181 ms)
galaxy.jobs.handler INFO 2025-03-02 07:07:59,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (277) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:59,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 277
galaxy.jobs DEBUG 2025-03-02 07:07:59,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [277] prepared (46.477 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:59,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:07:59,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:59,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:07:59,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/277/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/277/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/d/a/1/dataset_da1d9981-2e3b-424e-ae71-cab41dd8e353.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp  -c 'test' -s 'test'   -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/d/e/6/dataset_de633144-9af8-4d37-90e2-62f157ba13b7.dat' && mv cnv_tmp/summary.tab '/galaxy/server/database/objects/9/d/c/dataset_9dc7d4e0-308f-4908-ba8e-41c5c7a26cec.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/b/e/0/dataset_be0f92b4-c7e5-49e9-9ed3-9638607ae4ab.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/b/e/0/dataset_be0f92b4-c7e5-49e9-9ed3-9638607ae4ab.dat; python -m base64 $plot >> /galaxy/server/database/objects/b/e/0/dataset_be0f92b4-c7e5-49e9-9ed3-9638607ae4ab.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/b/e/0/dataset_be0f92b4-c7e5-49e9-9ed3-9638607ae4ab.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/b/e/0/dataset_be0f92b4-c7e5-49e9-9ed3-9638607ae4ab.dat;)]
galaxy.jobs.runners DEBUG 2025-03-02 07:07:59,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (277) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/277/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/277/galaxy_277.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:59,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 277 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:59,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:07:59,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:07:59,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:07:59,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 277 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:00,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:06,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bkpxj with k8s id: gxy-bkpxj succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:08:06,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 277: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:08:13,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 277 finished
galaxy.model.metadata DEBUG 2025-03-02 07:08:13,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 321
galaxy.model.metadata DEBUG 2025-03-02 07:08:13,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 322
galaxy.model.metadata DEBUG 2025-03-02 07:08:13,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 323
galaxy.util WARNING 2025-03-02 07:08:13,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/e/6/dataset_de633144-9af8-4d37-90e2-62f157ba13b7.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/e/6/dataset_de633144-9af8-4d37-90e2-62f157ba13b7.dat'
galaxy.util WARNING 2025-03-02 07:08:13,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/d/c/dataset_9dc7d4e0-308f-4908-ba8e-41c5c7a26cec.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/d/c/dataset_9dc7d4e0-308f-4908-ba8e-41c5c7a26cec.dat'
galaxy.jobs INFO 2025-03-02 07:08:13,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 277 in /galaxy/server/database/jobs_directory/000/277
galaxy.jobs DEBUG 2025-03-02 07:08:13,811 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 277 executed (103.112 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:13,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 277 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:08:14,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 278
tpv.core.entities DEBUG 2025-03-02 07:08:14,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:08:14,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (278) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:08:14,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (278) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:08:14,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (278) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:08:14,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (278) Working directory for job is: /galaxy/server/database/jobs_directory/000/278
galaxy.jobs.runners DEBUG 2025-03-02 07:08:14,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [278] queued (29.307 ms)
galaxy.jobs.handler INFO 2025-03-02 07:08:14,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (278) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:14,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 278
galaxy.jobs DEBUG 2025-03-02 07:08:14,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [278] prepared (72.578 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:08:15,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/278/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/278/registry.xml' '/galaxy/server/database/jobs_directory/000/278/upload_params.json' '324:/galaxy/server/database/objects/b/a/5/dataset_ba54b60a-9246-455f-b131-61adeb562c83_files:/galaxy/server/database/objects/b/a/5/dataset_ba54b60a-9246-455f-b131-61adeb562c83.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:08:15,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (278) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/278/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/278/galaxy_278.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:15,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 278 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:15,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 278 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:15,245 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:24,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nkrb4 with k8s id: gxy-nkrb4 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:08:24,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 278: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:08:32,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 278 finished
galaxy.model.metadata DEBUG 2025-03-02 07:08:32,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 324
galaxy.jobs INFO 2025-03-02 07:08:32,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 278 in /galaxy/server/database/jobs_directory/000/278
galaxy.jobs DEBUG 2025-03-02 07:08:32,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 278 executed (119.811 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:32,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 278 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:08:33,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 279
tpv.core.entities DEBUG 2025-03-02 07:08:33,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:08:33,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (279) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:08:33,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (279) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:08:33,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (279) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:08:33,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (279) Working directory for job is: /galaxy/server/database/jobs_directory/000/279
galaxy.jobs.runners DEBUG 2025-03-02 07:08:33,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [279] queued (32.267 ms)
galaxy.jobs.handler INFO 2025-03-02 07:08:33,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (279) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:33,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 279
galaxy.jobs DEBUG 2025-03-02 07:08:33,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [279] prepared (61.281 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:08:33,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:08:33,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:08:33,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:08:33,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/279/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/279/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/a/5/dataset_ba54b60a-9246-455f-b131-61adeb562c83.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp     --aberrant "1.0,1.0" --BAF-weight 1 --BAF-dev "0.04,0.04" --LRR-weight 0 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/7/2/c/dataset_72cfe28b-089a-41b5-8a42-a0e20ed0c0ea.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/7/9/f/dataset_79f7d231-53d1-4890-895d-30c9835f0606.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:08:33,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (279) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/279/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/279/galaxy_279.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:33,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 279 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:08:33,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:08:33,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:08:33,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:33,505 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 279 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:34,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:38,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fhczl with k8s id: gxy-fhczl succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:08:38,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 279: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:08:45,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 279 finished
galaxy.model.metadata DEBUG 2025-03-02 07:08:45,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 325
galaxy.model.metadata DEBUG 2025-03-02 07:08:45,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 326
galaxy.util WARNING 2025-03-02 07:08:45,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/2/c/dataset_72cfe28b-089a-41b5-8a42-a0e20ed0c0ea.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/2/c/dataset_72cfe28b-089a-41b5-8a42-a0e20ed0c0ea.dat'
galaxy.util WARNING 2025-03-02 07:08:45,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/9/f/dataset_79f7d231-53d1-4890-895d-30c9835f0606.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/9/f/dataset_79f7d231-53d1-4890-895d-30c9835f0606.dat'
galaxy.jobs INFO 2025-03-02 07:08:45,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 279 in /galaxy/server/database/jobs_directory/000/279
galaxy.jobs DEBUG 2025-03-02 07:08:45,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 279 executed (117.421 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:45,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 279 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:08:46,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 280
tpv.core.entities DEBUG 2025-03-02 07:08:46,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:08:46,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (280) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:08:46,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (280) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:08:46,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (280) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:08:46,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (280) Working directory for job is: /galaxy/server/database/jobs_directory/000/280
galaxy.jobs.runners DEBUG 2025-03-02 07:08:46,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [280] queued (34.221 ms)
galaxy.jobs.handler INFO 2025-03-02 07:08:46,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (280) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:46,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 280
galaxy.jobs DEBUG 2025-03-02 07:08:46,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [280] prepared (83.992 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:08:46,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/280/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/280/registry.xml' '/galaxy/server/database/jobs_directory/000/280/upload_params.json' '327:/galaxy/server/database/objects/f/f/6/dataset_ff62d90a-fe71-4b4a-81a2-c1ceb4b46f2e_files:/galaxy/server/database/objects/f/f/6/dataset_ff62d90a-fe71-4b4a-81a2-c1ceb4b46f2e.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:08:46,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (280) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/280/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/280/galaxy_280.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:46,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 280 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:46,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 280 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:47,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:08:56,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ql8hh with k8s id: gxy-ql8hh succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:08:56,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 280: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:09:04,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 280 finished
galaxy.model.metadata DEBUG 2025-03-02 07:09:04,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 327
galaxy.jobs INFO 2025-03-02 07:09:04,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 280 in /galaxy/server/database/jobs_directory/000/280
galaxy.jobs DEBUG 2025-03-02 07:09:04,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 280 executed (106.436 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:04,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 280 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:09:04,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 281
tpv.core.entities DEBUG 2025-03-02 07:09:04,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:09:04,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (281) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:09:04,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (281) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:09:04,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (281) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:09:05,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (281) Working directory for job is: /galaxy/server/database/jobs_directory/000/281
galaxy.jobs.runners DEBUG 2025-03-02 07:09:05,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [281] queued (53.378 ms)
galaxy.jobs.handler INFO 2025-03-02 07:09:05,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (281) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:05,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 281
galaxy.jobs DEBUG 2025-03-02 07:09:05,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [281] prepared (59.579 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:05,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:09:05,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:05,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:09:05,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/281/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/281/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/f/f/6/dataset_ff62d90a-fe71-4b4a-81a2-c1ceb4b46f2e.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp    -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09    --regions-overlap 1          input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/0/9/a/dataset_09a597ba-66fa-459a-87de-7c142a3c6acd.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/6/3/0/dataset_630cbc69-5458-40b4-81e4-b1444a8b8a41.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/d/2/8/dataset_d28d556a-1358-480c-b5c1-e65b60ba9559.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/d/2/8/dataset_d28d556a-1358-480c-b5c1-e65b60ba9559.dat; python -m base64 $plot >> /galaxy/server/database/objects/d/2/8/dataset_d28d556a-1358-480c-b5c1-e65b60ba9559.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/d/2/8/dataset_d28d556a-1358-480c-b5c1-e65b60ba9559.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/d/2/8/dataset_d28d556a-1358-480c-b5c1-e65b60ba9559.dat;)]
galaxy.jobs.runners DEBUG 2025-03-02 07:09:05,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (281) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/281/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/281/galaxy_281.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:05,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 281 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:05,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:09:05,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:05,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:05,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 281 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:05,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:12,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nr56l with k8s id: gxy-nr56l succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:09:12,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 281: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:09:19,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 281 finished
galaxy.model.metadata DEBUG 2025-03-02 07:09:19,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 328
galaxy.model.metadata DEBUG 2025-03-02 07:09:19,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 329
galaxy.model.metadata DEBUG 2025-03-02 07:09:19,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 330
galaxy.util WARNING 2025-03-02 07:09:19,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/9/a/dataset_09a597ba-66fa-459a-87de-7c142a3c6acd.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/9/a/dataset_09a597ba-66fa-459a-87de-7c142a3c6acd.dat'
galaxy.util WARNING 2025-03-02 07:09:19,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/3/0/dataset_630cbc69-5458-40b4-81e4-b1444a8b8a41.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/3/0/dataset_630cbc69-5458-40b4-81e4-b1444a8b8a41.dat'
galaxy.jobs INFO 2025-03-02 07:09:19,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 281 in /galaxy/server/database/jobs_directory/000/281
galaxy.jobs DEBUG 2025-03-02 07:09:19,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 281 executed (105.348 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:19,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 281 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:09:22,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 282
tpv.core.entities DEBUG 2025-03-02 07:09:22,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:09:22,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (282) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:09:22,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (282) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:09:22,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (282) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:09:22,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (282) Working directory for job is: /galaxy/server/database/jobs_directory/000/282
galaxy.jobs.runners DEBUG 2025-03-02 07:09:22,374 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [282] queued (27.890 ms)
galaxy.jobs.handler INFO 2025-03-02 07:09:22,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (282) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:22,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 282
galaxy.jobs DEBUG 2025-03-02 07:09:22,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [282] prepared (84.038 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:09:22,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/282/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/282/registry.xml' '/galaxy/server/database/jobs_directory/000/282/upload_params.json' '331:/galaxy/server/database/objects/a/e/e/dataset_aee21c16-14af-4599-85c7-36072fec0b9a_files:/galaxy/server/database/objects/a/e/e/dataset_aee21c16-14af-4599-85c7-36072fec0b9a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:09:22,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (282) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/282/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/282/galaxy_282.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:22,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 282 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:22,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 282 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:23,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:33,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-s64xx with k8s id: gxy-s64xx succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:09:33,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 282: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:09:40,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 282 finished
galaxy.model.metadata DEBUG 2025-03-02 07:09:40,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 331
galaxy.jobs INFO 2025-03-02 07:09:40,942 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 282 in /galaxy/server/database/jobs_directory/000/282
galaxy.jobs DEBUG 2025-03-02 07:09:40,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 282 executed (104.474 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:41,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 282 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:09:41,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 283
tpv.core.entities DEBUG 2025-03-02 07:09:41,740 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:09:41,741 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (283) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:09:41,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (283) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:09:41,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (283) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:09:41,784 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (283) Working directory for job is: /galaxy/server/database/jobs_directory/000/283
galaxy.jobs.runners DEBUG 2025-03-02 07:09:41,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [283] queued (47.901 ms)
galaxy.jobs.handler INFO 2025-03-02 07:09:41,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (283) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:41,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 283
galaxy.jobs DEBUG 2025-03-02 07:09:41,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [283] prepared (61.707 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:41,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:09:41,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:42,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:09:42,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/283/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/283/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/a/e/e/dataset_aee21c16-14af-4599-85c7-36072fec0b9a.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"                 input.vcf.gz  > '/galaxy/server/database/objects/8/9/b/dataset_89b3f948-4694-4aab-8e58-88f3eea382d3.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:09:42,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (283) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/283/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/283/galaxy_283.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:42,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 283 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:42,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:09:42,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-02 07:09:42,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:42,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 283 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:42,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:09:52,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v7zxb with k8s id: gxy-v7zxb succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:09:52,628 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 283: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:10:00,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 283 finished
galaxy.model.metadata DEBUG 2025-03-02 07:10:00,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 332
galaxy.jobs INFO 2025-03-02 07:10:00,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 283 in /galaxy/server/database/jobs_directory/000/283
galaxy.jobs DEBUG 2025-03-02 07:10:00,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 283 executed (105.734 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:00,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 283 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:10:02,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 284
tpv.core.entities DEBUG 2025-03-02 07:10:02,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:10:02,232 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (284) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:10:02,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (284) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:10:02,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (284) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:10:02,269 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (284) Working directory for job is: /galaxy/server/database/jobs_directory/000/284
galaxy.jobs.runners DEBUG 2025-03-02 07:10:02,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [284] queued (40.446 ms)
galaxy.jobs.handler INFO 2025-03-02 07:10:02,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (284) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:02,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 284
galaxy.jobs DEBUG 2025-03-02 07:10:02,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [284] prepared (92.103 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:10:02,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/284/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/284/registry.xml' '/galaxy/server/database/jobs_directory/000/284/upload_params.json' '333:/galaxy/server/database/objects/9/c/6/dataset_9c63e664-9447-4e4b-8548-e08d0da3a978_files:/galaxy/server/database/objects/9/c/6/dataset_9c63e664-9447-4e4b-8548-e08d0da3a978.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:10:02,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (284) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/284/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/284/galaxy_284.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:02,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 284 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:02,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 284 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:02,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:11,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-8lrfp failed and it is not a deletion case. Current state: running
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:11,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Trying with error file in _handle_job_failure
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8lrfp.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Checking if job 284 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes ERROR 2025-03-02 07:10:12,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://34.118.224.1:443/apis/batch/v1/namespaces/edge-25-03-02-06-11-1/jobs/gxy-8lrfp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 872, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 879, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 108, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.12/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-8lrfp": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] PP Getting into fail_job in k8s runner
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (284/gxy-8lrfp) tool_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (284/gxy-8lrfp) tool_stderr: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (284/gxy-8lrfp) job_stdout: 
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (284/gxy-8lrfp) job_stderr: An unknown error occurred in this job and the maximum number of retries have been exceeded for job: gxy-8lrfp.
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Attempting to stop job 284 (gxy-8lrfp)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Found job with id gxy-8lrfp to delete
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 284 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:12,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (284/gxy-8lrfp) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-03-02 07:10:13,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 285
tpv.core.entities DEBUG 2025-03-02 07:10:13,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:10:13,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (285) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:10:13,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (285) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:10:13,522 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (285) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:10:13,535 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (285) Working directory for job is: /galaxy/server/database/jobs_directory/000/285
galaxy.jobs.runners DEBUG 2025-03-02 07:10:13,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [285] queued (32.100 ms)
galaxy.jobs.handler INFO 2025-03-02 07:10:13,545 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (285) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:13,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 285
galaxy.jobs DEBUG 2025-03-02 07:10:13,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [285] prepared (63.416 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:10:13,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/285/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/285/registry.xml' '/galaxy/server/database/jobs_directory/000/285/upload_params.json' '334:/galaxy/server/database/objects/8/e/7/dataset_8e7febf9-1560-4303-acd1-1cac29b6218d_files:/galaxy/server/database/objects/8/e/7/dataset_8e7febf9-1560-4303-acd1-1cac29b6218d.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:10:13,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (285) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/285/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/285/galaxy_285.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:13,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 285 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:13,685 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 285 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:14,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:23,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7qn6n with k8s id: gxy-7qn6n succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:10:23,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 285: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:10:30,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 285 finished
galaxy.model.metadata DEBUG 2025-03-02 07:10:31,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 334
galaxy.jobs INFO 2025-03-02 07:10:31,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 285 in /galaxy/server/database/jobs_directory/000/285
galaxy.jobs DEBUG 2025-03-02 07:10:31,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 285 executed (131.510 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:31,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 285 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:10:31,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 286
tpv.core.entities DEBUG 2025-03-02 07:10:31,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:10:31,904 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (286) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:10:31,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (286) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:10:31,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (286) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:10:31,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (286) Working directory for job is: /galaxy/server/database/jobs_directory/000/286
galaxy.jobs.runners DEBUG 2025-03-02 07:10:31,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [286] queued (28.689 ms)
galaxy.jobs.handler INFO 2025-03-02 07:10:31,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (286) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:31,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 286
galaxy.jobs DEBUG 2025-03-02 07:10:31,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [286] prepared (48.129 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:10:31,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:10:31,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-02 07:10:32,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:10:32,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/286/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/286/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/8/e/7/dataset_8e7febf9-1560-4303-acd1-1cac29b6218d.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"  --ignore-homref --include-noalt  --hw-to-az "6.7e-08" --az-to-hw "5e-09"               --output-type r   input.vcf.gz  > '/galaxy/server/database/objects/7/6/4/dataset_76453568-4e6b-4f45-97bb-aca814b219ac.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:10:32,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (286) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/286/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/286/galaxy_286.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:32,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 286 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:10:32,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:10:32,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-02 07:10:32,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:32,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 286 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:32,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:37,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xrvql with k8s id: gxy-xrvql succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:10:37,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 286: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:10:44,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 286 finished
galaxy.model.metadata DEBUG 2025-03-02 07:10:44,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 335
galaxy.jobs INFO 2025-03-02 07:10:44,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 286 in /galaxy/server/database/jobs_directory/000/286
galaxy.jobs DEBUG 2025-03-02 07:10:45,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 286 executed (96.929 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:45,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 286 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:10:46,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 287
tpv.core.entities DEBUG 2025-03-02 07:10:46,210 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:10:46,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (287) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:10:46,214 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (287) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:10:46,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (287) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:10:46,235 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (287) Working directory for job is: /galaxy/server/database/jobs_directory/000/287
galaxy.jobs.runners DEBUG 2025-03-02 07:10:46,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [287] queued (27.217 ms)
galaxy.jobs.handler INFO 2025-03-02 07:10:46,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (287) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:46,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 287
galaxy.jobs DEBUG 2025-03-02 07:10:46,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [287] prepared (87.197 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:10:46,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/287/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/287/registry.xml' '/galaxy/server/database/jobs_directory/000/287/upload_params.json' '336:/galaxy/server/database/objects/d/8/c/dataset_d8c2ca75-9123-4eb7-8012-25d5f5d9bf60_files:/galaxy/server/database/objects/d/8/c/dataset_d8c2ca75-9123-4eb7-8012-25d5f5d9bf60.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:10:46,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (287) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/287/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/287/galaxy_287.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:46,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 287 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:46,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 287 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:47,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:10:56,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkqcj with k8s id: gxy-hkqcj succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:10:56,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 287: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:11:04,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 287 finished
galaxy.model.metadata DEBUG 2025-03-02 07:11:04,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 336
galaxy.jobs INFO 2025-03-02 07:11:04,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 287 in /galaxy/server/database/jobs_directory/000/287
galaxy.jobs DEBUG 2025-03-02 07:11:04,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 287 executed (121.647 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:04,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 287 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:11:05,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 288
tpv.core.entities DEBUG 2025-03-02 07:11:05,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:11:05,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (288) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:11:05,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (288) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:11:05,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (288) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:11:05,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (288) Working directory for job is: /galaxy/server/database/jobs_directory/000/288
galaxy.jobs.runners DEBUG 2025-03-02 07:11:05,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [288] queued (30.941 ms)
galaxy.jobs.handler INFO 2025-03-02 07:11:05,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (288) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:05,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 288
galaxy.jobs DEBUG 2025-03-02 07:11:05,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [288] prepared (46.144 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:05,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:11:05,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:05,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:11:05,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/288/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/288/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/d/8/c/dataset_d8c2ca75-9123-4eb7-8012-25d5f5d9bf60.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"     --regions-overlap 1             input.vcf.gz  > '/galaxy/server/database/objects/b/5/3/dataset_b536138c-a33d-4deb-b452-d8a67d2537ff.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:11:05,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (288) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/288/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/288/galaxy_288.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:05,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 288 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:05,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:11:05,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:05,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:05,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 288 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:06,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:10,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-mvw2p with k8s id: gxy-mvw2p succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:11:11,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 288: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:11:18,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 288 finished
galaxy.model.metadata DEBUG 2025-03-02 07:11:18,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 337
galaxy.jobs INFO 2025-03-02 07:11:18,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 288 in /galaxy/server/database/jobs_directory/000/288
galaxy.jobs DEBUG 2025-03-02 07:11:18,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 288 executed (80.498 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:18,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 288 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:11:20,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 290, 289
tpv.core.entities DEBUG 2025-03-02 07:11:20,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:11:20,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (289) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:11:20,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (289) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:11:20,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (289) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:11:21,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (289) Working directory for job is: /galaxy/server/database/jobs_directory/000/289
galaxy.jobs.runners DEBUG 2025-03-02 07:11:21,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [289] queued (39.075 ms)
galaxy.jobs.handler INFO 2025-03-02 07:11:21,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (289) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:21,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 289
tpv.core.entities DEBUG 2025-03-02 07:11:21,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:11:21,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (290) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:11:21,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (290) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:11:21,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (290) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:11:21,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (290) Working directory for job is: /galaxy/server/database/jobs_directory/000/290
galaxy.jobs.runners DEBUG 2025-03-02 07:11:21,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [290] queued (49.741 ms)
galaxy.jobs.handler INFO 2025-03-02 07:11:21,094 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (290) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:21,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 290
galaxy.jobs DEBUG 2025-03-02 07:11:21,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [289] prepared (104.850 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:11:21,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/289/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/289/registry.xml' '/galaxy/server/database/jobs_directory/000/289/upload_params.json' '338:/galaxy/server/database/objects/e/e/d/dataset_eed4233e-1339-4c3d-819d-1c4e7b9aad83_files:/galaxy/server/database/objects/e/e/d/dataset_eed4233e-1339-4c3d-819d-1c4e7b9aad83.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:11:21,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (289) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/289/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/289/galaxy_289.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:21,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 289 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 07:11:21,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [290] prepared (99.676 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:21,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 289 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:11:21,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/290/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/290/registry.xml' '/galaxy/server/database/jobs_directory/000/290/upload_params.json' '339:/galaxy/server/database/objects/4/2/7/dataset_427ff890-cd24-4809-b202-5159d653063b_files:/galaxy/server/database/objects/4/2/7/dataset_427ff890-cd24-4809-b202-5159d653063b.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:11:21,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (290) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/290/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/290/galaxy_290.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:21,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 290 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:21,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 290 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:21,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:22,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:31,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wd6n5 with k8s id: gxy-wd6n5 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:31,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qw6gn with k8s id: gxy-qw6gn succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:11:31,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 289: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:11:31,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 290: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:11:39,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 290 finished
galaxy.jobs.runners DEBUG 2025-03-02 07:11:39,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 289 finished
galaxy.model.metadata DEBUG 2025-03-02 07:11:39,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 339
galaxy.model.metadata DEBUG 2025-03-02 07:11:39,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 338
galaxy.jobs INFO 2025-03-02 07:11:39,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 290 in /galaxy/server/database/jobs_directory/000/290
galaxy.jobs INFO 2025-03-02 07:11:39,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 289 in /galaxy/server/database/jobs_directory/000/289
galaxy.jobs DEBUG 2025-03-02 07:11:39,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 290 executed (121.686 ms)
galaxy.jobs DEBUG 2025-03-02 07:11:39,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 289 executed (126.599 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:39,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 290 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:39,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 289 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:11:40,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 291
tpv.core.entities DEBUG 2025-03-02 07:11:40,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:11:40,519 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (291) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:11:40,522 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (291) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:11:40,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (291) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:11:40,548 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (291) Working directory for job is: /galaxy/server/database/jobs_directory/000/291
galaxy.jobs.runners DEBUG 2025-03-02 07:11:40,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [291] queued (33.662 ms)
galaxy.jobs.handler INFO 2025-03-02 07:11:40,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (291) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:40,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 291
galaxy.jobs DEBUG 2025-03-02 07:11:40,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [291] prepared (52.645 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:40,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:11:40,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:40,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:11:40,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/291/tool_script.sh] for tool command [vcfintersect -b '/galaxy/server/database/objects/4/2/7/dataset_427ff890-cd24-4809-b202-5159d653063b.dat'  '/galaxy/server/database/objects/e/e/d/dataset_eed4233e-1339-4c3d-819d-1c4e7b9aad83.dat' > '/galaxy/server/database/objects/4/a/4/dataset_4a46724d-6dd2-4d75-a7a6-de7adca84d9d.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:11:40,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (291) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/291/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/291/galaxy_291.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:40,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 291 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:40,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:11:40,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:11:40,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:40,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 291 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:41,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:11:53,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4lrtx with k8s id: gxy-4lrtx succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:11:53,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 291: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:12:00,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 291 finished
galaxy.model.metadata DEBUG 2025-03-02 07:12:00,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 340
galaxy.jobs INFO 2025-03-02 07:12:00,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 291 in /galaxy/server/database/jobs_directory/000/291
galaxy.jobs DEBUG 2025-03-02 07:12:00,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 291 executed (112.325 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:00,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 291 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:12:01,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 292
tpv.core.entities DEBUG 2025-03-02 07:12:01,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:12:01,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (292) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:12:01,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (292) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:12:01,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (292) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:12:01,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (292) Working directory for job is: /galaxy/server/database/jobs_directory/000/292
galaxy.jobs.runners DEBUG 2025-03-02 07:12:02,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [292] queued (36.281 ms)
galaxy.jobs.handler INFO 2025-03-02 07:12:02,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (292) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:02,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 292
galaxy.jobs DEBUG 2025-03-02 07:12:02,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [292] prepared (89.458 ms)
galaxy.jobs.command_factory INFO 2025-03-02 07:12:02,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/292/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/292/registry.xml' '/galaxy/server/database/jobs_directory/000/292/upload_params.json' '341:/galaxy/server/database/objects/f/8/0/dataset_f8015be5-1972-4688-ae75-c490c054ba54_files:/galaxy/server/database/objects/f/8/0/dataset_f8015be5-1972-4688-ae75-c490c054ba54.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:12:02,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (292) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/292/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/292/galaxy_292.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:02,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 292 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:02,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 292 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:03,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:12,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-568hn with k8s id: gxy-568hn succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:12:12,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 292: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:12:19,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 292 finished
galaxy.model.metadata DEBUG 2025-03-02 07:12:20,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 341
galaxy.jobs INFO 2025-03-02 07:12:20,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 292 in /galaxy/server/database/jobs_directory/000/292
galaxy.jobs DEBUG 2025-03-02 07:12:20,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 292 executed (112.435 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:20,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 292 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:12:20,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 293
tpv.core.entities DEBUG 2025-03-02 07:12:20,371 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:12:20,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (293) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:12:20,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (293) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:12:20,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (293) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:12:20,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (293) Working directory for job is: /galaxy/server/database/jobs_directory/000/293
galaxy.jobs.runners DEBUG 2025-03-02 07:12:20,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [293] queued (46.953 ms)
galaxy.jobs.handler INFO 2025-03-02 07:12:20,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (293) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:20,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 293
galaxy.jobs DEBUG 2025-03-02 07:12:20,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [293] prepared (40.425 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:20,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:12:20,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:20,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:12:20,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/293/tool_script.sh] for tool command [vcfintersect -R '20:1-30000'  '/galaxy/server/database/objects/f/8/0/dataset_f8015be5-1972-4688-ae75-c490c054ba54.dat' > '/galaxy/server/database/objects/4/b/5/dataset_4b5f4503-898e-430a-b075-07baa12be515.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:12:20,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (293) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/293/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/293/galaxy_293.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:20,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 293 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:20,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:12:20,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:20,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:20,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 293 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:21,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:24,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wzssm with k8s id: gxy-wzssm succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:12:24,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 293: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:12:31,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 293 finished
galaxy.model.metadata DEBUG 2025-03-02 07:12:31,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 342
galaxy.jobs INFO 2025-03-02 07:12:31,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 293 in /galaxy/server/database/jobs_directory/000/293
galaxy.jobs DEBUG 2025-03-02 07:12:31,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 293 executed (103.213 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:31,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 293 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:12:34,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 296, 295, 294
tpv.core.entities DEBUG 2025-03-02 07:12:34,699 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:12:34,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (294) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:12:34,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (294) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:12:34,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (294) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:12:34,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (294) Working directory for job is: /galaxy/server/database/jobs_directory/000/294
galaxy.jobs.runners DEBUG 2025-03-02 07:12:34,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [294] queued (45.578 ms)
galaxy.jobs.handler INFO 2025-03-02 07:12:34,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (294) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:34,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 294
tpv.core.entities DEBUG 2025-03-02 07:12:34,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:12:34,772 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (295) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:12:34,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (295) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:12:34,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (295) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:12:34,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (295) Working directory for job is: /galaxy/server/database/jobs_directory/000/295
galaxy.jobs.runners DEBUG 2025-03-02 07:12:34,841 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [295] queued (63.190 ms)
galaxy.jobs.handler INFO 2025-03-02 07:12:34,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (295) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:34,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 295
tpv.core.entities DEBUG 2025-03-02 07:12:34,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:12:34,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (296) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:12:34,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (296) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:12:34,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (296) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:12:34,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [294] prepared (126.211 ms)
galaxy.jobs DEBUG 2025-03-02 07:12:34,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (296) Working directory for job is: /galaxy/server/database/jobs_directory/000/296
galaxy.jobs.runners DEBUG 2025-03-02 07:12:34,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [296] queued (66.081 ms)
galaxy.jobs.handler INFO 2025-03-02 07:12:34,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (296) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:34,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 296
galaxy.jobs.command_factory INFO 2025-03-02 07:12:34,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/294/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/294/registry.xml' '/galaxy/server/database/jobs_directory/000/294/upload_params.json' '343:/galaxy/server/database/objects/a/e/8/dataset_ae8be407-3b47-460f-b956-9a293863d37d_files:/galaxy/server/database/objects/a/e/8/dataset_ae8be407-3b47-460f-b956-9a293863d37d.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:12:34,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (294) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/294/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/294/galaxy_294.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:34,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 294 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 07:12:34,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [295] prepared (126.282 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 294 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:12:35,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/295/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/295/registry.xml' '/galaxy/server/database/jobs_directory/000/295/upload_params.json' '344:/galaxy/server/database/objects/6/2/f/dataset_62f7e63f-07c0-4ba3-804f-c961edfba645_files:/galaxy/server/database/objects/6/2/f/dataset_62f7e63f-07c0-4ba3-804f-c961edfba645.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:12:35,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (295) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/295/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/295/galaxy_295.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-02 07:12:35,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [296] prepared (106.840 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 295 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 295 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:12:35,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/296/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/296/registry.xml' '/galaxy/server/database/jobs_directory/000/296/upload_params.json' '345:/galaxy/server/database/objects/0/e/7/dataset_0e711e39-a540-439c-8316-cc4dcff17a90_files:/galaxy/server/database/objects/0/e/7/dataset_0e711e39-a540-439c-8316-cc4dcff17a90.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:12:35,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (296) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/296/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/296/galaxy_296.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 296 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 296 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:35,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:45,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cmwws with k8s id: gxy-cmwws succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:45,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tzd85 with k8s id: gxy-tzd85 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:45,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-f8rgm with k8s id: gxy-f8rgm succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:12:45,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 294: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:12:45,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 295: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:12:45,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 296: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:12:57,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 294 finished
galaxy.model.metadata DEBUG 2025-03-02 07:12:57,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 343
galaxy.jobs.runners DEBUG 2025-03-02 07:12:57,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 295 finished
galaxy.jobs INFO 2025-03-02 07:12:57,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 294 in /galaxy/server/database/jobs_directory/000/294
galaxy.jobs.runners DEBUG 2025-03-02 07:12:57,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 296 finished
galaxy.model.metadata DEBUG 2025-03-02 07:12:57,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 344
galaxy.model.metadata DEBUG 2025-03-02 07:12:57,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 345
galaxy.jobs INFO 2025-03-02 07:12:57,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 295 in /galaxy/server/database/jobs_directory/000/295
galaxy.jobs DEBUG 2025-03-02 07:12:57,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 294 executed (183.123 ms)
galaxy.jobs INFO 2025-03-02 07:12:57,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 296 in /galaxy/server/database/jobs_directory/000/296
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:57,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 294 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 07:12:57,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 295 executed (131.974 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:57,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 295 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 07:12:57,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 296 executed (140.007 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:58,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 296 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:12:58,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 297
tpv.core.entities DEBUG 2025-03-02 07:12:58,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:12:58,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (297) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:12:58,581 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (297) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:12:58,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (297) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:12:58,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (297) Working directory for job is: /galaxy/server/database/jobs_directory/000/297
galaxy.jobs.runners DEBUG 2025-03-02 07:12:58,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [297] queued (30.215 ms)
galaxy.jobs.handler INFO 2025-03-02 07:12:58,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (297) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:58,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 297
galaxy.jobs DEBUG 2025-03-02 07:12:58,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [297] prepared (60.575 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:58,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:12:58,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:58,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:12:58,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/297/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/0/e/7/dataset_0e711e39-a540-439c-8316-cc4dcff17a90.dat' 'localref.fa' &&  vcfintersect   -r 'localref.fa' -w "30" -i '/galaxy/server/database/objects/a/e/8/dataset_ae8be407-3b47-460f-b956-9a293863d37d.dat' '/galaxy/server/database/objects/6/2/f/dataset_62f7e63f-07c0-4ba3-804f-c961edfba645.dat' > '/galaxy/server/database/objects/a/e/a/dataset_aea1bb4a-46e9-4a1a-82fb-c7a3deef4fe6.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:12:58,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (297) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/297/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/297/galaxy_297.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:58,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 297 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:58,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:12:58,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:12:58,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:58,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 297 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:12:59,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:03,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fwzxc with k8s id: gxy-fwzxc succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:13:03,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 297: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:13:11,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 297 finished
galaxy.model.metadata DEBUG 2025-03-02 07:13:11,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 346
galaxy.jobs INFO 2025-03-02 07:13:11,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 297 in /galaxy/server/database/jobs_directory/000/297
galaxy.jobs DEBUG 2025-03-02 07:13:11,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 297 executed (116.699 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:11,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 297 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:13:12,873 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 300, 299, 298
tpv.core.entities DEBUG 2025-03-02 07:13:12,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:13:12,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (298) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:13:12,909 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (298) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:13:12,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (298) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:13:12,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (298) Working directory for job is: /galaxy/server/database/jobs_directory/000/298
galaxy.jobs.runners DEBUG 2025-03-02 07:13:12,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [298] queued (32.120 ms)
galaxy.jobs.handler INFO 2025-03-02 07:13:12,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (298) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:12,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 298
tpv.core.entities DEBUG 2025-03-02 07:13:12,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:13:12,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (299) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:13:12,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (299) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:13:12,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (299) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:13:12,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (299) Working directory for job is: /galaxy/server/database/jobs_directory/000/299
galaxy.jobs.runners DEBUG 2025-03-02 07:13:13,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [299] queued (45.518 ms)
galaxy.jobs.handler INFO 2025-03-02 07:13:13,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (299) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 299
tpv.core.entities DEBUG 2025-03-02 07:13:13,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:24.1.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:13:13,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (300) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:13:13,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (300) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:13:13,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (300) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:13:13,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [298] prepared (114.331 ms)
galaxy.jobs DEBUG 2025-03-02 07:13:13,074 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (300) Working directory for job is: /galaxy/server/database/jobs_directory/000/300
galaxy.jobs.runners DEBUG 2025-03-02 07:13:13,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [300] queued (55.814 ms)
galaxy.jobs.handler INFO 2025-03-02 07:13:13,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (300) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 300
galaxy.jobs.command_factory INFO 2025-03-02 07:13:13,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/298/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/298/registry.xml' '/galaxy/server/database/jobs_directory/000/298/upload_params.json' '347:/galaxy/server/database/objects/b/e/7/dataset_be7339e2-e0ea-419e-bc5f-e90d70488dc3_files:/galaxy/server/database/objects/b/e/7/dataset_be7339e2-e0ea-419e-bc5f-e90d70488dc3.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:13:13,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (298) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/298/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/298/galaxy_298.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 298 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 07:13:13,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [299] prepared (116.033 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 298 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:13:13,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/299/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/299/registry.xml' '/galaxy/server/database/jobs_directory/000/299/upload_params.json' '348:/galaxy/server/database/objects/1/f/f/dataset_1ff98fb3-175a-4342-8eb4-dda8bee67c18_files:/galaxy/server/database/objects/1/f/f/dataset_1ff98fb3-175a-4342-8eb4-dda8bee67c18.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:13:13,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (299) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/299/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/299/galaxy_299.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 299 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs DEBUG 2025-03-02 07:13:13,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [300] prepared (108.252 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 299 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-02 07:13:13,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/300/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/300/registry.xml' '/galaxy/server/database/jobs_directory/000/300/upload_params.json' '349:/galaxy/server/database/objects/9/d/c/dataset_9dc3f362-d438-460d-918e-365cb8e1eff4_files:/galaxy/server/database/objects/9/d/c/dataset_9dc3f362-d438-460d-918e-365cb8e1eff4.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:13:13,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (300) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/300/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/300/galaxy_300.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 300 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 300 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:13,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:23,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7srsv with k8s id: gxy-7srsv succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:23,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-4q9x7 with k8s id: gxy-4q9x7 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:23,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wpjx7 with k8s id: gxy-wpjx7 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:13:23,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 298: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:13:23,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 299: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:13:23,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 300: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:13:35,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 298 finished
galaxy.model.metadata DEBUG 2025-03-02 07:13:35,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 347
galaxy.jobs.runners DEBUG 2025-03-02 07:13:35,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 299 finished
galaxy.jobs INFO 2025-03-02 07:13:35,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 298 in /galaxy/server/database/jobs_directory/000/298
galaxy.jobs.runners DEBUG 2025-03-02 07:13:35,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 300 finished
galaxy.model.metadata DEBUG 2025-03-02 07:13:35,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 348
galaxy.model.metadata DEBUG 2025-03-02 07:13:35,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 349
galaxy.jobs DEBUG 2025-03-02 07:13:35,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 298 executed (192.506 ms)
galaxy.jobs INFO 2025-03-02 07:13:35,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 299 in /galaxy/server/database/jobs_directory/000/299
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:35,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 298 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs INFO 2025-03-02 07:13:35,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 300 in /galaxy/server/database/jobs_directory/000/300
galaxy.jobs DEBUG 2025-03-02 07:13:35,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 299 executed (138.369 ms)
galaxy.jobs DEBUG 2025-03-02 07:13:35,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 300 executed (142.720 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:35,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 299 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:35,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 300 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.handler DEBUG 2025-03-02 07:13:35,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 301
tpv.core.entities DEBUG 2025-03-02 07:13:35,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:24.1.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-02 07:13:35,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (301) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-02 07:13:35,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (301) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-02 07:13:35,685 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (301) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-02 07:13:35,697 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (301) Working directory for job is: /galaxy/server/database/jobs_directory/000/301
galaxy.jobs.runners DEBUG 2025-03-02 07:13:35,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [301] queued (32.222 ms)
galaxy.jobs.handler INFO 2025-03-02 07:13:35,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (301) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:35,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 301
galaxy.jobs DEBUG 2025-03-02 07:13:35,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [301] prepared (48.322 ms)
galaxy.tool_util.deps.containers INFO 2025-03-02 07:13:35,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:13:35,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:13:35,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-02 07:13:35,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/301/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/9/d/c/dataset_9dc3f362-d438-460d-918e-365cb8e1eff4.dat' 'localref.fa' &&  vcfintersect   -r 'localref.fa' -w "30" -u '/galaxy/server/database/objects/b/e/7/dataset_be7339e2-e0ea-419e-bc5f-e90d70488dc3.dat' '/galaxy/server/database/objects/1/f/f/dataset_1ff98fb3-175a-4342-8eb4-dda8bee67c18.dat' > '/galaxy/server/database/objects/b/c/c/dataset_bccc82ed-54e5-44ec-a47e-acd67aa5b75a.dat']
galaxy.jobs.runners DEBUG 2025-03-02 07:13:35,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (301) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/301/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/301/galaxy_301.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:35,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 301 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-02 07:13:35,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-02 07:13:35,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-03-02 07:13:35,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:35,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 301 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:36,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:39,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7srf6 with k8s id: gxy-7srf6 succeeded
galaxy.jobs.runners DEBUG 2025-03-02 07:13:39,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 301: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-02 07:13:47,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 301 finished
galaxy.model.metadata DEBUG 2025-03-02 07:13:47,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 350
galaxy.jobs INFO 2025-03-02 07:13:47,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 301 in /galaxy/server/database/jobs_directory/000/301
galaxy.jobs DEBUG 2025-03-02 07:13:47,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 301 executed (110.804 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-02 07:13:47,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 301 is an interactive tool. guest ports: []. interactive entry points: []
