galaxy.jobs.runners DEBUG 2025-03-19 06:52:38,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 136 finished
galaxy.jobs DEBUG 2025-03-19 06:52:38,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/136/outputs/dataset_e0b152c6-c5a6-4571-94aa-63d99a911bc3.dat to /galaxy/server/database/objects/e/0/b/dataset_e0b152c6-c5a6-4571-94aa-63d99a911bc3.dat
galaxy.model.metadata DEBUG 2025-03-19 06:52:38,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 180
galaxy.jobs INFO 2025-03-19 06:52:38,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 136 in /galaxy/server/database/jobs_directory/000/136
galaxy.objectstore CRITICAL 2025-03-19 06:52:38,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/136/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/136/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:52:38,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 136 executed (93.579 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:38,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 136 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:38,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-2ww2s (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:52:41,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 137
tpv.core.entities DEBUG 2025-03-19 06:52:41,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:52:41,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:52:41,371 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:52:41,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:52:41,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Working directory for job is: /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-03-19 06:52:41,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [137] queued (29.922 ms)
galaxy.jobs.handler INFO 2025-03-19 06:52:41,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:41,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 137
galaxy.jobs DEBUG 2025-03-19 06:52:41,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [137] prepared (60.095 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:52:41,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/137/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/137/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/137/configs/tmp_8rf3x2c']
galaxy.jobs.runners DEBUG 2025-03-19 06:52:41,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (137) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/137/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/137/galaxy_137.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:41,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:41,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:42,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rgs96 with k8s id: gxy-rgs96 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:43,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-rgs96 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:51,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-rgs96 with k8s id: gxy-rgs96 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:52:51,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 137: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:52:59,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 137 finished
galaxy.jobs DEBUG 2025-03-19 06:52:59,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/137/outputs/dataset_b3a72669-2078-496a-957c-d113d04e9f71.dat to /galaxy/server/database/objects/b/3/a/dataset_b3a72669-2078-496a-957c-d113d04e9f71.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:52:59,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/137/working/data_fetch_upload_oyp5e1lc', 'object_id': 181}]}]}]
galaxy.jobs INFO 2025-03-19 06:52:59,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 137 in /galaxy/server/database/jobs_directory/000/137
galaxy.jobs DEBUG 2025-03-19 06:52:59,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 137 executed (118.743 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:59,572 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 137 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:52:59,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-rgs96 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:53:00,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 138
tpv.core.entities DEBUG 2025-03-19 06:53:00,699 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:53:00,699 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:53:00,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:53:00,711 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:53:00,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Working directory for job is: /galaxy/server/database/jobs_directory/000/138
galaxy.jobs.runners DEBUG 2025-03-19 06:53:00,737 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [138] queued (34.443 ms)
galaxy.jobs.handler INFO 2025-03-19 06:53:00,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:00,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 138
galaxy.jobs DEBUG 2025-03-19 06:53:00,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [138] prepared (73.673 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:53:00,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:53:00,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:53:00,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:53:01,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/138/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/138/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/138/configs/tmpiidzd3hm' 2> '/galaxy/server/database/jobs_directory/000/138/outputs/dataset_4682d115-6ed5-4c2b-bbc5-96a3ab4aeb90.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:53:01,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (138) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/138/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/138/galaxy_138.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:01,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:53:01,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:53:01,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:53:01,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:01,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:01,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:02,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:03,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:04,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:05,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:06,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:07,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:08,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:10,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:11,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:12,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:13,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:14,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:15,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:16,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:17,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:18,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:19,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:20,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:21,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:22,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-hkjcb set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:40,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hkjcb with k8s id: gxy-hkjcb succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:53:41,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 138: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:53:53,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 138 finished
galaxy.jobs DEBUG 2025-03-19 06:53:53,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/138/outputs/dataset_5ce56cc4-d493-402a-b56c-ed759c199be4.dat to /galaxy/server/database/objects/5/c/e/dataset_5ce56cc4-d493-402a-b56c-ed759c199be4.dat
galaxy.jobs DEBUG 2025-03-19 06:53:53,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/138/outputs/dataset_b440dc3c-dede-4232-a532-5d40065e94c5.dat to /galaxy/server/database/objects/b/4/4/dataset_b440dc3c-dede-4232-a532-5d40065e94c5.dat
galaxy.jobs DEBUG 2025-03-19 06:53:53,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/138/outputs/dataset_2fce2779-9f17-4a54-8fcf-bbb983cf988e.dat to /galaxy/server/database/objects/2/f/c/dataset_2fce2779-9f17-4a54-8fcf-bbb983cf988e.dat
galaxy.jobs DEBUG 2025-03-19 06:53:53,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/138/outputs/dataset_4682d115-6ed5-4c2b-bbc5-96a3ab4aeb90.dat to /galaxy/server/database/objects/4/6/8/dataset_4682d115-6ed5-4c2b-bbc5-96a3ab4aeb90.dat
galaxy.model.metadata DEBUG 2025-03-19 06:53:53,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 182
galaxy.model.metadata DEBUG 2025-03-19 06:53:53,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 183
galaxy.model.metadata DEBUG 2025-03-19 06:53:53,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 184
galaxy.model.metadata DEBUG 2025-03-19 06:53:53,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 185
galaxy.jobs INFO 2025-03-19 06:53:53,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 138 in /galaxy/server/database/jobs_directory/000/138
galaxy.objectstore CRITICAL 2025-03-19 06:53:53,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/138/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/138/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:53:53,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 138 executed (127.341 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:53,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 138 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:53,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-hkjcb (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:53:55,596 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 139
tpv.core.entities DEBUG 2025-03-19 06:53:55,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:53:55,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:53:55,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:53:55,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:53:55,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Working directory for job is: /galaxy/server/database/jobs_directory/000/139
galaxy.jobs.runners DEBUG 2025-03-19 06:53:55,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [139] queued (29.321 ms)
galaxy.jobs.handler INFO 2025-03-19 06:53:55,652 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:55,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 139
galaxy.jobs DEBUG 2025-03-19 06:53:55,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [139] prepared (54.591 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:53:55,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/139/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/139/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/139/configs/tmpiwsad7p9']
galaxy.jobs.runners DEBUG 2025-03-19 06:53:55,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (139) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/139/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/139/galaxy_139.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:55,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:55,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:56,003 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d8vtk with k8s id: gxy-d8vtk scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:57,035 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d8vtk with k8s id: gxy-d8vtk scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:53:58,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-d8vtk set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:06,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d8vtk with k8s id: gxy-d8vtk succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:54:06,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 139: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:54:13,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 139 finished
galaxy.jobs DEBUG 2025-03-19 06:54:13,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/139/outputs/dataset_ffccf467-2772-42f0-8934-36f00a9c92a7.dat to /galaxy/server/database/objects/f/f/c/dataset_ffccf467-2772-42f0-8934-36f00a9c92a7.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:54:13,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.tabular', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/139/working/data_fetch_upload_9q_b7b_1', 'object_id': 186}]}]}]
galaxy.jobs INFO 2025-03-19 06:54:14,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 139 in /galaxy/server/database/jobs_directory/000/139
galaxy.jobs DEBUG 2025-03-19 06:54:14,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 139 executed (413.641 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:14,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 139 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:14,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-d8vtk (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:54:14,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 140
tpv.core.entities DEBUG 2025-03-19 06:54:14,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:54:14,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:54:14,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:54:14,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:54:14,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Working directory for job is: /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-03-19 06:54:14,971 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [140] queued (34.948 ms)
galaxy.jobs.handler INFO 2025-03-19 06:54:14,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:14,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 140
galaxy.jobs DEBUG 2025-03-19 06:54:15,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [140] prepared (65.817 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:15,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:54:15,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:15,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:54:15,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/140/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/140/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/140/configs/tmprz5lreog' 2> '/galaxy/server/database/jobs_directory/000/140/outputs/dataset_fd5f1c0a-34ae-47af-b12f-62370608c3bd.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:54:15,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (140) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/140/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/140/galaxy_140.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:15,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:15,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:54:15,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:15,155 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:15,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:15,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pmr72 with k8s id: gxy-pmr72 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:16,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-pmr72 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:27,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pmr72 with k8s id: gxy-pmr72 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:54:27,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 140: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:54:35,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 140 finished
galaxy.jobs DEBUG 2025-03-19 06:54:35,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/140/outputs/dataset_8e91c311-2036-4b63-b19c-42a7fa97d374.dat to /galaxy/server/database/objects/8/e/9/dataset_8e91c311-2036-4b63-b19c-42a7fa97d374.dat
galaxy.jobs DEBUG 2025-03-19 06:54:35,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/140/outputs/dataset_8dd8d884-0e54-4910-a5bb-2a692596f6c8.dat to /galaxy/server/database/objects/8/d/d/dataset_8dd8d884-0e54-4910-a5bb-2a692596f6c8.dat
galaxy.jobs DEBUG 2025-03-19 06:54:35,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/140/outputs/dataset_8a6782e3-5a68-4cc4-83f2-1ecfd2b71019.dat to /galaxy/server/database/objects/8/a/6/dataset_8a6782e3-5a68-4cc4-83f2-1ecfd2b71019.dat
galaxy.jobs DEBUG 2025-03-19 06:54:35,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/140/outputs/dataset_fd5f1c0a-34ae-47af-b12f-62370608c3bd.dat to /galaxy/server/database/objects/f/d/5/dataset_fd5f1c0a-34ae-47af-b12f-62370608c3bd.dat
galaxy.model.metadata DEBUG 2025-03-19 06:54:35,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 187
galaxy.model.metadata DEBUG 2025-03-19 06:54:35,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 188
galaxy.model.metadata DEBUG 2025-03-19 06:54:35,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 189
galaxy.model.metadata DEBUG 2025-03-19 06:54:35,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 190
galaxy.jobs INFO 2025-03-19 06:54:36,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 140 in /galaxy/server/database/jobs_directory/000/140
galaxy.objectstore CRITICAL 2025-03-19 06:54:36,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/140/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/140/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:54:36,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 140 executed (138.058 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:36,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 140 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:36,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-pmr72 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:54:37,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 141
tpv.core.entities DEBUG 2025-03-19 06:54:37,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:54:37,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:54:37,339 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:54:37,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:54:37,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Working directory for job is: /galaxy/server/database/jobs_directory/000/141
galaxy.jobs.runners DEBUG 2025-03-19 06:54:37,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [141] queued (23.876 ms)
galaxy.jobs.handler INFO 2025-03-19 06:54:37,367 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:37,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 141
galaxy.jobs DEBUG 2025-03-19 06:54:37,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [141] prepared (65.341 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:54:37,464 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/141/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/141/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/141/configs/tmpjgxro72e']
galaxy.jobs.runners DEBUG 2025-03-19 06:54:37,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (141) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/141/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/141/galaxy_141.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:37,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:37,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:37,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qzf4w with k8s id: gxy-qzf4w scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:38,725 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qzf4w with k8s id: gxy-qzf4w scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:39,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-qzf4w set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:48,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qzf4w with k8s id: gxy-qzf4w succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:54:48,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 141: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:54:55,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 141 finished
galaxy.jobs DEBUG 2025-03-19 06:54:55,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/141/outputs/dataset_ef624d09-595d-4ebb-a55b-42174350af21.dat to /galaxy/server/database/objects/e/f/6/dataset_ef624d09-595d-4ebb-a55b-42174350af21.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:54:55,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/141/working/data_fetch_upload_u7pfczhh', 'object_id': 191}]}]}]
galaxy.jobs INFO 2025-03-19 06:54:55,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 141 in /galaxy/server/database/jobs_directory/000/141
galaxy.jobs DEBUG 2025-03-19 06:54:55,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 141 executed (121.381 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:55,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 141 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:55,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-qzf4w (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:54:56,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 142
tpv.core.entities DEBUG 2025-03-19 06:54:56,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:54:56,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:54:56,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:54:56,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:54:56,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Working directory for job is: /galaxy/server/database/jobs_directory/000/142
galaxy.jobs.runners DEBUG 2025-03-19 06:54:56,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [142] queued (36.474 ms)
galaxy.jobs.handler INFO 2025-03-19 06:54:56,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:56,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 142
galaxy.jobs DEBUG 2025-03-19 06:54:56,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [142] prepared (74.301 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:56,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:54:56,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:56,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:54:56,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/142/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/142/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/142/configs/tmpsrf_hoj1' 2> '/galaxy/server/database/jobs_directory/000/142/outputs/dataset_2c86b030-ea19-4f1d-85ca-e56466d71d52.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:54:56,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (142) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/142/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/142/galaxy_142.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:56,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:56,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:54:56,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:54:56,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:56,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:58,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-28t2t with k8s id: gxy-28t2t scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:54:59,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-28t2t set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:11,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-28t2t with k8s id: gxy-28t2t succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:55:11,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 142: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:55:23,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 142 finished
galaxy.jobs DEBUG 2025-03-19 06:55:23,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/142/outputs/dataset_7dea1e1d-8538-4077-a6cb-a235a1803b8c.dat to /galaxy/server/database/objects/7/d/e/dataset_7dea1e1d-8538-4077-a6cb-a235a1803b8c.dat
galaxy.jobs DEBUG 2025-03-19 06:55:23,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/142/outputs/dataset_14bb5fda-e6a3-424b-bf69-9c84839b20b1.dat to /galaxy/server/database/objects/1/4/b/dataset_14bb5fda-e6a3-424b-bf69-9c84839b20b1.dat
galaxy.jobs DEBUG 2025-03-19 06:55:23,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/142/outputs/dataset_19c84535-73b3-4370-af9c-36b093051e40.dat to /galaxy/server/database/objects/1/9/c/dataset_19c84535-73b3-4370-af9c-36b093051e40.dat
galaxy.jobs DEBUG 2025-03-19 06:55:23,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/142/outputs/dataset_2c86b030-ea19-4f1d-85ca-e56466d71d52.dat to /galaxy/server/database/objects/2/c/8/dataset_2c86b030-ea19-4f1d-85ca-e56466d71d52.dat
galaxy.model.metadata DEBUG 2025-03-19 06:55:23,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 192
galaxy.model.metadata DEBUG 2025-03-19 06:55:23,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 193
galaxy.model.metadata DEBUG 2025-03-19 06:55:23,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 194
galaxy.model.metadata DEBUG 2025-03-19 06:55:23,928 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 195
galaxy.jobs INFO 2025-03-19 06:55:23,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 142 in /galaxy/server/database/jobs_directory/000/142
galaxy.objectstore CRITICAL 2025-03-19 06:55:24,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/142/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/142/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:55:24,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 142 executed (158.674 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:24,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 142 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:24,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-28t2t (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:55:25,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 143
tpv.core.entities DEBUG 2025-03-19 06:55:25,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:55:25,314 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:55:25,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:55:25,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:55:25,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Working directory for job is: /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-03-19 06:55:25,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [143] queued (23.517 ms)
galaxy.jobs.handler INFO 2025-03-19 06:55:25,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:25,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 143
galaxy.jobs DEBUG 2025-03-19 06:55:25,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [143] prepared (56.026 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:55:25,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/143/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/143/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/143/configs/tmpjoy32kgs']
galaxy.jobs.runners DEBUG 2025-03-19 06:55:25,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (143) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/143/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/143/galaxy_143.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:25,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:25,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:25,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wrx6v with k8s id: gxy-wrx6v scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:26,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-wrx6v set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:34,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wrx6v with k8s id: gxy-wrx6v succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:55:34,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 143: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:55:42,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 143 finished
galaxy.jobs DEBUG 2025-03-19 06:55:42,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/143/outputs/dataset_c56cd022-3fba-4b1c-8ef3-63b689f8cf83.dat to /galaxy/server/database/objects/c/5/6/dataset_c56cd022-3fba-4b1c-8ef3-63b689f8cf83.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:55:42,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/143/working/data_fetch_upload_w529wn2j', 'object_id': 196}]}]}]
galaxy.jobs INFO 2025-03-19 06:55:42,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 143 in /galaxy/server/database/jobs_directory/000/143
galaxy.jobs DEBUG 2025-03-19 06:55:42,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 143 executed (115.842 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:42,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 143 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:42,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-wrx6v (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:55:43,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 144
tpv.core.entities DEBUG 2025-03-19 06:55:43,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:55:43,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:55:43,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:55:43,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:55:43,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Working directory for job is: /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-03-19 06:55:43,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [144] queued (38.188 ms)
galaxy.jobs.handler INFO 2025-03-19 06:55:43,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:43,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 144
galaxy.jobs DEBUG 2025-03-19 06:55:43,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [144] prepared (66.777 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:55:43,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:55:43,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:55:43,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:55:43,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/144/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/144/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/144/configs/tmpp74jcxrn' 2> '/galaxy/server/database/jobs_directory/000/144/outputs/dataset_d5d1fca3-7ca6-4c13-ab61-3c62dfd45600.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:55:43,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (144) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/144/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/144/galaxy_144.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:43,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:55:43,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:55:43,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:55:43,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:43,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:44,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p6gh5 with k8s id: gxy-p6gh5 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:45,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-p6gh5 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:55:56,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-p6gh5 with k8s id: gxy-p6gh5 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:55:56,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 144: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:56:08,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 144 finished
galaxy.jobs DEBUG 2025-03-19 06:56:08,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/144/outputs/dataset_93c631f4-a230-4928-8682-1cb6130f9668.dat to /galaxy/server/database/objects/9/3/c/dataset_93c631f4-a230-4928-8682-1cb6130f9668.dat
galaxy.jobs DEBUG 2025-03-19 06:56:08,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/144/outputs/dataset_2b3255d4-1126-4155-80c5-8d6529fc7bd2.dat to /galaxy/server/database/objects/2/b/3/dataset_2b3255d4-1126-4155-80c5-8d6529fc7bd2.dat
galaxy.jobs DEBUG 2025-03-19 06:56:08,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/144/outputs/dataset_4baec395-6fda-42d5-9a9f-c25388ed8e64.dat to /galaxy/server/database/objects/4/b/a/dataset_4baec395-6fda-42d5-9a9f-c25388ed8e64.dat
galaxy.jobs DEBUG 2025-03-19 06:56:08,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/144/outputs/dataset_d5d1fca3-7ca6-4c13-ab61-3c62dfd45600.dat to /galaxy/server/database/objects/d/5/d/dataset_d5d1fca3-7ca6-4c13-ab61-3c62dfd45600.dat
galaxy.model.metadata DEBUG 2025-03-19 06:56:08,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 197
galaxy.model.metadata DEBUG 2025-03-19 06:56:08,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 198
galaxy.model.metadata DEBUG 2025-03-19 06:56:08,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 199
galaxy.model.metadata DEBUG 2025-03-19 06:56:08,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 200
galaxy.jobs INFO 2025-03-19 06:56:08,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 144 in /galaxy/server/database/jobs_directory/000/144
galaxy.objectstore CRITICAL 2025-03-19 06:56:08,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/144/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/144/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:56:08,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 144 executed (187.284 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:08,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 144 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:08,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-p6gh5 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:56:10,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 145
tpv.core.entities DEBUG 2025-03-19 06:56:10,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:56:10,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:56:10,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:56:10,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:56:10,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Working directory for job is: /galaxy/server/database/jobs_directory/000/145
galaxy.jobs.runners DEBUG 2025-03-19 06:56:10,095 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [145] queued (31.589 ms)
galaxy.jobs.handler INFO 2025-03-19 06:56:10,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:10,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 145
galaxy.jobs DEBUG 2025-03-19 06:56:10,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [145] prepared (53.388 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:56:10,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/145/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/145/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/145/configs/tmp7oaqvifu']
galaxy.jobs.runners DEBUG 2025-03-19 06:56:10,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (145) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/145/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/145/galaxy_145.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:10,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:10,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:11,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-znhqh with k8s id: gxy-znhqh scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:12,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-znhqh set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:20,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-znhqh with k8s id: gxy-znhqh succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:56:20,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 145: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:56:27,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 145 finished
galaxy.jobs DEBUG 2025-03-19 06:56:27,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/145/outputs/dataset_81048168-c410-45d9-bd13-d97fc4d675f6.dat to /galaxy/server/database/objects/8/1/0/dataset_81048168-c410-45d9-bd13-d97fc4d675f6.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:56:27,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/145/working/data_fetch_upload_1o83_8y2', 'object_id': 201}]}]}]
galaxy.jobs INFO 2025-03-19 06:56:27,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 145 in /galaxy/server/database/jobs_directory/000/145
galaxy.jobs DEBUG 2025-03-19 06:56:27,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 145 executed (130.781 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:27,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 145 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:27,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-znhqh (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:56:28,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 146
tpv.core.entities DEBUG 2025-03-19 06:56:28,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:56:28,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:56:28,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:56:28,392 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:56:28,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Working directory for job is: /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners DEBUG 2025-03-19 06:56:28,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [146] queued (40.412 ms)
galaxy.jobs.handler INFO 2025-03-19 06:56:28,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:28,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 146
galaxy.jobs DEBUG 2025-03-19 06:56:28,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [146] prepared (77.092 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:56:28,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:56:28,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:56:28,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:56:28,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/146/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/146/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/146/configs/tmpeqbx_5j3' 2> '/galaxy/server/database/jobs_directory/000/146/outputs/dataset_162f30af-4064-46a4-812c-c4854319ffa8.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:56:28,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (146) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/146/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/146/galaxy_146.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:28,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:56:28,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:56:28,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:56:28,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:28,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:29,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d7bw7 with k8s id: gxy-d7bw7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:30,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-d7bw7 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:56:51,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-d7bw7 with k8s id: gxy-d7bw7 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:56:51,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 146: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:57:04,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 146 finished
galaxy.jobs DEBUG 2025-03-19 06:57:04,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/146/outputs/dataset_8af82eac-8213-4b03-9f64-5b7da9e5845d.dat to /galaxy/server/database/objects/8/a/f/dataset_8af82eac-8213-4b03-9f64-5b7da9e5845d.dat
galaxy.jobs DEBUG 2025-03-19 06:57:04,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/146/outputs/dataset_5f0ad495-5082-4e51-9c60-70a875269081.dat to /galaxy/server/database/objects/5/f/0/dataset_5f0ad495-5082-4e51-9c60-70a875269081.dat
galaxy.jobs DEBUG 2025-03-19 06:57:04,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/146/outputs/dataset_fdd61f70-cf56-42a1-98b5-aaff272c7b57.dat to /galaxy/server/database/objects/f/d/d/dataset_fdd61f70-cf56-42a1-98b5-aaff272c7b57.dat
galaxy.jobs DEBUG 2025-03-19 06:57:04,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/146/outputs/dataset_162f30af-4064-46a4-812c-c4854319ffa8.dat to /galaxy/server/database/objects/1/6/2/dataset_162f30af-4064-46a4-812c-c4854319ffa8.dat
galaxy.model.metadata DEBUG 2025-03-19 06:57:04,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 202
galaxy.model.metadata DEBUG 2025-03-19 06:57:04,625 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 203
galaxy.model.metadata DEBUG 2025-03-19 06:57:04,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 204
galaxy.model.metadata DEBUG 2025-03-19 06:57:04,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 205
galaxy.jobs INFO 2025-03-19 06:57:04,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 146 in /galaxy/server/database/jobs_directory/000/146
galaxy.objectstore CRITICAL 2025-03-19 06:57:04,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/146/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/146/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:57:04,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 146 executed (127.925 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:04,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 146 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:04,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-d7bw7 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:57:06,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 147
tpv.core.entities DEBUG 2025-03-19 06:57:07,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:57:07,005 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:57:07,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:57:07,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:57:07,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Working directory for job is: /galaxy/server/database/jobs_directory/000/147
galaxy.jobs.runners DEBUG 2025-03-19 06:57:07,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [147] queued (28.221 ms)
galaxy.jobs.handler INFO 2025-03-19 06:57:07,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:07,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 147
galaxy.jobs DEBUG 2025-03-19 06:57:07,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [147] prepared (58.238 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:57:07,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/147/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/147/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/147/configs/tmpe1lqn4ns']
galaxy.jobs.runners DEBUG 2025-03-19 06:57:07,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (147) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/147/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/147/galaxy_147.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:07,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:07,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:07,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6628w with k8s id: gxy-6628w scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:08,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6628w with k8s id: gxy-6628w scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:10,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-6628w set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:17,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6628w with k8s id: gxy-6628w succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:57:17,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 147: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:57:24,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 147 finished
galaxy.jobs DEBUG 2025-03-19 06:57:24,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/147/outputs/dataset_2b5e3906-d452-477a-afb0-791560b3a775.dat to /galaxy/server/database/objects/2/b/5/dataset_2b5e3906-d452-477a-afb0-791560b3a775.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:57:24,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/147/working/data_fetch_upload_2fg2nwxe', 'object_id': 206}]}]}]
galaxy.jobs INFO 2025-03-19 06:57:24,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 147 in /galaxy/server/database/jobs_directory/000/147
galaxy.jobs DEBUG 2025-03-19 06:57:25,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 147 executed (118.070 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:25,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 147 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:25,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-6628w (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:57:26,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 148
tpv.core.entities DEBUG 2025-03-19 06:57:26,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:57:26,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:57:26,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:57:26,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:57:26,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Working directory for job is: /galaxy/server/database/jobs_directory/000/148
galaxy.jobs.runners DEBUG 2025-03-19 06:57:26,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [148] queued (42.098 ms)
galaxy.jobs.handler INFO 2025-03-19 06:57:26,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:26,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 148
galaxy.jobs DEBUG 2025-03-19 06:57:26,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [148] prepared (74.986 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:57:26,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:57:26,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:57:26,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:57:26,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/148/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/148/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/148/configs/tmpell6vcod' 2> '/galaxy/server/database/jobs_directory/000/148/outputs/dataset_bc41905c-6bd5-4f4f-861c-3cba4e8272ff.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:57:26,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (148) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/148/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/148/galaxy_148.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:26,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:57:26,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:57:26,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:57:26,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:26,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:27,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hh78p with k8s id: gxy-hh78p scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:28,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-hh78p set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:40,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hh78p with k8s id: gxy-hh78p succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:57:40,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 148: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:57:52,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 148 finished
galaxy.jobs DEBUG 2025-03-19 06:57:52,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/148/outputs/dataset_68050f79-c207-4e5a-ada2-c28cc125de0a.dat to /galaxy/server/database/objects/6/8/0/dataset_68050f79-c207-4e5a-ada2-c28cc125de0a.dat
galaxy.jobs DEBUG 2025-03-19 06:57:52,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/148/outputs/dataset_cfb2e3a6-93ef-4fe5-9fee-2d273d2e3642.dat to /galaxy/server/database/objects/c/f/b/dataset_cfb2e3a6-93ef-4fe5-9fee-2d273d2e3642.dat
galaxy.jobs DEBUG 2025-03-19 06:57:52,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/148/outputs/dataset_1c8ca75c-ad9a-4a78-a585-118ab31f3305.dat to /galaxy/server/database/objects/1/c/8/dataset_1c8ca75c-ad9a-4a78-a585-118ab31f3305.dat
galaxy.jobs DEBUG 2025-03-19 06:57:52,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/148/outputs/dataset_bc41905c-6bd5-4f4f-861c-3cba4e8272ff.dat to /galaxy/server/database/objects/b/c/4/dataset_bc41905c-6bd5-4f4f-861c-3cba4e8272ff.dat
galaxy.model.metadata DEBUG 2025-03-19 06:57:52,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 207
galaxy.model.metadata DEBUG 2025-03-19 06:57:52,937 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 208
galaxy.model.metadata DEBUG 2025-03-19 06:57:52,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 209
galaxy.model.metadata DEBUG 2025-03-19 06:57:52,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 210
galaxy.jobs INFO 2025-03-19 06:57:52,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 148 in /galaxy/server/database/jobs_directory/000/148
galaxy.objectstore CRITICAL 2025-03-19 06:57:53,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/148/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/148/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:57:53,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 148 executed (158.114 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:53,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 148 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:53,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-hh78p (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:57:55,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 149
tpv.core.entities DEBUG 2025-03-19 06:57:55,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:57:55,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:57:55,822 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:57:55,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:57:55,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Working directory for job is: /galaxy/server/database/jobs_directory/000/149
galaxy.jobs.runners DEBUG 2025-03-19 06:57:55,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [149] queued (28.368 ms)
galaxy.jobs.handler INFO 2025-03-19 06:57:55,853 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:55,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 149
galaxy.jobs DEBUG 2025-03-19 06:57:55,926 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [149] prepared (62.094 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:57:55,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/149/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/149/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/149/configs/tmpm5ag728f']
galaxy.jobs.runners DEBUG 2025-03-19 06:57:55,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (149) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/149/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/149/galaxy_149.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:55,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:55,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:56,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-plljt with k8s id: gxy-plljt scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:57:57,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-plljt set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:06,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-plljt with k8s id: gxy-plljt succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:58:06,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 149: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:58:14,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 149 finished
galaxy.jobs DEBUG 2025-03-19 06:58:14,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/149/outputs/dataset_91a2dc5d-f69b-4038-8b77-0c26d468cbc7.dat to /galaxy/server/database/objects/9/1/a/dataset_91a2dc5d-f69b-4038-8b77-0c26d468cbc7.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:58:14,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'matrix.tabular', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/149/working/data_fetch_upload_4l2r972s', 'object_id': 211}]}]}]
galaxy.jobs INFO 2025-03-19 06:58:14,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 149 in /galaxy/server/database/jobs_directory/000/149
galaxy.jobs DEBUG 2025-03-19 06:58:14,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 149 executed (394.924 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:14,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 149 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:14,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-plljt (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:58:15,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 150
tpv.core.entities DEBUG 2025-03-19 06:58:15,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:58:15,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:58:15,234 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:58:15,246 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:58:15,270 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Working directory for job is: /galaxy/server/database/jobs_directory/000/150
galaxy.jobs.runners DEBUG 2025-03-19 06:58:15,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [150] queued (44.572 ms)
galaxy.jobs.handler INFO 2025-03-19 06:58:15,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:15,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 150
galaxy.jobs DEBUG 2025-03-19 06:58:15,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [150] prepared (72.821 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:15,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:58:15,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:15,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:58:15,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/150/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/150/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/150/configs/tmplkqic397' 2> '/galaxy/server/database/jobs_directory/000/150/outputs/dataset_56113842-4bb7-4638-89cd-42022a062caf.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:58:15,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (150) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/150/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/150/galaxy_150.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:15,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:15,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:58:15,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:15,466 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:15,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:15,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5gmz7 with k8s id: gxy-5gmz7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:16,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5gmz7 with k8s id: gxy-5gmz7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:17,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-5gmz7 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:29,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5gmz7 with k8s id: gxy-5gmz7 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:58:29,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 150: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:58:37,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 150 finished
galaxy.jobs DEBUG 2025-03-19 06:58:37,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/150/outputs/dataset_9c6dd6a7-dd0c-4125-8dd4-e80c4af350ae.dat to /galaxy/server/database/objects/9/c/6/dataset_9c6dd6a7-dd0c-4125-8dd4-e80c4af350ae.dat
galaxy.jobs DEBUG 2025-03-19 06:58:37,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/150/outputs/dataset_5332bdd8-3ca1-4e78-a3eb-7aa40a3ff497.dat to /galaxy/server/database/objects/5/3/3/dataset_5332bdd8-3ca1-4e78-a3eb-7aa40a3ff497.dat
galaxy.jobs DEBUG 2025-03-19 06:58:37,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/150/outputs/dataset_df14a40b-c7fb-43c3-bb37-ea8063a81b7c.dat to /galaxy/server/database/objects/d/f/1/dataset_df14a40b-c7fb-43c3-bb37-ea8063a81b7c.dat
galaxy.jobs DEBUG 2025-03-19 06:58:37,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/150/outputs/dataset_56113842-4bb7-4638-89cd-42022a062caf.dat to /galaxy/server/database/objects/5/6/1/dataset_56113842-4bb7-4638-89cd-42022a062caf.dat
galaxy.model.metadata DEBUG 2025-03-19 06:58:37,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 212
galaxy.model.metadata DEBUG 2025-03-19 06:58:37,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 213
galaxy.model.metadata DEBUG 2025-03-19 06:58:37,424 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 214
galaxy.model.metadata DEBUG 2025-03-19 06:58:37,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 215
galaxy.jobs INFO 2025-03-19 06:58:37,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 150 in /galaxy/server/database/jobs_directory/000/150
galaxy.objectstore CRITICAL 2025-03-19 06:58:37,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/150/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/150/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:58:37,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 150 executed (134.689 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:37,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 150 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:37,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-5gmz7 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:58:39,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 151
tpv.core.entities DEBUG 2025-03-19 06:58:39,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:58:39,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:58:39,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:58:39,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:58:39,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Working directory for job is: /galaxy/server/database/jobs_directory/000/151
galaxy.jobs.runners DEBUG 2025-03-19 06:58:39,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [151] queued (26.671 ms)
galaxy.jobs.handler INFO 2025-03-19 06:58:39,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:39,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 151
galaxy.jobs DEBUG 2025-03-19 06:58:39,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [151] prepared (58.494 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:58:39,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/151/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/151/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/151/configs/tmpog25_v84']
galaxy.jobs.runners DEBUG 2025-03-19 06:58:39,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (151) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/151/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/151/galaxy_151.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:39,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:39,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:40,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tzqwb with k8s id: gxy-tzqwb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:41,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tzqwb with k8s id: gxy-tzqwb scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:42,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-tzqwb set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:49,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tzqwb with k8s id: gxy-tzqwb succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:58:49,604 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 151: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:58:57,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 151 finished
galaxy.jobs DEBUG 2025-03-19 06:58:57,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/151/outputs/dataset_405a1108-81c1-4614-9ac6-263b0a5fb266.dat to /galaxy/server/database/objects/4/0/5/dataset_405a1108-81c1-4614-9ac6-263b0a5fb266.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:58:57,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/151/working/data_fetch_upload_jj004f71', 'object_id': 216}]}]}]
galaxy.jobs INFO 2025-03-19 06:58:57,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 151 in /galaxy/server/database/jobs_directory/000/151
galaxy.jobs DEBUG 2025-03-19 06:58:57,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 151 executed (122.710 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:57,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 151 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:57,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-tzqwb (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:58:58,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 152
tpv.core.entities DEBUG 2025-03-19 06:58:58,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:58:58,065 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:58:58,067 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:58:58,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:58:58,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Working directory for job is: /galaxy/server/database/jobs_directory/000/152
galaxy.jobs.runners DEBUG 2025-03-19 06:58:58,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [152] queued (38.652 ms)
galaxy.jobs.handler INFO 2025-03-19 06:58:58,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:58,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 152
galaxy.jobs DEBUG 2025-03-19 06:58:58,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [152] prepared (72.874 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:58,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:58:58,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:58,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:58:58,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/152/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/152/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/152/configs/tmpzmp_jjf8' 2> '/galaxy/server/database/jobs_directory/000/152/outputs/dataset_ab66b096-b1ef-41a0-bca5-188239bbc355.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:58:58,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (152) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/152/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/152/galaxy_152.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:58,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:58,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:58:58,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:58:58,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:58,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:58,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6px6j with k8s id: gxy-6px6j scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:58:59,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-6px6j set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:10,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6px6j with k8s id: gxy-6px6j succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:59:11,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 152: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:59:23,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 152 finished
galaxy.jobs DEBUG 2025-03-19 06:59:23,164 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/152/outputs/dataset_21ae77f6-a4d3-4e22-83e2-d9e8d2cdaaea.dat to /galaxy/server/database/objects/2/1/a/dataset_21ae77f6-a4d3-4e22-83e2-d9e8d2cdaaea.dat
galaxy.jobs DEBUG 2025-03-19 06:59:23,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/152/outputs/dataset_3e941a66-a1c2-4ebf-92b4-6843bcf7980a.dat to /galaxy/server/database/objects/3/e/9/dataset_3e941a66-a1c2-4ebf-92b4-6843bcf7980a.dat
galaxy.jobs DEBUG 2025-03-19 06:59:23,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/152/outputs/dataset_de295616-7204-483d-ba63-8d4bbcda2938.dat to /galaxy/server/database/objects/d/e/2/dataset_de295616-7204-483d-ba63-8d4bbcda2938.dat
galaxy.jobs DEBUG 2025-03-19 06:59:23,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/152/outputs/dataset_ab66b096-b1ef-41a0-bca5-188239bbc355.dat to /galaxy/server/database/objects/a/b/6/dataset_ab66b096-b1ef-41a0-bca5-188239bbc355.dat
galaxy.model.metadata DEBUG 2025-03-19 06:59:23,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 217
galaxy.model.metadata DEBUG 2025-03-19 06:59:23,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 218
galaxy.model.metadata DEBUG 2025-03-19 06:59:23,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 219
galaxy.model.metadata DEBUG 2025-03-19 06:59:23,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 220
galaxy.jobs INFO 2025-03-19 06:59:23,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 152 in /galaxy/server/database/jobs_directory/000/152
galaxy.objectstore CRITICAL 2025-03-19 06:59:23,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/152/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/152/_outputs'
galaxy.jobs DEBUG 2025-03-19 06:59:23,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 152 executed (154.254 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:23,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 152 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:23,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-6px6j (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:59:24,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 153
tpv.core.entities DEBUG 2025-03-19 06:59:24,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:59:24,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:59:24,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:59:24,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:59:24,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Working directory for job is: /galaxy/server/database/jobs_directory/000/153
galaxy.jobs.runners DEBUG 2025-03-19 06:59:24,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [153] queued (24.455 ms)
galaxy.jobs.handler INFO 2025-03-19 06:59:24,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:24,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 153
galaxy.jobs DEBUG 2025-03-19 06:59:24,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [153] prepared (56.506 ms)
galaxy.jobs.command_factory INFO 2025-03-19 06:59:24,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/153/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/153/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/153/configs/tmpb6ahxo3v']
galaxy.jobs.runners DEBUG 2025-03-19 06:59:24,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (153) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/153/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/153/galaxy_153.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:24,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:24,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:25,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5fv65 with k8s id: gxy-5fv65 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:26,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5fv65 with k8s id: gxy-5fv65 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:28,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-5fv65 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:35,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5fv65 with k8s id: gxy-5fv65 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:59:35,229 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 153: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 06:59:42,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 153 finished
galaxy.jobs DEBUG 2025-03-19 06:59:42,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/153/outputs/dataset_af9bae67-1c2f-48b9-9dc1-4bdb2f8603b5.dat to /galaxy/server/database/objects/a/f/9/dataset_af9bae67-1c2f-48b9-9dc1-4bdb2f8603b5.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 06:59:42,726 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/153/working/data_fetch_upload_pr1y581c', 'object_id': 221}]}]}]
galaxy.jobs INFO 2025-03-19 06:59:42,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 153 in /galaxy/server/database/jobs_directory/000/153
galaxy.jobs DEBUG 2025-03-19 06:59:42,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 153 executed (117.335 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:42,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 153 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:42,848 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-5fv65 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 06:59:43,903 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 154
tpv.core.entities DEBUG 2025-03-19 06:59:43,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 06:59:43,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 06:59:43,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 06:59:43,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 06:59:43,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Working directory for job is: /galaxy/server/database/jobs_directory/000/154
galaxy.jobs.runners DEBUG 2025-03-19 06:59:43,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [154] queued (37.465 ms)
galaxy.jobs.handler INFO 2025-03-19 06:59:43,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:43,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 154
galaxy.jobs DEBUG 2025-03-19 06:59:44,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [154] prepared (72.625 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 06:59:44,056 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:59:44,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:59:44,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 06:59:44,101 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/154/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/154/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/154/configs/tmppupixfju' 2> '/galaxy/server/database/jobs_directory/000/154/outputs/dataset_262cb4ee-1ea9-42da-9fbf-5167cb3d1616.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 06:59:44,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (154) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/154/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/154/galaxy_154.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:44,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 06:59:44,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 06:59:44,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 06:59:44,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:44,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:45,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-49q6n with k8s id: gxy-49q6n scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:46,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-49q6n set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 06:59:57,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-49q6n with k8s id: gxy-49q6n succeeded
galaxy.jobs.runners DEBUG 2025-03-19 06:59:57,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 154: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:00:10,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 154 finished
galaxy.jobs DEBUG 2025-03-19 07:00:10,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/154/outputs/dataset_ec2e8611-4ec6-445d-9da8-93d3f78e592b.dat to /galaxy/server/database/objects/e/c/2/dataset_ec2e8611-4ec6-445d-9da8-93d3f78e592b.dat
galaxy.jobs DEBUG 2025-03-19 07:00:10,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/154/outputs/dataset_6b2f3d03-b659-4885-b792-5a6c5ad30dd3.dat to /galaxy/server/database/objects/6/b/2/dataset_6b2f3d03-b659-4885-b792-5a6c5ad30dd3.dat
galaxy.jobs DEBUG 2025-03-19 07:00:10,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/154/outputs/dataset_90878fae-a074-42ef-9f82-0b0ef3b56cfa.dat to /galaxy/server/database/objects/9/0/8/dataset_90878fae-a074-42ef-9f82-0b0ef3b56cfa.dat
galaxy.jobs DEBUG 2025-03-19 07:00:10,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/154/outputs/dataset_262cb4ee-1ea9-42da-9fbf-5167cb3d1616.dat to /galaxy/server/database/objects/2/6/2/dataset_262cb4ee-1ea9-42da-9fbf-5167cb3d1616.dat
galaxy.model.metadata DEBUG 2025-03-19 07:00:10,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 222
galaxy.model.metadata DEBUG 2025-03-19 07:00:10,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 223
galaxy.model.metadata DEBUG 2025-03-19 07:00:10,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 224
galaxy.model.metadata DEBUG 2025-03-19 07:00:10,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 225
galaxy.jobs INFO 2025-03-19 07:00:10,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 154 in /galaxy/server/database/jobs_directory/000/154
galaxy.objectstore CRITICAL 2025-03-19 07:00:10,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/154/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/154/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:00:10,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 154 executed (185.760 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:10,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 154 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:10,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-49q6n (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:00:12,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 155
tpv.core.entities DEBUG 2025-03-19 07:00:12,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:00:12,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:00:12,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:00:12,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:00:12,568 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Working directory for job is: /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-03-19 07:00:12,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [155] queued (35.647 ms)
galaxy.jobs.handler INFO 2025-03-19 07:00:12,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:12,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 155
galaxy.jobs DEBUG 2025-03-19 07:00:12,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [155] prepared (73.467 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:00:12,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/155/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/155/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/155/configs/tmpbqv4n9pd']
galaxy.jobs.runners DEBUG 2025-03-19 07:00:12,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (155) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/155/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/155/galaxy_155.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:12,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:12,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:13,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-htrg4 with k8s id: gxy-htrg4 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:14,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-htrg4 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:21,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-htrg4 with k8s id: gxy-htrg4 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:00:22,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 155: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:00:30,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 155 finished
galaxy.jobs DEBUG 2025-03-19 07:00:30,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/155/outputs/dataset_e90200b6-06a9-4d0d-ad80-a67b96ee3b5c.dat to /galaxy/server/database/objects/e/9/0/dataset_e90200b6-06a9-4d0d-ad80-a67b96ee3b5c.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:00:30,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'use.intestinal', 'dbkey': '?', 'ext': 'tabular', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded tabular file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/155/working/data_fetch_upload_j2g12sm2', 'object_id': 226}]}]}]
galaxy.jobs INFO 2025-03-19 07:00:30,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 155 in /galaxy/server/database/jobs_directory/000/155
galaxy.jobs DEBUG 2025-03-19 07:00:30,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 155 executed (131.718 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:30,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 155 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:30,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-htrg4 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:00:30,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 156
tpv.core.entities DEBUG 2025-03-19 07:00:30,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:00:30,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:00:30,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:00:30,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:00:30,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Working directory for job is: /galaxy/server/database/jobs_directory/000/156
galaxy.jobs.runners DEBUG 2025-03-19 07:00:30,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [156] queued (41.312 ms)
galaxy.jobs.handler INFO 2025-03-19 07:00:30,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:30,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 156
galaxy.jobs DEBUG 2025-03-19 07:00:31,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [156] prepared (86.864 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:00:31,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:00:31,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 07:00:31,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:00:31,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/156/tool_script.sh] for tool command [Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' 2>&1 | head -1 | cut -d' ' -f 2 > /galaxy/server/database/jobs_directory/000/156/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/b83fbc90161e/raceid_filtnormconf/scripts/cluster.R' '/galaxy/server/database/jobs_directory/000/156/configs/tmpzajnudg3' 2> '/galaxy/server/database/jobs_directory/000/156/outputs/dataset_d1d94f8b-c347-49a2-a134-35c844a079cc.dat' > /dev/null]
galaxy.jobs.runners DEBUG 2025-03-19 07:00:31,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (156) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/156/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/156/galaxy_156.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:31,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:00:31,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:00:31,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/raceid_filtnormconf/raceid_filtnormconf/0.2.3+galaxy3: r-raceid:0.2.3
galaxy.tool_util.deps.containers INFO 2025-03-19 07:00:31,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/r-raceid:0.2.3,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:31,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:31,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9kphj with k8s id: gxy-9kphj scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:33,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-9kphj set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:00:55,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9kphj with k8s id: gxy-9kphj succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:00:56,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 156: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:01:08,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 156 finished
galaxy.jobs DEBUG 2025-03-19 07:01:08,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/156/outputs/dataset_a269bebe-ac10-4b81-861a-036f3f8e4347.dat to /galaxy/server/database/objects/a/2/6/dataset_a269bebe-ac10-4b81-861a-036f3f8e4347.dat
galaxy.jobs DEBUG 2025-03-19 07:01:08,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/156/outputs/dataset_4c963e00-8773-450d-abe0-ce6bce2b7788.dat to /galaxy/server/database/objects/4/c/9/dataset_4c963e00-8773-450d-abe0-ce6bce2b7788.dat
galaxy.jobs DEBUG 2025-03-19 07:01:08,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/156/outputs/dataset_db19da80-a604-413d-a60c-4da69946efc3.dat to /galaxy/server/database/objects/d/b/1/dataset_db19da80-a604-413d-a60c-4da69946efc3.dat
galaxy.jobs DEBUG 2025-03-19 07:01:08,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/156/outputs/dataset_d1d94f8b-c347-49a2-a134-35c844a079cc.dat to /galaxy/server/database/objects/d/1/d/dataset_d1d94f8b-c347-49a2-a134-35c844a079cc.dat
galaxy.model.metadata DEBUG 2025-03-19 07:01:08,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 227
galaxy.model.metadata DEBUG 2025-03-19 07:01:08,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 228
galaxy.model.metadata DEBUG 2025-03-19 07:01:08,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 229
galaxy.model.metadata DEBUG 2025-03-19 07:01:08,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 230
galaxy.jobs INFO 2025-03-19 07:01:08,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 156 in /galaxy/server/database/jobs_directory/000/156
galaxy.objectstore CRITICAL 2025-03-19 07:01:08,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/156/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/156/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:01:08,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 156 executed (135.799 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:08,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 156 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:08,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-9kphj (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:01:11,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 157
tpv.core.entities DEBUG 2025-03-19 07:01:11,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:01:11,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:01:11,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:01:11,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:01:11,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Working directory for job is: /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners DEBUG 2025-03-19 07:01:11,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [157] queued (28.974 ms)
galaxy.jobs.handler INFO 2025-03-19 07:01:11,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:11,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 157
galaxy.jobs DEBUG 2025-03-19 07:01:11,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [157] prepared (59.688 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:01:11,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/157/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/157/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/157/configs/tmpy8tzq90_']
galaxy.jobs.runners DEBUG 2025-03-19 07:01:11,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (157) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/157/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/157/galaxy_157.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:11,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:11,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:12,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nkfgn with k8s id: gxy-nkfgn scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:13,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nkfgn with k8s id: gxy-nkfgn scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:14,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-nkfgn set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:21,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nkfgn with k8s id: gxy-nkfgn succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:01:21,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 157: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:01:29,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 157 finished
galaxy.jobs DEBUG 2025-03-19 07:01:29,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/157/outputs/dataset_40fa3957-b011-4492-9212-0f1506f4e915.dat to /galaxy/server/database/objects/4/0/f/dataset_40fa3957-b011-4492-9212-0f1506f4e915.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:01:29,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pp.neighbors_gauss_braycurtis.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/157/working/gxupload_0', 'object_id': 231}]}]}]
h5py._conv DEBUG 2025-03-19 07:01:29,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Creating converter from 3 to 5
galaxy.jobs INFO 2025-03-19 07:01:29,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 157 in /galaxy/server/database/jobs_directory/000/157
galaxy.jobs DEBUG 2025-03-19 07:01:29,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 157 executed (141.138 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:29,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 157 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:29,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-nkfgn (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:01:29,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 158
tpv.core.entities DEBUG 2025-03-19 07:01:29,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:01:29,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:01:29,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:01:29,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:01:29,993 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Working directory for job is: /galaxy/server/database/jobs_directory/000/158
galaxy.jobs.runners DEBUG 2025-03-19 07:01:30,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [158] queued (30.795 ms)
galaxy.jobs.handler INFO 2025-03-19 07:01:30,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:30,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 158
galaxy.jobs DEBUG 2025-03-19 07:01:30,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [158] prepared (73.571 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:01:30,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:01:30,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:01:30,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:01:30,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/158/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/158/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/4/0/f/dataset_40fa3957-b011-4492-9212-0f1506f4e915.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/158/configs/tmplqwg_x4j' > '/galaxy/server/database/jobs_directory/000/158/outputs/dataset_197cd00e-71f3-4819-9017-0e32b486eda1.dat' && python '/galaxy/server/database/jobs_directory/000/158/configs/tmplqwg_x4j' >> '/galaxy/server/database/jobs_directory/000/158/outputs/dataset_197cd00e-71f3-4819-9017-0e32b486eda1.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/158/outputs/dataset_197cd00e-71f3-4819-9017-0e32b486eda1.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:01:30,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (158) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/158/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/158/galaxy_158.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/158/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/158/outputs/dataset_13bf43a6-91ba-4c5c-98b0-ac9303bc1793.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/158/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/158/outputs/dataset_13bf43a6-91ba-4c5c-98b0-ac9303bc1793.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:30,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:01:30,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:01:30,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:01:30,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:30,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:31,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:32,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:33,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:34,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:35,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:36,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:37,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:38,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:39,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:40,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:41,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:42,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:43,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:44,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:45,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:46,835 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:47,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:49,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:50,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:51,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:52,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:53,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:54,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:55,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:56,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:57,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:58,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:01:59,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-fv288 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:16,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-fv288 with k8s id: gxy-fv288 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:02:16,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 158: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:02:24,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 158 finished
galaxy.jobs DEBUG 2025-03-19 07:02:24,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/158/outputs/dataset_13bf43a6-91ba-4c5c-98b0-ac9303bc1793.dat to /galaxy/server/database/objects/1/3/b/dataset_13bf43a6-91ba-4c5c-98b0-ac9303bc1793.dat
galaxy.jobs DEBUG 2025-03-19 07:02:24,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/158/outputs/dataset_197cd00e-71f3-4819-9017-0e32b486eda1.dat to /galaxy/server/database/objects/1/9/7/dataset_197cd00e-71f3-4819-9017-0e32b486eda1.dat
galaxy.model.metadata DEBUG 2025-03-19 07:02:24,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 232
galaxy.model.metadata DEBUG 2025-03-19 07:02:24,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 233
galaxy.util WARNING 2025-03-19 07:02:24,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/1/3/b/dataset_13bf43a6-91ba-4c5c-98b0-ac9303bc1793.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/3/b/dataset_13bf43a6-91ba-4c5c-98b0-ac9303bc1793.dat'
galaxy.jobs INFO 2025-03-19 07:02:24,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 158 in /galaxy/server/database/jobs_directory/000/158
galaxy.objectstore CRITICAL 2025-03-19 07:02:24,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/158/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/158/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:02:24,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 158 executed (104.046 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:24,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 158 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:24,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-fv288 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:02:25,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 159
tpv.core.entities DEBUG 2025-03-19 07:02:25,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:02:25,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:02:25,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:02:25,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:02:25,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Working directory for job is: /galaxy/server/database/jobs_directory/000/159
galaxy.jobs.runners DEBUG 2025-03-19 07:02:25,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [159] queued (33.350 ms)
galaxy.jobs.handler INFO 2025-03-19 07:02:25,910 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:25,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 159
galaxy.jobs DEBUG 2025-03-19 07:02:25,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [159] prepared (62.326 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:02:26,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/159/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/159/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/159/configs/tmpw3nx_muz']
galaxy.jobs.runners DEBUG 2025-03-19 07:02:26,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (159) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/159/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/159/galaxy_159.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:26,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:26,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:26,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5jw57 with k8s id: gxy-5jw57 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:27,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5jw57 with k8s id: gxy-5jw57 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:28,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-5jw57 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:35,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5jw57 with k8s id: gxy-5jw57 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:02:35,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 159: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:02:43,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 159 finished
galaxy.jobs DEBUG 2025-03-19 07:02:43,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/159/outputs/dataset_c893e623-3617-4c03-8380-9d5163ce9e20.dat to /galaxy/server/database/objects/c/8/9/dataset_c893e623-3617-4c03-8380-9d5163ce9e20.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:02:43,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pp.neighbors_gauss_braycurtis.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/159/working/gxupload_0', 'object_id': 234}]}]}]
galaxy.jobs INFO 2025-03-19 07:02:43,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 159 in /galaxy/server/database/jobs_directory/000/159
galaxy.jobs DEBUG 2025-03-19 07:02:43,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 159 executed (134.469 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:43,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 159 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:43,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-5jw57 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:02:44,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 160
tpv.core.entities DEBUG 2025-03-19 07:02:44,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:02:44,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:02:44,209 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:02:44,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:02:44,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Working directory for job is: /galaxy/server/database/jobs_directory/000/160
galaxy.jobs.runners DEBUG 2025-03-19 07:02:44,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [160] queued (28.719 ms)
galaxy.jobs.handler INFO 2025-03-19 07:02:44,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:44,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 160
galaxy.jobs DEBUG 2025-03-19 07:02:44,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [160] prepared (58.440 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:02:44,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:02:44,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:02:44,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:02:44,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/160/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/160/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/c/8/9/dataset_c893e623-3617-4c03-8380-9d5163ce9e20.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/160/configs/tmpvzpntrhb' > '/galaxy/server/database/jobs_directory/000/160/outputs/dataset_887252ce-af66-40da-8aff-f36d500c1538.dat' && python '/galaxy/server/database/jobs_directory/000/160/configs/tmpvzpntrhb' >> '/galaxy/server/database/jobs_directory/000/160/outputs/dataset_887252ce-af66-40da-8aff-f36d500c1538.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/160/outputs/dataset_887252ce-af66-40da-8aff-f36d500c1538.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:02:44,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (160) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/160/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/160/galaxy_160.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/160/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/160/outputs/dataset_bbf1e8f7-8663-4c27-9568-e23ca2658025.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/160/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/160/outputs/dataset_bbf1e8f7-8663-4c27-9568-e23ca2658025.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:44,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:02:44,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:02:44,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:02:44,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:44,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:45,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xsk6r with k8s id: gxy-xsk6r scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:46,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-xsk6r set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:02:53,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xsk6r with k8s id: gxy-xsk6r succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:02:53,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 160: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:03:01,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 160 finished
galaxy.jobs DEBUG 2025-03-19 07:03:01,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/160/outputs/dataset_bbf1e8f7-8663-4c27-9568-e23ca2658025.dat to /galaxy/server/database/objects/b/b/f/dataset_bbf1e8f7-8663-4c27-9568-e23ca2658025.dat
galaxy.jobs DEBUG 2025-03-19 07:03:01,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/160/outputs/dataset_887252ce-af66-40da-8aff-f36d500c1538.dat to /galaxy/server/database/objects/8/8/7/dataset_887252ce-af66-40da-8aff-f36d500c1538.dat
galaxy.model.metadata DEBUG 2025-03-19 07:03:01,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 235
galaxy.model.metadata DEBUG 2025-03-19 07:03:01,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 236
galaxy.util WARNING 2025-03-19 07:03:01,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/b/b/f/dataset_bbf1e8f7-8663-4c27-9568-e23ca2658025.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/b/f/dataset_bbf1e8f7-8663-4c27-9568-e23ca2658025.dat'
galaxy.jobs INFO 2025-03-19 07:03:01,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 160 in /galaxy/server/database/jobs_directory/000/160
galaxy.objectstore CRITICAL 2025-03-19 07:03:01,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/160/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/160/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:03:01,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 160 executed (113.729 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:01,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 160 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:01,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-xsk6r (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:03:03,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 161
tpv.core.entities DEBUG 2025-03-19 07:03:03,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:03:03,595 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:03:03,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:03:03,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:03:03,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Working directory for job is: /galaxy/server/database/jobs_directory/000/161
galaxy.jobs.runners DEBUG 2025-03-19 07:03:03,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [161] queued (28.950 ms)
galaxy.jobs.handler INFO 2025-03-19 07:03:03,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:03,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 161
galaxy.jobs DEBUG 2025-03-19 07:03:03,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [161] prepared (63.065 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:03:03,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/161/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/161/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/161/configs/tmp68x1onn1']
galaxy.jobs.runners DEBUG 2025-03-19 07:03:03,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (161) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/161/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/161/galaxy_161.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:03,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:03,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:04,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7w5c4 with k8s id: gxy-7w5c4 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:05,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-7w5c4 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:13,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7w5c4 with k8s id: gxy-7w5c4 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:03:14,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 161: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:03:22,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 161 finished
galaxy.jobs DEBUG 2025-03-19 07:03:22,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/161/outputs/dataset_4f6ef318-32ea-4bab-8ec8-e10a30275a0e.dat to /galaxy/server/database/objects/4/f/6/dataset_4f6ef318-32ea-4bab-8ec8-e10a30275a0e.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:03:22,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'krumsiek11.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/161/working/gxupload_0', 'object_id': 237}]}]}]
galaxy.jobs INFO 2025-03-19 07:03:22,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 161 in /galaxy/server/database/jobs_directory/000/161
galaxy.jobs DEBUG 2025-03-19 07:03:22,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 161 executed (149.741 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:22,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 161 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:22,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-7w5c4 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:03:22,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 162
tpv.core.entities DEBUG 2025-03-19 07:03:22,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:03:22,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:03:22,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:03:22,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:03:22,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Working directory for job is: /galaxy/server/database/jobs_directory/000/162
galaxy.jobs.runners DEBUG 2025-03-19 07:03:22,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [162] queued (33.610 ms)
galaxy.jobs.handler INFO 2025-03-19 07:03:22,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:22,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 162
galaxy.jobs DEBUG 2025-03-19 07:03:23,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [162] prepared (66.340 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:03:23,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:03:23,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:03:23,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:03:23,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/162/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/162/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/4/f/6/dataset_4f6ef318-32ea-4bab-8ec8-e10a30275a0e.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/162/configs/tmpobum6rwz' > '/galaxy/server/database/jobs_directory/000/162/outputs/dataset_5594a802-0e56-412c-baa7-26bc4fdb7cce.dat' && python '/galaxy/server/database/jobs_directory/000/162/configs/tmpobum6rwz' >> '/galaxy/server/database/jobs_directory/000/162/outputs/dataset_5594a802-0e56-412c-baa7-26bc4fdb7cce.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/162/outputs/dataset_5594a802-0e56-412c-baa7-26bc4fdb7cce.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:03:23,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (162) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/162/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/162/galaxy_162.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/162/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/162/outputs/dataset_73759a06-1c46-46f7-8f9e-a425aaef38af.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/162/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/162/outputs/dataset_73759a06-1c46-46f7-8f9e-a425aaef38af.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:23,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:03:23,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:03:23,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:03:23,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:23,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:23,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wk97m with k8s id: gxy-wk97m scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:25,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wk97m with k8s id: gxy-wk97m scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:26,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-wk97m set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:32,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wk97m with k8s id: gxy-wk97m succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:03:32,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 162: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:03:40,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 162 finished
galaxy.jobs DEBUG 2025-03-19 07:03:40,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/162/outputs/dataset_73759a06-1c46-46f7-8f9e-a425aaef38af.dat to /galaxy/server/database/objects/7/3/7/dataset_73759a06-1c46-46f7-8f9e-a425aaef38af.dat
galaxy.jobs DEBUG 2025-03-19 07:03:40,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/162/outputs/dataset_5594a802-0e56-412c-baa7-26bc4fdb7cce.dat to /galaxy/server/database/objects/5/5/9/dataset_5594a802-0e56-412c-baa7-26bc4fdb7cce.dat
galaxy.model.metadata DEBUG 2025-03-19 07:03:40,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 238
galaxy.model.metadata DEBUG 2025-03-19 07:03:40,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 239
galaxy.util WARNING 2025-03-19 07:03:40,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/7/3/7/dataset_73759a06-1c46-46f7-8f9e-a425aaef38af.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/3/7/dataset_73759a06-1c46-46f7-8f9e-a425aaef38af.dat'
galaxy.jobs INFO 2025-03-19 07:03:40,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 162 in /galaxy/server/database/jobs_directory/000/162
galaxy.objectstore CRITICAL 2025-03-19 07:03:40,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/162/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/162/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:03:40,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 162 executed (117.693 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:40,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 162 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:40,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-wk97m (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:03:42,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 163
tpv.core.entities DEBUG 2025-03-19 07:03:42,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:03:42,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:03:42,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:03:42,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:03:42,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Working directory for job is: /galaxy/server/database/jobs_directory/000/163
galaxy.jobs.runners DEBUG 2025-03-19 07:03:42,304 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [163] queued (25.340 ms)
galaxy.jobs.handler INFO 2025-03-19 07:03:42,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:42,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 163
galaxy.jobs DEBUG 2025-03-19 07:03:42,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [163] prepared (58.212 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:03:42,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/163/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/163/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/163/configs/tmpatc_nho2']
galaxy.jobs.runners DEBUG 2025-03-19 07:03:42,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (163) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/163/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/163/galaxy_163.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:42,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:42,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:43,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6nbdh with k8s id: gxy-6nbdh scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:44,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-6nbdh set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:51,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6nbdh with k8s id: gxy-6nbdh succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:03:51,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 163: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:03:59,292 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 163 finished
galaxy.jobs DEBUG 2025-03-19 07:03:59,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/163/outputs/dataset_15616b7c-e50f-4b0e-8c41-11fe80a851d1.dat to /galaxy/server/database/objects/1/5/6/dataset_15616b7c-e50f-4b0e-8c41-11fe80a851d1.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:03:59,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'krumsiek11.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/163/working/gxupload_0', 'object_id': 240}]}]}]
galaxy.jobs INFO 2025-03-19 07:03:59,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 163 in /galaxy/server/database/jobs_directory/000/163
galaxy.jobs DEBUG 2025-03-19 07:03:59,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 163 executed (165.938 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:59,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 163 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:03:59,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-6nbdh (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:04:00,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 164
tpv.core.entities DEBUG 2025-03-19 07:04:00,599 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:04:00,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:04:00,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:04:00,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:04:00,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Working directory for job is: /galaxy/server/database/jobs_directory/000/164
galaxy.jobs.runners DEBUG 2025-03-19 07:04:00,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [164] queued (37.396 ms)
galaxy.jobs.handler INFO 2025-03-19 07:04:00,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:00,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 164
galaxy.jobs DEBUG 2025-03-19 07:04:00,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [164] prepared (60.019 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:00,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:04:00,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:00,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:04:00,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/164/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/164/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/1/5/6/dataset_15616b7c-e50f-4b0e-8c41-11fe80a851d1.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/164/configs/tmpiv8uwhvp' > '/galaxy/server/database/jobs_directory/000/164/outputs/dataset_a5078104-98c9-4e63-81ab-9f131b31a341.dat' && python '/galaxy/server/database/jobs_directory/000/164/configs/tmpiv8uwhvp' >> '/galaxy/server/database/jobs_directory/000/164/outputs/dataset_a5078104-98c9-4e63-81ab-9f131b31a341.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/164/outputs/dataset_a5078104-98c9-4e63-81ab-9f131b31a341.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:04:00,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (164) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/164/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/164/galaxy_164.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/164/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/164/outputs/dataset_5d691a20-4bb7-4201-b16a-3e01bbc22a61.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/164/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/164/outputs/dataset_5d691a20-4bb7-4201-b16a-3e01bbc22a61.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:00,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:00,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:04:00,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:00,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:00,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:01,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cm8qz with k8s id: gxy-cm8qz scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:02,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-cm8qz set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:08,540 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cm8qz with k8s id: gxy-cm8qz succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:04:08,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 164: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:04:16,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 164 finished
galaxy.jobs DEBUG 2025-03-19 07:04:16,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/164/outputs/dataset_5d691a20-4bb7-4201-b16a-3e01bbc22a61.dat to /galaxy/server/database/objects/5/d/6/dataset_5d691a20-4bb7-4201-b16a-3e01bbc22a61.dat
galaxy.jobs DEBUG 2025-03-19 07:04:16,495 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/164/outputs/dataset_a5078104-98c9-4e63-81ab-9f131b31a341.dat to /galaxy/server/database/objects/a/5/0/dataset_a5078104-98c9-4e63-81ab-9f131b31a341.dat
galaxy.model.metadata DEBUG 2025-03-19 07:04:16,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 241
galaxy.model.metadata DEBUG 2025-03-19 07:04:16,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 242
galaxy.util WARNING 2025-03-19 07:04:16,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/5/d/6/dataset_5d691a20-4bb7-4201-b16a-3e01bbc22a61.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/d/6/dataset_5d691a20-4bb7-4201-b16a-3e01bbc22a61.dat'
galaxy.jobs INFO 2025-03-19 07:04:16,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 164 in /galaxy/server/database/jobs_directory/000/164
galaxy.objectstore CRITICAL 2025-03-19 07:04:16,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/164/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/164/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:04:16,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 164 executed (125.158 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:16,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 164 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:16,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-cm8qz (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:04:17,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 165
tpv.core.entities DEBUG 2025-03-19 07:04:17,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:04:17,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:04:17,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:04:17,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:04:17,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Working directory for job is: /galaxy/server/database/jobs_directory/000/165
galaxy.jobs.runners DEBUG 2025-03-19 07:04:17,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [165] queued (28.681 ms)
galaxy.jobs.handler INFO 2025-03-19 07:04:17,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:17,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 165
galaxy.jobs DEBUG 2025-03-19 07:04:18,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [165] prepared (54.884 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:04:18,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/165/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/165/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/165/configs/tmp4wm7n2mh']
galaxy.jobs.runners DEBUG 2025-03-19 07:04:18,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (165) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/165/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/165/galaxy_165.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:18,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:18,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:18,578 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7cthv with k8s id: gxy-7cthv scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:19,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-7cthv set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:27,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7cthv with k8s id: gxy-7cthv succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:04:27,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 165: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:04:35,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 165 finished
galaxy.jobs DEBUG 2025-03-19 07:04:35,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/165/outputs/dataset_64790e51-6538-45aa-abe7-3a96cdb99c33.dat to /galaxy/server/database/objects/6/4/7/dataset_64790e51-6538-45aa-abe7-3a96cdb99c33.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:04:35,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'krumsiek11.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/165/working/gxupload_0', 'object_id': 243}]}]}]
galaxy.jobs INFO 2025-03-19 07:04:35,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 165 in /galaxy/server/database/jobs_directory/000/165
galaxy.jobs DEBUG 2025-03-19 07:04:35,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 165 executed (140.052 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:35,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 165 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:35,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-7cthv (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:04:36,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 166
tpv.core.entities DEBUG 2025-03-19 07:04:36,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:04:36,237 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:04:36,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:04:36,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:04:36,259 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Working directory for job is: /galaxy/server/database/jobs_directory/000/166
galaxy.jobs.runners DEBUG 2025-03-19 07:04:36,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [166] queued (25.700 ms)
galaxy.jobs.handler INFO 2025-03-19 07:04:36,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:36,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 166
galaxy.jobs DEBUG 2025-03-19 07:04:36,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [166] prepared (50.949 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:36,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:04:36,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:36,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:04:36,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/166/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/166/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/6/4/7/dataset_64790e51-6538-45aa-abe7-3a96cdb99c33.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/166/configs/tmpj1r_zf_b' > '/galaxy/server/database/jobs_directory/000/166/outputs/dataset_d3855b1f-f062-4d76-92ab-13216b84de71.dat' && python '/galaxy/server/database/jobs_directory/000/166/configs/tmpj1r_zf_b' >> '/galaxy/server/database/jobs_directory/000/166/outputs/dataset_d3855b1f-f062-4d76-92ab-13216b84de71.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/166/outputs/dataset_d3855b1f-f062-4d76-92ab-13216b84de71.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:04:36,396 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (166) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/166/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/166/galaxy_166.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/166/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/166/outputs/dataset_37d4fe83-988a-4bb4-8b20-56892e69ddfc.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/166/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/166/outputs/dataset_37d4fe83-988a-4bb4-8b20-56892e69ddfc.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:36,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:36,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:04:36,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:04:36,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:36,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:36,843 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2p4mf with k8s id: gxy-2p4mf scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:37,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-2p4mf set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:45,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2p4mf with k8s id: gxy-2p4mf succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:04:45,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 166: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:04:52,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 166 finished
galaxy.jobs DEBUG 2025-03-19 07:04:53,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/166/outputs/dataset_37d4fe83-988a-4bb4-8b20-56892e69ddfc.dat to /galaxy/server/database/objects/3/7/d/dataset_37d4fe83-988a-4bb4-8b20-56892e69ddfc.dat
galaxy.jobs DEBUG 2025-03-19 07:04:53,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/166/outputs/dataset_d3855b1f-f062-4d76-92ab-13216b84de71.dat to /galaxy/server/database/objects/d/3/8/dataset_d3855b1f-f062-4d76-92ab-13216b84de71.dat
galaxy.model.metadata DEBUG 2025-03-19 07:04:53,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 244
galaxy.model.metadata DEBUG 2025-03-19 07:04:53,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 245
galaxy.util WARNING 2025-03-19 07:04:53,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/3/7/d/dataset_37d4fe83-988a-4bb4-8b20-56892e69ddfc.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/7/d/dataset_37d4fe83-988a-4bb4-8b20-56892e69ddfc.dat'
galaxy.jobs INFO 2025-03-19 07:04:53,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 166 in /galaxy/server/database/jobs_directory/000/166
galaxy.objectstore CRITICAL 2025-03-19 07:04:53,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/166/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/166/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:04:53,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 166 executed (117.991 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:53,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 166 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:53,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-2p4mf (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:04:54,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 167
tpv.core.entities DEBUG 2025-03-19 07:04:54,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:04:54,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:04:54,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:04:54,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:04:54,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Working directory for job is: /galaxy/server/database/jobs_directory/000/167
galaxy.jobs.runners DEBUG 2025-03-19 07:04:54,664 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [167] queued (22.590 ms)
galaxy.jobs.handler INFO 2025-03-19 07:04:54,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:54,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 167
galaxy.jobs DEBUG 2025-03-19 07:04:54,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [167] prepared (59.295 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:04:54,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/167/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/167/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/167/configs/tmpfluzpizw']
galaxy.jobs.runners DEBUG 2025-03-19 07:04:54,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (167) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/167/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/167/galaxy_167.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:54,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:54,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:56,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9nd8f with k8s id: gxy-9nd8f scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:04:57,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-9nd8f set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:05,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-9nd8f with k8s id: gxy-9nd8f succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:05:05,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 167: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:05:13,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 167 finished
galaxy.jobs DEBUG 2025-03-19 07:05:13,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/167/outputs/dataset_3af5074b-77f2-41aa-8fc9-e7c0fd97e2ff.dat to /galaxy/server/database/objects/3/a/f/dataset_3af5074b-77f2-41aa-8fc9-e7c0fd97e2ff.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:05:13,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pp.neighbors_gauss_braycurtis.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/167/working/gxupload_0', 'object_id': 246}]}]}]
galaxy.jobs INFO 2025-03-19 07:05:13,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 167 in /galaxy/server/database/jobs_directory/000/167
galaxy.jobs DEBUG 2025-03-19 07:05:13,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 167 executed (141.338 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:13,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 167 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:13,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-9nd8f (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:05:13,933 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 168
tpv.core.entities DEBUG 2025-03-19 07:05:13,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:05:13,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:05:13,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:05:13,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:05:13,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Working directory for job is: /galaxy/server/database/jobs_directory/000/168
galaxy.jobs.runners DEBUG 2025-03-19 07:05:13,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [168] queued (33.330 ms)
galaxy.jobs.handler INFO 2025-03-19 07:05:13,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:13,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 168
galaxy.jobs DEBUG 2025-03-19 07:05:14,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [168] prepared (63.958 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:14,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:05:14,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:14,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:05:14,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/168/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/168/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/3/a/f/dataset_3af5074b-77f2-41aa-8fc9-e7c0fd97e2ff.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/168/configs/tmpsdvap1rk' > '/galaxy/server/database/jobs_directory/000/168/outputs/dataset_11ac83e9-a093-4622-929b-48631e474c2a.dat' && python '/galaxy/server/database/jobs_directory/000/168/configs/tmpsdvap1rk' >> '/galaxy/server/database/jobs_directory/000/168/outputs/dataset_11ac83e9-a093-4622-929b-48631e474c2a.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/168/outputs/dataset_11ac83e9-a093-4622-929b-48631e474c2a.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:05:14,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (168) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/168/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/168/galaxy_168.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/168/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/168/outputs/dataset_3851e8a2-893f-4c57-9b0a-ecb4a6e64e9d.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/168/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/168/outputs/dataset_3851e8a2-893f-4c57-9b0a-ecb4a6e64e9d.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:14,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:14,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:05:14,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:14,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:14,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:14,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nllr7 with k8s id: gxy-nllr7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:15,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nllr7 with k8s id: gxy-nllr7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:16,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-nllr7 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:23,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-nllr7 with k8s id: gxy-nllr7 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:05:23,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 168: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:05:31,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 168 finished
galaxy.jobs DEBUG 2025-03-19 07:05:31,387 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/168/outputs/dataset_3851e8a2-893f-4c57-9b0a-ecb4a6e64e9d.dat to /galaxy/server/database/objects/3/8/5/dataset_3851e8a2-893f-4c57-9b0a-ecb4a6e64e9d.dat
galaxy.jobs DEBUG 2025-03-19 07:05:31,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/168/outputs/dataset_11ac83e9-a093-4622-929b-48631e474c2a.dat to /galaxy/server/database/objects/1/1/a/dataset_11ac83e9-a093-4622-929b-48631e474c2a.dat
galaxy.model.metadata DEBUG 2025-03-19 07:05:31,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 247
galaxy.model.metadata DEBUG 2025-03-19 07:05:31,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 248
galaxy.util WARNING 2025-03-19 07:05:31,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/3/8/5/dataset_3851e8a2-893f-4c57-9b0a-ecb4a6e64e9d.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/8/5/dataset_3851e8a2-893f-4c57-9b0a-ecb4a6e64e9d.dat'
galaxy.jobs INFO 2025-03-19 07:05:31,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 168 in /galaxy/server/database/jobs_directory/000/168
galaxy.objectstore CRITICAL 2025-03-19 07:05:31,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/168/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/168/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:05:31,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 168 executed (114.878 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:31,510 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 168 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:31,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-nllr7 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:05:33,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 169
tpv.core.entities DEBUG 2025-03-19 07:05:33,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:05:33,300 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:05:33,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:05:33,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:05:33,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Working directory for job is: /galaxy/server/database/jobs_directory/000/169
galaxy.jobs.runners DEBUG 2025-03-19 07:05:33,334 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [169] queued (31.192 ms)
galaxy.jobs.handler INFO 2025-03-19 07:05:33,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:33,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 169
galaxy.jobs DEBUG 2025-03-19 07:05:33,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [169] prepared (53.922 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:05:33,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/169/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/169/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/169/configs/tmp1t_nnck5']
galaxy.jobs.runners DEBUG 2025-03-19 07:05:33,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (169) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/169/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/169/galaxy_169.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:33,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:33,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:33,634 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bbffp with k8s id: gxy-bbffp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:34,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bbffp with k8s id: gxy-bbffp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:35,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-bbffp set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:42,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-bbffp with k8s id: gxy-bbffp succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:05:42,975 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 169: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:05:50,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 169 finished
galaxy.jobs DEBUG 2025-03-19 07:05:50,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/169/outputs/dataset_1450ef64-1376-44d6-be58-84dc1f2ff110.dat to /galaxy/server/database/objects/1/4/5/dataset_1450ef64-1376-44d6-be58-84dc1f2ff110.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:05:50,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'krumsiek11.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/169/working/gxupload_0', 'object_id': 249}]}]}]
galaxy.jobs INFO 2025-03-19 07:05:50,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 169 in /galaxy/server/database/jobs_directory/000/169
galaxy.jobs DEBUG 2025-03-19 07:05:51,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 169 executed (155.490 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:51,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 169 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:51,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-bbffp (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:05:51,596 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 170
tpv.core.entities DEBUG 2025-03-19 07:05:51,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:05:51,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:05:51,621 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:05:51,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:05:51,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Working directory for job is: /galaxy/server/database/jobs_directory/000/170
galaxy.jobs.runners DEBUG 2025-03-19 07:05:51,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [170] queued (27.950 ms)
galaxy.jobs.handler INFO 2025-03-19 07:05:51,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:51,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 170
galaxy.jobs DEBUG 2025-03-19 07:05:51,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [170] prepared (55.510 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:51,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:05:51,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:51,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:05:51,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/170/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/170/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/1/4/5/dataset_1450ef64-1376-44d6-be58-84dc1f2ff110.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/170/configs/tmppxdclr2i' > '/galaxy/server/database/jobs_directory/000/170/outputs/dataset_28f53e88-b3b3-4062-a33d-f1bbd4665ff0.dat' && python '/galaxy/server/database/jobs_directory/000/170/configs/tmppxdclr2i' >> '/galaxy/server/database/jobs_directory/000/170/outputs/dataset_28f53e88-b3b3-4062-a33d-f1bbd4665ff0.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/170/outputs/dataset_28f53e88-b3b3-4062-a33d-f1bbd4665ff0.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:05:51,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (170) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/170/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/170/galaxy_170.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/170/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/170/outputs/dataset_f41eef85-3a3f-4257-b1a5-cdb54f34ae2e.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/170/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/170/outputs/dataset_f41eef85-3a3f-4257-b1a5-cdb54f34ae2e.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:51,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:51,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:05:51,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:05:51,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:51,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:52,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-59ttw with k8s id: gxy-59ttw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:05:53,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-59ttw set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:01,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-59ttw with k8s id: gxy-59ttw succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:06:01,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 170: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:06:08,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 170 finished
galaxy.jobs DEBUG 2025-03-19 07:06:09,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/170/outputs/dataset_f41eef85-3a3f-4257-b1a5-cdb54f34ae2e.dat to /galaxy/server/database/objects/f/4/1/dataset_f41eef85-3a3f-4257-b1a5-cdb54f34ae2e.dat
galaxy.jobs DEBUG 2025-03-19 07:06:09,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/170/outputs/dataset_28f53e88-b3b3-4062-a33d-f1bbd4665ff0.dat to /galaxy/server/database/objects/2/8/f/dataset_28f53e88-b3b3-4062-a33d-f1bbd4665ff0.dat
galaxy.model.metadata DEBUG 2025-03-19 07:06:09,028 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 250
galaxy.model.metadata DEBUG 2025-03-19 07:06:09,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 251
galaxy.util WARNING 2025-03-19 07:06:09,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/f/4/1/dataset_f41eef85-3a3f-4257-b1a5-cdb54f34ae2e.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/4/1/dataset_f41eef85-3a3f-4257-b1a5-cdb54f34ae2e.dat'
galaxy.jobs INFO 2025-03-19 07:06:09,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 170 in /galaxy/server/database/jobs_directory/000/170
galaxy.objectstore CRITICAL 2025-03-19 07:06:09,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/170/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/170/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:06:09,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 170 executed (110.095 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:09,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 170 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:09,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-59ttw (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:06:10,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 171
tpv.core.entities DEBUG 2025-03-19 07:06:10,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:06:10,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:06:10,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:06:10,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:06:10,965 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Working directory for job is: /galaxy/server/database/jobs_directory/000/171
galaxy.jobs.runners DEBUG 2025-03-19 07:06:10,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [171] queued (27.236 ms)
galaxy.jobs.handler INFO 2025-03-19 07:06:10,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:10,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 171
galaxy.jobs DEBUG 2025-03-19 07:06:11,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [171] prepared (59.131 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:06:11,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/171/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/171/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/171/configs/tmpo8t2ceae']
galaxy.jobs.runners DEBUG 2025-03-19 07:06:11,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (171) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/171/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/171/galaxy_171.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:11,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:11,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:12,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dj2qj with k8s id: gxy-dj2qj scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:13,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-dj2qj set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:21,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dj2qj with k8s id: gxy-dj2qj succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:06:21,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 171: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:06:29,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 171 finished
galaxy.jobs DEBUG 2025-03-19 07:06:29,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/171/outputs/dataset_3443db16-9b0b-4105-8746-ff526e91bac6.dat to /galaxy/server/database/objects/3/4/4/dataset_3443db16-9b0b-4105-8746-ff526e91bac6.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:06:29,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pp.neighbors_umap_euclidean.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/171/working/gxupload_0', 'object_id': 252}]}]}]
galaxy.jobs INFO 2025-03-19 07:06:29,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 171 in /galaxy/server/database/jobs_directory/000/171
galaxy.jobs DEBUG 2025-03-19 07:06:29,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 171 executed (143.860 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:29,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 171 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:29,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-dj2qj (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:06:30,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 172
tpv.core.entities DEBUG 2025-03-19 07:06:30,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:06:30,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:06:30,269 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:06:30,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:06:30,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Working directory for job is: /galaxy/server/database/jobs_directory/000/172
galaxy.jobs.runners DEBUG 2025-03-19 07:06:30,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [172] queued (34.065 ms)
galaxy.jobs.handler INFO 2025-03-19 07:06:30,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:30,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 172
galaxy.jobs DEBUG 2025-03-19 07:06:30,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [172] prepared (64.930 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:06:30,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:06:30,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:06:30,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:06:30,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/172/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/172/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/3/4/4/dataset_3443db16-9b0b-4105-8746-ff526e91bac6.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/172/configs/tmpeemgclvq' > '/galaxy/server/database/jobs_directory/000/172/outputs/dataset_e540dc71-1fd4-484f-87ed-531bf77ac6b0.dat' && python '/galaxy/server/database/jobs_directory/000/172/configs/tmpeemgclvq' >> '/galaxy/server/database/jobs_directory/000/172/outputs/dataset_e540dc71-1fd4-484f-87ed-531bf77ac6b0.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/172/outputs/dataset_e540dc71-1fd4-484f-87ed-531bf77ac6b0.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:06:30,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (172) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/172/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/172/galaxy_172.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/172/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/172/outputs/dataset_a6b5eac6-5085-4715-8e55-705566ac546a.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/172/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/172/outputs/dataset_a6b5eac6-5085-4715-8e55-705566ac546a.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:30,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:06:30,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:06:30,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:06:30,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:30,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:31,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c6z7n with k8s id: gxy-c6z7n scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:32,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-c6z7n set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:46,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c6z7n with k8s id: gxy-c6z7n succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:06:46,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 172: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:06:54,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 172 finished
galaxy.jobs DEBUG 2025-03-19 07:06:54,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/172/outputs/dataset_a6b5eac6-5085-4715-8e55-705566ac546a.dat to /galaxy/server/database/objects/a/6/b/dataset_a6b5eac6-5085-4715-8e55-705566ac546a.dat
galaxy.jobs DEBUG 2025-03-19 07:06:54,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/172/outputs/dataset_e540dc71-1fd4-484f-87ed-531bf77ac6b0.dat to /galaxy/server/database/objects/e/5/4/dataset_e540dc71-1fd4-484f-87ed-531bf77ac6b0.dat
galaxy.model.metadata DEBUG 2025-03-19 07:06:54,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 253
galaxy.model.metadata DEBUG 2025-03-19 07:06:54,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 254
galaxy.util WARNING 2025-03-19 07:06:54,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/a/6/b/dataset_a6b5eac6-5085-4715-8e55-705566ac546a.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/6/b/dataset_a6b5eac6-5085-4715-8e55-705566ac546a.dat'
galaxy.jobs INFO 2025-03-19 07:06:54,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 172 in /galaxy/server/database/jobs_directory/000/172
galaxy.objectstore CRITICAL 2025-03-19 07:06:54,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/172/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/172/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:06:54,836 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 172 executed (109.653 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:54,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 172 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:54,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-c6z7n (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:06:56,695 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 173
tpv.core.entities DEBUG 2025-03-19 07:06:56,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:06:56,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:06:56,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:06:56,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:06:56,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Working directory for job is: /galaxy/server/database/jobs_directory/000/173
galaxy.jobs.runners DEBUG 2025-03-19 07:06:56,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [173] queued (31.066 ms)
galaxy.jobs.handler INFO 2025-03-19 07:06:56,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:56,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 173
galaxy.jobs DEBUG 2025-03-19 07:06:56,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [173] prepared (65.544 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:06:56,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/173/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/173/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/173/configs/tmpea75p8co']
galaxy.jobs.runners DEBUG 2025-03-19 07:06:56,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (173) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/173/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/173/galaxy_173.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:56,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:56,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:57,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hhghn with k8s id: gxy-hhghn scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:06:58,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-hhghn set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:06,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hhghn with k8s id: gxy-hhghn succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:07:07,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 173: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:07:14,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 173 finished
galaxy.jobs DEBUG 2025-03-19 07:07:14,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/173/outputs/dataset_24f4bdc4-38aa-4692-8d55-2e9f263e9719.dat to /galaxy/server/database/objects/2/4/f/dataset_24f4bdc4-38aa-4692-8d55-2e9f263e9719.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:07:14,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pp.neighbors_umap_euclidean.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/173/working/gxupload_0', 'object_id': 255}]}]}]
galaxy.jobs INFO 2025-03-19 07:07:14,672 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 173 in /galaxy/server/database/jobs_directory/000/173
galaxy.jobs DEBUG 2025-03-19 07:07:14,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 173 executed (118.885 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:14,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 173 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:14,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-hhghn (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:07:15,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 174
tpv.core.entities DEBUG 2025-03-19 07:07:15,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:07:15,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:07:15,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:07:15,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:07:15,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Working directory for job is: /galaxy/server/database/jobs_directory/000/174
galaxy.jobs.runners DEBUG 2025-03-19 07:07:15,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [174] queued (30.457 ms)
galaxy.jobs.handler INFO 2025-03-19 07:07:15,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:15,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 174
galaxy.jobs DEBUG 2025-03-19 07:07:15,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [174] prepared (53.514 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:15,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:07:15,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:15,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:07:15,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/174/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/174/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/2/4/f/dataset_24f4bdc4-38aa-4692-8d55-2e9f263e9719.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/174/configs/tmpzbvlcemr' > '/galaxy/server/database/jobs_directory/000/174/outputs/dataset_b1cad1e0-8ed3-47e1-9afe-ab2699a0daa6.dat' && python '/galaxy/server/database/jobs_directory/000/174/configs/tmpzbvlcemr' >> '/galaxy/server/database/jobs_directory/000/174/outputs/dataset_b1cad1e0-8ed3-47e1-9afe-ab2699a0daa6.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/174/outputs/dataset_b1cad1e0-8ed3-47e1-9afe-ab2699a0daa6.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:07:15,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (174) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/174/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/174/galaxy_174.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/174/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/174/outputs/dataset_68884d5a-4fe4-4aa1-9ec4-448af9490e77.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/174/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/174/outputs/dataset_68884d5a-4fe4-4aa1-9ec4-448af9490e77.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:15,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:15,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:07:15,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:15,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:15,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:15,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7pdx5 with k8s id: gxy-7pdx5 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:17,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-7pdx5 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:23,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7pdx5 with k8s id: gxy-7pdx5 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:07:23,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 174: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:07:30,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 174 finished
galaxy.jobs DEBUG 2025-03-19 07:07:30,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/174/outputs/dataset_68884d5a-4fe4-4aa1-9ec4-448af9490e77.dat to /galaxy/server/database/objects/6/8/8/dataset_68884d5a-4fe4-4aa1-9ec4-448af9490e77.dat
galaxy.jobs DEBUG 2025-03-19 07:07:30,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/174/outputs/dataset_b1cad1e0-8ed3-47e1-9afe-ab2699a0daa6.dat to /galaxy/server/database/objects/b/1/c/dataset_b1cad1e0-8ed3-47e1-9afe-ab2699a0daa6.dat
galaxy.model.metadata DEBUG 2025-03-19 07:07:30,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 256
galaxy.model.metadata DEBUG 2025-03-19 07:07:30,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 257
galaxy.util WARNING 2025-03-19 07:07:30,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/6/8/8/dataset_68884d5a-4fe4-4aa1-9ec4-448af9490e77.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/8/8/dataset_68884d5a-4fe4-4aa1-9ec4-448af9490e77.dat'
galaxy.jobs INFO 2025-03-19 07:07:31,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 174 in /galaxy/server/database/jobs_directory/000/174
galaxy.objectstore CRITICAL 2025-03-19 07:07:31,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/174/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/174/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:07:31,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 174 executed (109.322 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:31,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 174 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:31,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-7pdx5 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:07:32,359 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 175
tpv.core.entities DEBUG 2025-03-19 07:07:32,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:07:32,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:07:32,386 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:07:32,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:07:32,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Working directory for job is: /galaxy/server/database/jobs_directory/000/175
galaxy.jobs.runners DEBUG 2025-03-19 07:07:32,418 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [175] queued (31.433 ms)
galaxy.jobs.handler INFO 2025-03-19 07:07:32,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:32,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 175
galaxy.jobs DEBUG 2025-03-19 07:07:32,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [175] prepared (58.576 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:07:32,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/175/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/175/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/175/configs/tmpp6_0_swe']
galaxy.jobs.runners DEBUG 2025-03-19 07:07:32,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (175) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/175/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/175/galaxy_175.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:32,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:32,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:33,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qbmv5 with k8s id: gxy-qbmv5 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:34,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-qbmv5 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:42,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-qbmv5 with k8s id: gxy-qbmv5 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:07:42,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 175: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:07:50,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 175 finished
galaxy.jobs DEBUG 2025-03-19 07:07:50,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/175/outputs/dataset_263cf6be-d190-456d-b70b-239f69a15966.dat to /galaxy/server/database/objects/2/6/3/dataset_263cf6be-d190-456d-b70b-239f69a15966.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:07:50,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'pp.neighbors_gauss_braycurtis.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/175/working/gxupload_0', 'object_id': 258}]}]}]
galaxy.jobs INFO 2025-03-19 07:07:50,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 175 in /galaxy/server/database/jobs_directory/000/175
galaxy.jobs DEBUG 2025-03-19 07:07:50,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 175 executed (137.635 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:50,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 175 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:50,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-qbmv5 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:07:50,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 176
tpv.core.entities DEBUG 2025-03-19 07:07:50,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:07:50,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:07:50,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:07:50,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:07:50,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Working directory for job is: /galaxy/server/database/jobs_directory/000/176
galaxy.jobs.runners DEBUG 2025-03-19 07:07:50,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [176] queued (32.740 ms)
galaxy.jobs.handler INFO 2025-03-19 07:07:50,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:50,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 176
galaxy.jobs DEBUG 2025-03-19 07:07:50,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [176] prepared (56.903 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:50,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:07:50,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:51,020 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:07:51,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/176/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/176/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/2/6/3/dataset_263cf6be-d190-456d-b70b-239f69a15966.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/176/configs/tmpk1e1mbbv' > '/galaxy/server/database/jobs_directory/000/176/outputs/dataset_31bd8196-2a72-4dbf-9d0e-9774df7cbf5d.dat' && python '/galaxy/server/database/jobs_directory/000/176/configs/tmpk1e1mbbv' >> '/galaxy/server/database/jobs_directory/000/176/outputs/dataset_31bd8196-2a72-4dbf-9d0e-9774df7cbf5d.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/176/outputs/dataset_31bd8196-2a72-4dbf-9d0e-9774df7cbf5d.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:07:51,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (176) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/176/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/176/galaxy_176.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/176/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/176/outputs/dataset_2b1947b5-65ac-4a82-b82f-28e5e3b19c54.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/176/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/176/outputs/dataset_2b1947b5-65ac-4a82-b82f-28e5e3b19c54.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:51,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:51,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:07:51,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:07:51,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:51,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:51,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cxpgs with k8s id: gxy-cxpgs scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:52,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-cxpgs set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:07:59,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-cxpgs with k8s id: gxy-cxpgs succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:07:59,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 176: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:08:07,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 176 finished
galaxy.jobs DEBUG 2025-03-19 07:08:07,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/176/outputs/dataset_2b1947b5-65ac-4a82-b82f-28e5e3b19c54.dat to /galaxy/server/database/objects/2/b/1/dataset_2b1947b5-65ac-4a82-b82f-28e5e3b19c54.dat
galaxy.jobs DEBUG 2025-03-19 07:08:07,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/176/outputs/dataset_31bd8196-2a72-4dbf-9d0e-9774df7cbf5d.dat to /galaxy/server/database/objects/3/1/b/dataset_31bd8196-2a72-4dbf-9d0e-9774df7cbf5d.dat
galaxy.model.metadata DEBUG 2025-03-19 07:08:07,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 259
galaxy.model.metadata DEBUG 2025-03-19 07:08:07,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 260
galaxy.util WARNING 2025-03-19 07:08:07,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/2/b/1/dataset_2b1947b5-65ac-4a82-b82f-28e5e3b19c54.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/b/1/dataset_2b1947b5-65ac-4a82-b82f-28e5e3b19c54.dat'
galaxy.jobs INFO 2025-03-19 07:08:07,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 176 in /galaxy/server/database/jobs_directory/000/176
galaxy.objectstore CRITICAL 2025-03-19 07:08:07,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/176/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/176/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:08:07,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 176 executed (100.484 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:07,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 176 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:07,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-cxpgs (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:08:09,015 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 177
tpv.core.entities DEBUG 2025-03-19 07:08:09,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:08:09,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:08:09,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:08:09,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:08:09,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Working directory for job is: /galaxy/server/database/jobs_directory/000/177
galaxy.jobs.runners DEBUG 2025-03-19 07:08:09,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [177] queued (26.415 ms)
galaxy.jobs.handler INFO 2025-03-19 07:08:09,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:09,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 177
galaxy.jobs DEBUG 2025-03-19 07:08:09,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [177] prepared (51.708 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:08:09,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/177/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/177/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/177/configs/tmpctlyrv0v']
galaxy.jobs.runners DEBUG 2025-03-19 07:08:09,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (177) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/177/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/177/galaxy_177.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:09,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:09,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:10,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tpdk6 with k8s id: gxy-tpdk6 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:11,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-tpdk6 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:19,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-tpdk6 with k8s id: gxy-tpdk6 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:08:20,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 177: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:08:27,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 177 finished
galaxy.jobs DEBUG 2025-03-19 07:08:27,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/177/outputs/dataset_8b0dcca5-5178-47b9-ac4e-abab233d86a8.dat to /galaxy/server/database/objects/8/b/0/dataset_8b0dcca5-5178-47b9-ac4e-abab233d86a8.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:08:27,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'tl.diffmap.neighbors_gauss_braycurtis.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/177/working/gxupload_0', 'object_id': 261}]}]}]
galaxy.jobs INFO 2025-03-19 07:08:27,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 177 in /galaxy/server/database/jobs_directory/000/177
galaxy.jobs DEBUG 2025-03-19 07:08:27,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 177 executed (118.649 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:27,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 177 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:27,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-tpdk6 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:08:28,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 178
tpv.core.entities DEBUG 2025-03-19 07:08:28,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:08:28,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:08:28,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:08:28,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:08:28,365 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Working directory for job is: /galaxy/server/database/jobs_directory/000/178
galaxy.jobs.runners DEBUG 2025-03-19 07:08:28,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [178] queued (26.529 ms)
galaxy.jobs.handler INFO 2025-03-19 07:08:28,374 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:28,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 178
galaxy.jobs DEBUG 2025-03-19 07:08:28,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [178] prepared (56.587 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:08:28,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:08:28,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:08:28,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:08:28,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/178/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/178/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/8/b/0/dataset_8b0dcca5-5178-47b9-ac4e-abab233d86a8.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/178/configs/tmpynps_fzn' > '/galaxy/server/database/jobs_directory/000/178/outputs/dataset_c6c7a554-ae53-42c7-a83a-d04e9bd55555.dat' && python '/galaxy/server/database/jobs_directory/000/178/configs/tmpynps_fzn' >> '/galaxy/server/database/jobs_directory/000/178/outputs/dataset_c6c7a554-ae53-42c7-a83a-d04e9bd55555.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/178/outputs/dataset_c6c7a554-ae53-42c7-a83a-d04e9bd55555.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:08:28,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (178) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/178/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/178/galaxy_178.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/178/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/178/outputs/dataset_54690e76-6b6e-4bfc-9e65-4a137b169948.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/178/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/178/outputs/dataset_54690e76-6b6e-4bfc-9e65-4a137b169948.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:28,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:08:28,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:08:28,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:08:28,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:28,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:29,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2pg7l with k8s id: gxy-2pg7l scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:30,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-2pg7l set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:36,287 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2pg7l with k8s id: gxy-2pg7l succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:08:36,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 178: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:08:44,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 178 finished
galaxy.jobs DEBUG 2025-03-19 07:08:44,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/178/outputs/dataset_54690e76-6b6e-4bfc-9e65-4a137b169948.dat to /galaxy/server/database/objects/5/4/6/dataset_54690e76-6b6e-4bfc-9e65-4a137b169948.dat
galaxy.jobs DEBUG 2025-03-19 07:08:44,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/178/outputs/dataset_c6c7a554-ae53-42c7-a83a-d04e9bd55555.dat to /galaxy/server/database/objects/c/6/c/dataset_c6c7a554-ae53-42c7-a83a-d04e9bd55555.dat
galaxy.model.metadata DEBUG 2025-03-19 07:08:44,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 262
galaxy.model.metadata DEBUG 2025-03-19 07:08:44,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 263
galaxy.util WARNING 2025-03-19 07:08:44,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/5/4/6/dataset_54690e76-6b6e-4bfc-9e65-4a137b169948.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/4/6/dataset_54690e76-6b6e-4bfc-9e65-4a137b169948.dat'
galaxy.jobs INFO 2025-03-19 07:08:44,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 178 in /galaxy/server/database/jobs_directory/000/178
galaxy.objectstore CRITICAL 2025-03-19 07:08:44,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/178/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/178/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:08:44,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 178 executed (95.377 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:44,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 178 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:44,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-2pg7l (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:08:45,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 179
tpv.core.entities DEBUG 2025-03-19 07:08:45,612 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:08:45,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:08:45,616 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:08:45,627 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:08:45,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Working directory for job is: /galaxy/server/database/jobs_directory/000/179
galaxy.jobs.runners DEBUG 2025-03-19 07:08:45,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [179] queued (29.806 ms)
galaxy.jobs.handler INFO 2025-03-19 07:08:45,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:45,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 179
galaxy.jobs DEBUG 2025-03-19 07:08:45,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [179] prepared (54.978 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:08:45,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/179/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/179/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/179/configs/tmp4tlrqtqu']
galaxy.jobs.runners DEBUG 2025-03-19 07:08:45,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (179) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/179/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/179/galaxy_179.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:45,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:45,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:46,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7d9x7 with k8s id: gxy-7d9x7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:47,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-7d9x7 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:08:55,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-7d9x7 with k8s id: gxy-7d9x7 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:08:55,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 179: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:09:03,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 179 finished
galaxy.jobs DEBUG 2025-03-19 07:09:03,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/179/outputs/dataset_57aedfbb-566a-4eda-94d6-6d9232c268a1.dat to /galaxy/server/database/objects/5/7/a/dataset_57aedfbb-566a-4eda-94d6-6d9232c268a1.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:09:03,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'tl.umap.neighbors_umap_euclidean.recipe_weinreb17.paul15_subsample.h5ad', 'dbkey': '?', 'ext': 'h5ad', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded h5ad file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/179/working/gxupload_0', 'object_id': 264}]}]}]
galaxy.jobs INFO 2025-03-19 07:09:03,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 179 in /galaxy/server/database/jobs_directory/000/179
galaxy.jobs DEBUG 2025-03-19 07:09:03,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 179 executed (127.033 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:03,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 179 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:03,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-7d9x7 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:09:04,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 180
tpv.core.entities DEBUG 2025-03-19 07:09:04,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/.*, abstract=False, cores=4, mem=19.0, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:09:04,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:09:04,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:09:04,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:09:04,968 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Working directory for job is: /galaxy/server/database/jobs_directory/000/180
galaxy.jobs.runners DEBUG 2025-03-19 07:09:04,975 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [180] queued (29.747 ms)
galaxy.jobs.handler INFO 2025-03-19 07:09:04,978 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:04,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 180
galaxy.jobs DEBUG 2025-03-19 07:09:05,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [180] prepared (58.254 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:05,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:09:05,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:05,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:09:05,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/180/tool_script.sh] for tool command [python -c "import scanpy as sc;print('scanpy version: %s' % sc.__version__)" > /galaxy/server/database/jobs_directory/000/180/outputs/COMMAND_VERSION 2>&1;
cp '/galaxy/server/database/objects/5/7/a/dataset_57aedfbb-566a-4eda-94d6-6d9232c268a1.dat' 'anndata.h5ad' && cat '/galaxy/server/database/jobs_directory/000/180/configs/tmposrixhop' > '/galaxy/server/database/jobs_directory/000/180/outputs/dataset_14adf18f-8aa3-435c-9065-de8d9a2a1225.dat' && python '/galaxy/server/database/jobs_directory/000/180/configs/tmposrixhop' >> '/galaxy/server/database/jobs_directory/000/180/outputs/dataset_14adf18f-8aa3-435c-9065-de8d9a2a1225.dat' && ls . >> '/galaxy/server/database/jobs_directory/000/180/outputs/dataset_14adf18f-8aa3-435c-9065-de8d9a2a1225.dat' && touch 'anndata_info.txt' && cat 'anndata_info.txt'  | sed -r '1 s|AnnData object with (.+) = (.*)\s*|\1: \2|g' | sed "s|'||g"  | sed -r 's|^\s*(.*):\s(.*)|[\1]\n-    \2|g' | sed 's|, |\n-    |g']
galaxy.jobs.runners DEBUG 2025-03-19 07:09:05,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (180) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/180/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/180/galaxy_180.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/180/working/anndata.h5ad" -a -f "/galaxy/server/database/jobs_directory/000/180/outputs/dataset_160224ee-a19b-49f2-b786-c6392ffde2fb.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/180/working/anndata.h5ad" "/galaxy/server/database/jobs_directory/000/180/outputs/dataset_160224ee-a19b-49f2-b786-c6392ffde2fb.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:05,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:05,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:09:05,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.9.6+galaxy4: mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:05,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-784291a780593dcd9226bd4bde0c101805d870b1:a06f8b4d675e08b4697ef348fe4ca2eeb7fcaca5-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:05,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:05,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dzknt with k8s id: gxy-dzknt scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:06,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dzknt with k8s id: gxy-dzknt scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:07,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-dzknt set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:14,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dzknt with k8s id: gxy-dzknt succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:09:14,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 180: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:09:21,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 180 finished
galaxy.jobs DEBUG 2025-03-19 07:09:21,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/180/outputs/dataset_160224ee-a19b-49f2-b786-c6392ffde2fb.dat to /galaxy/server/database/objects/1/6/0/dataset_160224ee-a19b-49f2-b786-c6392ffde2fb.dat
galaxy.jobs DEBUG 2025-03-19 07:09:21,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/180/outputs/dataset_14adf18f-8aa3-435c-9065-de8d9a2a1225.dat to /galaxy/server/database/objects/1/4/a/dataset_14adf18f-8aa3-435c-9065-de8d9a2a1225.dat
galaxy.model.metadata DEBUG 2025-03-19 07:09:21,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 265
galaxy.model.metadata DEBUG 2025-03-19 07:09:21,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 266
galaxy.util WARNING 2025-03-19 07:09:21,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=10001, gr_mem=[])) for /galaxy/server/database/objects/1/6/0/dataset_160224ee-a19b-49f2-b786-c6392ffde2fb.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/6/0/dataset_160224ee-a19b-49f2-b786-c6392ffde2fb.dat'
galaxy.jobs INFO 2025-03-19 07:09:21,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 180 in /galaxy/server/database/jobs_directory/000/180
galaxy.objectstore CRITICAL 2025-03-19 07:09:21,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/180/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/180/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:09:21,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 180 executed (103.344 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:21,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 180 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:22,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-dzknt (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:09:27,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 181
tpv.core.entities DEBUG 2025-03-19 07:09:27,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:09:27,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:09:27,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:09:27,316 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:09:27,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Working directory for job is: /galaxy/server/database/jobs_directory/000/181
galaxy.jobs.runners DEBUG 2025-03-19 07:09:27,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [181] queued (29.762 ms)
galaxy.jobs.handler INFO 2025-03-19 07:09:27,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:27,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 181
galaxy.jobs DEBUG 2025-03-19 07:09:27,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [181] prepared (58.923 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:09:27,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/181/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/181/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/181/configs/tmplyv7w8c2']
galaxy.jobs.runners DEBUG 2025-03-19 07:09:27,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (181) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/181/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/181/galaxy_181.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:27,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:27,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:28,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zrcxd with k8s id: gxy-zrcxd scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:29,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zrcxd with k8s id: gxy-zrcxd scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:30,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-zrcxd set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:37,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zrcxd with k8s id: gxy-zrcxd succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:09:37,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 181: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:09:45,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 181 finished
galaxy.jobs DEBUG 2025-03-19 07:09:45,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/181/outputs/dataset_50211d79-4314-4e4c-b9ea-e71f3bcf8149.dat to /galaxy/server/database/objects/5/0/2/dataset_50211d79-4314-4e4c-b9ea-e71f3bcf8149.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:09:45,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'testdb.sqlite', 'dbkey': '?', 'ext': 'sqlite', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded sqlite file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/181/working/gxupload_0', 'object_id': 267}]}]}]
galaxy.jobs INFO 2025-03-19 07:09:45,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 181 in /galaxy/server/database/jobs_directory/000/181
galaxy.jobs DEBUG 2025-03-19 07:09:45,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 181 executed (143.711 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:45,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 181 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:45,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-zrcxd (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:09:45,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 182
tpv.core.entities DEBUG 2025-03-19 07:09:45,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:09:45,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:09:45,627 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:09:45,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:09:45,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Working directory for job is: /galaxy/server/database/jobs_directory/000/182
galaxy.jobs.runners DEBUG 2025-03-19 07:09:45,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [182] queued (25.630 ms)
galaxy.jobs.handler INFO 2025-03-19 07:09:45,655 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:45,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 182
galaxy.jobs DEBUG 2025-03-19 07:09:45,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [182] prepared (56.513 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:45,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:09:45,721 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/sqlite_to_tabular/sqlite_to_tabular/3.2.1: python:3.7
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:45,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/python:3.7--1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:09:45,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/182/tool_script.sh] for tool command [python '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/sqlite_to_tabular/c4d18aa4ec4a/sqlite_to_tabular/sqlite_to_tabular.py' --sqlitedb='/galaxy/server/database/objects/5/0/2/dataset_50211d79-4314-4e4c-b9ea-e71f3bcf8149.dat' --query_file='/galaxy/server/database/jobs_directory/000/182/configs/tmp3b1huc08'   --comment_char='#'   --output='/galaxy/server/database/jobs_directory/000/182/outputs/dataset_3cd05513-a539-4980-970e-bf23c64ea7c9.dat']
galaxy.jobs.runners DEBUG 2025-03-19 07:09:45,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (182) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/182/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/182/galaxy_182.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:45,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:45,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:09:45,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/sqlite_to_tabular/sqlite_to_tabular/3.2.1: python:3.7
galaxy.tool_util.deps.containers INFO 2025-03-19 07:09:45,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/python:3.7--1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:45,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:46,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xfhvx with k8s id: gxy-xfhvx scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:47,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xfhvx with k8s id: gxy-xfhvx scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:48,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xfhvx with k8s id: gxy-xfhvx scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:49,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xfhvx with k8s id: gxy-xfhvx scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:50,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xfhvx with k8s id: gxy-xfhvx scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:51,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-xfhvx set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:09:56,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-xfhvx with k8s id: gxy-xfhvx succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:09:57,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 182: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:10:04,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 182 finished
galaxy.jobs DEBUG 2025-03-19 07:10:04,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/182/outputs/dataset_3cd05513-a539-4980-970e-bf23c64ea7c9.dat to /galaxy/server/database/objects/3/c/d/dataset_3cd05513-a539-4980-970e-bf23c64ea7c9.dat
galaxy.model.metadata DEBUG 2025-03-19 07:10:04,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 268
galaxy.jobs INFO 2025-03-19 07:10:04,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 182 in /galaxy/server/database/jobs_directory/000/182
galaxy.objectstore CRITICAL 2025-03-19 07:10:04,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/182/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/182/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:10:04,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 182 executed (104.119 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:04,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 182 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:04,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-xfhvx (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:10:07,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 183, 184
tpv.core.entities DEBUG 2025-03-19 07:10:08,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:10:08,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:10:08,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:10:08,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:10:08,033 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Working directory for job is: /galaxy/server/database/jobs_directory/000/183
galaxy.jobs.runners DEBUG 2025-03-19 07:10:08,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [183] queued (30.730 ms)
galaxy.jobs.handler INFO 2025-03-19 07:10:08,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 183
tpv.core.entities DEBUG 2025-03-19 07:10:08,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:10:08,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:10:08,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:10:08,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:10:08,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Working directory for job is: /galaxy/server/database/jobs_directory/000/184
galaxy.jobs.runners DEBUG 2025-03-19 07:10:08,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [184] queued (44.867 ms)
galaxy.jobs.handler INFO 2025-03-19 07:10:08,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,115 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 184
galaxy.jobs DEBUG 2025-03-19 07:10:08,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [183] prepared (80.862 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:10:08,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/183/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/183/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/183/configs/tmpspjw0yrj']
galaxy.jobs.runners DEBUG 2025-03-19 07:10:08,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (183) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/183/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/183/galaxy_183.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-19 07:10:08,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [184] prepared (75.533 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-19 07:10:08,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/184/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/184/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/184/configs/tmpa8j7fhcv']
galaxy.jobs.runners DEBUG 2025-03-19 07:10:08,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (184) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/184/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/184/galaxy_184.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 184 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 184 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2wx95 with k8s id: gxy-2wx95 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:08,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c4jk2 with k8s id: gxy-c4jk2 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:10,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-2wx95 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:10,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c4jk2 with k8s id: gxy-c4jk2 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:11,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-c4jk2 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:18,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2wx95 with k8s id: gxy-2wx95 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:18,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-c4jk2 with k8s id: gxy-c4jk2 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:10:18,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 183: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:10:18,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 184: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:10:26,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 183 finished
galaxy.jobs DEBUG 2025-03-19 07:10:26,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/183/outputs/dataset_5f53b7ff-221c-48bc-9202-cb26c527c50a.dat to /galaxy/server/database/objects/5/f/5/dataset_5f53b7ff-221c-48bc-9202-cb26c527c50a.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:10:26,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/183/working/data_fetch_upload_nirxsgd3', 'object_id': 269}]}]}]
galaxy.jobs.runners DEBUG 2025-03-19 07:10:26,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 184 finished
galaxy.jobs DEBUG 2025-03-19 07:10:26,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/184/outputs/dataset_368f9cec-6523-42a4-9989-aac094f32f85.dat to /galaxy/server/database/objects/3/6/8/dataset_368f9cec-6523-42a4-9989-aac094f32f85.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:10:26,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/184/working/data_fetch_upload_9eqjpxfa', 'object_id': 270}]}]}]
galaxy.jobs INFO 2025-03-19 07:10:26,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 183 in /galaxy/server/database/jobs_directory/000/183
galaxy.jobs INFO 2025-03-19 07:10:26,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 184 in /galaxy/server/database/jobs_directory/000/184
galaxy.jobs DEBUG 2025-03-19 07:10:26,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 183 executed (169.420 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:26,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 183 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:26,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-2wx95 (attempt 1).
galaxy.jobs DEBUG 2025-03-19 07:10:26,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 184 executed (157.880 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:26,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 184 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:26,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-c4jk2 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:10:27,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 185
tpv.core.entities DEBUG 2025-03-19 07:10:27,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:10:27,425 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:10:27,427 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:10:27,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:10:27,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Working directory for job is: /galaxy/server/database/jobs_directory/000/185
galaxy.jobs.runners DEBUG 2025-03-19 07:10:27,458 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [185] queued (30.865 ms)
galaxy.jobs.handler INFO 2025-03-19 07:10:27,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:27,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 185
galaxy.jobs DEBUG 2025-03-19 07:10:27,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [185] prepared (74.078 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:10:27,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:10:27,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/jasminesv/jasminesv/1.0.11+galaxy1: jasminesv:1.0.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:10:28,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/jasminesv:1.0.11--hdfd78af_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:10:28,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/185/tool_script.sh] for tool command [jasmine > /galaxy/server/database/jobs_directory/000/185/outputs/COMMAND_VERSION 2>&1;
jasmine 'max_dist=1000' 'min_dist=-1' 'kd_tree_norm=2' 'min_seq_id=0.0' 'k_jaccard=9' 'max_dup_length=10000' 'min_support=1' threads=${GALAXY_SLOTS:-4} 'spec_reads=10' 'spec_len=30'  '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''  ''  file_list='/galaxy/server/database/jobs_directory/000/185/configs/tmpbzomctqw' out_file='/galaxy/server/database/jobs_directory/000/185/outputs/dataset_7fb01046-0360-4348-b704-2e626548527f.dat']
galaxy.jobs.runners DEBUG 2025-03-19 07:10:28,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (185) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/185/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/185/galaxy_185.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:28,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 185 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:10:28,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:10:28,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/jasminesv/jasminesv/1.0.11+galaxy1: jasminesv:1.0.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:10:28,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/jasminesv:1.0.11--hdfd78af_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:28,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 185 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:29,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:30,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:31,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:32,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:33,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:34,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:35,699 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:36,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:37,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:38,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:40,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:41,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:42,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-n8tzw set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:48,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-n8tzw with k8s id: gxy-n8tzw succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:10:48,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 185: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:10:55,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 185 finished
galaxy.jobs DEBUG 2025-03-19 07:10:55,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/185/outputs/dataset_7fb01046-0360-4348-b704-2e626548527f.dat to /galaxy/server/database/objects/7/f/b/dataset_7fb01046-0360-4348-b704-2e626548527f.dat
galaxy.model.metadata DEBUG 2025-03-19 07:10:55,994 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 271
galaxy.jobs INFO 2025-03-19 07:10:56,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 185 in /galaxy/server/database/jobs_directory/000/185
galaxy.objectstore CRITICAL 2025-03-19 07:10:56,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/185/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/185/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:10:56,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 185 executed (114.472 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:56,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 185 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:56,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-n8tzw (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:10:57,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 186, 187, 188
tpv.core.entities DEBUG 2025-03-19 07:10:57,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:10:57,940 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:10:57,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:10:57,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:10:57,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Working directory for job is: /galaxy/server/database/jobs_directory/000/186
galaxy.jobs.runners DEBUG 2025-03-19 07:10:57,975 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [186] queued (31.726 ms)
galaxy.jobs.handler INFO 2025-03-19 07:10:57,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:57,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 186
tpv.core.entities DEBUG 2025-03-19 07:10:57,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:10:57,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:10:57,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:10:58,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:10:58,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Working directory for job is: /galaxy/server/database/jobs_directory/000/187
galaxy.jobs.runners DEBUG 2025-03-19 07:10:58,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [187] queued (45.384 ms)
galaxy.jobs.handler INFO 2025-03-19 07:10:58,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 187
tpv.core.entities DEBUG 2025-03-19 07:10:58,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:10:58,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:10:58,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:10:58,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:10:58,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [186] prepared (110.037 ms)
galaxy.jobs DEBUG 2025-03-19 07:10:58,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Working directory for job is: /galaxy/server/database/jobs_directory/000/188
galaxy.jobs.runners DEBUG 2025-03-19 07:10:58,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [188] queued (58.609 ms)
galaxy.jobs.handler INFO 2025-03-19 07:10:58,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 188
galaxy.jobs.command_factory INFO 2025-03-19 07:10:58,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/186/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/186/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/186/configs/tmp4nym7i9e']
galaxy.jobs.runners DEBUG 2025-03-19 07:10:58,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (186) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/186/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/186/galaxy_186.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-19 07:10:58,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [187] prepared (116.668 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 186 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 186 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-19 07:10:58,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/187/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/187/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/187/configs/tmp3ishghlj']
galaxy.jobs.runners DEBUG 2025-03-19 07:10:58,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (187) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/187/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/187/galaxy_187.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-19 07:10:58,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [188] prepared (112.298 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 187 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 187 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-19 07:10:58,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/188/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/188/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/188/configs/tmpnhy5qdx2']
galaxy.jobs.runners DEBUG 2025-03-19 07:10:58,332 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (188) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/188/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/188/galaxy_188.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 188 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:58,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 188 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:59,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6xzzn with k8s id: gxy-6xzzn scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:59,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dz7lg with k8s id: gxy-dz7lg scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:10:59,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v2rkt with k8s id: gxy-v2rkt scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:00,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-6xzzn set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:00,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-dz7lg set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:00,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-v2rkt set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:07,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6xzzn with k8s id: gxy-6xzzn succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:07,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dz7lg with k8s id: gxy-dz7lg succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:07,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v2rkt with k8s id: gxy-v2rkt succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:11:08,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 186: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:11:08,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 187: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:11:08,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 188: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:11:20,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 186 finished
galaxy.jobs DEBUG 2025-03-19 07:11:20,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/186/outputs/dataset_3b49fe7c-afcc-4918-aac0-26d39e4968bb.dat to /galaxy/server/database/objects/3/b/4/dataset_3b49fe7c-afcc-4918-aac0-26d39e4968bb.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:11:20,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'c.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/186/working/data_fetch_upload_u9fso8xh', 'object_id': 272}]}]}]
galaxy.jobs.runners DEBUG 2025-03-19 07:11:20,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 187 finished
galaxy.jobs DEBUG 2025-03-19 07:11:20,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/187/outputs/dataset_91d5f5fc-5acf-46e0-b0ec-05c6758ccb30.dat to /galaxy/server/database/objects/9/1/d/dataset_91d5f5fc-5acf-46e0-b0ec-05c6758ccb30.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:11:20,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'd.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/187/working/data_fetch_upload_g02wucfe', 'object_id': 273}]}]}]
galaxy.jobs INFO 2025-03-19 07:11:20,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 186 in /galaxy/server/database/jobs_directory/000/186
galaxy.jobs.runners DEBUG 2025-03-19 07:11:20,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 188 finished
galaxy.jobs DEBUG 2025-03-19 07:11:20,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/188/outputs/dataset_11d4372a-9bc4-44d7-b49f-ae4a1f2fdf3f.dat to /galaxy/server/database/objects/1/1/d/dataset_11d4372a-9bc4-44d7-b49f-ae4a1f2fdf3f.dat
galaxy.jobs INFO 2025-03-19 07:11:20,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 187 in /galaxy/server/database/jobs_directory/000/187
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:11:20,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'chr_norm_file.txt', 'dbkey': '?', 'ext': 'gg', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded gg file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/188/working/data_fetch_upload_608uubfu', 'object_id': 274}]}]}]
galaxy.jobs DEBUG 2025-03-19 07:11:20,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 186 executed (229.320 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:20,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 186 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:20,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-6xzzn (attempt 1).
galaxy.jobs INFO 2025-03-19 07:11:20,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 188 in /galaxy/server/database/jobs_directory/000/188
galaxy.jobs DEBUG 2025-03-19 07:11:20,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 187 executed (197.414 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:20,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 187 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:20,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-dz7lg (attempt 1).
galaxy.jobs DEBUG 2025-03-19 07:11:20,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 188 executed (173.653 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:20,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 188 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:20,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-v2rkt (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:11:21,647 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 189
tpv.core.entities DEBUG 2025-03-19 07:11:21,676 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:11:21,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:11:21,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:11:21,691 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:11:21,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Working directory for job is: /galaxy/server/database/jobs_directory/000/189
galaxy.jobs.runners DEBUG 2025-03-19 07:11:21,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [189] queued (33.731 ms)
galaxy.jobs.handler INFO 2025-03-19 07:11:21,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:21,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 189
galaxy.jobs DEBUG 2025-03-19 07:11:21,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [189] prepared (57.490 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:11:21,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:11:21,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/jasminesv/jasminesv/1.0.11+galaxy1: jasminesv:1.0.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:11:21,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/jasminesv:1.0.11--hdfd78af_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:11:21,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/189/tool_script.sh] for tool command [jasmine > /galaxy/server/database/jobs_directory/000/189/outputs/COMMAND_VERSION 2>&1;
jasmine 'max_dist=1000' 'min_dist=-1' 'kd_tree_norm=2' 'min_seq_id=0.0' 'k_jaccard=9' 'max_dup_length=10000' 'min_support=1' threads=${GALAXY_SLOTS:-4} 'spec_reads=10' 'spec_len=30'  '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''  '--normalize_chrs' 'chr_norm_file=/galaxy/server/database/objects/1/1/d/dataset_11d4372a-9bc4-44d7-b49f-ae4a1f2fdf3f.dat'  file_list='/galaxy/server/database/jobs_directory/000/189/configs/tmpkkd7t30f' out_file='/galaxy/server/database/jobs_directory/000/189/outputs/dataset_f81dc978-2788-41bf-be24-3d23dcc2a9e9.dat']
galaxy.jobs.runners DEBUG 2025-03-19 07:11:21,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (189) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/189/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/189/galaxy_189.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:21,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 189 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:11:21,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:11:21,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/jasminesv/jasminesv/1.0.11+galaxy1: jasminesv:1.0.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:11:21,876 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/jasminesv:1.0.11--hdfd78af_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:21,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 189 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:22,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-snfj7 with k8s id: gxy-snfj7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:23,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-snfj7 with k8s id: gxy-snfj7 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:24,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-snfj7 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:27,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-snfj7 with k8s id: gxy-snfj7 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:11:27,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 189: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:11:35,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 189 finished
galaxy.jobs DEBUG 2025-03-19 07:11:35,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/189/outputs/dataset_f81dc978-2788-41bf-be24-3d23dcc2a9e9.dat to /galaxy/server/database/objects/f/8/1/dataset_f81dc978-2788-41bf-be24-3d23dcc2a9e9.dat
galaxy.model.metadata DEBUG 2025-03-19 07:11:35,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 275
galaxy.jobs INFO 2025-03-19 07:11:35,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 189 in /galaxy/server/database/jobs_directory/000/189
galaxy.objectstore CRITICAL 2025-03-19 07:11:35,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/189/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/189/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:11:35,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 189 executed (109.172 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:35,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 189 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:35,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-snfj7 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:11:36,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 191, 190, 192
tpv.core.entities DEBUG 2025-03-19 07:11:36,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:11:36,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:11:36,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:11:36,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:11:36,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Working directory for job is: /galaxy/server/database/jobs_directory/000/190
galaxy.jobs.runners DEBUG 2025-03-19 07:11:36,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [190] queued (25.320 ms)
galaxy.jobs.handler INFO 2025-03-19 07:11:36,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:36,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 190
tpv.core.entities DEBUG 2025-03-19 07:11:36,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:11:36,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:11:37,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:11:37,016 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:11:37,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Working directory for job is: /galaxy/server/database/jobs_directory/000/191
galaxy.jobs.runners DEBUG 2025-03-19 07:11:37,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [191] queued (41.228 ms)
galaxy.jobs.handler INFO 2025-03-19 07:11:37,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 191
tpv.core.entities DEBUG 2025-03-19 07:11:37,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:11:37,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:11:37,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:11:37,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:11:37,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [190] prepared (93.388 ms)
galaxy.jobs DEBUG 2025-03-19 07:11:37,106 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Working directory for job is: /galaxy/server/database/jobs_directory/000/192
galaxy.jobs.runners DEBUG 2025-03-19 07:11:37,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [192] queued (50.711 ms)
galaxy.jobs.handler INFO 2025-03-19 07:11:37,115 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 192
galaxy.jobs.command_factory INFO 2025-03-19 07:11:37,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/190/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/190/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/190/configs/tmpjvcmm_oa']
galaxy.jobs.runners DEBUG 2025-03-19 07:11:37,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (190) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/190/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/190/galaxy_190.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-19 07:11:37,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [191] prepared (96.208 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 190 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 190 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-19 07:11:37,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/191/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/191/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/191/configs/tmprp3ivjhf']
galaxy.jobs.runners DEBUG 2025-03-19 07:11:37,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (191) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/191/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/191/galaxy_191.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-19 07:11:37,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [192] prepared (85.373 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 191 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 191 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-19 07:11:37,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/192/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/192/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/192/configs/tmp0l4unnp1']
galaxy.jobs.runners DEBUG 2025-03-19 07:11:37,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (192) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/192/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/192/galaxy_192.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 192 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 192 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ml4rl with k8s id: gxy-ml4rl scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wgcj9 with k8s id: gxy-wgcj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:37,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-98b7g with k8s id: gxy-98b7g scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:38,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ml4rl with k8s id: gxy-ml4rl scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:38,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wgcj9 with k8s id: gxy-wgcj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:38,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-98b7g set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:40,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-ml4rl set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:40,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-wgcj9 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:47,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-ml4rl with k8s id: gxy-ml4rl succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:47,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-wgcj9 with k8s id: gxy-wgcj9 succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:11:47,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-98b7g with k8s id: gxy-98b7g succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:11:47,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 190: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:11:47,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 191: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:11:47,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 192: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:12:00,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 190 finished
galaxy.jobs DEBUG 2025-03-19 07:12:00,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/190/outputs/dataset_d1b5531c-c4e8-4ca1-a16f-037195d623b6.dat to /galaxy/server/database/objects/d/1/b/dataset_d1b5531c-c4e8-4ca1-a16f-037195d623b6.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:12:00,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'a.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/190/working/data_fetch_upload_xyxe90qq', 'object_id': 276}]}]}]
galaxy.jobs INFO 2025-03-19 07:12:00,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 190 in /galaxy/server/database/jobs_directory/000/190
galaxy.jobs.runners DEBUG 2025-03-19 07:12:00,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 192 finished
galaxy.jobs.runners DEBUG 2025-03-19 07:12:00,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 191 finished
galaxy.jobs DEBUG 2025-03-19 07:12:00,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/192/outputs/dataset_4ea65b7a-24a3-46f7-8304-429ada0c6cf6.dat to /galaxy/server/database/objects/4/e/a/dataset_4ea65b7a-24a3-46f7-8304-429ada0c6cf6.dat
galaxy.jobs DEBUG 2025-03-19 07:12:00,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/191/outputs/dataset_c2fcb12f-892f-480a-9908-4e10bd98cf0c.dat to /galaxy/server/database/objects/c/2/f/dataset_c2fcb12f-892f-480a-9908-4e10bd98cf0c.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:12:00,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'genome.fa.gz', 'dbkey': '?', 'ext': 'fasta.gz', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded fasta.gz file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/192/working/gxupload_0', 'object_id': 278}]}]}]
galaxy.jobs DEBUG 2025-03-19 07:12:00,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 190 executed (180.988 ms)
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:12:00,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'b.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/191/working/data_fetch_upload_k2f0j8n2', 'object_id': 277}]}]}]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:00,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 190 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:00,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-ml4rl (attempt 1).
galaxy.jobs INFO 2025-03-19 07:12:00,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 192 in /galaxy/server/database/jobs_directory/000/192
galaxy.jobs INFO 2025-03-19 07:12:00,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 191 in /galaxy/server/database/jobs_directory/000/191
galaxy.jobs DEBUG 2025-03-19 07:12:00,547 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 192 executed (173.341 ms)
galaxy.jobs DEBUG 2025-03-19 07:12:00,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 191 executed (177.576 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:00,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 192 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:00,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-98b7g (attempt 1).
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:00,581 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 191 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:00,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-wgcj9 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:12:01,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 193
tpv.core.entities DEBUG 2025-03-19 07:12:01,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:12:01,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:12:01,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:12:01,683 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:12:01,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Working directory for job is: /galaxy/server/database/jobs_directory/000/193
galaxy.jobs.runners DEBUG 2025-03-19 07:12:01,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [193] queued (28.488 ms)
galaxy.jobs.handler INFO 2025-03-19 07:12:01,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:01,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 193
galaxy.jobs DEBUG 2025-03-19 07:12:01,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [193] prepared (70.291 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:01,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:12:01,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/jasminesv/jasminesv/1.0.11+galaxy1: jasminesv:1.0.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:01,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/jasminesv:1.0.11--hdfd78af_1,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:12:01,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/193/tool_script.sh] for tool command [jasmine > /galaxy/server/database/jobs_directory/000/193/outputs/COMMAND_VERSION 2>&1;
gunzip -c '/galaxy/server/database/objects/4/e/a/dataset_4ea65b7a-24a3-46f7-8304-429ada0c6cf6.dat' > reference &&   jasmine 'max_dist=1000' 'min_dist=-1' 'kd_tree_norm=2' 'min_seq_id=0.0' 'k_jaccard=9' 'max_dup_length=10000' 'min_support=1' threads=${GALAXY_SLOTS:-4} 'spec_reads=10' 'spec_len=30' 'genome_file=reference'  '' '' '{SafeStringWrapper__str__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis object at 7ed7b6456450 on: __sq__dup_to_ins__sq__: <galaxy.tools.wrappers.SelectToolParameterWrapper object at 0x7ed7b64328d0>, SafeStringWrapper__str__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis object at 7ed7b6457590 on: __sq____current_case____sq__: 0, SafeStringWrapper__str__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis object at 7ed7b6455490 on: __sq__reference_source__sq__: {SafeStringWrapper__str__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis object at 7ed7b6456e10 on: __sq__reference_source_selector__sq__: <galaxy.tools.wrappers.SelectToolParameterWrapper object at 0x7ed7b6433da0>, SafeStringWrapper__str__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis object at 7ed7b6457890 on: __sq____current_case____sq__: 1, SafeStringWrapper__str__NoneType__NotImplementedType__Number__SafeStringWrapper__ToolParameterValueWrapper__bool__bytearray__ellipsis object at 7ed7b6456630 on: __sq__ref_file__sq__: <galaxy.tools.wrappers.DatasetFilenameWrapper object at 0x7ed7b6565c70>}}' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''  ''  file_list='/galaxy/server/database/jobs_directory/000/193/configs/tmpn638vo7w' out_file='/galaxy/server/database/jobs_directory/000/193/outputs/dataset_3cf0105a-52db-42e3-a60d-61527646e464.dat']
galaxy.jobs.runners DEBUG 2025-03-19 07:12:01,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (193) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/193/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/193/galaxy_193.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:01,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 193 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:01,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:12:01,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/jasminesv/jasminesv/1.0.11+galaxy1: jasminesv:1.0.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:01,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/jasminesv:1.0.11--hdfd78af_1,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:01,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 193 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:02,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jgh68 with k8s id: gxy-jgh68 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:03,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-jgh68 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:06,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jgh68 with k8s id: gxy-jgh68 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:12:06,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 193: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:12:14,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 193 finished
galaxy.jobs DEBUG 2025-03-19 07:12:14,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/193/outputs/dataset_3cf0105a-52db-42e3-a60d-61527646e464.dat to /galaxy/server/database/objects/3/c/f/dataset_3cf0105a-52db-42e3-a60d-61527646e464.dat
galaxy.model.metadata DEBUG 2025-03-19 07:12:14,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 279
galaxy.jobs INFO 2025-03-19 07:12:14,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 193 in /galaxy/server/database/jobs_directory/000/193
galaxy.objectstore CRITICAL 2025-03-19 07:12:14,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/193/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/193/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:12:14,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 193 executed (90.767 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:14,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 193 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:14,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-jgh68 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:12:16,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 194
tpv.core.entities DEBUG 2025-03-19 07:12:16,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:12:16,936 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:12:16,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:12:16,948 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:12:16,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Working directory for job is: /galaxy/server/database/jobs_directory/000/194
galaxy.jobs.runners DEBUG 2025-03-19 07:12:16,963 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [194] queued (24.471 ms)
galaxy.jobs.handler INFO 2025-03-19 07:12:16,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:16,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 194
galaxy.jobs DEBUG 2025-03-19 07:12:17,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [194] prepared (53.560 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:12:17,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/194/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/194/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/194/configs/tmpb1x58wau']
galaxy.jobs.runners DEBUG 2025-03-19 07:12:17,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (194) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/194/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/194/galaxy_194.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:17,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 194 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:17,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 194 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:17,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dzbsj with k8s id: gxy-dzbsj scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:18,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-dzbsj set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:26,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-dzbsj with k8s id: gxy-dzbsj succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:12:27,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 194: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:12:34,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 194 finished
galaxy.jobs DEBUG 2025-03-19 07:12:34,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/194/outputs/dataset_362cd0d6-a029-4eeb-96ce-ab496646766b.dat to /galaxy/server/database/objects/3/6/2/dataset_362cd0d6-a029-4eeb-96ce-ab496646766b.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:12:34,667 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/194/working/data_fetch_upload_u8_yz5kh', 'object_id': 280}]}]}]
galaxy.jobs INFO 2025-03-19 07:12:34,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 194 in /galaxy/server/database/jobs_directory/000/194
galaxy.jobs DEBUG 2025-03-19 07:12:34,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 194 executed (135.001 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:34,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 194 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:34,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-dzbsj (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:12:35,225 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 195
tpv.core.entities DEBUG 2025-03-19 07:12:35,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:12:35,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:12:35,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:12:35,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:12:35,277 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Working directory for job is: /galaxy/server/database/jobs_directory/000/195
galaxy.jobs.runners DEBUG 2025-03-19 07:12:35,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [195] queued (30.180 ms)
galaxy.jobs.handler INFO 2025-03-19 07:12:35,289 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:35,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 195
galaxy.jobs DEBUG 2025-03-19 07:12:35,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [195] prepared (72.720 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:35,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:12:35,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:36,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:12:36,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/195/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/195/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/6/2/dataset_362cd0d6-a029-4eeb-96ce-ab496646766b.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GT --gensample "/galaxy/server/database/jobs_directory/000/195/outputs/dataset_4b797ff7-21e7-440f-aba1-d39800c6c532.dat,/galaxy/server/database/jobs_directory/000/195/outputs/dataset_b6de4f56-9021-4ba7-9446-f5beab8376ec.dat"                      "/galaxy/server/database/objects/3/6/2/dataset_362cd0d6-a029-4eeb-96ce-ab496646766b.dat" .]
galaxy.jobs.runners DEBUG 2025-03-19 07:12:36,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (195) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/195/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/195/galaxy_195.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:36,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 195 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:36,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:12:36,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:12:36,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:36,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 195 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:37,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lc9xp with k8s id: gxy-lc9xp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:38,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lc9xp with k8s id: gxy-lc9xp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:39,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lc9xp with k8s id: gxy-lc9xp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:40,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lc9xp with k8s id: gxy-lc9xp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:41,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lc9xp with k8s id: gxy-lc9xp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:42,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-lc9xp set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:45,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-lc9xp with k8s id: gxy-lc9xp succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:12:45,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 195: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:12:53,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 195 finished
galaxy.jobs DEBUG 2025-03-19 07:12:53,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/195/outputs/dataset_4b797ff7-21e7-440f-aba1-d39800c6c532.dat to /galaxy/server/database/objects/4/b/7/dataset_4b797ff7-21e7-440f-aba1-d39800c6c532.dat
galaxy.jobs DEBUG 2025-03-19 07:12:53,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/195/outputs/dataset_b6de4f56-9021-4ba7-9446-f5beab8376ec.dat to /galaxy/server/database/objects/b/6/d/dataset_b6de4f56-9021-4ba7-9446-f5beab8376ec.dat
galaxy.model.metadata DEBUG 2025-03-19 07:12:53,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 281
galaxy.model.metadata DEBUG 2025-03-19 07:12:53,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 282
galaxy.jobs INFO 2025-03-19 07:12:53,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 195 in /galaxy/server/database/jobs_directory/000/195
galaxy.objectstore CRITICAL 2025-03-19 07:12:53,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/195/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/195/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:12:53,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 195 executed (107.044 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:53,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 195 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:53,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-lc9xp (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:12:54,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 196
tpv.core.entities DEBUG 2025-03-19 07:12:54,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:12:54,604 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:12:54,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:12:54,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:12:54,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Working directory for job is: /galaxy/server/database/jobs_directory/000/196
galaxy.jobs.runners DEBUG 2025-03-19 07:12:54,630 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [196] queued (24.346 ms)
galaxy.jobs.handler INFO 2025-03-19 07:12:54,633 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:54,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 196
galaxy.jobs DEBUG 2025-03-19 07:12:54,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [196] prepared (52.986 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:12:54,723 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/196/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/196/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/196/configs/tmpbm7456bf']
galaxy.jobs.runners DEBUG 2025-03-19 07:12:54,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (196) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/196/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/196/galaxy_196.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:54,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 196 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:54,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 196 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:55,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jhbrc with k8s id: gxy-jhbrc scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:12:56,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-jhbrc set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:04,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-jhbrc with k8s id: gxy-jhbrc succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:13:04,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 196: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:13:12,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 196 finished
galaxy.jobs DEBUG 2025-03-19 07:13:12,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/196/outputs/dataset_3fd5e8a0-c47a-4065-b170-4a26249bd0f0.dat to /galaxy/server/database/objects/3/f/d/dataset_3fd5e8a0-c47a-4065-b170-4a26249bd0f0.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:13:12,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/196/working/data_fetch_upload_aju5uym3', 'object_id': 283}]}]}]
galaxy.jobs INFO 2025-03-19 07:13:12,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 196 in /galaxy/server/database/jobs_directory/000/196
galaxy.jobs DEBUG 2025-03-19 07:13:12,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 196 executed (132.615 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:12,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 196 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:12,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-jhbrc (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:13:12,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 197
tpv.core.entities DEBUG 2025-03-19 07:13:12,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:13:12,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:13:12,915 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:13:12,924 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:13:12,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Working directory for job is: /galaxy/server/database/jobs_directory/000/197
galaxy.jobs.runners DEBUG 2025-03-19 07:13:12,949 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [197] queued (33.880 ms)
galaxy.jobs.handler INFO 2025-03-19 07:13:12,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:12,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 197
galaxy.jobs DEBUG 2025-03-19 07:13:13,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [197] prepared (67.734 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:13,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:13:13,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:13,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:13:13,527 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/197/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/197/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/f/d/dataset_3fd5e8a0-c47a-4065-b170-4a26249bd0f0.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GP --3N6 --gensample "/galaxy/server/database/jobs_directory/000/197/outputs/dataset_1b197d4e-4016-4b49-addf-613f6e525ec2.dat,/galaxy/server/database/jobs_directory/000/197/outputs/dataset_3770dd29-0a18-47ab-87bd-258fad422d7c.dat"                      "/galaxy/server/database/objects/3/f/d/dataset_3fd5e8a0-c47a-4065-b170-4a26249bd0f0.dat" .]
galaxy.jobs.runners DEBUG 2025-03-19 07:13:13,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (197) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/197/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/197/galaxy_197.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:13,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 197 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:13,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:13:13,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:13,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:13,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 197 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:14,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v4gs9 with k8s id: gxy-v4gs9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:15,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-v4gs9 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:18,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v4gs9 with k8s id: gxy-v4gs9 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:13:18,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 197: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:13:26,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 197 finished
galaxy.jobs DEBUG 2025-03-19 07:13:26,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/197/outputs/dataset_1b197d4e-4016-4b49-addf-613f6e525ec2.dat to /galaxy/server/database/objects/1/b/1/dataset_1b197d4e-4016-4b49-addf-613f6e525ec2.dat
galaxy.jobs DEBUG 2025-03-19 07:13:26,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/197/outputs/dataset_3770dd29-0a18-47ab-87bd-258fad422d7c.dat to /galaxy/server/database/objects/3/7/7/dataset_3770dd29-0a18-47ab-87bd-258fad422d7c.dat
galaxy.model.metadata DEBUG 2025-03-19 07:13:26,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 284
galaxy.model.metadata DEBUG 2025-03-19 07:13:26,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 285
galaxy.jobs INFO 2025-03-19 07:13:26,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 197 in /galaxy/server/database/jobs_directory/000/197
galaxy.objectstore CRITICAL 2025-03-19 07:13:26,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/197/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/197/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:13:26,433 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 197 executed (123.893 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:26,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 197 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:26,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-v4gs9 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:13:27,298 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 198
tpv.core.entities DEBUG 2025-03-19 07:13:27,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:13:27,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:13:27,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:13:27,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:13:27,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Working directory for job is: /galaxy/server/database/jobs_directory/000/198
galaxy.jobs.runners DEBUG 2025-03-19 07:13:27,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [198] queued (27.040 ms)
galaxy.jobs.handler INFO 2025-03-19 07:13:27,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:27,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 198
galaxy.jobs DEBUG 2025-03-19 07:13:27,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [198] prepared (58.128 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:13:27,442 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/198/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/198/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/198/configs/tmpfdgv3wl7']
galaxy.jobs.runners DEBUG 2025-03-19 07:13:27,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (198) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/198/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/198/galaxy_198.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:27,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 198 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:27,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 198 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:27,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5qc2b with k8s id: gxy-5qc2b scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:28,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5qc2b with k8s id: gxy-5qc2b scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:29,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-5qc2b set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:36,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5qc2b with k8s id: gxy-5qc2b succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:13:37,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 198: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:13:44,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 198 finished
galaxy.jobs DEBUG 2025-03-19 07:13:44,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/198/outputs/dataset_9ab36bc6-45e3-4187-8b64-8a05449ad1ed.dat to /galaxy/server/database/objects/9/a/b/dataset_9ab36bc6-45e3-4187-8b64-8a05449ad1ed.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:13:44,760 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'check.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/198/working/data_fetch_upload_menq3imr', 'object_id': 286}]}]}]
galaxy.jobs INFO 2025-03-19 07:13:44,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 198 in /galaxy/server/database/jobs_directory/000/198
galaxy.jobs DEBUG 2025-03-19 07:13:44,852 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 198 executed (110.206 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:44,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 198 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:44,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-5qc2b (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:13:45,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 199
tpv.core.entities DEBUG 2025-03-19 07:13:45,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:13:45,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:13:45,640 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:13:45,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:13:45,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Working directory for job is: /galaxy/server/database/jobs_directory/000/199
galaxy.jobs.runners DEBUG 2025-03-19 07:13:45,670 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [199] queued (29.942 ms)
galaxy.jobs.handler INFO 2025-03-19 07:13:45,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:45,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 199
galaxy.jobs DEBUG 2025-03-19 07:13:45,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [199] prepared (46.021 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:45,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:13:45,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:45,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:13:45,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/199/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/199/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/9/a/b/dataset_9ab36bc6-45e3-4187-8b64-8a05449ad1ed.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GT --3N6 --vcf-ids --gensample "/galaxy/server/database/jobs_directory/000/199/outputs/dataset_d9ab02f3-cf33-4ded-a32f-ab17c1b7b38b.dat,/galaxy/server/database/jobs_directory/000/199/outputs/dataset_44f816dc-29dd-409a-ad45-e42be20e7dd7.dat"                      "/galaxy/server/database/objects/9/a/b/dataset_9ab36bc6-45e3-4187-8b64-8a05449ad1ed.dat" .]
galaxy.jobs.runners DEBUG 2025-03-19 07:13:45,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (199) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/199/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/199/galaxy_199.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:45,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 199 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:45,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:13:45,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:13:45,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:45,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 199 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:46,038 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5f594 with k8s id: gxy-5f594 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:47,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-5f594 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:50,774 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5f594 with k8s id: gxy-5f594 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:13:50,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 199: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:13:58,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 199 finished
galaxy.jobs DEBUG 2025-03-19 07:13:58,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/199/outputs/dataset_d9ab02f3-cf33-4ded-a32f-ab17c1b7b38b.dat to /galaxy/server/database/objects/d/9/a/dataset_d9ab02f3-cf33-4ded-a32f-ab17c1b7b38b.dat
galaxy.jobs DEBUG 2025-03-19 07:13:58,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/199/outputs/dataset_44f816dc-29dd-409a-ad45-e42be20e7dd7.dat to /galaxy/server/database/objects/4/4/f/dataset_44f816dc-29dd-409a-ad45-e42be20e7dd7.dat
galaxy.model.metadata DEBUG 2025-03-19 07:13:58,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 287
galaxy.model.metadata DEBUG 2025-03-19 07:13:58,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 288
galaxy.jobs INFO 2025-03-19 07:13:58,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 199 in /galaxy/server/database/jobs_directory/000/199
galaxy.objectstore CRITICAL 2025-03-19 07:13:58,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/199/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/199/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:13:58,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 199 executed (119.290 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:58,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 199 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:58,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-5f594 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:13:59,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 200
tpv.core.entities DEBUG 2025-03-19 07:13:59,917 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:13:59,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:13:59,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:13:59,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:13:59,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Working directory for job is: /galaxy/server/database/jobs_directory/000/200
galaxy.jobs.runners DEBUG 2025-03-19 07:13:59,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [200] queued (31.825 ms)
galaxy.jobs.handler INFO 2025-03-19 07:13:59,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:13:59,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 200
galaxy.jobs DEBUG 2025-03-19 07:14:00,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [200] prepared (65.532 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:14:00,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/200/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/200/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/200/configs/tmpyvlfayai']
galaxy.jobs.runners DEBUG 2025-03-19 07:14:00,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (200) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/200/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/200/galaxy_200.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:00,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 200 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:00,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 200 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:00,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zxxd4 with k8s id: gxy-zxxd4 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:01,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-zxxd4 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:09,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-zxxd4 with k8s id: gxy-zxxd4 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:14:10,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 200: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:14:17,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 200 finished
galaxy.jobs DEBUG 2025-03-19 07:14:17,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/200/outputs/dataset_a445a3d4-adcb-4ee7-848b-3872e3c78290.dat to /galaxy/server/database/objects/a/4/4/dataset_a445a3d4-adcb-4ee7-848b-3872e3c78290.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:14:17,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/200/working/data_fetch_upload_r7m99y1_', 'object_id': 289}]}]}]
galaxy.jobs INFO 2025-03-19 07:14:17,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 200 in /galaxy/server/database/jobs_directory/000/200
galaxy.jobs DEBUG 2025-03-19 07:14:17,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 200 executed (132.878 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:17,788 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 200 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:17,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-zxxd4 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:14:18,210 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 201
tpv.core.entities DEBUG 2025-03-19 07:14:18,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:14:18,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:14:18,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:14:18,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:14:18,268 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Working directory for job is: /galaxy/server/database/jobs_directory/000/201
galaxy.jobs.runners DEBUG 2025-03-19 07:14:18,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [201] queued (33.539 ms)
galaxy.jobs.handler INFO 2025-03-19 07:14:18,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:18,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 201
galaxy.jobs DEBUG 2025-03-19 07:14:18,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [201] prepared (89.873 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:18,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:14:18,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:18,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:14:18,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/201/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/201/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/a/4/4/dataset_a445a3d4-adcb-4ee7-848b-3872e3c78290.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert   --hapsample "/galaxy/server/database/jobs_directory/000/201/outputs/dataset_21e138f3-4749-4e7a-b836-eb16edf48638.dat,/galaxy/server/database/jobs_directory/000/201/outputs/dataset_05b92295-2486-4ff9-81e1-d442325d34b6.dat"                      "/galaxy/server/database/objects/a/4/4/dataset_a445a3d4-adcb-4ee7-848b-3872e3c78290.dat" .]
galaxy.jobs.runners DEBUG 2025-03-19 07:14:18,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (201) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/201/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/201/galaxy_201.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:18,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 201 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:18,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:14:18,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:18,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:18,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 201 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:19,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gkslt with k8s id: gxy-gkslt scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:20,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gkslt with k8s id: gxy-gkslt scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:21,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-gkslt set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:24,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gkslt with k8s id: gxy-gkslt succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:14:24,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 201: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:14:32,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 201 finished
galaxy.jobs DEBUG 2025-03-19 07:14:32,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/201/outputs/dataset_21e138f3-4749-4e7a-b836-eb16edf48638.dat to /galaxy/server/database/objects/2/1/e/dataset_21e138f3-4749-4e7a-b836-eb16edf48638.dat
galaxy.jobs DEBUG 2025-03-19 07:14:32,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/201/outputs/dataset_05b92295-2486-4ff9-81e1-d442325d34b6.dat to /galaxy/server/database/objects/0/5/b/dataset_05b92295-2486-4ff9-81e1-d442325d34b6.dat
galaxy.model.metadata DEBUG 2025-03-19 07:14:32,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 290
galaxy.model.metadata DEBUG 2025-03-19 07:14:32,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 291
galaxy.jobs INFO 2025-03-19 07:14:32,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 201 in /galaxy/server/database/jobs_directory/000/201
galaxy.objectstore CRITICAL 2025-03-19 07:14:32,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/201/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/201/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:14:32,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 201 executed (106.592 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:32,386 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 201 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:32,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-gkslt (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:14:33,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 202
tpv.core.entities DEBUG 2025-03-19 07:14:33,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:14:33,535 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:14:33,538 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:14:33,548 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:14:33,561 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Working directory for job is: /galaxy/server/database/jobs_directory/000/202
galaxy.jobs.runners DEBUG 2025-03-19 07:14:33,568 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [202] queued (30.081 ms)
galaxy.jobs.handler INFO 2025-03-19 07:14:33,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:33,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 202
galaxy.jobs DEBUG 2025-03-19 07:14:33,638 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [202] prepared (57.840 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:14:33,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/202/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/202/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/202/configs/tmpv34qf2ws']
galaxy.jobs.runners DEBUG 2025-03-19 07:14:33,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (202) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/202/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/202/galaxy_202.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:33,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 202 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:33,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 202 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:34,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5ktrk with k8s id: gxy-5ktrk scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:35,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-5ktrk set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:43,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5ktrk with k8s id: gxy-5ktrk succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:14:43,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 202: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:14:51,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 202 finished
galaxy.jobs DEBUG 2025-03-19 07:14:51,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] finish(): Moved /galaxy/server/database/jobs_directory/000/202/outputs/dataset_d17c5c2d-2288-4065-9d27-1c5440e81daf.dat to /galaxy/server/database/objects/d/1/7/dataset_d17c5c2d-2288-4065-9d27-1c5440e81daf.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:14:51,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/202/working/data_fetch_upload_e1xchk7c', 'object_id': 292}]}]}]
galaxy.jobs INFO 2025-03-19 07:14:51,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 202 in /galaxy/server/database/jobs_directory/000/202
galaxy.jobs DEBUG 2025-03-19 07:14:51,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 202 executed (112.425 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:51,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 202 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:51,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-5ktrk (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:14:52,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 203
tpv.core.entities DEBUG 2025-03-19 07:14:52,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:14:52,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:14:52,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:14:52,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:14:52,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Working directory for job is: /galaxy/server/database/jobs_directory/000/203
galaxy.jobs.runners DEBUG 2025-03-19 07:14:52,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [203] queued (39.730 ms)
galaxy.jobs.handler INFO 2025-03-19 07:14:52,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:52,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 203
galaxy.jobs DEBUG 2025-03-19 07:14:52,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [203] prepared (57.694 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:52,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:14:52,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:53,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:14:53,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/203/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/203/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/d/1/7/dataset_d17c5c2d-2288-4065-9d27-1c5440e81daf.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert   --haplegendsample "/galaxy/server/database/jobs_directory/000/203/outputs/dataset_fdd9f273-da1e-4396-ad66-99a86648471e.dat,/galaxy/server/database/jobs_directory/000/203/outputs/dataset_ae6d8c74-49d4-49b9-b2fa-4c59088c4579.dat,/galaxy/server/database/jobs_directory/000/203/outputs/dataset_929707e2-c403-44d1-bd31-a307ccac66b9.dat"                      "/galaxy/server/database/objects/d/1/7/dataset_d17c5c2d-2288-4065-9d27-1c5440e81daf.dat" .]
galaxy.jobs.runners DEBUG 2025-03-19 07:14:53,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (203) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/203/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/203/galaxy_203.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:53,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 203 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:53,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:14:53,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:14:53,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:53,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 203 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:53,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t9w4v with k8s id: gxy-t9w4v scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:55,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t9w4v with k8s id: gxy-t9w4v scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:56,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t9w4v with k8s id: gxy-t9w4v scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:14:57,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-t9w4v with k8s id: gxy-t9w4v succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:14:57,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 203: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:15:04,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 203 finished
galaxy.jobs DEBUG 2025-03-19 07:15:04,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/203/outputs/dataset_fdd9f273-da1e-4396-ad66-99a86648471e.dat to /galaxy/server/database/objects/f/d/d/dataset_fdd9f273-da1e-4396-ad66-99a86648471e.dat
galaxy.jobs DEBUG 2025-03-19 07:15:04,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/203/outputs/dataset_ae6d8c74-49d4-49b9-b2fa-4c59088c4579.dat to /galaxy/server/database/objects/a/e/6/dataset_ae6d8c74-49d4-49b9-b2fa-4c59088c4579.dat
galaxy.jobs DEBUG 2025-03-19 07:15:04,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/203/outputs/dataset_929707e2-c403-44d1-bd31-a307ccac66b9.dat to /galaxy/server/database/objects/9/2/9/dataset_929707e2-c403-44d1-bd31-a307ccac66b9.dat
galaxy.model.metadata DEBUG 2025-03-19 07:15:04,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 293
galaxy.model.metadata DEBUG 2025-03-19 07:15:04,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 294
galaxy.model.metadata DEBUG 2025-03-19 07:15:04,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 295
galaxy.jobs INFO 2025-03-19 07:15:04,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 203 in /galaxy/server/database/jobs_directory/000/203
galaxy.objectstore CRITICAL 2025-03-19 07:15:04,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/203/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/203/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:15:04,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 203 executed (123.455 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:05,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 203 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:05,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-t9w4v (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:15:06,109 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 204
tpv.core.entities DEBUG 2025-03-19 07:15:06,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:15:06,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:15:06,134 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:15:06,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:15:06,156 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Working directory for job is: /galaxy/server/database/jobs_directory/000/204
galaxy.jobs.runners DEBUG 2025-03-19 07:15:06,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [204] queued (30.030 ms)
galaxy.jobs.handler INFO 2025-03-19 07:15:06,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (204) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:06,169 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for Galaxy job 204
galaxy.jobs DEBUG 2025-03-19 07:15:06,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [204] prepared (56.323 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:15:06,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/204/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/204/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/204/configs/tmp_254zhu6']
galaxy.jobs.runners DEBUG 2025-03-19 07:15:06,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (204) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/204/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/204/galaxy_204.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:06,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 204 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:06,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking if job 204 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:07,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5h4vx with k8s id: gxy-5h4vx scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:08,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-5h4vx set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:16,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-5h4vx with k8s id: gxy-5h4vx succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:15:16,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 204: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:15:24,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 204 finished
galaxy.jobs DEBUG 2025-03-19 07:15:24,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/204/outputs/dataset_9a75aaa5-263c-459a-a9e4-af0dae208ff7.dat to /galaxy/server/database/objects/9/a/7/dataset_9a75aaa5-263c-459a-a9e4-af0dae208ff7.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:15:24,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/204/working/data_fetch_upload_s44vxoo9', 'object_id': 296}]}]}]
galaxy.jobs INFO 2025-03-19 07:15:24,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 204 in /galaxy/server/database/jobs_directory/000/204
galaxy.jobs DEBUG 2025-03-19 07:15:24,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 204 executed (139.446 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:24,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 204 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:24,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Cleaning up job with K8s id gxy-5h4vx (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:15:25,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 205
tpv.core.entities DEBUG 2025-03-19 07:15:25,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:15:25,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:15:25,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:15:25,457 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:15:25,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Working directory for job is: /galaxy/server/database/jobs_directory/000/205
galaxy.jobs.runners DEBUG 2025-03-19 07:15:25,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [205] queued (28.696 ms)
galaxy.jobs.handler INFO 2025-03-19 07:15:25,480 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (205) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:25,482 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 205
galaxy.jobs DEBUG 2025-03-19 07:15:25,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [205] prepared (54.955 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:25,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:15:25,546 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:25,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:15:25,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/205/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/205/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/9/a/7/dataset_9a75aaa5-263c-459a-a9e4-af0dae208ff7.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert   --haplegendsample "/galaxy/server/database/jobs_directory/000/205/outputs/dataset_f2220770-8ef2-402c-bf9b-ba2dc3e18afc.dat,/galaxy/server/database/jobs_directory/000/205/outputs/dataset_ba9fbed8-f909-46a5-9b2f-250a03998e88.dat,/galaxy/server/database/jobs_directory/000/205/outputs/dataset_4e30a166-b4be-4249-847d-529a594dbef6.dat"    --keep-duplicates --keep-duplicates                   "/galaxy/server/database/objects/9/a/7/dataset_9a75aaa5-263c-459a-a9e4-af0dae208ff7.dat" .]
galaxy.jobs.runners DEBUG 2025-03-19 07:15:25,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (205) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/205/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/205/galaxy_205.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:25,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 205 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:25,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:15:25,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:25,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:25,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 205 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:26,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-hqs56 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:30,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-hqs56 with k8s id: gxy-hqs56 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:15:30,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 205: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:15:38,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 205 finished
galaxy.jobs DEBUG 2025-03-19 07:15:38,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/205/outputs/dataset_f2220770-8ef2-402c-bf9b-ba2dc3e18afc.dat to /galaxy/server/database/objects/f/2/2/dataset_f2220770-8ef2-402c-bf9b-ba2dc3e18afc.dat
galaxy.jobs DEBUG 2025-03-19 07:15:38,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/205/outputs/dataset_ba9fbed8-f909-46a5-9b2f-250a03998e88.dat to /galaxy/server/database/objects/b/a/9/dataset_ba9fbed8-f909-46a5-9b2f-250a03998e88.dat
galaxy.jobs DEBUG 2025-03-19 07:15:38,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/205/outputs/dataset_4e30a166-b4be-4249-847d-529a594dbef6.dat to /galaxy/server/database/objects/4/e/3/dataset_4e30a166-b4be-4249-847d-529a594dbef6.dat
galaxy.model.metadata DEBUG 2025-03-19 07:15:38,368 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 297
galaxy.model.metadata DEBUG 2025-03-19 07:15:38,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 298
galaxy.model.metadata DEBUG 2025-03-19 07:15:38,420 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 299
galaxy.jobs INFO 2025-03-19 07:15:38,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 205 in /galaxy/server/database/jobs_directory/000/205
galaxy.objectstore CRITICAL 2025-03-19 07:15:38,514 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/205/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/205/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:15:38,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 205 executed (177.775 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:38,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 205 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:38,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-hqs56 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:15:39,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 206
tpv.core.entities DEBUG 2025-03-19 07:15:39,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:15:39,701 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:15:39,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:15:39,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:15:39,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Working directory for job is: /galaxy/server/database/jobs_directory/000/206
galaxy.jobs.runners DEBUG 2025-03-19 07:15:39,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [206] queued (24.674 ms)
galaxy.jobs.handler INFO 2025-03-19 07:15:39,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (206) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:39,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 206
galaxy.jobs DEBUG 2025-03-19 07:15:39,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [206] prepared (55.541 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:15:39,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/206/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/206/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/206/configs/tmpow8mg0i5']
galaxy.jobs.runners DEBUG 2025-03-19 07:15:39,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (206) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/206/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/206/galaxy_206.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:39,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 206 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:39,855 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 206 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:40,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pbp65 with k8s id: gxy-pbp65 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:41,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-pbp65 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:50,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-pbp65 with k8s id: gxy-pbp65 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:15:50,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 206: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:15:57,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 206 finished
galaxy.jobs DEBUG 2025-03-19 07:15:57,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/206/outputs/dataset_1bd9677c-2f2c-4e9f-bc70-26edd621567f.dat to /galaxy/server/database/objects/1/b/d/dataset_1bd9677c-2f2c-4e9f-bc70-26edd621567f.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:15:57,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'convert.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/206/working/data_fetch_upload_ckw9v540', 'object_id': 300}]}]}]
galaxy.jobs INFO 2025-03-19 07:15:57,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 206 in /galaxy/server/database/jobs_directory/000/206
galaxy.jobs DEBUG 2025-03-19 07:15:57,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 206 executed (118.128 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:57,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 206 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:57,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-pbp65 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:15:59,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 207
tpv.core.entities DEBUG 2025-03-19 07:15:59,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:15:59,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:15:59,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:15:59,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:15:59,060 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Working directory for job is: /galaxy/server/database/jobs_directory/000/207
galaxy.jobs.runners DEBUG 2025-03-19 07:15:59,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [207] queued (33.900 ms)
galaxy.jobs.handler INFO 2025-03-19 07:15:59,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (207) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:59,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 207
galaxy.jobs DEBUG 2025-03-19 07:15:59,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [207] prepared (47.504 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:59,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:15:59,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:59,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:15:59,170 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/207/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/207/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/1/b/d/dataset_1bd9677c-2f2c-4e9f-bc70-26edd621567f.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools convert  --tag GT --gensample "/galaxy/server/database/jobs_directory/000/207/outputs/dataset_c589351b-d1ee-423e-8054-cc4ad4102dd6.dat,/galaxy/server/database/jobs_directory/000/207/outputs/dataset_741ba13f-4eee-42a1-a701-eba1d4f9bc70.dat"           --regions-overlap 1            "/galaxy/server/database/objects/1/b/d/dataset_1bd9677c-2f2c-4e9f-bc70-26edd621567f.dat" .]
galaxy.jobs.runners DEBUG 2025-03-19 07:15:59,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (207) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/207/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/207/galaxy_207.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:59,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 207 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:59,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:15:59,198 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_convert_from_vcf/bcftools_convert_from_vcf/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:15:59,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:15:59,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 207 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:00,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m2h56 with k8s id: gxy-m2h56 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:01,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m2h56 with k8s id: gxy-m2h56 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:02,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m2h56 with k8s id: gxy-m2h56 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:03,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-m2h56 with k8s id: gxy-m2h56 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:16:03,362 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 207: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:16:10,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 207 finished
galaxy.jobs DEBUG 2025-03-19 07:16:10,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/207/outputs/dataset_c589351b-d1ee-423e-8054-cc4ad4102dd6.dat to /galaxy/server/database/objects/c/5/8/dataset_c589351b-d1ee-423e-8054-cc4ad4102dd6.dat
galaxy.jobs DEBUG 2025-03-19 07:16:10,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/207/outputs/dataset_741ba13f-4eee-42a1-a701-eba1d4f9bc70.dat to /galaxy/server/database/objects/7/4/1/dataset_741ba13f-4eee-42a1-a701-eba1d4f9bc70.dat
galaxy.model.metadata DEBUG 2025-03-19 07:16:10,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 301
galaxy.model.metadata DEBUG 2025-03-19 07:16:10,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 302
galaxy.jobs INFO 2025-03-19 07:16:10,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 207 in /galaxy/server/database/jobs_directory/000/207
galaxy.objectstore CRITICAL 2025-03-19 07:16:11,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/207/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/207/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:16:11,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 207 executed (123.118 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:11,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 207 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:11,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Cleaning up job with K8s id gxy-m2h56 (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:16:13,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 208
tpv.core.entities DEBUG 2025-03-19 07:16:13,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:16:13,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:16:13,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:16:13,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:16:13,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Working directory for job is: /galaxy/server/database/jobs_directory/000/208
galaxy.jobs.runners DEBUG 2025-03-19 07:16:13,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [208] queued (30.375 ms)
galaxy.jobs.handler INFO 2025-03-19 07:16:13,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (208) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:13,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 208
galaxy.jobs DEBUG 2025-03-19 07:16:13,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [208] prepared (60.882 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:16:13,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/208/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/208/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/208/configs/tmpi5q5bwly']
galaxy.jobs.runners DEBUG 2025-03-19 07:16:13,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (208) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/208/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/208/galaxy_208.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:13,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 208 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:13,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 208 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:14,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gxw7g with k8s id: gxy-gxw7g scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:15,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-gxw7g set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:23,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-gxw7g with k8s id: gxy-gxw7g succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:16:23,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 208: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:16:31,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 208 finished
galaxy.jobs DEBUG 2025-03-19 07:16:31,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] finish(): Moved /galaxy/server/database/jobs_directory/000/208/outputs/dataset_af095252-1c04-4467-b806-16e7be4a27cd.dat to /galaxy/server/database/objects/a/f/0/dataset_af095252-1c04-4467-b806-16e7be4a27cd.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:16:31,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'plugin1.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/208/working/data_fetch_upload__m9vkxay', 'object_id': 303}]}]}]
galaxy.jobs INFO 2025-03-19 07:16:31,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 208 in /galaxy/server/database/jobs_directory/000/208
galaxy.jobs DEBUG 2025-03-19 07:16:31,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 208 executed (125.820 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:31,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 208 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:31,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-gxw7g (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:16:31,580 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 209
tpv.core.entities DEBUG 2025-03-19 07:16:31,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:16:31,607 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:16:31,610 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:16:31,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:16:31,629 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Working directory for job is: /galaxy/server/database/jobs_directory/000/209
galaxy.jobs.runners DEBUG 2025-03-19 07:16:31,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [209] queued (25.213 ms)
galaxy.jobs.handler INFO 2025-03-19 07:16:31,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (209) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:31,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 209
galaxy.jobs DEBUG 2025-03-19 07:16:31,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [209] prepared (56.497 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:16:31,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:16:31,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_dosage/bcftools_plugin_dosage/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:16:31,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:16:31,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/209/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/209/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/a/f/0/dataset_af095252-1c04-4467-b806-16e7be4a27cd.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools plugin dosage                 input.vcf.gz   > '/galaxy/server/database/jobs_directory/000/209/outputs/dataset_ae9a8746-1e52-4d91-a511-b017c615fb88.dat']
galaxy.jobs.runners DEBUG 2025-03-19 07:16:31,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (209) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/209/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/209/galaxy_209.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:31,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 209 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:16:31,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:16:31,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_plugin_dosage/bcftools_plugin_dosage/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-03-19 07:16:31,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:31,802 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 209 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:32,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6g5nn with k8s id: gxy-6g5nn scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:33,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-6g5nn set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:36,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-6g5nn with k8s id: gxy-6g5nn succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:16:36,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 209: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:16:44,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 209 finished
galaxy.jobs DEBUG 2025-03-19 07:16:44,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/209/outputs/dataset_ae9a8746-1e52-4d91-a511-b017c615fb88.dat to /galaxy/server/database/objects/a/e/9/dataset_ae9a8746-1e52-4d91-a511-b017c615fb88.dat
galaxy.model.metadata DEBUG 2025-03-19 07:16:44,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 304
galaxy.jobs INFO 2025-03-19 07:16:44,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 209 in /galaxy/server/database/jobs_directory/000/209
galaxy.objectstore CRITICAL 2025-03-19 07:16:44,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/209/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/209/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:16:44,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 209 executed (91.127 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:44,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 209 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:44,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-6g5nn (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:16:47,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 210, 211
tpv.core.entities DEBUG 2025-03-19 07:16:47,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:16:47,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:16:47,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:16:47,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:16:47,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Working directory for job is: /galaxy/server/database/jobs_directory/000/210
galaxy.jobs.runners DEBUG 2025-03-19 07:16:47,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [210] queued (32.458 ms)
galaxy.jobs.handler INFO 2025-03-19 07:16:47,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (210) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:47,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for Galaxy job 210
tpv.core.entities DEBUG 2025-03-19 07:16:47,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-min:24.2-uid'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:16:47,939 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:16:47,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:16:47,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:16:47,976 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Working directory for job is: /galaxy/server/database/jobs_directory/000/211
galaxy.jobs.runners DEBUG 2025-03-19 07:16:47,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [211] queued (42.192 ms)
galaxy.jobs.handler INFO 2025-03-19 07:16:47,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (211) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:47,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for Galaxy job 211
galaxy.jobs DEBUG 2025-03-19 07:16:48,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [210] prepared (77.419 ms)
galaxy.jobs.command_factory INFO 2025-03-19 07:16:48,049 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/210/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/210/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/210/configs/tmp20iyv_xk']
galaxy.jobs.runners DEBUG 2025-03-19 07:16:48,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (210) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/210/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/210/galaxy_210.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-03-19 07:16:48,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [211] prepared (68.516 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:48,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 210 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:48,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 210 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.command_factory INFO 2025-03-19 07:16:48,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/211/tool_script.sh] for tool command [python '/galaxy/server/lib/galaxy/tools/data_fetch.py' --galaxy-root '/galaxy/server' --datatypes-registry '/galaxy/server/database/jobs_directory/000/211/registry.xml' --request-version '1' --request '/galaxy/server/database/jobs_directory/000/211/configs/tmpg4i8m_bv']
galaxy.jobs.runners DEBUG 2025-03-19 07:16:48,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (211) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/211/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/211/galaxy_211.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:48,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 211 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:48,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking if job 211 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:48,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v2qxp with k8s id: gxy-v2qxp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:48,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2r7c2 with k8s id: gxy-2r7c2 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:49,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v2qxp with k8s id: gxy-v2qxp scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:49,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2r7c2 with k8s id: gxy-2r7c2 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:50,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-v2qxp set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:50,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-2r7c2 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:58,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-v2qxp with k8s id: gxy-v2qxp succeeded
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:16:58,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-2r7c2 with k8s id: gxy-2r7c2 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:16:58,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 210: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:16:58,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 211: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:17:06,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 211 finished
galaxy.jobs.runners DEBUG 2025-03-19 07:17:06,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 210 finished
galaxy.jobs DEBUG 2025-03-19 07:17:06,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] finish(): Moved /galaxy/server/database/jobs_directory/000/211/outputs/dataset_1a714f94-2f56-4a06-826a-7aa9da522c46.dat to /galaxy/server/database/objects/1/a/7/dataset_1a714f94-2f56-4a06-826a-7aa9da522c46.dat
galaxy.jobs DEBUG 2025-03-19 07:17:06,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/210/outputs/dataset_83665ab1-9611-4ff8-a809-fca831977440.dat to /galaxy/server/database/objects/8/3/6/dataset_83665ab1-9611-4ff8-a809-fca831977440.dat
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:17:06,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'regions.bed', 'dbkey': '?', 'ext': 'bed', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded bed file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/211/working/data_fetch_upload_3hx_defx', 'object_id': 306}]}]}]
galaxy.tool_util.provided_metadata DEBUG 2025-03-19 07:17:06,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] unnamed outputs [{'__unnamed_outputs': [{'destination': {'type': 'hdas'}, 'elements': [{'name': 'slice_in.vcf', 'dbkey': '?', 'ext': 'vcf', 'link_data_only': False, 'sources': [], 'hashes': [], 'info': 'uploaded vcf file', 'state': 'ok', 'filename': '/galaxy/server/database/jobs_directory/000/210/working/data_fetch_upload_xmbmnom6', 'object_id': 305}]}]}]
galaxy.jobs INFO 2025-03-19 07:17:06,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 211 in /galaxy/server/database/jobs_directory/000/211
galaxy.jobs INFO 2025-03-19 07:17:06,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 210 in /galaxy/server/database/jobs_directory/000/210
galaxy.jobs DEBUG 2025-03-19 07:17:06,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 211 executed (161.864 ms)
galaxy.jobs DEBUG 2025-03-19 07:17:06,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 210 executed (161.429 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:06,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 211 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:06,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Cleaning up job with K8s id gxy-2r7c2 (attempt 1).
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:06,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 210 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:06,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-v2qxp (attempt 1).
galaxy.jobs.handler DEBUG 2025-03-19 07:17:07,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 212
tpv.core.entities DEBUG 2025-03-19 07:17:07,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-min:24.2-uid', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-03-19 07:17:07,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-03-19 07:17:07,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-03-19 07:17:07,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-03-19 07:17:07,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Working directory for job is: /galaxy/server/database/jobs_directory/000/212
galaxy.jobs.runners DEBUG 2025-03-19 07:17:07,348 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [212] queued (29.494 ms)
galaxy.jobs.handler INFO 2025-03-19 07:17:07,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (212) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:07,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for Galaxy job 212
galaxy.jobs DEBUG 2025-03-19 07:17:07,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [212] prepared (49.849 ms)
galaxy.tool_util.deps.containers INFO 2025-03-19 07:17:07,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:17:07,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcftools_slice/vcftools_slice/0.1: vcftools:0.1.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:17:07,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcftools:0.1.11--2,type=docker]]
galaxy.jobs.command_factory INFO 2025-03-19 07:17:07,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/212/tool_script.sh] for tool command [vcf-sort /galaxy/server/database/objects/8/3/6/dataset_83665ab1-9611-4ff8-a809-fca831977440.dat > sorted.vcf  ; echo '#dummy header' | cat - /galaxy/server/database/objects/1/a/7/dataset_1a714f94-2f56-4a06-826a-7aa9da522c46.dat > regions.bed  ; vcftools --vcf sorted.vcf --out output --bed regions.bed --recode]
galaxy.jobs.runners DEBUG 2025-03-19 07:17:07,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (212) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/212/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/212/galaxy_212.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/212/working/output.recode.vcf" -a -f "/galaxy/server/database/jobs_directory/000/212/outputs/dataset_f829bc6e-08c5-4ad4-908b-28b9a0626215.dat" ] ; then cp "/galaxy/server/database/jobs_directory/000/212/working/output.recode.vcf" "/galaxy/server/database/jobs_directory/000/212/outputs/dataset_f829bc6e-08c5-4ad4-908b-28b9a0626215.dat" ; fi; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:07,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 212 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.tool_util.deps.containers INFO 2025-03-19 07:17:07,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-03-19 07:17:07,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcftools_slice/vcftools_slice/0.1: vcftools:0.1.11
galaxy.tool_util.deps.containers INFO 2025-03-19 07:17:07,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcftools:0.1.11--2,type=docker]]
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:07,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking if job 212 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:08,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:09,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:10,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:11,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:12,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:13,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:15,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:16,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:17,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 scheduled and is waiting to start...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:18,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job gxy-vbrj9 set to running...
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:21,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Job id: gxy-vbrj9 with k8s id: gxy-vbrj9 succeeded
galaxy.jobs.runners DEBUG 2025-03-19 07:17:21,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 212: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-03-19 07:17:28,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 212 finished
galaxy.tool_util.output_checker INFO 2025-03-19 07:17:28,834 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job failed because of contents in the standard error stream: [/galaxy/server/database/jobs_directory/000/212/tool_script.sh: line 22: vcf-sort: command not found
]
galaxy.jobs DEBUG 2025-03-19 07:17:28,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] finish(): Moved /galaxy/server/database/jobs_directory/000/212/outputs/dataset_f829bc6e-08c5-4ad4-908b-28b9a0626215.dat to /galaxy/server/database/objects/f/8/2/dataset_f829bc6e-08c5-4ad4-908b-28b9a0626215.dat
galaxy.jobs DEBUG 2025-03-19 07:17:28,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (212) setting dataset 307 state to ERROR
galaxy.jobs INFO 2025-03-19 07:17:28,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 212 in /galaxy/server/database/jobs_directory/000/212
galaxy.objectstore CRITICAL 2025-03-19 07:17:28,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] /galaxy/server/database/jobs_directory delete error [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/212/_outputs'
Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/objectstore/__init__.py", line 1082, in _delete
    shutil.rmtree(path)
  File "/usr/local/lib/python3.12/shutil.py", line 759, in rmtree
    _rmtree_safe_fd(stack, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 703, in _rmtree_safe_fd
    onexc(func, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 700, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)
  File "/usr/local/lib/python3.12/shutil.py", line 698, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: '/galaxy/server/database/jobs_directory/000/212/_outputs'
galaxy.jobs DEBUG 2025-03-19 07:17:28,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 212 executed (121.545 ms)
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:28,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking if job 212 is an interactive tool. guest ports: []. interactive entry points: []
galaxy.jobs.runners.kubernetes DEBUG 2025-03-19 07:17:28,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Cleaning up job with K8s id gxy-vbrj9 (attempt 1).
